
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[tip:x86/mm] x86/mm: Provide general kernel support for memory encryption - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [tip:x86/mm] x86/mm: Provide general kernel support for memory encryption</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=60001">tip-bot for Jacob Shin</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>July 18, 2017, 10:50 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;tip-21729f81ce8ae76a6995681d40e16f7ce8075db4@git.kernel.org&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9847883/mbox/"
   >mbox</a>
|
   <a href="/patch/9847883/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9847883/">/patch/9847883/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	D34FD602A7 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 18 Jul 2017 10:59:15 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id D67AB2854A
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 18 Jul 2017 10:59:15 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id CAA2C28573; Tue, 18 Jul 2017 10:59:15 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 5B3FA2854A
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 18 Jul 2017 10:59:14 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752109AbdGRK7K (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 18 Jul 2017 06:59:10 -0400
Received: from terminus.zytor.com ([65.50.211.136]:35957 &quot;EHLO
	terminus.zytor.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1752070AbdGRK7E (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 18 Jul 2017 06:59:04 -0400
Received: from terminus.zytor.com (localhost [127.0.0.1])
	by terminus.zytor.com (8.15.2/8.15.2) with ESMTP id v6IAooaG023987;
	Tue, 18 Jul 2017 03:50:50 -0700
Received: (from tipbot@localhost)
	by terminus.zytor.com (8.15.2/8.15.2/Submit) id v6IAooN7023983;
	Tue, 18 Jul 2017 03:50:50 -0700
Date: Tue, 18 Jul 2017 03:50:50 -0700
X-Authentication-Warning: terminus.zytor.com: tipbot set sender to
	tipbot@zytor.com using -f
From: tip-bot for Tom Lendacky &lt;tipbot@zytor.com&gt;
Message-ID: &lt;tip-21729f81ce8ae76a6995681d40e16f7ce8075db4@git.kernel.org&gt;
Cc: matt@codeblueprint.co.uk, tglx@linutronix.de, dyoung@redhat.com,
	glider@google.com, pbonzini@redhat.com, hpa@zytor.com,
	corbet@lwn.net, rkrcmar@redhat.com, toshi.kani@hpe.com,
	luto@kernel.org, konrad.wilk@oracle.com, riel@redhat.com,
	arnd@arndb.de, linux-kernel@vger.kernel.org, lwoodman@redhat.com,
	bp@alien8.de, aryabinin@virtuozzo.com, mingo@kernel.org,
	mst@redhat.com, bp@suse.de, thomas.lendacky@amd.com,
	brijesh.singh@amd.com, torvalds@linux-foundation.org,
	dvyukov@google.com, peterz@infradead.org
Reply-To: pbonzini@redhat.com, glider@google.com, hpa@zytor.com,
	matt@codeblueprint.co.uk, tglx@linutronix.de, dyoung@redhat.com,
	toshi.kani@hpe.com, rkrcmar@redhat.com, corbet@lwn.net,
	linux-kernel@vger.kernel.org, lwoodman@redhat.com,
	konrad.wilk@oracle.com, riel@redhat.com, luto@kernel.org,
	arnd@arndb.de, brijesh.singh@amd.com, bp@suse.de,
	thomas.lendacky@amd.com, peterz@infradead.org,
	torvalds@linux-foundation.org, dvyukov@google.com,
	mst@redhat.com, mingo@kernel.org, bp@alien8.de,
	aryabinin@virtuozzo.com
In-Reply-To: &lt;b36e952c4c39767ae7f0a41cf5345adf27438480.1500319216.git.thomas.lendacky@amd.com&gt;
References: &lt;b36e952c4c39767ae7f0a41cf5345adf27438480.1500319216.git.thomas.lendacky@amd.com&gt;
To: linux-tip-commits@vger.kernel.org
Subject: [tip:x86/mm] x86/mm: Provide general kernel support for memory
	encryption
Git-Commit-ID: 21729f81ce8ae76a6995681d40e16f7ce8075db4
X-Mailer: tip-git-log-daemon
Robot-ID: &lt;tip-bot.git.kernel.org&gt;
Robot-Unsubscribe: Contact &lt;mailto:hpa@kernel.org&gt; to get blacklisted from
	these emails
MIME-Version: 1.0
Content-Transfer-Encoding: 8bit
Content-Type: text/plain; charset=UTF-8
Content-Disposition: inline
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=60001">tip-bot for Jacob Shin</a> - July 18, 2017, 10:50 a.m.</div>
<pre class="content">
Commit-ID:  21729f81ce8ae76a6995681d40e16f7ce8075db4
Gitweb:     http://git.kernel.org/tip/21729f81ce8ae76a6995681d40e16f7ce8075db4
Author:     Tom Lendacky &lt;thomas.lendacky@amd.com&gt;
AuthorDate: Mon, 17 Jul 2017 16:10:07 -0500
Committer:  Ingo Molnar &lt;mingo@kernel.org&gt;
CommitDate: Tue, 18 Jul 2017 11:38:00 +0200

x86/mm: Provide general kernel support for memory encryption

Changes to the existing page table macros will allow the SME support to
be enabled in a simple fashion with minimal changes to files that use these
macros.  Since the memory encryption mask will now be part of the regular
pagetable macros, we introduce two new macros (_PAGE_TABLE_NOENC and
_KERNPG_TABLE_NOENC) to allow for early pagetable creation/initialization
without the encryption mask before SME becomes active.  Two new pgprot()
macros are defined to allow setting or clearing the page encryption mask.

The FIXMAP_PAGE_NOCACHE define is introduced for use with MMIO.  SME does
not support encryption for MMIO areas so this define removes the encryption
mask from the page attribute.

Two new macros are introduced (__sme_pa() / __sme_pa_nodebug()) to allow
creating a physical address with the encryption mask.  These are used when
working with the cr3 register so that the PGD can be encrypted. The current
__va() macro is updated so that the virtual address is generated based off
of the physical address without the encryption mask thus allowing the same
virtual address to be generated regardless of whether encryption is enabled
for that physical location or not.

Also, an early initialization function is added for SME.  If SME is active,
this function:

 - Updates the early_pmd_flags so that early page faults create mappings
   with the encryption mask.

 - Updates the __supported_pte_mask to include the encryption mask.

 - Updates the protection_map entries to include the encryption mask so
   that user-space allocations will automatically have the encryption mask
   applied.
<span class="signed-off-by">
Signed-off-by: Tom Lendacky &lt;thomas.lendacky@amd.com&gt;</span>
<span class="reviewed-by">Reviewed-by: Thomas Gleixner &lt;tglx@linutronix.de&gt;</span>
<span class="reviewed-by">Reviewed-by: Borislav Petkov &lt;bp@suse.de&gt;</span>
Cc: Alexander Potapenko &lt;glider@google.com&gt;
Cc: Andrey Ryabinin &lt;aryabinin@virtuozzo.com&gt;
Cc: Andy Lutomirski &lt;luto@kernel.org&gt;
Cc: Arnd Bergmann &lt;arnd@arndb.de&gt;
Cc: Borislav Petkov &lt;bp@alien8.de&gt;
Cc: Brijesh Singh &lt;brijesh.singh@amd.com&gt;
Cc: Dave Young &lt;dyoung@redhat.com&gt;
Cc: Dmitry Vyukov &lt;dvyukov@google.com&gt;
Cc: Jonathan Corbet &lt;corbet@lwn.net&gt;
Cc: Konrad Rzeszutek Wilk &lt;konrad.wilk@oracle.com&gt;
Cc: Larry Woodman &lt;lwoodman@redhat.com&gt;
Cc: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;
Cc: Matt Fleming &lt;matt@codeblueprint.co.uk&gt;
Cc: Michael S. Tsirkin &lt;mst@redhat.com&gt;
Cc: Paolo Bonzini &lt;pbonzini@redhat.com&gt;
Cc: Peter Zijlstra &lt;peterz@infradead.org&gt;
Cc: Radim Krčmář &lt;rkrcmar@redhat.com&gt;
Cc: Rik van Riel &lt;riel@redhat.com&gt;
Cc: Toshimitsu Kani &lt;toshi.kani@hpe.com&gt;
Cc: kasan-dev@googlegroups.com
Cc: kvm@vger.kernel.org
Cc: linux-arch@vger.kernel.org
Cc: linux-doc@vger.kernel.org
Cc: linux-efi@vger.kernel.org
Cc: linux-mm@kvack.org
Link: http://lkml.kernel.org/r/b36e952c4c39767ae7f0a41cf5345adf27438480.1500319216.git.thomas.lendacky@amd.com
<span class="signed-off-by">Signed-off-by: Ingo Molnar &lt;mingo@kernel.org&gt;</span>
---
 arch/x86/boot/compressed/pagetable.c |  7 ++++++
 arch/x86/include/asm/fixmap.h        |  7 ++++++
 arch/x86/include/asm/mem_encrypt.h   | 13 +++++++++++
 arch/x86/include/asm/page_types.h    |  3 ++-
 arch/x86/include/asm/pgtable.h       |  9 ++++++++
 arch/x86/include/asm/pgtable_types.h | 45 +++++++++++++++++++++++-------------
 arch/x86/include/asm/processor.h     |  3 ++-
 arch/x86/kernel/espfix_64.c          |  2 +-
 arch/x86/kernel/head64.c             | 11 +++++++--
 arch/x86/kernel/head_64.S            | 20 ++++++++--------
 arch/x86/mm/kasan_init_64.c          |  4 ++--
 arch/x86/mm/mem_encrypt.c            | 17 ++++++++++++++
 arch/x86/mm/pageattr.c               |  3 +++
 arch/x86/mm/tlb.c                    |  4 ++--
 include/asm-generic/pgtable.h        | 12 ++++++++++
 include/linux/mem_encrypt.h          |  8 +++++++
 16 files changed, 133 insertions(+), 35 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/boot/compressed/pagetable.c b/arch/x86/boot/compressed/pagetable.c</span>
<span class="p_header">index 28029be..f1aa438 100644</span>
<span class="p_header">--- a/arch/x86/boot/compressed/pagetable.c</span>
<span class="p_header">+++ b/arch/x86/boot/compressed/pagetable.c</span>
<span class="p_chunk">@@ -15,6 +15,13 @@</span> <span class="p_context"></span>
 #define __pa(x)  ((unsigned long)(x))
 #define __va(x)  ((void *)((unsigned long)(x)))
 
<span class="p_add">+/*</span>
<span class="p_add">+ * The pgtable.h and mm/ident_map.c includes make use of the SME related</span>
<span class="p_add">+ * information which is not used in the compressed image support. Un-define</span>
<span class="p_add">+ * the SME support to avoid any compile and link errors.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#undef CONFIG_AMD_MEM_ENCRYPT</span>
<span class="p_add">+</span>
 #include &quot;misc.h&quot;
 
 /* These actually do the work of building the kernel identity maps. */
<span class="p_header">diff --git a/arch/x86/include/asm/fixmap.h b/arch/x86/include/asm/fixmap.h</span>
<span class="p_header">index b65155c..d9ff226 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/fixmap.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/fixmap.h</span>
<span class="p_chunk">@@ -157,6 +157,13 @@</span> <span class="p_context"> static inline void __set_fixmap(enum fixed_addresses idx,</span>
 }
 #endif
 
<span class="p_add">+/*</span>
<span class="p_add">+ * FIXMAP_PAGE_NOCACHE is used for MMIO. Memory encryption is not</span>
<span class="p_add">+ * supported for MMIO addresses, so make sure that the memory encryption</span>
<span class="p_add">+ * mask is not part of the page attributes.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define FIXMAP_PAGE_NOCACHE PAGE_KERNEL_IO_NOCACHE</span>
<span class="p_add">+</span>
 #include &lt;asm-generic/fixmap.h&gt;
 
 #define __late_set_fixmap(idx, phys, flags) __set_fixmap(idx, phys, flags)
<span class="p_header">diff --git a/arch/x86/include/asm/mem_encrypt.h b/arch/x86/include/asm/mem_encrypt.h</span>
<span class="p_header">index 475e34f..dbae7a5 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/mem_encrypt.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/mem_encrypt.h</span>
<span class="p_chunk">@@ -21,6 +21,8 @@</span> <span class="p_context"></span>
 
 extern unsigned long sme_me_mask;
 
<span class="p_add">+void __init sme_early_init(void);</span>
<span class="p_add">+</span>
 void __init sme_encrypt_kernel(void);
 void __init sme_enable(void);
 
<span class="p_chunk">@@ -28,11 +30,22 @@</span> <span class="p_context"> void __init sme_enable(void);</span>
 
 #define sme_me_mask	0UL
 
<span class="p_add">+static inline void __init sme_early_init(void) { }</span>
<span class="p_add">+</span>
 static inline void __init sme_encrypt_kernel(void) { }
 static inline void __init sme_enable(void) { }
 
 #endif	/* CONFIG_AMD_MEM_ENCRYPT */
 
<span class="p_add">+/*</span>
<span class="p_add">+ * The __sme_pa() and __sme_pa_nodebug() macros are meant for use when</span>
<span class="p_add">+ * writing to or comparing values from the cr3 register.  Having the</span>
<span class="p_add">+ * encryption mask set in cr3 enables the PGD entry to be encrypted and</span>
<span class="p_add">+ * avoid special case handling of PGD allocations.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define __sme_pa(x)		(__pa(x) | sme_me_mask)</span>
<span class="p_add">+#define __sme_pa_nodebug(x)	(__pa_nodebug(x) | sme_me_mask)</span>
<span class="p_add">+</span>
 #endif	/* __ASSEMBLY__ */
 
 #endif	/* __X86_MEM_ENCRYPT_H__ */
<span class="p_header">diff --git a/arch/x86/include/asm/page_types.h b/arch/x86/include/asm/page_types.h</span>
<span class="p_header">index 7bd0099..b98ed9d 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/page_types.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/page_types.h</span>
<span class="p_chunk">@@ -3,6 +3,7 @@</span> <span class="p_context"></span>
 
 #include &lt;linux/const.h&gt;
 #include &lt;linux/types.h&gt;
<span class="p_add">+#include &lt;linux/mem_encrypt.h&gt;</span>
 
 /* PAGE_SHIFT determines the page size */
 #define PAGE_SHIFT		12
<span class="p_chunk">@@ -15,7 +16,7 @@</span> <span class="p_context"></span>
 #define PUD_PAGE_SIZE		(_AC(1, UL) &lt;&lt; PUD_SHIFT)
 #define PUD_PAGE_MASK		(~(PUD_PAGE_SIZE-1))
 
<span class="p_del">-#define __PHYSICAL_MASK		((phys_addr_t)((1ULL &lt;&lt; __PHYSICAL_MASK_SHIFT) - 1))</span>
<span class="p_add">+#define __PHYSICAL_MASK		((phys_addr_t)(__sme_clr((1ULL &lt;&lt; __PHYSICAL_MASK_SHIFT) - 1)))</span>
 #define __VIRTUAL_MASK		((1UL &lt;&lt; __VIRTUAL_MASK_SHIFT) - 1)
 
 /* Cast *PAGE_MASK to a signed type so that it is sign-extended if
<span class="p_header">diff --git a/arch/x86/include/asm/pgtable.h b/arch/x86/include/asm/pgtable.h</span>
<span class="p_header">index b64ea52..c6452cb 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/pgtable.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/pgtable.h</span>
<span class="p_chunk">@@ -1,6 +1,7 @@</span> <span class="p_context"></span>
 #ifndef _ASM_X86_PGTABLE_H
 #define _ASM_X86_PGTABLE_H
 
<span class="p_add">+#include &lt;linux/mem_encrypt.h&gt;</span>
 #include &lt;asm/page.h&gt;
 #include &lt;asm/pgtable_types.h&gt;
 
<span class="p_chunk">@@ -13,6 +14,12 @@</span> <span class="p_context"></span>
 		     cachemode2protval(_PAGE_CACHE_MODE_UC_MINUS)))	\
 	 : (prot))
 
<span class="p_add">+/*</span>
<span class="p_add">+ * Macros to add or remove encryption attribute</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define pgprot_encrypted(prot)	__pgprot(__sme_set(pgprot_val(prot)))</span>
<span class="p_add">+#define pgprot_decrypted(prot)	__pgprot(__sme_clr(pgprot_val(prot)))</span>
<span class="p_add">+</span>
 #ifndef __ASSEMBLY__
 #include &lt;asm/x86_init.h&gt;
 
<span class="p_chunk">@@ -38,6 +45,8 @@</span> <span class="p_context"> extern struct list_head pgd_list;</span>
 
 extern struct mm_struct *pgd_page_get_mm(struct page *page);
 
<span class="p_add">+extern pmdval_t early_pmd_flags;</span>
<span class="p_add">+</span>
 #ifdef CONFIG_PARAVIRT
 #include &lt;asm/paravirt.h&gt;
 #else  /* !CONFIG_PARAVIRT */
<span class="p_header">diff --git a/arch/x86/include/asm/pgtable_types.h b/arch/x86/include/asm/pgtable_types.h</span>
<span class="p_header">index bf9638e..de32ca3 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/pgtable_types.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/pgtable_types.h</span>
<span class="p_chunk">@@ -2,6 +2,8 @@</span> <span class="p_context"></span>
 #define _ASM_X86_PGTABLE_DEFS_H
 
 #include &lt;linux/const.h&gt;
<span class="p_add">+#include &lt;linux/mem_encrypt.h&gt;</span>
<span class="p_add">+</span>
 #include &lt;asm/page_types.h&gt;
 
 #define FIRST_USER_ADDRESS	0UL
<span class="p_chunk">@@ -121,10 +123,10 @@</span> <span class="p_context"></span>
 
 #define _PAGE_PROTNONE	(_AT(pteval_t, 1) &lt;&lt; _PAGE_BIT_PROTNONE)
 
<span class="p_del">-#define _PAGE_TABLE	(_PAGE_PRESENT | _PAGE_RW | _PAGE_USER |	\</span>
<span class="p_del">-			 _PAGE_ACCESSED | _PAGE_DIRTY)</span>
<span class="p_del">-#define _KERNPG_TABLE	(_PAGE_PRESENT | _PAGE_RW | _PAGE_ACCESSED |	\</span>
<span class="p_del">-			 _PAGE_DIRTY)</span>
<span class="p_add">+#define _PAGE_TABLE_NOENC	(_PAGE_PRESENT | _PAGE_RW | _PAGE_USER |\</span>
<span class="p_add">+				 _PAGE_ACCESSED | _PAGE_DIRTY)</span>
<span class="p_add">+#define _KERNPG_TABLE_NOENC	(_PAGE_PRESENT | _PAGE_RW |		\</span>
<span class="p_add">+				 _PAGE_ACCESSED | _PAGE_DIRTY)</span>
 
 /*
  * Set of bits not changed in pte_modify.  The pte&#39;s
<span class="p_chunk">@@ -191,18 +193,29 @@</span> <span class="p_context"> enum page_cache_mode {</span>
 #define __PAGE_KERNEL_IO		(__PAGE_KERNEL)
 #define __PAGE_KERNEL_IO_NOCACHE	(__PAGE_KERNEL_NOCACHE)
 
<span class="p_del">-#define PAGE_KERNEL			__pgprot(__PAGE_KERNEL)</span>
<span class="p_del">-#define PAGE_KERNEL_RO			__pgprot(__PAGE_KERNEL_RO)</span>
<span class="p_del">-#define PAGE_KERNEL_EXEC		__pgprot(__PAGE_KERNEL_EXEC)</span>
<span class="p_del">-#define PAGE_KERNEL_RX			__pgprot(__PAGE_KERNEL_RX)</span>
<span class="p_del">-#define PAGE_KERNEL_NOCACHE		__pgprot(__PAGE_KERNEL_NOCACHE)</span>
<span class="p_del">-#define PAGE_KERNEL_LARGE		__pgprot(__PAGE_KERNEL_LARGE)</span>
<span class="p_del">-#define PAGE_KERNEL_LARGE_EXEC		__pgprot(__PAGE_KERNEL_LARGE_EXEC)</span>
<span class="p_del">-#define PAGE_KERNEL_VSYSCALL		__pgprot(__PAGE_KERNEL_VSYSCALL)</span>
<span class="p_del">-#define PAGE_KERNEL_VVAR		__pgprot(__PAGE_KERNEL_VVAR)</span>
<span class="p_del">-</span>
<span class="p_del">-#define PAGE_KERNEL_IO			__pgprot(__PAGE_KERNEL_IO)</span>
<span class="p_del">-#define PAGE_KERNEL_IO_NOCACHE		__pgprot(__PAGE_KERNEL_IO_NOCACHE)</span>
<span class="p_add">+#ifndef __ASSEMBLY__</span>
<span class="p_add">+</span>
<span class="p_add">+#define _PAGE_ENC	(_AT(pteval_t, sme_me_mask))</span>
<span class="p_add">+</span>
<span class="p_add">+#define _PAGE_TABLE	(_PAGE_PRESENT | _PAGE_RW | _PAGE_USER |	\</span>
<span class="p_add">+			 _PAGE_ACCESSED | _PAGE_DIRTY | _PAGE_ENC)</span>
<span class="p_add">+#define _KERNPG_TABLE	(_PAGE_PRESENT | _PAGE_RW | _PAGE_ACCESSED |	\</span>
<span class="p_add">+			 _PAGE_DIRTY | _PAGE_ENC)</span>
<span class="p_add">+</span>
<span class="p_add">+#define PAGE_KERNEL		__pgprot(__PAGE_KERNEL | _PAGE_ENC)</span>
<span class="p_add">+#define PAGE_KERNEL_RO		__pgprot(__PAGE_KERNEL_RO | _PAGE_ENC)</span>
<span class="p_add">+#define PAGE_KERNEL_EXEC	__pgprot(__PAGE_KERNEL_EXEC | _PAGE_ENC)</span>
<span class="p_add">+#define PAGE_KERNEL_RX		__pgprot(__PAGE_KERNEL_RX | _PAGE_ENC)</span>
<span class="p_add">+#define PAGE_KERNEL_NOCACHE	__pgprot(__PAGE_KERNEL_NOCACHE | _PAGE_ENC)</span>
<span class="p_add">+#define PAGE_KERNEL_LARGE	__pgprot(__PAGE_KERNEL_LARGE | _PAGE_ENC)</span>
<span class="p_add">+#define PAGE_KERNEL_LARGE_EXEC	__pgprot(__PAGE_KERNEL_LARGE_EXEC | _PAGE_ENC)</span>
<span class="p_add">+#define PAGE_KERNEL_VSYSCALL	__pgprot(__PAGE_KERNEL_VSYSCALL | _PAGE_ENC)</span>
<span class="p_add">+#define PAGE_KERNEL_VVAR	__pgprot(__PAGE_KERNEL_VVAR | _PAGE_ENC)</span>
<span class="p_add">+</span>
<span class="p_add">+#define PAGE_KERNEL_IO		__pgprot(__PAGE_KERNEL_IO)</span>
<span class="p_add">+#define PAGE_KERNEL_IO_NOCACHE	__pgprot(__PAGE_KERNEL_IO_NOCACHE)</span>
<span class="p_add">+</span>
<span class="p_add">+#endif	/* __ASSEMBLY__ */</span>
 
 /*         xwr */
 #define __P000	PAGE_NONE
<span class="p_header">diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h</span>
<span class="p_header">index 6a79547..a68f70c 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/processor.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/processor.h</span>
<span class="p_chunk">@@ -29,6 +29,7 @@</span> <span class="p_context"> struct vm86;</span>
 #include &lt;linux/math64.h&gt;
 #include &lt;linux/err.h&gt;
 #include &lt;linux/irqflags.h&gt;
<span class="p_add">+#include &lt;linux/mem_encrypt.h&gt;</span>
 
 /*
  * We handle most unaligned accesses in hardware.  On the other hand
<span class="p_chunk">@@ -241,7 +242,7 @@</span> <span class="p_context"> static inline unsigned long read_cr3_pa(void)</span>
 
 static inline void load_cr3(pgd_t *pgdir)
 {
<span class="p_del">-	write_cr3(__pa(pgdir));</span>
<span class="p_add">+	write_cr3(__sme_pa(pgdir));</span>
 }
 
 #ifdef CONFIG_X86_32
<span class="p_header">diff --git a/arch/x86/kernel/espfix_64.c b/arch/x86/kernel/espfix_64.c</span>
<span class="p_header">index 6b91e2e..9c4e7ba 100644</span>
<span class="p_header">--- a/arch/x86/kernel/espfix_64.c</span>
<span class="p_header">+++ b/arch/x86/kernel/espfix_64.c</span>
<span class="p_chunk">@@ -195,7 +195,7 @@</span> <span class="p_context"> void init_espfix_ap(int cpu)</span>
 
 	pte_p = pte_offset_kernel(&amp;pmd, addr);
 	stack_page = page_address(alloc_pages_node(node, GFP_KERNEL, 0));
<span class="p_del">-	pte = __pte(__pa(stack_page) | (__PAGE_KERNEL_RO &amp; ptemask));</span>
<span class="p_add">+	pte = __pte(__pa(stack_page) | ((__PAGE_KERNEL_RO | _PAGE_ENC) &amp; ptemask));</span>
 	for (n = 0; n &lt; ESPFIX_PTE_CLONES; n++)
 		set_pte(&amp;pte_p[n*PTE_STRIDE], pte);
 
<span class="p_header">diff --git a/arch/x86/kernel/head64.c b/arch/x86/kernel/head64.c</span>
<span class="p_header">index 1f0ddcc..5cd0b72 100644</span>
<span class="p_header">--- a/arch/x86/kernel/head64.c</span>
<span class="p_header">+++ b/arch/x86/kernel/head64.c</span>
<span class="p_chunk">@@ -102,7 +102,7 @@</span> <span class="p_context"> unsigned long __head __startup_64(unsigned long physaddr)</span>
 
 	pud = fixup_pointer(early_dynamic_pgts[next_early_pgt++], physaddr);
 	pmd = fixup_pointer(early_dynamic_pgts[next_early_pgt++], physaddr);
<span class="p_del">-	pgtable_flags = _KERNPG_TABLE + sme_get_me_mask();</span>
<span class="p_add">+	pgtable_flags = _KERNPG_TABLE_NOENC + sme_get_me_mask();</span>
 
 	if (IS_ENABLED(CONFIG_X86_5LEVEL)) {
 		p4d = fixup_pointer(early_dynamic_pgts[next_early_pgt++], physaddr);
<span class="p_chunk">@@ -177,7 +177,7 @@</span> <span class="p_context"> static void __init reset_early_page_tables(void)</span>
 {
 	memset(early_top_pgt, 0, sizeof(pgd_t)*(PTRS_PER_PGD-1));
 	next_early_pgt = 0;
<span class="p_del">-	write_cr3(__pa_nodebug(early_top_pgt));</span>
<span class="p_add">+	write_cr3(__sme_pa_nodebug(early_top_pgt));</span>
 }
 
 /* Create a new PMD entry */
<span class="p_chunk">@@ -310,6 +310,13 @@</span> <span class="p_context"> asmlinkage __visible void __init x86_64_start_kernel(char * real_mode_data)</span>
 
 	clear_page(init_top_pgt);
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * SME support may update early_pmd_flags to include the memory</span>
<span class="p_add">+	 * encryption mask, so it needs to be called before anything</span>
<span class="p_add">+	 * that may generate a page fault.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	sme_early_init();</span>
<span class="p_add">+</span>
 	kasan_early_init();
 
 	for (i = 0; i &lt; NUM_EXCEPTION_VECTORS; i++)
<span class="p_header">diff --git a/arch/x86/kernel/head_64.S b/arch/x86/kernel/head_64.S</span>
<span class="p_header">index ec5d5e9..513cbb0 100644</span>
<span class="p_header">--- a/arch/x86/kernel/head_64.S</span>
<span class="p_header">+++ b/arch/x86/kernel/head_64.S</span>
<span class="p_chunk">@@ -351,9 +351,9 @@</span> <span class="p_context"> GLOBAL(name)</span>
 NEXT_PAGE(early_top_pgt)
 	.fill	511,8,0
 #ifdef CONFIG_X86_5LEVEL
<span class="p_del">-	.quad	level4_kernel_pgt - __START_KERNEL_map + _PAGE_TABLE</span>
<span class="p_add">+	.quad	level4_kernel_pgt - __START_KERNEL_map + _PAGE_TABLE_NOENC</span>
 #else
<span class="p_del">-	.quad	level3_kernel_pgt - __START_KERNEL_map + _PAGE_TABLE</span>
<span class="p_add">+	.quad	level3_kernel_pgt - __START_KERNEL_map + _PAGE_TABLE_NOENC</span>
 #endif
 
 NEXT_PAGE(early_dynamic_pgts)
<span class="p_chunk">@@ -366,15 +366,15 @@</span> <span class="p_context"> NEXT_PAGE(init_top_pgt)</span>
 	.fill	512,8,0
 #else
 NEXT_PAGE(init_top_pgt)
<span class="p_del">-	.quad   level3_ident_pgt - __START_KERNEL_map + _KERNPG_TABLE</span>
<span class="p_add">+	.quad   level3_ident_pgt - __START_KERNEL_map + _KERNPG_TABLE_NOENC</span>
 	.org    init_top_pgt + PGD_PAGE_OFFSET*8, 0
<span class="p_del">-	.quad   level3_ident_pgt - __START_KERNEL_map + _KERNPG_TABLE</span>
<span class="p_add">+	.quad   level3_ident_pgt - __START_KERNEL_map + _KERNPG_TABLE_NOENC</span>
 	.org    init_top_pgt + PGD_START_KERNEL*8, 0
 	/* (2^48-(2*1024*1024*1024))/(2^39) = 511 */
<span class="p_del">-	.quad   level3_kernel_pgt - __START_KERNEL_map + _PAGE_TABLE</span>
<span class="p_add">+	.quad   level3_kernel_pgt - __START_KERNEL_map + _PAGE_TABLE_NOENC</span>
 
 NEXT_PAGE(level3_ident_pgt)
<span class="p_del">-	.quad	level2_ident_pgt - __START_KERNEL_map + _KERNPG_TABLE</span>
<span class="p_add">+	.quad	level2_ident_pgt - __START_KERNEL_map + _KERNPG_TABLE_NOENC</span>
 	.fill	511, 8, 0
 NEXT_PAGE(level2_ident_pgt)
 	/* Since I easily can, map the first 1G.
<span class="p_chunk">@@ -386,14 +386,14 @@</span> <span class="p_context"> NEXT_PAGE(level2_ident_pgt)</span>
 #ifdef CONFIG_X86_5LEVEL
 NEXT_PAGE(level4_kernel_pgt)
 	.fill	511,8,0
<span class="p_del">-	.quad	level3_kernel_pgt - __START_KERNEL_map + _PAGE_TABLE</span>
<span class="p_add">+	.quad	level3_kernel_pgt - __START_KERNEL_map + _PAGE_TABLE_NOENC</span>
 #endif
 
 NEXT_PAGE(level3_kernel_pgt)
 	.fill	L3_START_KERNEL,8,0
 	/* (2^48-(2*1024*1024*1024)-((2^39)*511))/(2^30) = 510 */
<span class="p_del">-	.quad	level2_kernel_pgt - __START_KERNEL_map + _KERNPG_TABLE</span>
<span class="p_del">-	.quad	level2_fixmap_pgt - __START_KERNEL_map + _PAGE_TABLE</span>
<span class="p_add">+	.quad	level2_kernel_pgt - __START_KERNEL_map + _KERNPG_TABLE_NOENC</span>
<span class="p_add">+	.quad	level2_fixmap_pgt - __START_KERNEL_map + _PAGE_TABLE_NOENC</span>
 
 NEXT_PAGE(level2_kernel_pgt)
 	/*
<span class="p_chunk">@@ -411,7 +411,7 @@</span> <span class="p_context"> NEXT_PAGE(level2_kernel_pgt)</span>
 
 NEXT_PAGE(level2_fixmap_pgt)
 	.fill	506,8,0
<span class="p_del">-	.quad	level1_fixmap_pgt - __START_KERNEL_map + _PAGE_TABLE</span>
<span class="p_add">+	.quad	level1_fixmap_pgt - __START_KERNEL_map + _PAGE_TABLE_NOENC</span>
 	/* 8MB reserved for vsyscalls + a 2MB hole = 4 + 1 entries */
 	.fill	5,8,0
 
<span class="p_header">diff --git a/arch/x86/mm/kasan_init_64.c b/arch/x86/mm/kasan_init_64.c</span>
<span class="p_header">index 02c9d75..39d4daf 100644</span>
<span class="p_header">--- a/arch/x86/mm/kasan_init_64.c</span>
<span class="p_header">+++ b/arch/x86/mm/kasan_init_64.c</span>
<span class="p_chunk">@@ -87,7 +87,7 @@</span> <span class="p_context"> static struct notifier_block kasan_die_notifier = {</span>
 void __init kasan_early_init(void)
 {
 	int i;
<span class="p_del">-	pteval_t pte_val = __pa_nodebug(kasan_zero_page) | __PAGE_KERNEL;</span>
<span class="p_add">+	pteval_t pte_val = __pa_nodebug(kasan_zero_page) | __PAGE_KERNEL | _PAGE_ENC;</span>
 	pmdval_t pmd_val = __pa_nodebug(kasan_zero_pte) | _KERNPG_TABLE;
 	pudval_t pud_val = __pa_nodebug(kasan_zero_pmd) | _KERNPG_TABLE;
 	p4dval_t p4d_val = __pa_nodebug(kasan_zero_pud) | _KERNPG_TABLE;
<span class="p_chunk">@@ -153,7 +153,7 @@</span> <span class="p_context"> void __init kasan_init(void)</span>
 	 */
 	memset(kasan_zero_page, 0, PAGE_SIZE);
 	for (i = 0; i &lt; PTRS_PER_PTE; i++) {
<span class="p_del">-		pte_t pte = __pte(__pa(kasan_zero_page) | __PAGE_KERNEL_RO);</span>
<span class="p_add">+		pte_t pte = __pte(__pa(kasan_zero_page) | __PAGE_KERNEL_RO | _PAGE_ENC);</span>
 		set_pte(&amp;kasan_zero_pte[i], pte);
 	}
 	/* Flush TLBs again to be sure that write protection applied. */
<span class="p_header">diff --git a/arch/x86/mm/mem_encrypt.c b/arch/x86/mm/mem_encrypt.c</span>
<span class="p_header">index 3ac6f99..f973d3d 100644</span>
<span class="p_header">--- a/arch/x86/mm/mem_encrypt.c</span>
<span class="p_header">+++ b/arch/x86/mm/mem_encrypt.c</span>
<span class="p_chunk">@@ -12,6 +12,7 @@</span> <span class="p_context"></span>
 
 #include &lt;linux/linkage.h&gt;
 #include &lt;linux/init.h&gt;
<span class="p_add">+#include &lt;linux/mm.h&gt;</span>
 
 /*
  * Since SME related variables are set early in the boot process they must
<span class="p_chunk">@@ -21,6 +22,22 @@</span> <span class="p_context"></span>
 unsigned long sme_me_mask __section(.data) = 0;
 EXPORT_SYMBOL_GPL(sme_me_mask);
 
<span class="p_add">+void __init sme_early_init(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned int i;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!sme_me_mask)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	early_pmd_flags = __sme_set(early_pmd_flags);</span>
<span class="p_add">+</span>
<span class="p_add">+	__supported_pte_mask = __sme_set(__supported_pte_mask);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Update the protection map with memory encryption mask */</span>
<span class="p_add">+	for (i = 0; i &lt; ARRAY_SIZE(protection_map); i++)</span>
<span class="p_add">+		protection_map[i] = pgprot_encrypted(protection_map[i]);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 void __init sme_encrypt_kernel(void)
 {
 }
<span class="p_header">diff --git a/arch/x86/mm/pageattr.c b/arch/x86/mm/pageattr.c</span>
<span class="p_header">index 757b0bc..7e2d6c0 100644</span>
<span class="p_header">--- a/arch/x86/mm/pageattr.c</span>
<span class="p_header">+++ b/arch/x86/mm/pageattr.c</span>
<span class="p_chunk">@@ -2020,6 +2020,9 @@</span> <span class="p_context"> int kernel_map_pages_in_pgd(pgd_t *pgd, u64 pfn, unsigned long address,</span>
 	if (!(page_flags &amp; _PAGE_RW))
 		cpa.mask_clr = __pgprot(_PAGE_RW);
 
<span class="p_add">+	if (!(page_flags &amp; _PAGE_ENC))</span>
<span class="p_add">+		cpa.mask_clr = pgprot_encrypted(cpa.mask_clr);</span>
<span class="p_add">+</span>
 	cpa.mask_set = __pgprot(_PAGE_PRESENT | page_flags);
 
 	retval = __change_page_attr_set_clr(&amp;cpa, 0);
<span class="p_header">diff --git a/arch/x86/mm/tlb.c b/arch/x86/mm/tlb.c</span>
<span class="p_header">index 2c1b888..593d2f7 100644</span>
<span class="p_header">--- a/arch/x86/mm/tlb.c</span>
<span class="p_header">+++ b/arch/x86/mm/tlb.c</span>
<span class="p_chunk">@@ -115,7 +115,7 @@</span> <span class="p_context"> void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,</span>
 			 */
 			this_cpu_write(cpu_tlbstate.ctxs[0].tlb_gen,
 				       next_tlb_gen);
<span class="p_del">-			write_cr3(__pa(next-&gt;pgd));</span>
<span class="p_add">+			write_cr3(__sme_pa(next-&gt;pgd));</span>
 			trace_tlb_flush(TLB_FLUSH_ON_TASK_SWITCH,
 					TLB_FLUSH_ALL);
 		}
<span class="p_chunk">@@ -157,7 +157,7 @@</span> <span class="p_context"> void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,</span>
 		this_cpu_write(cpu_tlbstate.ctxs[0].ctx_id, next-&gt;context.ctx_id);
 		this_cpu_write(cpu_tlbstate.ctxs[0].tlb_gen, next_tlb_gen);
 		this_cpu_write(cpu_tlbstate.loaded_mm, next);
<span class="p_del">-		write_cr3(__pa(next-&gt;pgd));</span>
<span class="p_add">+		write_cr3(__sme_pa(next-&gt;pgd));</span>
 
 		trace_tlb_flush(TLB_FLUSH_ON_TASK_SWITCH, TLB_FLUSH_ALL);
 	}
<span class="p_header">diff --git a/include/asm-generic/pgtable.h b/include/asm-generic/pgtable.h</span>
<span class="p_header">index 7dfa767..4d7bb98 100644</span>
<span class="p_header">--- a/include/asm-generic/pgtable.h</span>
<span class="p_header">+++ b/include/asm-generic/pgtable.h</span>
<span class="p_chunk">@@ -583,6 +583,18 @@</span> <span class="p_context"> static inline void ptep_modify_prot_commit(struct mm_struct *mm,</span>
 #endif /* CONFIG_MMU */
 
 /*
<span class="p_add">+ * No-op macros that just return the current protection value. Defined here</span>
<span class="p_add">+ * because these macros can be used used even if CONFIG_MMU is not defined.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#ifndef pgprot_encrypted</span>
<span class="p_add">+#define pgprot_encrypted(prot)	(prot)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef pgprot_decrypted</span>
<span class="p_add">+#define pgprot_decrypted(prot)	(prot)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
  * A facility to provide lazy MMU batching.  This allows PTE updates and
  * page invalidations to be delayed until a call to leave lazy MMU mode
  * is issued.  Some architectures may benefit from doing this, and it is
<span class="p_header">diff --git a/include/linux/mem_encrypt.h b/include/linux/mem_encrypt.h</span>
<span class="p_header">index 570f4fc..1255f09 100644</span>
<span class="p_header">--- a/include/linux/mem_encrypt.h</span>
<span class="p_header">+++ b/include/linux/mem_encrypt.h</span>
<span class="p_chunk">@@ -35,6 +35,14 @@</span> <span class="p_context"> static inline unsigned long sme_get_me_mask(void)</span>
 	return sme_me_mask;
 }
 
<span class="p_add">+/*</span>
<span class="p_add">+ * The __sme_set() and __sme_clr() macros are useful for adding or removing</span>
<span class="p_add">+ * the encryption mask from a value (e.g. when dealing with pagetable</span>
<span class="p_add">+ * entries).</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define __sme_set(x)		((unsigned long)(x) | sme_me_mask)</span>
<span class="p_add">+#define __sme_clr(x)		((unsigned long)(x) &amp; ~sme_me_mask)</span>
<span class="p_add">+</span>
 #endif	/* __ASSEMBLY__ */
 
 #endif	/* __MEM_ENCRYPT_H__ */

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



