
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[RFC] mm, oom: introduce oom reaper - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [RFC] mm, oom: introduce oom reaper</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Nov. 25, 2015, 3:56 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1448467018-20603-1-git-send-email-mhocko@kernel.org&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/7700711/mbox/"
   >mbox</a>
|
   <a href="/patch/7700711/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/7700711/">/patch/7700711/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork1.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork1.web.kernel.org (Postfix) with ESMTP id AD3F29F2E9
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 25 Nov 2015 15:57:22 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id C2D62207C7
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 25 Nov 2015 15:57:16 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id 50CD620826
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 25 Nov 2015 15:57:15 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752413AbbKYP5M (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Wed, 25 Nov 2015 10:57:12 -0500
Received: from mail-wm0-f53.google.com ([74.125.82.53]:36020 &quot;EHLO
	mail-wm0-f53.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1750914AbbKYP5K (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Wed, 25 Nov 2015 10:57:10 -0500
Received: by wmww144 with SMTP id w144so185845871wmw.1
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Wed, 25 Nov 2015 07:57:09 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20130820;
	h=from:to:cc:subject:date:message-id;
	bh=p8g0U+GiOmmnqtVBRaPLUJxTZ3qBVApQZ4xZGPc6k30=;
	b=hUfeLqb1FcgcDGcDZ2HiYahGsSF4tZtNAAiVcKBeUUyQNCWJz9zqLvqc/GEIkx3Mc2
	zxDE4Zjc/VxCWTy8bDoayEYPyxYshKigeGVA290OCgepNOrMtUpqOsSOyLzBMzbTwaxm
	4hbeyp+aKZ1jMY2MP/H3OsEVGIVvTUHD00U5QRSSrRJfx4mpaBBB4lYcJu5+eoCnIwCa
	VesZ9fwI3TBbRETXDEAM0qwTT8hWbmxKVIakqQiV6vlQnX70FwmHTIaZTcBnEh7NTPpN
	KDVSCtrArlRiG3LfaaoD31Ps+usWi3Ojy/K1Up/LsUDo4v89SRvkNrAx26KvGoTf90AB
	aEZA==
X-Received: by 10.194.192.106 with SMTP id
	hf10mr43524908wjc.131.1448467028907; 
	Wed, 25 Nov 2015 07:57:08 -0800 (PST)
Received: from tiehlicka.suse.cz (ip-86-49-65-8.net.upcbroadband.cz.
	[86.49.65.8]) by smtp.gmail.com with ESMTPSA id
	pc2sm23742391wjb.11.2015.11.25.07.57.07
	(version=TLS1_2 cipher=ECDHE-RSA-AES128-SHA bits=128/128);
	Wed, 25 Nov 2015 07:57:08 -0800 (PST)
From: Michal Hocko &lt;mhocko@kernel.org&gt;
To: &lt;linux-mm@kvack.org&gt;
Cc: Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	Linus Torvalds &lt;torvalds@linux-foundation.org&gt;,
	Mel Gorman &lt;mgorman@suse.de&gt;, David Rientjes &lt;rientjes@google.com&gt;,
	Tetsuo Handa &lt;penguin-kernel@I-love.SAKURA.ne.jp&gt;,
	Rik van Riel &lt;riel@redhat.com&gt;, Hugh Dickins &lt;hughd@google.com&gt;,
	Oleg Nesterov &lt;oleg@redhat.com&gt;, Andrea Argangeli &lt;andrea@kernel.org&gt;,
	LKML &lt;linux-kernel@vger.kernel.org&gt;, Michal Hocko &lt;mhocko@suse.com&gt;
Subject: [RFC PATCH] mm, oom: introduce oom reaper
Date: Wed, 25 Nov 2015 16:56:58 +0100
Message-Id: &lt;1448467018-20603-1-git-send-email-mhocko@kernel.org&gt;
X-Mailer: git-send-email 2.6.2
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-7.5 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - Nov. 25, 2015, 3:56 p.m.</div>
<pre class="content">
<span class="from">From: Michal Hocko &lt;mhocko@suse.com&gt;</span>

This is based on the idea from Mel Gorman discussed during LSFMM 2015 and
independently brought up by Oleg Nesterov.

The OOM killer currently allows to kill only a single task in a good
hope that the task will terminate in a reasonable time and frees up its
memory.  Such a task (oom victim) will get an access to memory reserves
via mark_oom_victim to allow a forward progress should there be a need
for additional memory during exit path.

It has been shown (e.g. by Tetsuo Handa) that it is not that hard to
construct workloads which break the core assumption mentioned above and
the OOM victim might take unbounded amount of time to exit because it
might be blocked in the uninterruptible state waiting for on an event
(e.g. lock) which is blocked by another task looping in the page
allocator.

This patch reduces the probability of such a lockup by introducing a
specialized kernel thread (oom_reaper) which tries to reclaim additional
memory by preemptively reaping the anonymous or swapped out memory
owned by the oom victim under an assumption that such a memory won&#39;t
be needed when its owner is killed and kicked from the userspace anyway.
There is one notable exception to this, though, if the OOM victim was
in the process of coredumping the result would be incomplete. This is
considered a reasonable constrain because the overall system health is
more important than debugability of a particular application.

A kernel thread has been chosen because we need a reliable way of
invocation so workqueue context is not appropriate because all the
workers might be busy (e.g. allocating memory). Kswapd which sounds
like another good fit is not appropriate as well because it might get
blocked on locks during reclaim as well.

oom_reaper has to take mmap_sem on the target task for reading so the
solution is not 100% because the semaphore might be held or blocked
for write while write but the probability is reduced considerably wrt.
basically any lock blocking forward progress as described above. In
order to prevent from blocking on the lock without any forward progress
we are using only a trylock and retry 10 times with a short sleep
in between.
Users of mmap_sem which need it for write should be carefully reviewed
to use _killable waiting as much as possible and reduce allocations
requests done with the lock held to absolute minimum to reduce the risk
even further.

The API between oom killer and oom reaper is quite trivial. wake_oom_reaper
updates mm_to_reap with cmpxchg to guarantee only NUll-&gt;mm transition
and oom_reaper clear this atomically once it is done with the work. This
means that only a single mm_struct can be reaped at the time. As the
operation is potentially disruptive we are trying to limit it to the
ncessary minimum and the reaper blocks any updates while it operates on
an mm. mm_struct is pinned by mm_count to allow parallel exit_mmap and a
race is detected by atomic_inc_not_zero(mm_users).
<span class="signed-off-by">
Signed-off-by: Michal Hocko &lt;mhocko@suse.com&gt;</span>
---

Hi,
this is another step into making OOM killing more reliable. We are still
not 100% of course because we still depend on mmap_sem for read and the
oom victim might not be holding a lot of private anonymous memory. But I
think this is an improvement over the current situation already without
too much of additional cost/complexity. There is a room for improvements
I guess but I wanted to start as easy as possible. This has survived
my oom hammering but I am not claiming it is 100% safe. There might be
side effects I have never thought about so this really needs a _careful_
review (it doesn&#39;t help that changes outside of oom_kill.c are few
lines, right ;).

Any feedback is welcome.

 include/linux/mm.h |   2 +
 mm/internal.h      |   5 ++
 mm/memory.c        |  10 ++--
 mm/oom_kill.c      | 131 +++++++++++++++++++++++++++++++++++++++++++++++++++++
 4 files changed, 143 insertions(+), 5 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=45">Johannes Weiner</a> - Nov. 25, 2015, 8:08 p.m.</div>
<pre class="content">
Hi Michal,

I think whatever we end up doing to smoothen things for the &quot;common
case&quot; (as much as OOM kills can be considered common), we need a plan
to resolve the memory deadlock situations in a finite amount of time.

Eventually we have to attempt killing another task. Or kill all of
them to save the kernel.

It just strikes me as odd to start with smoothening the common case,
rather than making it functionally correct first.

On Wed, Nov 25, 2015 at 04:56:58PM +0100, Michal Hocko wrote:
<span class="quote">&gt; A kernel thread has been chosen because we need a reliable way of</span>
<span class="quote">&gt; invocation so workqueue context is not appropriate because all the</span>
<span class="quote">&gt; workers might be busy (e.g. allocating memory). Kswapd which sounds</span>
<span class="quote">&gt; like another good fit is not appropriate as well because it might get</span>
<span class="quote">&gt; blocked on locks during reclaim as well.</span>

Why not do it directly from the allocating context? I.e. when entering
the OOM killer and finding a lingering TIF_MEMDIE from a previous kill
just reap its memory directly then and there. It&#39;s not like the
allocating task has anything else to do in the meantime...
<span class="quote">
&gt; @@ -1123,7 +1126,7 @@ static unsigned long zap_pte_range(struct mmu_gather *tlb,</span>
<span class="quote">&gt;  			continue;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  		/* If details-&gt;check_mapping, we leave swap entries. */</span>
<span class="quote">&gt; -		if (unlikely(details))</span>
<span class="quote">&gt; +		if (unlikely(details || !details-&gt;check_swap_entries))</span>
<span class="quote">&gt;  			continue;</span>

&amp;&amp;
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/include/linux/mm.h b/include/linux/mm.h</span>
<span class="p_header">index 25cdec395f2c..d1ce03569942 100644</span>
<span class="p_header">--- a/include/linux/mm.h</span>
<span class="p_header">+++ b/include/linux/mm.h</span>
<span class="p_chunk">@@ -1061,6 +1061,8 @@</span> <span class="p_context"> struct zap_details {</span>
 	struct address_space *check_mapping;	/* Check page-&gt;mapping if set */
 	pgoff_t	first_index;			/* Lowest page-&gt;index to unmap */
 	pgoff_t last_index;			/* Highest page-&gt;index to unmap */
<span class="p_add">+	bool ignore_dirty;			/* Ignore dirty pages */</span>
<span class="p_add">+	bool check_swap_entries;		/* Check also swap entries */</span>
 };
 
 struct page *vm_normal_page(struct vm_area_struct *vma, unsigned long addr,
<span class="p_header">diff --git a/mm/internal.h b/mm/internal.h</span>
<span class="p_header">index 4ae7b7c7462b..9006ce1960ff 100644</span>
<span class="p_header">--- a/mm/internal.h</span>
<span class="p_header">+++ b/mm/internal.h</span>
<span class="p_chunk">@@ -41,6 +41,11 @@</span> <span class="p_context"> extern int do_swap_page(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 void free_pgtables(struct mmu_gather *tlb, struct vm_area_struct *start_vma,
 		unsigned long floor, unsigned long ceiling);
 
<span class="p_add">+void unmap_page_range(struct mmu_gather *tlb,</span>
<span class="p_add">+			     struct vm_area_struct *vma,</span>
<span class="p_add">+			     unsigned long addr, unsigned long end,</span>
<span class="p_add">+			     struct zap_details *details);</span>
<span class="p_add">+</span>
 static inline void set_page_count(struct page *page, int v)
 {
 	atomic_set(&amp;page-&gt;_count, v);
<span class="p_header">diff --git a/mm/memory.c b/mm/memory.c</span>
<span class="p_header">index f5b8e8c9f4c3..4750d7e942a3 100644</span>
<span class="p_header">--- a/mm/memory.c</span>
<span class="p_header">+++ b/mm/memory.c</span>
<span class="p_chunk">@@ -1104,6 +1104,9 @@</span> <span class="p_context"> static unsigned long zap_pte_range(struct mmu_gather *tlb,</span>
 
 			if (!PageAnon(page)) {
 				if (pte_dirty(ptent)) {
<span class="p_add">+					/* oom_repear cannot tear down dirty pages */</span>
<span class="p_add">+					if (unlikely(details &amp;&amp; details-&gt;ignore_dirty))</span>
<span class="p_add">+						continue;</span>
 					force_flush = 1;
 					set_page_dirty(page);
 				}
<span class="p_chunk">@@ -1123,7 +1126,7 @@</span> <span class="p_context"> static unsigned long zap_pte_range(struct mmu_gather *tlb,</span>
 			continue;
 		}
 		/* If details-&gt;check_mapping, we leave swap entries. */
<span class="p_del">-		if (unlikely(details))</span>
<span class="p_add">+		if (unlikely(details || !details-&gt;check_swap_entries))</span>
 			continue;
 
 		entry = pte_to_swp_entry(ptent);
<span class="p_chunk">@@ -1228,7 +1231,7 @@</span> <span class="p_context"> static inline unsigned long zap_pud_range(struct mmu_gather *tlb,</span>
 	return addr;
 }
 
<span class="p_del">-static void unmap_page_range(struct mmu_gather *tlb,</span>
<span class="p_add">+void unmap_page_range(struct mmu_gather *tlb,</span>
 			     struct vm_area_struct *vma,
 			     unsigned long addr, unsigned long end,
 			     struct zap_details *details)
<span class="p_chunk">@@ -1236,9 +1239,6 @@</span> <span class="p_context"> static void unmap_page_range(struct mmu_gather *tlb,</span>
 	pgd_t *pgd;
 	unsigned long next;
 
<span class="p_del">-	if (details &amp;&amp; !details-&gt;check_mapping)</span>
<span class="p_del">-		details = NULL;</span>
<span class="p_del">-</span>
 	BUG_ON(addr &gt;= end);
 	tlb_start_vma(tlb, vma);
 	pgd = pgd_offset(vma-&gt;vm_mm, addr);
<span class="p_header">diff --git a/mm/oom_kill.c b/mm/oom_kill.c</span>
<span class="p_header">index 5314b206caa5..47c9f584038b 100644</span>
<span class="p_header">--- a/mm/oom_kill.c</span>
<span class="p_header">+++ b/mm/oom_kill.c</span>
<span class="p_chunk">@@ -35,6 +35,11 @@</span> <span class="p_context"></span>
 #include &lt;linux/freezer.h&gt;
 #include &lt;linux/ftrace.h&gt;
 #include &lt;linux/ratelimit.h&gt;
<span class="p_add">+#include &lt;linux/kthread.h&gt;</span>
<span class="p_add">+#include &lt;linux/module.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/tlb.h&gt;</span>
<span class="p_add">+#include &quot;internal.h&quot;</span>
 
 #define CREATE_TRACE_POINTS
 #include &lt;trace/events/oom.h&gt;
<span class="p_chunk">@@ -408,6 +413,108 @@</span> <span class="p_context"> static DECLARE_WAIT_QUEUE_HEAD(oom_victims_wait);</span>
 
 bool oom_killer_disabled __read_mostly;
 
<span class="p_add">+/*</span>
<span class="p_add">+ * OOM Reaper kernel thread which tries to reap the memory used by the OOM</span>
<span class="p_add">+ * victim (if that is possible) to help the OOM killer to move on.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static struct task_struct *oom_reaper_th;</span>
<span class="p_add">+static struct mm_struct *mm_to_reap;</span>
<span class="p_add">+static DECLARE_WAIT_QUEUE_HEAD(oom_reaper_wait);</span>
<span class="p_add">+</span>
<span class="p_add">+static bool __oom_reap_vmas(struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct mmu_gather tlb;</span>
<span class="p_add">+	struct vm_area_struct *vma;</span>
<span class="p_add">+	struct zap_details details = {.check_swap_entries = true,</span>
<span class="p_add">+				      .ignore_dirty = true};</span>
<span class="p_add">+	bool ret = true;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* We might have raced with exit path */</span>
<span class="p_add">+	if (!atomic_inc_not_zero(&amp;mm-&gt;mm_users))</span>
<span class="p_add">+		return true;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!down_read_trylock(&amp;mm-&gt;mmap_sem)) {</span>
<span class="p_add">+		ret = false;</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	tlb_gather_mmu(&amp;tlb, mm, 0, -1);</span>
<span class="p_add">+	for (vma = mm-&gt;mmap ; vma; vma = vma-&gt;vm_next) {</span>
<span class="p_add">+		if (is_vm_hugetlb_page(vma))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Only anonymous pages have a good chance to be dropped</span>
<span class="p_add">+		 * without additional steps which we cannot afford as we</span>
<span class="p_add">+		 * are OOM already.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (vma_is_anonymous(vma) || !(vma-&gt;vm_flags &amp; VM_SHARED))</span>
<span class="p_add">+			unmap_page_range(&amp;tlb, vma, vma-&gt;vm_start, vma-&gt;vm_end,</span>
<span class="p_add">+					 &amp;details);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	tlb_finish_mmu(&amp;tlb, 0, -1);</span>
<span class="p_add">+	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+out:</span>
<span class="p_add">+	mmput(mm);</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void oom_reap_vmas(struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int attempts = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	while (attempts++ &lt; 10 &amp;&amp; !__oom_reap_vmas(mm))</span>
<span class="p_add">+		schedule_timeout(HZ/10);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Drop a reference taken by wake_oom_reaper */</span>
<span class="p_add">+	mmdrop(mm);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int oom_reaper(void *unused)</span>
<span class="p_add">+{</span>
<span class="p_add">+	DEFINE_WAIT(wait);</span>
<span class="p_add">+</span>
<span class="p_add">+	while (!kthread_should_stop()) {</span>
<span class="p_add">+		struct mm_struct *mm;</span>
<span class="p_add">+</span>
<span class="p_add">+		prepare_to_wait(&amp;oom_reaper_wait, &amp;wait, TASK_UNINTERRUPTIBLE);</span>
<span class="p_add">+		mm = READ_ONCE(mm_to_reap);</span>
<span class="p_add">+		if (!mm) {</span>
<span class="p_add">+			freezable_schedule();</span>
<span class="p_add">+			finish_wait(&amp;oom_reaper_wait, &amp;wait);</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			finish_wait(&amp;oom_reaper_wait, &amp;wait);</span>
<span class="p_add">+			oom_reap_vmas(mm);</span>
<span class="p_add">+			WRITE_ONCE(mm_to_reap, NULL);</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void wake_oom_reaper(struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct mm_struct *old_mm;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!oom_reaper_th)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Make sure that only a single mm is ever queued for the reaper</span>
<span class="p_add">+	 * because multiple are not necessary and the operation might be</span>
<span class="p_add">+	 * disruptive so better reduce it to the bare minimum.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	old_mm = cmpxchg(&amp;mm_to_reap, NULL, mm);</span>
<span class="p_add">+	if (!old_mm) {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Pin the given mm. Use mm_count instead of mm_users because</span>
<span class="p_add">+		 * we do not want to delay the address space tear down.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		atomic_inc(&amp;mm-&gt;mm_count);</span>
<span class="p_add">+		wake_up(&amp;oom_reaper_wait);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /**
  * mark_oom_victim - mark the given task as OOM victim
  * @tsk: task to mark
<span class="p_chunk">@@ -421,6 +528,11 @@</span> <span class="p_context"> void mark_oom_victim(struct task_struct *tsk)</span>
 	/* OOM killer might race with memcg OOM */
 	if (test_and_set_tsk_thread_flag(tsk, TIF_MEMDIE))
 		return;
<span class="p_add">+</span>
<span class="p_add">+	/* Kick oom reaper to help us release some memory */</span>
<span class="p_add">+	if (tsk-&gt;mm)</span>
<span class="p_add">+		wake_oom_reaper(tsk-&gt;mm);</span>
<span class="p_add">+</span>
 	/*
 	 * Make sure that the task is woken up from uninterruptible sleep
 	 * if it is frozen because OOM killer wouldn&#39;t be able to free
<span class="p_chunk">@@ -767,3 +879,22 @@</span> <span class="p_context"> void pagefault_out_of_memory(void)</span>
 
 	mutex_unlock(&amp;oom_lock);
 }
<span class="p_add">+</span>
<span class="p_add">+static int __init oom_init(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	oom_reaper_th = kthread_run(oom_reaper, NULL, &quot;oom_reaper&quot;);</span>
<span class="p_add">+	if (IS_ERR(oom_reaper_th)) {</span>
<span class="p_add">+		pr_err(&quot;Unable to start OOM reaper %ld. Continuing regardless\n&quot;,</span>
<span class="p_add">+				PTR_ERR(oom_reaper_th));</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		struct sched_param param = { .sched_priority = MAX_RT_PRIO-1 };</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Make sure our oom reaper thread will get scheduled when</span>
<span class="p_add">+		 * ASAP and that it won&#39;t get preempted by malicious userspace.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		sched_setscheduler(oom_reaper_th, SCHED_FIFO, &amp;param);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+module_init(oom_init)</span>

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



