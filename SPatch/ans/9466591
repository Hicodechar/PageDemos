
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[HMM,v14,05/16] mm/ZONE_DEVICE/unaddressable: add support for un-addressable device memory - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [HMM,v14,05/16] mm/ZONE_DEVICE/unaddressable: add support for un-addressable device memory</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=2554">Jerome Glisse</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Dec. 8, 2016, 4:39 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1481215184-18551-6-git-send-email-jglisse@redhat.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9466591/mbox/"
   >mbox</a>
|
   <a href="/patch/9466591/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9466591/">/patch/9466591/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	0190C6071E for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  8 Dec 2016 15:44:14 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id E77522855B
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  8 Dec 2016 15:44:13 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id DC3C328577; Thu,  8 Dec 2016 15:44:13 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 993332855B
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  8 Dec 2016 15:44:12 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S932761AbcLHPoF (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 8 Dec 2016 10:44:05 -0500
Received: from mx1.redhat.com ([209.132.183.28]:54618 &quot;EHLO mx1.redhat.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S932588AbcLHPjb (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 8 Dec 2016 10:39:31 -0500
Received: from int-mx10.intmail.prod.int.phx2.redhat.com
	(int-mx10.intmail.prod.int.phx2.redhat.com [10.5.11.23])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256
	bits)) (No client certificate requested)
	by mx1.redhat.com (Postfix) with ESMTPS id 5304CC04B94E;
	Thu,  8 Dec 2016 15:39:31 +0000 (UTC)
Received: from xgl-cortex.ml2.eng.bos.redhat.com
	(xgl-cortex.ml2.eng.bos.redhat.com [10.19.160.80])
	by int-mx10.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with
	ESMTP id uB8FdM1O003774; Thu, 8 Dec 2016 10:39:30 -0500
From: =?UTF-8?q?J=C3=A9r=C3=B4me=20Glisse?= &lt;jglisse@redhat.com&gt;
To: akpm@linux-foundation.org, &lt;linux-kernel@vger.kernel.org&gt;,
	linux-mm@kvack.org
Cc: John Hubbard &lt;jhubbard@nvidia.com&gt;,
	=?UTF-8?q?J=C3=A9r=C3=B4me=20Glisse?= &lt;jglisse@redhat.com&gt;,
	Dan Williams &lt;dan.j.williams@intel.com&gt;,
	Ross Zwisler &lt;ross.zwisler@linux.intel.com&gt;
Subject: [HMM v14 05/16] mm/ZONE_DEVICE/unaddressable: add support for
	un-addressable device memory
Date: Thu,  8 Dec 2016 11:39:33 -0500
Message-Id: &lt;1481215184-18551-6-git-send-email-jglisse@redhat.com&gt;
In-Reply-To: &lt;1481215184-18551-1-git-send-email-jglisse@redhat.com&gt;
References: &lt;1481215184-18551-1-git-send-email-jglisse@redhat.com&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.23
X-Greylist: Sender IP whitelisted, not delayed by milter-greylist-4.5.16
	(mx1.redhat.com [10.5.110.31]);
	Thu, 08 Dec 2016 15:39:31 +0000 (UTC)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=65121">Dave Hansen</a> - Dec. 8, 2016, 4:21 p.m.</div>
<pre class="content">
On 12/08/2016 08:39 AM, Jérôme Glisse wrote:
<span class="quote">&gt; Architecture that wish to support un-addressable device memory should make</span>
<span class="quote">&gt; sure to never populate the kernel linar mapping for the physical range.</span>

Does the platform somehow provide a range of physical addresses for this
unaddressable area?  How do we know no memory will be hot-added in a
range we&#39;re using for unaddressable device memory, for instance?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2554">Jerome Glisse</a> - Dec. 8, 2016, 4:39 p.m.</div>
<pre class="content">
This add support for un-addressable device memory. Such memory is hotpluged
only so we can have struct page but we should never map them as such memory
can not be accessed by CPU. For that reason it uses a special swap entry for
CPU page table entry.

This patch implement all the logic from special swap type to handling CPU
page fault through a callback specified in the ZONE_DEVICE pgmap struct.

Architecture that wish to support un-addressable device memory should make
sure to never populate the kernel linar mapping for the physical range.
<span class="signed-off-by">
Signed-off-by: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
Cc: Dan Williams &lt;dan.j.williams@intel.com&gt;
Cc: Ross Zwisler &lt;ross.zwisler@linux.intel.com&gt;
---
 drivers/dax/pmem.c                |  4 +--
 drivers/nvdimm/pmem.c             |  6 ++--
 fs/proc/task_mmu.c                | 10 +++++-
 include/linux/memory_hotplug.h    |  7 ++++
 include/linux/memremap.h          | 29 +++++++++++++++--
 include/linux/swap.h              | 18 +++++++++--
 include/linux/swapops.h           | 67 +++++++++++++++++++++++++++++++++++++++
 kernel/memremap.c                 | 43 +++++++++++++++++++++++--
 mm/Kconfig                        | 12 +++++++
 mm/memory.c                       | 62 ++++++++++++++++++++++++++++++++++++
 mm/mprotect.c                     | 12 +++++++
 tools/testing/nvdimm/test/iomap.c |  3 +-
 12 files changed, 259 insertions(+), 14 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2554">Jerome Glisse</a> - Dec. 8, 2016, 4:39 p.m.</div>
<pre class="content">
<span class="quote">&gt; On 12/08/2016 08:39 AM, Jérôme Glisse wrote:</span>
<span class="quote">&gt; &gt; Architecture that wish to support un-addressable device memory should make</span>
<span class="quote">&gt; &gt; sure to never populate the kernel linar mapping for the physical range.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Does the platform somehow provide a range of physical addresses for this</span>
<span class="quote">&gt; unaddressable area?  How do we know no memory will be hot-added in a</span>
<span class="quote">&gt; range we&#39;re using for unaddressable device memory, for instance?</span>

That&#39;s what one of the big issue. No platform does not reserve any range so
there is a possibility that some memory get hotpluged and assign this range.

I pushed the range decision to higher level (ie it is the device driver that
pick one) so right now for device driver using HMM (NVidia close driver as
we don&#39;t have nouveau ready for that yet) it goes from the highest physical
address and scan down until finding an empty range big enough.

I don&#39;t think i can control or enforce at platform level how to choose
specific physical address for hotplug.

So right now with my patchset what happens is that the hotplug will fail
because i already registered a resource for the physical range. What i can
add is a way to migrate the device memory to a different physical range.
I am bit afraid on how complex this can be.

The ideal solution would be to increase the MAX_PHYSMEM_BITS by one and use
physical address that can never be valid. We would not need to increase the
the direct mapping size of memory (this memory is not mappable by CPU). But
i am afraid of complication this might cause.

I think for sparse memory model it should be easy enough and i already rely
on sparse for HMM.

In any case i think this is something that can be solve after. If it becomes
a real issue. Maybe i should add a debug printk that when hotplug fails
because of an existing un-addressable ZONE_DEVICE resource.

Cheers,
Jérôme
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=65121">Dave Hansen</a> - Dec. 8, 2016, 8:07 p.m.</div>
<pre class="content">
On 12/08/2016 08:39 AM, Jerome Glisse wrote:
<span class="quote">&gt;&gt; On 12/08/2016 08:39 AM, Jérôme Glisse wrote:</span>
<span class="quote">&gt;&gt;&gt; &gt; &gt; Architecture that wish to support un-addressable device memory should make</span>
<span class="quote">&gt;&gt;&gt; &gt; &gt; sure to never populate the kernel linar mapping for the physical range.</span>
<span class="quote">&gt;&gt; &gt; </span>
<span class="quote">&gt;&gt; &gt; Does the platform somehow provide a range of physical addresses for this</span>
<span class="quote">&gt;&gt; &gt; unaddressable area?  How do we know no memory will be hot-added in a</span>
<span class="quote">&gt;&gt; &gt; range we&#39;re using for unaddressable device memory, for instance?</span>
<span class="quote">&gt; That&#39;s what one of the big issue. No platform does not reserve any range so</span>
<span class="quote">&gt; there is a possibility that some memory get hotpluged and assign this range.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I pushed the range decision to higher level (ie it is the device driver that</span>
<span class="quote">&gt; pick one) so right now for device driver using HMM (NVidia close driver as</span>
<span class="quote">&gt; we don&#39;t have nouveau ready for that yet) it goes from the highest physical</span>
<span class="quote">&gt; address and scan down until finding an empty range big enough.</span>

I don&#39;t think you should be stealing physical address space for things
that don&#39;t and can&#39;t have physical addresses.  Delegating this to
individual device drivers and hoping that they all get it right seems
like a recipe for disaster.

Maybe worth adding to the changelog:

	This feature potentially breaks memory hotplug unless every
	driver using it magically predicts the future addresses of
	where memory will be hotplugged.

BTW, how many more of these &quot;big issues&quot; does this set have?  I didn&#39;t
see any mention of this in the changelogs.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2554">Jerome Glisse</a> - Dec. 8, 2016, 8:37 p.m.</div>
<pre class="content">
<span class="quote">&gt; On 12/08/2016 08:39 AM, Jerome Glisse wrote:</span>
<span class="quote">&gt; &gt;&gt; On 12/08/2016 08:39 AM, Jérôme Glisse wrote:</span>
<span class="quote">&gt; &gt;&gt;&gt; &gt; &gt; Architecture that wish to support un-addressable device memory should</span>
<span class="quote">&gt; &gt;&gt;&gt; &gt; &gt; make</span>
<span class="quote">&gt; &gt;&gt;&gt; &gt; &gt; sure to never populate the kernel linar mapping for the physical</span>
<span class="quote">&gt; &gt;&gt;&gt; &gt; &gt; range.</span>
<span class="quote">&gt; &gt;&gt; &gt; </span>
<span class="quote">&gt; &gt;&gt; &gt; Does the platform somehow provide a range of physical addresses for this</span>
<span class="quote">&gt; &gt;&gt; &gt; unaddressable area?  How do we know no memory will be hot-added in a</span>
<span class="quote">&gt; &gt;&gt; &gt; range we&#39;re using for unaddressable device memory, for instance?</span>
<span class="quote">&gt; &gt; That&#39;s what one of the big issue. No platform does not reserve any range so</span>
<span class="quote">&gt; &gt; there is a possibility that some memory get hotpluged and assign this</span>
<span class="quote">&gt; &gt; range.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I pushed the range decision to higher level (ie it is the device driver</span>
<span class="quote">&gt; &gt; that</span>
<span class="quote">&gt; &gt; pick one) so right now for device driver using HMM (NVidia close driver as</span>
<span class="quote">&gt; &gt; we don&#39;t have nouveau ready for that yet) it goes from the highest physical</span>
<span class="quote">&gt; &gt; address and scan down until finding an empty range big enough.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I don&#39;t think you should be stealing physical address space for things</span>
<span class="quote">&gt; that don&#39;t and can&#39;t have physical addresses.  Delegating this to</span>
<span class="quote">&gt; individual device drivers and hoping that they all get it right seems</span>
<span class="quote">&gt; like a recipe for disaster.</span>

Well i expected device driver to use hmm_devmem_add() which does not take
physical address but use the above logic to pick one.
<span class="quote">
&gt; </span>
<span class="quote">&gt; Maybe worth adding to the changelog:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	This feature potentially breaks memory hotplug unless every</span>
<span class="quote">&gt; 	driver using it magically predicts the future addresses of</span>
<span class="quote">&gt; 	where memory will be hotplugged.</span>

I will add debug printk to memory hotplug in case it fails because of some
un-addressable resource. If you really dislike memory hotplug being broken
then i can go down the way of allowing to hotplug memory above the max
physical memory limit. This require more changes but i believe this is
doable for some of the memory model (sparsemem and sparsemem extreme).
<span class="quote">
&gt; </span>
<span class="quote">&gt; BTW, how many more of these &quot;big issues&quot; does this set have?  I didn&#39;t</span>
<span class="quote">&gt; see any mention of this in the changelogs.</span>
 
I am not sure what to say here. If you don&#39;t use HMM ie no device that
hotplug it. Then there is no chance of having issue. If you have a device
that use it then someone might try to do something stupid (try to kmap
and access such un-addressable page for instance). So i am not sure where
to draw the line.

Cheers,
Jérôme
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36302">Anshuman Khandual</a> - Dec. 26, 2016, 9:12 a.m.</div>
<pre class="content">
On 12/09/2016 02:07 AM, Jerome Glisse wrote:
<span class="quote">&gt;&gt; On 12/08/2016 08:39 AM, Jerome Glisse wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; &gt; &gt;&gt; On 12/08/2016 08:39 AM, Jérôme Glisse wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; &gt; &gt;&gt;&gt; &gt; &gt; Architecture that wish to support un-addressable device memory should</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; &gt; &gt;&gt;&gt; &gt; &gt; make</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; &gt; &gt;&gt;&gt; &gt; &gt; sure to never populate the kernel linar mapping for the physical</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; &gt; &gt;&gt;&gt; &gt; &gt; range.</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; &gt; &gt;&gt; &gt; </span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; &gt; &gt;&gt; &gt; Does the platform somehow provide a range of physical addresses for this</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; &gt; &gt;&gt; &gt; unaddressable area?  How do we know no memory will be hot-added in a</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; &gt; &gt;&gt; &gt; range we&#39;re using for unaddressable device memory, for instance?</span>
<span class="quote">&gt;&gt;&gt; &gt; &gt; That&#39;s what one of the big issue. No platform does not reserve any range so</span>
<span class="quote">&gt;&gt;&gt; &gt; &gt; there is a possibility that some memory get hotpluged and assign this</span>
<span class="quote">&gt;&gt;&gt; &gt; &gt; range.</span>
<span class="quote">&gt;&gt;&gt; &gt; &gt; </span>
<span class="quote">&gt;&gt;&gt; &gt; &gt; I pushed the range decision to higher level (ie it is the device driver</span>
<span class="quote">&gt;&gt;&gt; &gt; &gt; that</span>
<span class="quote">&gt;&gt;&gt; &gt; &gt; pick one) so right now for device driver using HMM (NVidia close driver as</span>
<span class="quote">&gt;&gt;&gt; &gt; &gt; we don&#39;t have nouveau ready for that yet) it goes from the highest physical</span>
<span class="quote">&gt;&gt;&gt; &gt; &gt; address and scan down until finding an empty range big enough.</span>
<span class="quote">&gt;&gt; &gt; </span>
<span class="quote">&gt;&gt; &gt; I don&#39;t think you should be stealing physical address space for things</span>
<span class="quote">&gt;&gt; &gt; that don&#39;t and can&#39;t have physical addresses.  Delegating this to</span>
<span class="quote">&gt;&gt; &gt; individual device drivers and hoping that they all get it right seems</span>
<span class="quote">&gt;&gt; &gt; like a recipe for disaster.</span>
<span class="quote">&gt; Well i expected device driver to use hmm_devmem_add() which does not take</span>
<span class="quote">&gt; physical address but use the above logic to pick one.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; &gt; </span>
<span class="quote">&gt;&gt; &gt; Maybe worth adding to the changelog:</span>
<span class="quote">&gt;&gt; &gt; </span>
<span class="quote">&gt;&gt; &gt; 	This feature potentially breaks memory hotplug unless every</span>
<span class="quote">&gt;&gt; &gt; 	driver using it magically predicts the future addresses of</span>
<span class="quote">&gt;&gt; &gt; 	where memory will be hotplugged.</span>
<span class="quote">&gt; I will add debug printk to memory hotplug in case it fails because of some</span>
<span class="quote">&gt; un-addressable resource. If you really dislike memory hotplug being broken</span>
<span class="quote">&gt; then i can go down the way of allowing to hotplug memory above the max</span>
<span class="quote">&gt; physical memory limit. This require more changes but i believe this is</span>
<span class="quote">&gt; doable for some of the memory model (sparsemem and sparsemem extreme).</span>

Did not get that. Hotplug memory request will come within the max physical
memory limit as they are real RAM. The address range also would have been
specified. How it can be added beyond the physical limit irrespective of
which we memory model we use.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2554">Jerome Glisse</a> - Dec. 26, 2016, 7:02 p.m.</div>
<pre class="content">
<span class="quote">&gt; On 12/09/2016 02:07 AM, Jerome Glisse wrote:</span>
<span class="quote">&gt; &gt;&gt; On 12/08/2016 08:39 AM, Jerome Glisse wrote:</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; &gt; &gt;&gt; On 12/08/2016 08:39 AM, Jérôme Glisse wrote:</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt; &gt; &gt;&gt;&gt; &gt; &gt; Architecture that wish to support un-addressable device</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt; &gt; &gt;&gt;&gt; &gt; &gt; memory should</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt; &gt; &gt;&gt;&gt; &gt; &gt; make</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt; &gt; &gt;&gt;&gt; &gt; &gt; sure to never populate the kernel linar mapping for the</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt; &gt; &gt;&gt;&gt; &gt; &gt; physical</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt; &gt; &gt;&gt;&gt; &gt; &gt; range.</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt; &gt; &gt;&gt; &gt; </span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt; &gt; &gt;&gt; &gt; Does the platform somehow provide a range of physical addresses</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt; &gt; &gt;&gt; &gt; for this</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt; &gt; &gt;&gt; &gt; unaddressable area?  How do we know no memory will be hot-added</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt; &gt; &gt;&gt; &gt; in a</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt; &gt; &gt;&gt; &gt; range we&#39;re using for unaddressable device memory, for instance?</span>
<span class="quote">&gt; &gt;&gt;&gt; &gt; &gt; That&#39;s what one of the big issue. No platform does not reserve any</span>
<span class="quote">&gt; &gt;&gt;&gt; &gt; &gt; range so</span>
<span class="quote">&gt; &gt;&gt;&gt; &gt; &gt; there is a possibility that some memory get hotpluged and assign this</span>
<span class="quote">&gt; &gt;&gt;&gt; &gt; &gt; range.</span>
<span class="quote">&gt; &gt;&gt;&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt;&gt;&gt; &gt; &gt; I pushed the range decision to higher level (ie it is the device</span>
<span class="quote">&gt; &gt;&gt;&gt; &gt; &gt; driver</span>
<span class="quote">&gt; &gt;&gt;&gt; &gt; &gt; that</span>
<span class="quote">&gt; &gt;&gt;&gt; &gt; &gt; pick one) so right now for device driver using HMM (NVidia close</span>
<span class="quote">&gt; &gt;&gt;&gt; &gt; &gt; driver as</span>
<span class="quote">&gt; &gt;&gt;&gt; &gt; &gt; we don&#39;t have nouveau ready for that yet) it goes from the highest</span>
<span class="quote">&gt; &gt;&gt;&gt; &gt; &gt; physical</span>
<span class="quote">&gt; &gt;&gt;&gt; &gt; &gt; address and scan down until finding an empty range big enough.</span>
<span class="quote">&gt; &gt;&gt; &gt; </span>
<span class="quote">&gt; &gt;&gt; &gt; I don&#39;t think you should be stealing physical address space for things</span>
<span class="quote">&gt; &gt;&gt; &gt; that don&#39;t and can&#39;t have physical addresses.  Delegating this to</span>
<span class="quote">&gt; &gt;&gt; &gt; individual device drivers and hoping that they all get it right seems</span>
<span class="quote">&gt; &gt;&gt; &gt; like a recipe for disaster.</span>
<span class="quote">&gt; &gt; Well i expected device driver to use hmm_devmem_add() which does not take</span>
<span class="quote">&gt; &gt; physical address but use the above logic to pick one.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt;&gt; &gt; </span>
<span class="quote">&gt; &gt;&gt; &gt; Maybe worth adding to the changelog:</span>
<span class="quote">&gt; &gt;&gt; &gt; </span>
<span class="quote">&gt; &gt;&gt; &gt; 	This feature potentially breaks memory hotplug unless every</span>
<span class="quote">&gt; &gt;&gt; &gt; 	driver using it magically predicts the future addresses of</span>
<span class="quote">&gt; &gt;&gt; &gt; 	where memory will be hotplugged.</span>
<span class="quote">&gt; &gt; I will add debug printk to memory hotplug in case it fails because of some</span>
<span class="quote">&gt; &gt; un-addressable resource. If you really dislike memory hotplug being broken</span>
<span class="quote">&gt; &gt; then i can go down the way of allowing to hotplug memory above the max</span>
<span class="quote">&gt; &gt; physical memory limit. This require more changes but i believe this is</span>
<span class="quote">&gt; &gt; doable for some of the memory model (sparsemem and sparsemem extreme).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Did not get that. Hotplug memory request will come within the max physical</span>
<span class="quote">&gt; memory limit as they are real RAM. The address range also would have been</span>
<span class="quote">&gt; specified. How it can be added beyond the physical limit irrespective of</span>
<span class="quote">&gt; which we memory model we use.</span>
<span class="quote">&gt; </span>

Maybe what you do not know is that on x86 we do not have resource reserve by the
patform for the device memory (the PCIE bar never cover the whole memory so this
range can not be use).

Right now i pick random unuse physical address range for device memory and thus
real memory might later be hotplug just inside the range i took and hotplug will
fail because i already registered a resource for my device memory. This is an
x86 platform limitation.

Now if i bump the maximum physical memory by one bit than i can hotplug device
memory inside that extra bit range and be sure that i will never have any real
memory conflict (as i am above the architectural limit).

Allowing to bump the maximum physical memory have implication and i can not just
bump MAX_PHYSMEM_BITS as it will have repercusion that i don&#39;t want. Now in some
memory model i can allow hotplug to happen above the MAX_PHYSMEM_BITS without
having to change MAX_PHYSMEM_BITS and allowing page_to_pfn() and pfn_to_page()
to work above MAX_PHYSMEM_BITS again without changing it.

Memory model like SPARSEMEM_VMEMMAP are problematic as i would need to change the
kernel virtual memory map for the architecture and it is not something i want to
do.

In the meantime people using HMM are &quot;~happy~&quot; enough with memory hotplug failing.

Cheers,
Jérôme
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/drivers/dax/pmem.c b/drivers/dax/pmem.c</span>
<span class="p_header">index 52ff674..f65a68a 100644</span>
<span class="p_header">--- a/drivers/dax/pmem.c</span>
<span class="p_header">+++ b/drivers/dax/pmem.c</span>
<span class="p_chunk">@@ -107,8 +107,8 @@</span> <span class="p_context"> static int dax_pmem_probe(struct device *dev)</span>
 	if (rc)
 		return rc;
 
<span class="p_del">-	addr = devm_memremap_pages(dev, &amp;res, &amp;dax_pmem-&gt;ref,</span>
<span class="p_del">-				   altmap, NULL, NULL);</span>
<span class="p_add">+	addr = devm_memremap_pages(dev, &amp;res, &amp;dax_pmem-&gt;ref, altmap,</span>
<span class="p_add">+				   NULL, NULL, NULL, NULL, MEMORY_DEVICE);</span>
 	if (IS_ERR(addr))
 		return PTR_ERR(addr);
 
<span class="p_header">diff --git a/drivers/nvdimm/pmem.c b/drivers/nvdimm/pmem.c</span>
<span class="p_header">index c261d12..dcad86f 100644</span>
<span class="p_header">--- a/drivers/nvdimm/pmem.c</span>
<span class="p_header">+++ b/drivers/nvdimm/pmem.c</span>
<span class="p_chunk">@@ -260,7 +260,8 @@</span> <span class="p_context"> static int pmem_attach_disk(struct device *dev,</span>
 	pmem-&gt;pfn_flags = PFN_DEV;
 	if (is_nd_pfn(dev)) {
 		addr = devm_memremap_pages(dev, &amp;pfn_res, &amp;q-&gt;q_usage_counter,
<span class="p_del">-					   altmap, NULL, NULL);</span>
<span class="p_add">+					   altmap, NULL, NULL, NULL,</span>
<span class="p_add">+					   NULL, MEMORY_DEVICE);</span>
 		pfn_sb = nd_pfn-&gt;pfn_sb;
 		pmem-&gt;data_offset = le64_to_cpu(pfn_sb-&gt;dataoff);
 		pmem-&gt;pfn_pad = resource_size(res) - resource_size(&amp;pfn_res);
<span class="p_chunk">@@ -270,7 +271,8 @@</span> <span class="p_context"> static int pmem_attach_disk(struct device *dev,</span>
 	} else if (pmem_should_map_pages(dev)) {
 		addr = devm_memremap_pages(dev, &amp;nsio-&gt;res,
 					   &amp;q-&gt;q_usage_counter,
<span class="p_del">-					   NULL, NULL, NULL);</span>
<span class="p_add">+					   NULL, NULL, NULL, NULL,</span>
<span class="p_add">+					   NULL, MEMORY_DEVICE);</span>
 		pmem-&gt;pfn_flags |= PFN_MAP;
 	} else
 		addr = devm_memremap(dev, pmem-&gt;phys_addr,
<span class="p_header">diff --git a/fs/proc/task_mmu.c b/fs/proc/task_mmu.c</span>
<span class="p_header">index 6909582..0726d39 100644</span>
<span class="p_header">--- a/fs/proc/task_mmu.c</span>
<span class="p_header">+++ b/fs/proc/task_mmu.c</span>
<span class="p_chunk">@@ -544,8 +544,11 @@</span> <span class="p_context"> static void smaps_pte_entry(pte_t *pte, unsigned long addr,</span>
 			} else {
 				mss-&gt;swap_pss += (u64)PAGE_SIZE &lt;&lt; PSS_SHIFT;
 			}
<span class="p_del">-		} else if (is_migration_entry(swpent))</span>
<span class="p_add">+		} else if (is_migration_entry(swpent)) {</span>
 			page = migration_entry_to_page(swpent);
<span class="p_add">+		} else if (is_device_entry(swpent)) {</span>
<span class="p_add">+			page = device_entry_to_page(swpent);</span>
<span class="p_add">+		}</span>
 	} else if (unlikely(IS_ENABLED(CONFIG_SHMEM) &amp;&amp; mss-&gt;check_shmem_swap
 							&amp;&amp; pte_none(*pte))) {
 		page = find_get_entry(vma-&gt;vm_file-&gt;f_mapping,
<span class="p_chunk">@@ -708,6 +711,8 @@</span> <span class="p_context"> static int smaps_hugetlb_range(pte_t *pte, unsigned long hmask,</span>
 
 		if (is_migration_entry(swpent))
 			page = migration_entry_to_page(swpent);
<span class="p_add">+		if (is_device_entry(swpent))</span>
<span class="p_add">+			page = device_entry_to_page(swpent);</span>
 	}
 	if (page) {
 		int mapcount = page_mapcount(page);
<span class="p_chunk">@@ -1191,6 +1196,9 @@</span> <span class="p_context"> static pagemap_entry_t pte_to_pagemap_entry(struct pagemapread *pm,</span>
 		flags |= PM_SWAP;
 		if (is_migration_entry(entry))
 			page = migration_entry_to_page(entry);
<span class="p_add">+</span>
<span class="p_add">+		if (is_device_entry(entry))</span>
<span class="p_add">+			page = device_entry_to_page(entry);</span>
 	}
 
 	if (page &amp;&amp; !PageAnon(page))
<span class="p_header">diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h</span>
<span class="p_header">index 3f50eb8..e7c5dc6 100644</span>
<span class="p_header">--- a/include/linux/memory_hotplug.h</span>
<span class="p_header">+++ b/include/linux/memory_hotplug.h</span>
<span class="p_chunk">@@ -285,15 +285,22 @@</span> <span class="p_context"> extern int zone_for_memory(int nid, u64 start, u64 size, int zone_default,</span>
  * never relied on struct page migration so far and new user of might also
  * prefer avoiding struct page migration.
  *
<span class="p_add">+ * For device memory (which use ZONE_DEVICE) we want differentiate between CPU</span>
<span class="p_add">+ * accessible memory (persitent memory, device memory on an architecture with a</span>
<span class="p_add">+ * system bus that allow transparent access to device memory) and unaddressable</span>
<span class="p_add">+ * memory (device memory that can not be accessed by CPU directly).</span>
<span class="p_add">+ *</span>
  * New non device memory specific flags can be added if ever needed.
  *
  * MEMORY_REGULAR: regular system memory
  * DEVICE_MEMORY: device memory create a ZONE_DEVICE zone for it
  * DEVICE_MEMORY_ALLOW_MIGRATE: page in that device memory ca be migrated
<span class="p_add">+ * MEMORY_DEVICE_UNADDRESSABLE: un-addressable memory (CPU can not access it)</span>
  */
 #define MEMORY_NORMAL 0
 #define MEMORY_DEVICE (1 &lt;&lt; 0)
 #define MEMORY_DEVICE_ALLOW_MIGRATE (1 &lt;&lt; 1)
<span class="p_add">+#define MEMORY_DEVICE_UNADDRESSABLE (1 &lt;&lt; 2)</span>
 
 extern int arch_add_memory(int nid, u64 start, u64 size, int flags);
 extern int offline_pages(unsigned long start_pfn, unsigned long nr_pages);
<span class="p_header">diff --git a/include/linux/memremap.h b/include/linux/memremap.h</span>
<span class="p_header">index 7845f2e..a646c47 100644</span>
<span class="p_header">--- a/include/linux/memremap.h</span>
<span class="p_header">+++ b/include/linux/memremap.h</span>
<span class="p_chunk">@@ -35,31 +35,42 @@</span> <span class="p_context"> static inline struct vmem_altmap *to_vmem_altmap(unsigned long memmap_start)</span>
 }
 #endif
 
<span class="p_add">+typedef int (*dev_page_fault_t)(struct vm_area_struct *vma,</span>
<span class="p_add">+				unsigned long addr,</span>
<span class="p_add">+				struct page *page,</span>
<span class="p_add">+				unsigned flags,</span>
<span class="p_add">+				pmd_t *pmdp);</span>
 typedef void (*dev_page_free_t)(struct page *page, void *data);
 
 /**
  * struct dev_pagemap - metadata for ZONE_DEVICE mappings
<span class="p_add">+ * @page_fault: callback when CPU fault on an un-addressable device page</span>
  * @page_free: free page callback when page refcount reach 1
  * @altmap: pre-allocated/reserved memory for vmemmap allocations
  * @res: physical address range covered by @ref
  * @ref: reference count that pins the devm_memremap_pages() mapping
  * @dev: host device of the mapping for debug
  * @data: privata data pointer for page_free
<span class="p_add">+ * @flags: device memory flags (look for MEMORY_DEVICE_* memory_hotplug.h)</span>
  */
 struct dev_pagemap {
<span class="p_add">+	dev_page_fault_t page_fault;</span>
 	dev_page_free_t page_free;
 	struct vmem_altmap *altmap;
 	const struct resource *res;
 	struct percpu_ref *ref;
 	struct device *dev;
 	void *data;
<span class="p_add">+	int flags;</span>
 };
 
 #ifdef CONFIG_ZONE_DEVICE
 void *devm_memremap_pages(struct device *dev, struct resource *res,
 			  struct percpu_ref *ref, struct vmem_altmap *altmap,
<span class="p_add">+			  struct dev_pagemap **ppgmap,</span>
<span class="p_add">+			  dev_page_fault_t page_fault,</span>
 			  dev_page_free_t page_free,
<span class="p_del">-			  void *data);</span>
<span class="p_add">+			  void *data, int flags);</span>
 struct dev_pagemap *find_dev_pagemap(resource_size_t phys);
 int devm_memremap_pages_remove(struct device *dev, struct dev_pagemap *pgmap);
 
<span class="p_chunk">@@ -68,13 +79,22 @@</span> <span class="p_context"> static inline bool dev_page_allow_migrate(const struct page *page)</span>
 	return ((page_zonenum(page) == ZONE_DEVICE) &amp;&amp;
 		(page-&gt;pgmap-&gt;flags &amp; MEMORY_DEVICE_ALLOW_MIGRATE));
 }
<span class="p_add">+</span>
<span class="p_add">+static inline bool is_addressable_page(const struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return ((page_zonenum(page) != ZONE_DEVICE) ||</span>
<span class="p_add">+		!(page-&gt;pgmap-&gt;flags &amp; MEMORY_DEVICE_UNADDRESSABLE));</span>
<span class="p_add">+}</span>
 #else
 static inline void *devm_memremap_pages(struct device *dev,
 					struct resource *res,
 					struct percpu_ref *ref,
 					struct vmem_altmap *altmap,
<span class="p_add">+					struct dev_pagemap **ppgmap,</span>
<span class="p_add">+					dev_page_fault_t page_fault,</span>
 					dev_page_free_t page_free,
<span class="p_del">-					void *data)</span>
<span class="p_add">+					void *data,</span>
<span class="p_add">+					int flags)</span>
 {
 	/*
 	 * Fail attempts to call devm_memremap_pages() without
<span class="p_chunk">@@ -100,6 +120,11 @@</span> <span class="p_context"> static inline bool dev_page_allow_migrate(const struct page *page)</span>
 {
 	return false;
 }
<span class="p_add">+</span>
<span class="p_add">+static inline bool is_addressable_page(const struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return true;</span>
<span class="p_add">+}</span>
 #endif
 
 /**
<span class="p_header">diff --git a/include/linux/swap.h b/include/linux/swap.h</span>
<span class="p_header">index 7e553e1..599cb54 100644</span>
<span class="p_header">--- a/include/linux/swap.h</span>
<span class="p_header">+++ b/include/linux/swap.h</span>
<span class="p_chunk">@@ -50,6 +50,17 @@</span> <span class="p_context"> static inline int current_is_kswapd(void)</span>
  */
 
 /*
<span class="p_add">+ * Un-addressable device memory support</span>
<span class="p_add">+ */</span>
<span class="p_add">+#ifdef CONFIG_DEVICE_UNADDRESSABLE</span>
<span class="p_add">+#define SWP_DEVICE_NUM 2</span>
<span class="p_add">+#define SWP_DEVICE_WRITE (MAX_SWAPFILES + SWP_HWPOISON_NUM + SWP_MIGRATION_NUM)</span>
<span class="p_add">+#define SWP_DEVICE (MAX_SWAPFILES + SWP_HWPOISON_NUM + SWP_MIGRATION_NUM + 1)</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define SWP_DEVICE_NUM 0</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
  * NUMA node memory migration support
  */
 #ifdef CONFIG_MIGRATION
<span class="p_chunk">@@ -71,7 +82,8 @@</span> <span class="p_context"> static inline int current_is_kswapd(void)</span>
 #endif
 
 #define MAX_SWAPFILES \
<span class="p_del">-	((1 &lt;&lt; MAX_SWAPFILES_SHIFT) - SWP_MIGRATION_NUM - SWP_HWPOISON_NUM)</span>
<span class="p_add">+	((1 &lt;&lt; MAX_SWAPFILES_SHIFT) - SWP_DEVICE_NUM - \</span>
<span class="p_add">+	SWP_MIGRATION_NUM - SWP_HWPOISON_NUM)</span>
 
 /*
  * Magic header for a swap area. The first part of the union is
<span class="p_chunk">@@ -442,8 +454,8 @@</span> <span class="p_context"> static inline void show_swap_cache_info(void)</span>
 {
 }
 
<span class="p_del">-#define free_swap_and_cache(swp)	is_migration_entry(swp)</span>
<span class="p_del">-#define swapcache_prepare(swp)		is_migration_entry(swp)</span>
<span class="p_add">+#define free_swap_and_cache(e) (is_migration_entry(e) || is_device_entry(e))</span>
<span class="p_add">+#define swapcache_prepare(e) (is_migration_entry(e) || is_device_entry(e))</span>
 
 static inline int add_swap_count_continuation(swp_entry_t swp, gfp_t gfp_mask)
 {
<span class="p_header">diff --git a/include/linux/swapops.h b/include/linux/swapops.h</span>
<span class="p_header">index 5c3a5f3..0e339f0 100644</span>
<span class="p_header">--- a/include/linux/swapops.h</span>
<span class="p_header">+++ b/include/linux/swapops.h</span>
<span class="p_chunk">@@ -100,6 +100,73 @@</span> <span class="p_context"> static inline void *swp_to_radix_entry(swp_entry_t entry)</span>
 	return (void *)(value | RADIX_TREE_EXCEPTIONAL_ENTRY);
 }
 
<span class="p_add">+#if IS_ENABLED(CONFIG_DEVICE_UNADDRESSABLE)</span>
<span class="p_add">+static inline swp_entry_t make_device_entry(struct page *page, bool write)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return swp_entry(write?SWP_DEVICE_WRITE:SWP_DEVICE, page_to_pfn(page));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline bool is_device_entry(swp_entry_t entry)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int type = swp_type(entry);</span>
<span class="p_add">+	return type == SWP_DEVICE || type == SWP_DEVICE_WRITE;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void make_device_entry_read(swp_entry_t *entry)</span>
<span class="p_add">+{</span>
<span class="p_add">+	*entry = swp_entry(SWP_DEVICE, swp_offset(*entry));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline bool is_write_device_entry(swp_entry_t entry)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return unlikely(swp_type(entry) == SWP_DEVICE_WRITE);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline struct page *device_entry_to_page(swp_entry_t entry)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return pfn_to_page(swp_offset(entry));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+int device_entry_fault(struct vm_area_struct *vma,</span>
<span class="p_add">+		       unsigned long addr,</span>
<span class="p_add">+		       swp_entry_t entry,</span>
<span class="p_add">+		       unsigned flags,</span>
<span class="p_add">+		       pmd_t *pmdp);</span>
<span class="p_add">+#else /* CONFIG_DEVICE_UNADDRESSABLE */</span>
<span class="p_add">+static inline swp_entry_t make_device_entry(struct page *page, bool write)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return swp_entry(0, 0);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void make_device_entry_read(swp_entry_t *entry)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline bool is_device_entry(swp_entry_t entry)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return false;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline bool is_write_device_entry(swp_entry_t entry)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return false;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline struct page *device_entry_to_page(swp_entry_t entry)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return NULL;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int device_entry_fault(struct vm_area_struct *vma,</span>
<span class="p_add">+				     unsigned long addr,</span>
<span class="p_add">+				     swp_entry_t entry,</span>
<span class="p_add">+				     unsigned flags,</span>
<span class="p_add">+				     pmd_t *pmdp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return VM_FAULT_SIGBUS;</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif /* CONFIG_DEVICE_UNADDRESSABLE */</span>
<span class="p_add">+</span>
 #ifdef CONFIG_MIGRATION
 static inline swp_entry_t make_migration_entry(struct page *page, int write)
 {
<span class="p_header">diff --git a/kernel/memremap.c b/kernel/memremap.c</span>
<span class="p_header">index bc1e400..3df08f4 100644</span>
<span class="p_header">--- a/kernel/memremap.c</span>
<span class="p_header">+++ b/kernel/memremap.c</span>
<span class="p_chunk">@@ -18,6 +18,8 @@</span> <span class="p_context"></span>
 #include &lt;linux/io.h&gt;
 #include &lt;linux/mm.h&gt;
 #include &lt;linux/memory_hotplug.h&gt;
<span class="p_add">+#include &lt;linux/swap.h&gt;</span>
<span class="p_add">+#include &lt;linux/swapops.h&gt;</span>
 
 #ifndef ioremap_cache
 /* temporary while we convert existing ioremap_cache users to memremap */
<span class="p_chunk">@@ -200,6 +202,21 @@</span> <span class="p_context"> void put_zone_device_page(struct page *page)</span>
 }
 EXPORT_SYMBOL(put_zone_device_page);
 
<span class="p_add">+#if IS_ENABLED(CONFIG_DEVICE_UNADDRESSABLE)</span>
<span class="p_add">+int device_entry_fault(struct vm_area_struct *vma,</span>
<span class="p_add">+		       unsigned long addr,</span>
<span class="p_add">+		       swp_entry_t entry,</span>
<span class="p_add">+		       unsigned flags,</span>
<span class="p_add">+		       pmd_t *pmdp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct page *page = device_entry_to_page(entry);</span>
<span class="p_add">+</span>
<span class="p_add">+	BUG_ON(!page-&gt;pgmap-&gt;page_fault);</span>
<span class="p_add">+	return page-&gt;pgmap-&gt;page_fault(vma, addr, page, flags, pmdp);</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(device_entry_fault);</span>
<span class="p_add">+#endif /* CONFIG_DEVICE_UNADDRESSABLE */</span>
<span class="p_add">+</span>
 static void pgmap_radix_release(struct resource *res)
 {
 	resource_size_t key, align_start, align_size, align_end;
<span class="p_chunk">@@ -252,7 +269,7 @@</span> <span class="p_context"> static void devm_memremap_pages_release(struct device *dev, void *data)</span>
 	/* pages are dead and unused, undo the arch mapping */
 	align_start = res-&gt;start &amp; ~(SECTION_SIZE - 1);
 	align_size = ALIGN(resource_size(res), SECTION_SIZE);
<span class="p_del">-	arch_remove_memory(align_start, align_size, MEMORY_DEVICE);</span>
<span class="p_add">+	arch_remove_memory(align_start, align_size, pgmap-&gt;flags);</span>
 	untrack_pfn(NULL, PHYS_PFN(align_start), align_size);
 	pgmap_radix_release(res);
 	dev_WARN_ONCE(dev, pgmap-&gt;altmap &amp;&amp; pgmap-&gt;altmap-&gt;alloc,
<span class="p_chunk">@@ -276,8 +293,11 @@</span> <span class="p_context"> struct dev_pagemap *find_dev_pagemap(resource_size_t phys)</span>
  * @res: &quot;host memory&quot; address range
  * @ref: a live per-cpu reference count
  * @altmap: optional descriptor for allocating the memmap from @res
<span class="p_add">+ * @ppgmap: pointer set to new page dev_pagemap on success</span>
<span class="p_add">+ * @page_fault: callback for CPU page fault on un-addressable memory</span>
  * @page_free: callback call when page refcount reach 1 ie it is free
  * @data: privata data pointer for page_free
<span class="p_add">+ * @flags: device memory flags (look for MEMORY_DEVICE_* memory_hotplug.h)</span>
  *
  * Notes:
  * 1/ @ref must be &#39;live&#39; on entry and &#39;dead&#39; before devm_memunmap_pages() time
<span class="p_chunk">@@ -289,8 +309,10 @@</span> <span class="p_context"> struct dev_pagemap *find_dev_pagemap(resource_size_t phys)</span>
  */
 void *devm_memremap_pages(struct device *dev, struct resource *res,
 			  struct percpu_ref *ref, struct vmem_altmap *altmap,
<span class="p_add">+			  struct dev_pagemap **ppgmap,</span>
<span class="p_add">+			  dev_page_fault_t page_fault,</span>
 			  dev_page_free_t page_free,
<span class="p_del">-			  void *data)</span>
<span class="p_add">+			  void *data, int flags)</span>
 {
 	resource_size_t key, align_start, align_size, align_end;
 	pgprot_t pgprot = PAGE_KERNEL;
<span class="p_chunk">@@ -299,6 +321,17 @@</span> <span class="p_context"> void *devm_memremap_pages(struct device *dev, struct resource *res,</span>
 	int error, nid, is_ram;
 	unsigned long pfn;
 
<span class="p_add">+	if (!(flags &amp; MEMORY_DEVICE)) {</span>
<span class="p_add">+		WARN_ONCE(1, &quot;%s attempted on non device memory\n&quot;, __func__);</span>
<span class="p_add">+		return ERR_PTR(-EINVAL);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (altmap &amp;&amp; (flags &amp; MEMORY_DEVICE_UNADDRESSABLE)) {</span>
<span class="p_add">+		WARN_ONCE(1, &quot;%s with altmap for un-addressable &quot;</span>
<span class="p_add">+			  &quot;device memory\n&quot;, __func__);</span>
<span class="p_add">+		return ERR_PTR(-EINVAL);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	align_start = res-&gt;start &amp; ~(SECTION_SIZE - 1);
 	align_size = ALIGN(res-&gt;start + resource_size(res), SECTION_SIZE)
 		- align_start;
<span class="p_chunk">@@ -332,8 +365,10 @@</span> <span class="p_context"> void *devm_memremap_pages(struct device *dev, struct resource *res,</span>
 	}
 	pgmap-&gt;ref = ref;
 	pgmap-&gt;res = &amp;page_map-&gt;res;
<span class="p_add">+	pgmap-&gt;page_fault = page_fault;</span>
 	pgmap-&gt;page_free = page_free;
 	pgmap-&gt;data = data;
<span class="p_add">+	pgmap-&gt;flags = flags;</span>
 
 	mutex_lock(&amp;pgmap_lock);
 	error = 0;
<span class="p_chunk">@@ -370,7 +405,7 @@</span> <span class="p_context"> void *devm_memremap_pages(struct device *dev, struct resource *res,</span>
 	if (error)
 		goto err_pfn_remap;
 
<span class="p_del">-	error = arch_add_memory(nid, align_start, align_size, MEMORY_DEVICE);</span>
<span class="p_add">+	error = arch_add_memory(nid, align_start, align_size, pgmap-&gt;flags);</span>
 	if (error)
 		goto err_add_memory;
 
<span class="p_chunk">@@ -387,6 +422,8 @@</span> <span class="p_context"> void *devm_memremap_pages(struct device *dev, struct resource *res,</span>
 		page-&gt;pgmap = pgmap;
 	}
 	devres_add(dev, page_map);
<span class="p_add">+	if (ppgmap)</span>
<span class="p_add">+		*ppgmap = pgmap;</span>
 	return __va(res-&gt;start);
 
  err_add_memory:
<span class="p_header">diff --git a/mm/Kconfig b/mm/Kconfig</span>
<span class="p_header">index be0ee11..8564a5f 100644</span>
<span class="p_header">--- a/mm/Kconfig</span>
<span class="p_header">+++ b/mm/Kconfig</span>
<span class="p_chunk">@@ -704,6 +704,18 @@</span> <span class="p_context"> config ZONE_DEVICE</span>
 
 	  If FS_DAX is enabled, then say Y.
 
<span class="p_add">+config DEVICE_UNADDRESSABLE</span>
<span class="p_add">+	bool &quot;Un-addressable device memory (GPU memory, ...)&quot;</span>
<span class="p_add">+	depends on ZONE_DEVICE</span>
<span class="p_add">+</span>
<span class="p_add">+	help</span>
<span class="p_add">+	  Allow to create struct page for un-addressable device memory</span>
<span class="p_add">+	  ie memory that is only accessible by the device (or group of</span>
<span class="p_add">+	  devices).</span>
<span class="p_add">+</span>
<span class="p_add">+	  Having struct page is necessary for process memory migration</span>
<span class="p_add">+	  to device memory.</span>
<span class="p_add">+</span>
 config FRAME_VECTOR
 	bool
 
<span class="p_header">diff --git a/mm/memory.c b/mm/memory.c</span>
<span class="p_header">index 840adc6..03306cf 100644</span>
<span class="p_header">--- a/mm/memory.c</span>
<span class="p_header">+++ b/mm/memory.c</span>
<span class="p_chunk">@@ -45,6 +45,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/swap.h&gt;
 #include &lt;linux/highmem.h&gt;
 #include &lt;linux/pagemap.h&gt;
<span class="p_add">+#include &lt;linux/memremap.h&gt;</span>
 #include &lt;linux/ksm.h&gt;
 #include &lt;linux/rmap.h&gt;
 #include &lt;linux/export.h&gt;
<span class="p_chunk">@@ -888,6 +889,25 @@</span> <span class="p_context"> copy_one_pte(struct mm_struct *dst_mm, struct mm_struct *src_mm,</span>
 					pte = pte_swp_mksoft_dirty(pte);
 				set_pte_at(src_mm, addr, src_pte, pte);
 			}
<span class="p_add">+		} else if (is_device_entry(entry)) {</span>
<span class="p_add">+			page = device_entry_to_page(entry);</span>
<span class="p_add">+</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * Update rss count even for un-addressable page as</span>
<span class="p_add">+			 * they should be consider just like any other page.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			get_page(page);</span>
<span class="p_add">+			rss[mm_counter(page)]++;</span>
<span class="p_add">+			page_dup_rmap(page, false);</span>
<span class="p_add">+</span>
<span class="p_add">+			if (is_write_device_entry(entry) &amp;&amp;</span>
<span class="p_add">+			    is_cow_mapping(vm_flags)) {</span>
<span class="p_add">+				make_device_entry_read(&amp;entry);</span>
<span class="p_add">+				pte = swp_entry_to_pte(entry);</span>
<span class="p_add">+				if (pte_swp_soft_dirty(*src_pte))</span>
<span class="p_add">+					pte = pte_swp_mksoft_dirty(pte);</span>
<span class="p_add">+				set_pte_at(src_mm, addr, src_pte, pte);</span>
<span class="p_add">+			}</span>
 		}
 		goto out_set_pte;
 	}
<span class="p_chunk">@@ -1178,6 +1198,32 @@</span> <span class="p_context"> again:</span>
 			}
 			continue;
 		}
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Un-addressable page must always be check that are not like</span>
<span class="p_add">+		 * other swap entries and thus should be check no matter what</span>
<span class="p_add">+		 * details-&gt;check_swap_entries value is.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		entry = pte_to_swp_entry(ptent);</span>
<span class="p_add">+		if (non_swap_entry(entry) &amp;&amp; is_device_entry(entry)) {</span>
<span class="p_add">+			struct page *page = device_entry_to_page(entry);</span>
<span class="p_add">+</span>
<span class="p_add">+			if (unlikely(details)) {</span>
<span class="p_add">+				/*</span>
<span class="p_add">+				 * unmap_shared_mapping_pages() wants to</span>
<span class="p_add">+				 * invalidate cache without truncating:</span>
<span class="p_add">+				 * unmap shared but keep private pages.</span>
<span class="p_add">+				 */</span>
<span class="p_add">+				if (details-&gt;check_mapping &amp;&amp;</span>
<span class="p_add">+				    details-&gt;check_mapping != page_rmapping(page))</span>
<span class="p_add">+					continue;</span>
<span class="p_add">+			}</span>
<span class="p_add">+</span>
<span class="p_add">+			rss[mm_counter(page)]--;</span>
<span class="p_add">+			page_remove_rmap(page, false);</span>
<span class="p_add">+			put_page(page);</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
 		/* only check swap_entries if explicitly asked for in details */
 		if (unlikely(details &amp;&amp; !details-&gt;check_swap_entries))
 			continue;
<span class="p_chunk">@@ -2535,6 +2581,14 @@</span> <span class="p_context"> int do_swap_page(struct fault_env *fe, pte_t orig_pte)</span>
 	if (unlikely(non_swap_entry(entry))) {
 		if (is_migration_entry(entry)) {
 			migration_entry_wait(vma-&gt;vm_mm, fe-&gt;pmd, fe-&gt;address);
<span class="p_add">+		} else if (is_device_entry(entry)) {</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * For un-addressable device memory we call the pgmap</span>
<span class="p_add">+			 * fault handler callback. The callback must migrate</span>
<span class="p_add">+			 * the page back to some CPU accessible page.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			ret = device_entry_fault(vma, fe-&gt;address, entry,</span>
<span class="p_add">+						 fe-&gt;flags, fe-&gt;pmd);</span>
 		} else if (is_hwpoison_entry(entry)) {
 			ret = VM_FAULT_HWPOISON;
 		} else {
<span class="p_chunk">@@ -3482,6 +3536,7 @@</span> <span class="p_context"> static inline bool vma_is_accessible(struct vm_area_struct *vma)</span>
 static int handle_pte_fault(struct fault_env *fe)
 {
 	pte_t entry;
<span class="p_add">+	struct page *page;</span>
 
 	if (unlikely(pmd_none(*fe-&gt;pmd))) {
 		/*
<span class="p_chunk">@@ -3533,6 +3588,13 @@</span> <span class="p_context"> static int handle_pte_fault(struct fault_env *fe)</span>
 	if (pte_protnone(entry) &amp;&amp; vma_is_accessible(fe-&gt;vma))
 		return do_numa_page(fe, entry);
 
<span class="p_add">+	/* Catch mapping of un-addressable memory this should never happen */</span>
<span class="p_add">+	page = pfn_to_page(pte_pfn(entry));</span>
<span class="p_add">+	if (!is_addressable_page(page)) {</span>
<span class="p_add">+		print_bad_pte(fe-&gt;vma, fe-&gt;address, entry, page);</span>
<span class="p_add">+		return VM_FAULT_SIGBUS;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	fe-&gt;ptl = pte_lockptr(fe-&gt;vma-&gt;vm_mm, fe-&gt;pmd);
 	spin_lock(fe-&gt;ptl);
 	if (unlikely(!pte_same(*fe-&gt;pte, entry)))
<span class="p_header">diff --git a/mm/mprotect.c b/mm/mprotect.c</span>
<span class="p_header">index 1bc1eb3..70aff3a 100644</span>
<span class="p_header">--- a/mm/mprotect.c</span>
<span class="p_header">+++ b/mm/mprotect.c</span>
<span class="p_chunk">@@ -139,6 +139,18 @@</span> <span class="p_context"> static unsigned long change_pte_range(struct vm_area_struct *vma, pmd_t *pmd,</span>
 
 				pages++;
 			}
<span class="p_add">+</span>
<span class="p_add">+			if (is_write_device_entry(entry)) {</span>
<span class="p_add">+				pte_t newpte;</span>
<span class="p_add">+</span>
<span class="p_add">+				make_device_entry_read(&amp;entry);</span>
<span class="p_add">+				newpte = swp_entry_to_pte(entry);</span>
<span class="p_add">+				if (pte_swp_soft_dirty(oldpte))</span>
<span class="p_add">+					newpte = pte_swp_mksoft_dirty(newpte);</span>
<span class="p_add">+				set_pte_at(mm, addr, pte, newpte);</span>
<span class="p_add">+</span>
<span class="p_add">+				pages++;</span>
<span class="p_add">+			}</span>
 		}
 	} while (pte++, addr += PAGE_SIZE, addr != end);
 	arch_leave_lazy_mmu_mode();
<span class="p_header">diff --git a/tools/testing/nvdimm/test/iomap.c b/tools/testing/nvdimm/test/iomap.c</span>
<span class="p_header">index 6505a87..0c8696c 100644</span>
<span class="p_header">--- a/tools/testing/nvdimm/test/iomap.c</span>
<span class="p_header">+++ b/tools/testing/nvdimm/test/iomap.c</span>
<span class="p_chunk">@@ -108,7 +108,8 @@</span> <span class="p_context"> void *__wrap_devm_memremap_pages(struct device *dev, struct resource *res,</span>
 
 	if (nfit_res)
 		return nfit_res-&gt;buf + offset - nfit_res-&gt;res-&gt;start;
<span class="p_del">-	return devm_memremap_pages(dev, res, ref, altmap, NULL, NULL);</span>
<span class="p_add">+	return devm_memremap_pages(dev, res, ref, altmap, NULL,</span>
<span class="p_add">+				   NULL, NULL, NULL, MEMORY_DEVICE);</span>
 }
 EXPORT_SYMBOL(__wrap_devm_memremap_pages);
 

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



