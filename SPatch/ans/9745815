
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[RFC,v2,09/10] mm: Change mmap_sem to range lock - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [RFC,v2,09/10] mm: Change mmap_sem to range lock</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=89031">Laurent Dufour</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>May 24, 2017, 11:20 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1495624801-8063-10-git-send-email-ldufour@linux.vnet.ibm.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9745815/mbox/"
   >mbox</a>
|
   <a href="/patch/9745815/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9745815/">/patch/9745815/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	E1ADB601C2 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 24 May 2017 11:23:42 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id C40D428545
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 24 May 2017 11:23:42 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id B54E7285E3; Wed, 24 May 2017 11:23:42 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 7BF5028545
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 24 May 2017 11:23:35 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1163557AbdEXLXd (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Wed, 24 May 2017 07:23:33 -0400
Received: from mx0a-001b2d01.pphosted.com ([148.163.156.1]:37132 &quot;EHLO
	mx0a-001b2d01.pphosted.com&quot; rhost-flags-OK-OK-OK-OK)
	by vger.kernel.org with ESMTP id S1762479AbdEXLUf (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Wed, 24 May 2017 07:20:35 -0400
Received: from pps.filterd (m0098394.ppops.net [127.0.0.1])
	by mx0a-001b2d01.pphosted.com (8.16.0.20/8.16.0.20) with SMTP id
	v4OBDSYk051152
	for &lt;linux-kernel@vger.kernel.org&gt;; Wed, 24 May 2017 07:20:35 -0400
Received: from e06smtp12.uk.ibm.com (e06smtp12.uk.ibm.com [195.75.94.108])
	by mx0a-001b2d01.pphosted.com with ESMTP id 2an1m13k29-1
	(version=TLSv1.2 cipher=AES256-SHA bits=256 verify=NOT)
	for &lt;linux-kernel@vger.kernel.org&gt;; Wed, 24 May 2017 07:20:31 -0400
Received: from localhost
	by e06smtp12.uk.ibm.com with IBM ESMTP SMTP Gateway: Authorized Use
	Only! Violators will be prosecuted
	for &lt;linux-kernel@vger.kernel.org&gt; from &lt;ldufour@linux.vnet.ibm.com&gt;; 
	Wed, 24 May 2017 12:20:27 +0100
Received: from b06cxnps4075.portsmouth.uk.ibm.com (9.149.109.197)
	by e06smtp12.uk.ibm.com (192.168.101.142) with IBM ESMTP SMTP
	Gateway: Authorized Use Only! Violators will be prosecuted; 
	Wed, 24 May 2017 12:20:25 +0100
Received: from d06av26.portsmouth.uk.ibm.com (d06av26.portsmouth.uk.ibm.com
	[9.149.105.62])
	by b06cxnps4075.portsmouth.uk.ibm.com (8.14.9/8.14.9/NCO v10.0) with
	ESMTP id v4OBKORq31654092; Wed, 24 May 2017 11:20:24 GMT
Received: from d06av26.portsmouth.uk.ibm.com (unknown [127.0.0.1])
	by IMSVA (Postfix) with ESMTP id 2B4DCAE04D;
	Wed, 24 May 2017 12:18:22 +0100 (BST)
Received: from d06av26.portsmouth.uk.ibm.com (unknown [127.0.0.1])
	by IMSVA (Postfix) with ESMTP id 55A2CAE073;
	Wed, 24 May 2017 12:18:20 +0100 (BST)
Received: from nimbus.lab.toulouse-stg.fr.ibm.com (unknown [9.145.185.18])
	by d06av26.portsmouth.uk.ibm.com (Postfix) with ESMTP;
	Wed, 24 May 2017 12:18:19 +0100 (BST)
From: Laurent Dufour &lt;ldufour@linux.vnet.ibm.com&gt;
To: linux-mm@kvack.org
Cc: Davidlohr Bueso &lt;dave@stgolabs.net&gt;, akpm@linux-foundation.org,
	Jan Kara &lt;jack@suse.cz&gt;, &quot;Kirill A . Shutemov&quot; &lt;kirill@shutemov.name&gt;,
	Michal Hocko &lt;mhocko@kernel.org&gt;, Peter Zijlstra &lt;peterz@infradead.org&gt;,
	Mel Gorman &lt;mgorman@techsingularity.net&gt;,
	Andi Kleen &lt;andi@firstfloor.org&gt;, haren@linux.vnet.ibm.com,
	aneesh.kumar@linux.vnet.ibm.com, khandual@linux.vnet.ibm.com,
	paulmck@linux.vnet.ibm.com, linux-kernel@vger.kernel.org
Subject: [RFC v2 09/10] mm: Change mmap_sem to range lock
Date: Wed, 24 May 2017 13:20:00 +0200
X-Mailer: git-send-email 2.7.4
In-Reply-To: &lt;1495624801-8063-1-git-send-email-ldufour@linux.vnet.ibm.com&gt;
References: &lt;1495624801-8063-1-git-send-email-ldufour@linux.vnet.ibm.com&gt;
X-TM-AS-GCONF: 00
x-cbid: 17052411-0008-0000-0000-000004532C8D
X-IBM-AV-DETECTION: SAVI=unused REMOTE=unused XFE=unused
x-cbparentid: 17052411-0009-0000-0000-00001DCE4214
Message-Id: &lt;1495624801-8063-10-git-send-email-ldufour@linux.vnet.ibm.com&gt;
X-Proofpoint-Virus-Version: vendor=fsecure engine=2.50.10432:, ,
	definitions=2017-05-24_08:, , signatures=0
X-Proofpoint-Spam-Details: rule=outbound_notspam policy=outbound score=0
	spamscore=0 suspectscore=4
	malwarescore=0 phishscore=0 adultscore=0 bulkscore=0 classifier=spam
	adjust=0 reason=mlx scancount=1 engine=8.0.1-1703280000
	definitions=main-1705240054
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=89031">Laurent Dufour</a> - May 24, 2017, 11:20 a.m.</div>
<pre class="content">
Change the mmap_sem to a range lock to allow finer grain locking on
the memory layout of a task.

This patch move the mmap_sem to a range lock.
To achieve that in a configurable way, all call to down_read(),
up_read(), etc. to the mmap_sem are encapsulated into new mm specific
services. This will allow to change this call to range lock operation.

The range lock operation requires an additional parameter which
declare using a dedicated macro. This avoids declaration of unused
variable in the case CONFIG_MEM_RANGE_LOCK is not defined.
This macro create a full range variable so no functional changes is
expected through this patch even if CONFIG_MEM_RANGE_LOCK is defined.

Currently, this patch only supports x86 and PowerPc architectures,
furthermore it should break the build of any others.
<span class="signed-off-by">
Signed-off-by: Laurent Dufour &lt;ldufour@linux.vnet.ibm.com&gt;</span>
---
 arch/powerpc/kernel/vdso.c                         |  7 ++-
 arch/powerpc/kvm/book3s_64_mmu_hv.c                |  5 +-
 arch/powerpc/kvm/book3s_64_mmu_radix.c             |  5 +-
 arch/powerpc/kvm/book3s_64_vio.c                   |  5 +-
 arch/powerpc/kvm/book3s_hv.c                       |  7 ++-
 arch/powerpc/kvm/e500_mmu_host.c                   |  6 +-
 arch/powerpc/mm/copro_fault.c                      |  5 +-
 arch/powerpc/mm/fault.c                            | 11 ++--
 arch/powerpc/mm/mmu_context_iommu.c                |  5 +-
 arch/powerpc/mm/subpage-prot.c                     | 14 +++--
 arch/powerpc/oprofile/cell/spu_task_sync.c         |  7 ++-
 arch/powerpc/platforms/cell/spufs/file.c           |  4 +-
 arch/x86/entry/vdso/vma.c                          | 12 ++--
 arch/x86/kernel/tboot.c                            |  6 +-
 arch/x86/kernel/vm86_32.c                          |  5 +-
 arch/x86/mm/fault.c                                | 67 ++++++++++++++++------
 arch/x86/mm/mpx.c                                  | 15 +++--
 drivers/android/binder.c                           |  7 ++-
 drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c             |  5 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c            |  7 ++-
 drivers/gpu/drm/amd/amdgpu/amdgpu_mn.c             |  7 ++-
 drivers/gpu/drm/amd/amdkfd/kfd_events.c            |  5 +-
 drivers/gpu/drm/amd/amdkfd/kfd_process.c           |  5 +-
 drivers/gpu/drm/etnaviv/etnaviv_gem.c              |  5 +-
 drivers/gpu/drm/i915/i915_gem.c                    |  5 +-
 drivers/gpu/drm/i915/i915_gem_userptr.c            | 10 ++--
 drivers/gpu/drm/radeon/radeon_cs.c                 |  5 +-
 drivers/gpu/drm/radeon/radeon_gem.c                |  8 ++-
 drivers/gpu/drm/radeon/radeon_mn.c                 |  7 ++-
 drivers/gpu/drm/ttm/ttm_bo_vm.c                    |  4 +-
 drivers/infiniband/core/umem.c                     | 17 +++---
 drivers/infiniband/core/umem_odp.c                 |  5 +-
 drivers/infiniband/hw/hfi1/user_pages.c            | 16 ++++--
 drivers/infiniband/hw/mlx4/main.c                  |  5 +-
 drivers/infiniband/hw/mlx5/main.c                  |  5 +-
 drivers/infiniband/hw/qib/qib_user_pages.c         | 11 ++--
 drivers/infiniband/hw/usnic/usnic_uiom.c           | 17 +++---
 drivers/iommu/amd_iommu_v2.c                       |  7 ++-
 drivers/iommu/intel-svm.c                          |  5 +-
 drivers/media/v4l2-core/videobuf-core.c            |  5 +-
 drivers/media/v4l2-core/videobuf-dma-contig.c      |  5 +-
 drivers/media/v4l2-core/videobuf-dma-sg.c          |  5 +-
 drivers/misc/cxl/fault.c                           |  5 +-
 drivers/misc/mic/scif/scif_rma.c                   | 16 ++++--
 drivers/oprofile/buffer_sync.c                     | 12 ++--
 drivers/staging/lustre/lustre/llite/llite_mmap.c   |  3 +-
 drivers/staging/lustre/lustre/llite/vvp_io.c       |  5 +-
 .../interface/vchiq_arm/vchiq_2835_arm.c           |  6 +-
 .../vc04_services/interface/vchiq_arm/vchiq_arm.c  |  5 +-
 drivers/vfio/vfio_iommu_spapr_tce.c                | 11 ++--
 drivers/vfio/vfio_iommu_type1.c                    | 16 +++---
 drivers/xen/gntdev.c                               |  5 +-
 drivers/xen/privcmd.c                              | 12 ++--
 fs/aio.c                                           |  5 +-
 fs/coredump.c                                      |  5 +-
 fs/exec.c                                          | 20 ++++---
 fs/proc/base.c                                     | 32 ++++++-----
 fs/proc/internal.h                                 |  3 +
 fs/proc/task_mmu.c                                 | 24 ++++----
 fs/proc/task_nommu.c                               | 24 ++++----
 fs/userfaultfd.c                                   | 21 ++++---
 ipc/shm.c                                          | 10 ++--
 kernel/acct.c                                      |  5 +-
 kernel/events/core.c                               |  5 +-
 kernel/events/uprobes.c                            | 20 ++++---
 kernel/exit.c                                      |  9 +--
 kernel/fork.c                                      | 20 +++++--
 kernel/futex.c                                     |  7 ++-
 kernel/sched/fair.c                                |  6 +-
 kernel/sys.c                                       | 22 ++++---
 kernel/trace/trace_output.c                        |  5 +-
 mm/filemap.c                                       |  4 +-
 mm/frame_vector.c                                  |  8 ++-
 mm/gup.c                                           | 18 +++---
 mm/init-mm.c                                       |  4 ++
 mm/khugepaged.c                                    | 35 +++++------
 mm/ksm.c                                           | 36 +++++++-----
 mm/madvise.c                                       | 17 +++---
 mm/memcontrol.c                                    | 12 ++--
 mm/memory.c                                        | 17 ++++--
 mm/mempolicy.c                                     | 26 +++++----
 mm/migrate.c                                       | 10 ++--
 mm/mincore.c                                       |  5 +-
 mm/mlock.c                                         | 20 ++++---
 mm/mmap.c                                          | 34 ++++++-----
 mm/mmu_notifier.c                                  |  5 +-
 mm/mprotect.c                                      | 15 +++--
 mm/mremap.c                                        |  5 +-
 mm/msync.c                                         |  9 +--
 mm/nommu.c                                         | 26 +++++----
 mm/oom_kill.c                                      |  7 ++-
 mm/process_vm_access.c                             |  7 ++-
 mm/shmem.c                                         |  2 +-
 mm/swapfile.c                                      |  7 ++-
 mm/userfaultfd.c                                   | 15 ++---
 mm/util.c                                          | 11 ++--
 virt/kvm/async_pf.c                                |  7 ++-
 virt/kvm/kvm_main.c                                | 29 +++++++---
 98 files changed, 667 insertions(+), 432 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/powerpc/kernel/vdso.c b/arch/powerpc/kernel/vdso.c</span>
<span class="p_header">index 22b01a3962f0..338da057c24e 100644</span>
<span class="p_header">--- a/arch/powerpc/kernel/vdso.c</span>
<span class="p_header">+++ b/arch/powerpc/kernel/vdso.c</span>
<span class="p_chunk">@@ -155,6 +155,7 @@</span> <span class="p_context"> int arch_setup_additional_pages(struct linux_binprm *bprm, int uses_interp)</span>
 	unsigned long vdso_pages;
 	unsigned long vdso_base;
 	int rc;
<span class="p_add">+	mm_range_define(range);</span>
 
 	if (!vdso_ready)
 		return 0;
<span class="p_chunk">@@ -196,7 +197,7 @@</span> <span class="p_context"> int arch_setup_additional_pages(struct linux_binprm *bprm, int uses_interp)</span>
 	 * and end up putting it elsewhere.
 	 * Add enough to the size so that the result can be aligned.
 	 */
<span class="p_del">-	if (down_write_killable(&amp;mm-&gt;mmap_sem))</span>
<span class="p_add">+	if (mm_write_lock_killable(mm, &amp;range))</span>
 		return -EINTR;
 	vdso_base = get_unmapped_area(NULL, vdso_base,
 				      (vdso_pages &lt;&lt; PAGE_SHIFT) +
<span class="p_chunk">@@ -236,11 +237,11 @@</span> <span class="p_context"> int arch_setup_additional_pages(struct linux_binprm *bprm, int uses_interp)</span>
 		goto fail_mmapsem;
 	}
 
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(mm, &amp;range);</span>
 	return 0;
 
  fail_mmapsem:
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(mm, &amp;range);</span>
 	return rc;
 }
 
<span class="p_header">diff --git a/arch/powerpc/kvm/book3s_64_mmu_hv.c b/arch/powerpc/kvm/book3s_64_mmu_hv.c</span>
<span class="p_header">index 710e491206ed..3260d3fa49c0 100644</span>
<span class="p_header">--- a/arch/powerpc/kvm/book3s_64_mmu_hv.c</span>
<span class="p_header">+++ b/arch/powerpc/kvm/book3s_64_mmu_hv.c</span>
<span class="p_chunk">@@ -485,6 +485,7 @@</span> <span class="p_context"> int kvmppc_book3s_hv_page_fault(struct kvm_run *run, struct kvm_vcpu *vcpu,</span>
 	struct vm_area_struct *vma;
 	unsigned long rcbits;
 	long mmio_update;
<span class="p_add">+	mm_range_define(range);</span>
 
 	if (kvm_is_radix(kvm))
 		return kvmppc_book3s_radix_page_fault(run, vcpu, ea, dsisr);
<span class="p_chunk">@@ -568,7 +569,7 @@</span> <span class="p_context"> int kvmppc_book3s_hv_page_fault(struct kvm_run *run, struct kvm_vcpu *vcpu,</span>
 	npages = get_user_pages_fast(hva, 1, writing, pages);
 	if (npages &lt; 1) {
 		/* Check if it&#39;s an I/O mapping */
<span class="p_del">-		down_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_lock(current-&gt;mm, &amp;range);</span>
 		vma = find_vma(current-&gt;mm, hva);
 		if (vma &amp;&amp; vma-&gt;vm_start &lt;= hva &amp;&amp; hva + psize &lt;= vma-&gt;vm_end &amp;&amp;
 		    (vma-&gt;vm_flags &amp; VM_PFNMAP)) {
<span class="p_chunk">@@ -578,7 +579,7 @@</span> <span class="p_context"> int kvmppc_book3s_hv_page_fault(struct kvm_run *run, struct kvm_vcpu *vcpu,</span>
 			is_ci = pte_ci(__pte((pgprot_val(vma-&gt;vm_page_prot))));
 			write_ok = vma-&gt;vm_flags &amp; VM_WRITE;
 		}
<span class="p_del">-		up_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_unlock(current-&gt;mm, &amp;range);</span>
 		if (!pfn)
 			goto out_put;
 	} else {
<span class="p_header">diff --git a/arch/powerpc/kvm/book3s_64_mmu_radix.c b/arch/powerpc/kvm/book3s_64_mmu_radix.c</span>
<span class="p_header">index f6b3e67c5762..9aa215cb87a2 100644</span>
<span class="p_header">--- a/arch/powerpc/kvm/book3s_64_mmu_radix.c</span>
<span class="p_header">+++ b/arch/powerpc/kvm/book3s_64_mmu_radix.c</span>
<span class="p_chunk">@@ -305,6 +305,7 @@</span> <span class="p_context"> int kvmppc_book3s_radix_page_fault(struct kvm_run *run, struct kvm_vcpu *vcpu,</span>
 	pte_t pte, *ptep;
 	unsigned long pgflags;
 	unsigned int shift, level;
<span class="p_add">+	mm_range_define(range);</span>
 
 	/* Check for unusual errors */
 	if (dsisr &amp; DSISR_UNSUPP_MMU) {
<span class="p_chunk">@@ -394,7 +395,7 @@</span> <span class="p_context"> int kvmppc_book3s_radix_page_fault(struct kvm_run *run, struct kvm_vcpu *vcpu,</span>
 	npages = get_user_pages_fast(hva, 1, writing, pages);
 	if (npages &lt; 1) {
 		/* Check if it&#39;s an I/O mapping */
<span class="p_del">-		down_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_lock(current-&gt;mm, &amp;range);</span>
 		vma = find_vma(current-&gt;mm, hva);
 		if (vma &amp;&amp; vma-&gt;vm_start &lt;= hva &amp;&amp; hva &lt; vma-&gt;vm_end &amp;&amp;
 		    (vma-&gt;vm_flags &amp; VM_PFNMAP)) {
<span class="p_chunk">@@ -402,7 +403,7 @@</span> <span class="p_context"> int kvmppc_book3s_radix_page_fault(struct kvm_run *run, struct kvm_vcpu *vcpu,</span>
 				((hva - vma-&gt;vm_start) &gt;&gt; PAGE_SHIFT);
 			pgflags = pgprot_val(vma-&gt;vm_page_prot);
 		}
<span class="p_del">-		up_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_unlock(current-&gt;mm, &amp;range);</span>
 		if (!pfn)
 			return -EFAULT;
 	} else {
<span class="p_header">diff --git a/arch/powerpc/kvm/book3s_64_vio.c b/arch/powerpc/kvm/book3s_64_vio.c</span>
<span class="p_header">index a160c14304eb..599d7a882597 100644</span>
<span class="p_header">--- a/arch/powerpc/kvm/book3s_64_vio.c</span>
<span class="p_header">+++ b/arch/powerpc/kvm/book3s_64_vio.c</span>
<span class="p_chunk">@@ -60,11 +60,12 @@</span> <span class="p_context"> static unsigned long kvmppc_stt_pages(unsigned long tce_pages)</span>
 static long kvmppc_account_memlimit(unsigned long stt_pages, bool inc)
 {
 	long ret = 0;
<span class="p_add">+	mm_range_define(range);</span>
 
 	if (!current || !current-&gt;mm)
 		return ret; /* process exited */
 
<span class="p_del">-	down_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_lock(current-&gt;mm, &amp;range);</span>
 
 	if (inc) {
 		unsigned long locked, lock_limit;
<span class="p_chunk">@@ -89,7 +90,7 @@</span> <span class="p_context"> static long kvmppc_account_memlimit(unsigned long stt_pages, bool inc)</span>
 			rlimit(RLIMIT_MEMLOCK),
 			ret ? &quot; - exceeded&quot; : &quot;&quot;);
 
<span class="p_del">-	up_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(current-&gt;mm, &amp;range);</span>
 
 	return ret;
 }
<span class="p_header">diff --git a/arch/powerpc/kvm/book3s_hv.c b/arch/powerpc/kvm/book3s_hv.c</span>
<span class="p_header">index 42b7a4fd57d9..88005961a816 100644</span>
<span class="p_header">--- a/arch/powerpc/kvm/book3s_hv.c</span>
<span class="p_header">+++ b/arch/powerpc/kvm/book3s_hv.c</span>
<span class="p_chunk">@@ -3201,6 +3201,7 @@</span> <span class="p_context"> static int kvmppc_hv_setup_htab_rma(struct kvm_vcpu *vcpu)</span>
 	unsigned long lpcr = 0, senc;
 	unsigned long psize, porder;
 	int srcu_idx;
<span class="p_add">+	mm_range_define(range);</span>
 
 	mutex_lock(&amp;kvm-&gt;lock);
 	if (kvm-&gt;arch.hpte_setup_done)
<span class="p_chunk">@@ -3237,7 +3238,7 @@</span> <span class="p_context"> static int kvmppc_hv_setup_htab_rma(struct kvm_vcpu *vcpu)</span>
 
 	/* Look up the VMA for the start of this memory slot */
 	hva = memslot-&gt;userspace_addr;
<span class="p_del">-	down_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_lock(current-&gt;mm, &amp;range);</span>
 	vma = find_vma(current-&gt;mm, hva);
 	if (!vma || vma-&gt;vm_start &gt; hva || (vma-&gt;vm_flags &amp; VM_IO))
 		goto up_out;
<span class="p_chunk">@@ -3245,7 +3246,7 @@</span> <span class="p_context"> static int kvmppc_hv_setup_htab_rma(struct kvm_vcpu *vcpu)</span>
 	psize = vma_kernel_pagesize(vma);
 	porder = __ilog2(psize);
 
<span class="p_del">-	up_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(current-&gt;mm, &amp;range);</span>
 
 	/* We can handle 4k, 64k or 16M pages in the VRMA */
 	err = -EINVAL;
<span class="p_chunk">@@ -3279,7 +3280,7 @@</span> <span class="p_context"> static int kvmppc_hv_setup_htab_rma(struct kvm_vcpu *vcpu)</span>
 	return err;
 
  up_out:
<span class="p_del">-	up_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(current-&gt;mm, &amp;range);</span>
 	goto out_srcu;
 }
 
<span class="p_header">diff --git a/arch/powerpc/kvm/e500_mmu_host.c b/arch/powerpc/kvm/e500_mmu_host.c</span>
<span class="p_header">index 77fd043b3ecc..1539f977d5c7 100644</span>
<span class="p_header">--- a/arch/powerpc/kvm/e500_mmu_host.c</span>
<span class="p_header">+++ b/arch/powerpc/kvm/e500_mmu_host.c</span>
<span class="p_chunk">@@ -357,7 +357,9 @@</span> <span class="p_context"> static inline int kvmppc_e500_shadow_map(struct kvmppc_vcpu_e500 *vcpu_e500,</span>
 
 	if (tlbsel == 1) {
 		struct vm_area_struct *vma;
<span class="p_del">-		down_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_range_define(range);</span>
<span class="p_add">+</span>
<span class="p_add">+		mm_read_lock(current-&gt;mm, &amp;range);</span>
 
 		vma = find_vma(current-&gt;mm, hva);
 		if (vma &amp;&amp; hva &gt;= vma-&gt;vm_start &amp;&amp;
<span class="p_chunk">@@ -443,7 +445,7 @@</span> <span class="p_context"> static inline int kvmppc_e500_shadow_map(struct kvmppc_vcpu_e500 *vcpu_e500,</span>
 			tsize = max(BOOK3E_PAGESZ_4K, tsize &amp; ~1);
 		}
 
<span class="p_del">-		up_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_unlock(current-&gt;mm, &amp;range);</span>
 	}
 
 	if (likely(!pfnmap)) {
<span class="p_header">diff --git a/arch/powerpc/mm/copro_fault.c b/arch/powerpc/mm/copro_fault.c</span>
<span class="p_header">index 81fbf79d2e97..f7d8766369af 100644</span>
<span class="p_header">--- a/arch/powerpc/mm/copro_fault.c</span>
<span class="p_header">+++ b/arch/powerpc/mm/copro_fault.c</span>
<span class="p_chunk">@@ -39,6 +39,7 @@</span> <span class="p_context"> int copro_handle_mm_fault(struct mm_struct *mm, unsigned long ea,</span>
 	struct vm_area_struct *vma;
 	unsigned long is_write;
 	int ret;
<span class="p_add">+	mm_range_define(range);</span>
 
 	if (mm == NULL)
 		return -EFAULT;
<span class="p_chunk">@@ -46,7 +47,7 @@</span> <span class="p_context"> int copro_handle_mm_fault(struct mm_struct *mm, unsigned long ea,</span>
 	if (mm-&gt;pgd == NULL)
 		return -EFAULT;
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_lock(mm, &amp;range);</span>
 	ret = -EFAULT;
 	vma = find_vma(mm, ea);
 	if (!vma)
<span class="p_chunk">@@ -95,7 +96,7 @@</span> <span class="p_context"> int copro_handle_mm_fault(struct mm_struct *mm, unsigned long ea,</span>
 		current-&gt;min_flt++;
 
 out_unlock:
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(mm, &amp;range);</span>
 	return ret;
 }
 EXPORT_SYMBOL_GPL(copro_handle_mm_fault);
<span class="p_header">diff --git a/arch/powerpc/mm/fault.c b/arch/powerpc/mm/fault.c</span>
<span class="p_header">index 278550794dea..824143e12873 100644</span>
<span class="p_header">--- a/arch/powerpc/mm/fault.c</span>
<span class="p_header">+++ b/arch/powerpc/mm/fault.c</span>
<span class="p_chunk">@@ -208,6 +208,7 @@</span> <span class="p_context"> int do_page_fault(struct pt_regs *regs, unsigned long address,</span>
  	int is_exec = trap == 0x400;
 	int fault;
 	int rc = 0, store_update_sp = 0;
<span class="p_add">+	mm_range_define(range);</span>
 
 #if !(defined(CONFIG_4xx) || defined(CONFIG_BOOKE))
 	/*
<span class="p_chunk">@@ -308,12 +309,12 @@</span> <span class="p_context"> int do_page_fault(struct pt_regs *regs, unsigned long address,</span>
 	 * source.  If this is invalid we can skip the address space check,
 	 * thus avoiding the deadlock.
 	 */
<span class="p_del">-	if (!down_read_trylock(&amp;mm-&gt;mmap_sem)) {</span>
<span class="p_add">+	if (!mm_read_trylock(mm, &amp;range)) {</span>
 		if (!user_mode(regs) &amp;&amp; !search_exception_tables(regs-&gt;nip))
 			goto bad_area_nosemaphore;
 
 retry:
<span class="p_del">-		down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_lock(mm, &amp;range);</span>
 	} else {
 		/*
 		 * The above down_read_trylock() might have succeeded in
<span class="p_chunk">@@ -446,7 +447,7 @@</span> <span class="p_context"> int do_page_fault(struct pt_regs *regs, unsigned long address,</span>
 	 * make sure we exit gracefully rather than endlessly redo
 	 * the fault.
 	 */
<span class="p_del">-	fault = handle_mm_fault(vma, address, flags, NULL);</span>
<span class="p_add">+	fault = handle_mm_fault(vma, address, flags, &amp;range);</span>
 
 	/*
 	 * Handle the retry right now, the mmap_sem has been released in that
<span class="p_chunk">@@ -466,7 +467,7 @@</span> <span class="p_context"> int do_page_fault(struct pt_regs *regs, unsigned long address,</span>
 		}
 		/* We will enter mm_fault_error() below */
 	} else
<span class="p_del">-		up_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_unlock(mm, &amp;range);</span>
 
 	if (unlikely(fault &amp; (VM_FAULT_RETRY|VM_FAULT_ERROR))) {
 		if (fault &amp; VM_FAULT_SIGSEGV)
<span class="p_chunk">@@ -505,7 +506,7 @@</span> <span class="p_context"> int do_page_fault(struct pt_regs *regs, unsigned long address,</span>
 	goto bail;
 
 bad_area:
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(mm, &amp;range);</span>
 
 bad_area_nosemaphore:
 	/* User mode accesses cause a SIGSEGV */
<span class="p_header">diff --git a/arch/powerpc/mm/mmu_context_iommu.c b/arch/powerpc/mm/mmu_context_iommu.c</span>
<span class="p_header">index e0a2d8e806ed..b8e051b55e00 100644</span>
<span class="p_header">--- a/arch/powerpc/mm/mmu_context_iommu.c</span>
<span class="p_header">+++ b/arch/powerpc/mm/mmu_context_iommu.c</span>
<span class="p_chunk">@@ -36,11 +36,12 @@</span> <span class="p_context"> static long mm_iommu_adjust_locked_vm(struct mm_struct *mm,</span>
 		unsigned long npages, bool incr)
 {
 	long ret = 0, locked, lock_limit;
<span class="p_add">+	mm_range_define(range);</span>
 
 	if (!npages)
 		return 0;
 
<span class="p_del">-	down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_lock(mm, &amp;range);</span>
 
 	if (incr) {
 		locked = mm-&gt;locked_vm + npages;
<span class="p_chunk">@@ -61,7 +62,7 @@</span> <span class="p_context"> static long mm_iommu_adjust_locked_vm(struct mm_struct *mm,</span>
 			npages &lt;&lt; PAGE_SHIFT,
 			mm-&gt;locked_vm &lt;&lt; PAGE_SHIFT,
 			rlimit(RLIMIT_MEMLOCK));
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(mm, &amp;range);</span>
 
 	return ret;
 }
<span class="p_header">diff --git a/arch/powerpc/mm/subpage-prot.c b/arch/powerpc/mm/subpage-prot.c</span>
<span class="p_header">index e94fbd4c8845..f6e64c050ea4 100644</span>
<span class="p_header">--- a/arch/powerpc/mm/subpage-prot.c</span>
<span class="p_header">+++ b/arch/powerpc/mm/subpage-prot.c</span>
<span class="p_chunk">@@ -98,8 +98,9 @@</span> <span class="p_context"> static void subpage_prot_clear(unsigned long addr, unsigned long len)</span>
 	unsigned long i;
 	size_t nw;
 	unsigned long next, limit;
<span class="p_add">+	mm_range_define(range);</span>
 
<span class="p_del">-	down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_lock(mm, &amp;range);</span>
 	limit = addr + len;
 	if (limit &gt; spt-&gt;maxaddr)
 		limit = spt-&gt;maxaddr;
<span class="p_chunk">@@ -127,7 +128,7 @@</span> <span class="p_context"> static void subpage_prot_clear(unsigned long addr, unsigned long len)</span>
 		/* now flush any existing HPTEs for the range */
 		hpte_flush_range(mm, addr, nw);
 	}
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(mm, &amp;range);</span>
 }
 
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE
<span class="p_chunk">@@ -194,6 +195,7 @@</span> <span class="p_context"> long sys_subpage_prot(unsigned long addr, unsigned long len, u32 __user *map)</span>
 	size_t nw;
 	unsigned long next, limit;
 	int err;
<span class="p_add">+	mm_range_define(range);</span>
 
 	/* Check parameters */
 	if ((addr &amp; ~PAGE_MASK) || (len &amp; ~PAGE_MASK) ||
<span class="p_chunk">@@ -213,7 +215,7 @@</span> <span class="p_context"> long sys_subpage_prot(unsigned long addr, unsigned long len, u32 __user *map)</span>
 	if (!access_ok(VERIFY_READ, map, (len &gt;&gt; PAGE_SHIFT) * sizeof(u32)))
 		return -EFAULT;
 
<span class="p_del">-	down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_lock(mm, &amp;range);</span>
 	subpage_mark_vma_nohuge(mm, addr, len);
 	for (limit = addr + len; addr &lt; limit; addr = next) {
 		next = pmd_addr_end(addr, limit);
<span class="p_chunk">@@ -248,11 +250,11 @@</span> <span class="p_context"> long sys_subpage_prot(unsigned long addr, unsigned long len, u32 __user *map)</span>
 		if (addr + (nw &lt;&lt; PAGE_SHIFT) &gt; next)
 			nw = (next - addr) &gt;&gt; PAGE_SHIFT;
 
<span class="p_del">-		up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_write_unlock(mm, &amp;range);</span>
 		if (__copy_from_user(spp, map, nw * sizeof(u32)))
 			return -EFAULT;
 		map += nw;
<span class="p_del">-		down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_write_lock(mm, &amp;range);</span>
 
 		/* now flush any existing HPTEs for the range */
 		hpte_flush_range(mm, addr, nw);
<span class="p_chunk">@@ -261,6 +263,6 @@</span> <span class="p_context"> long sys_subpage_prot(unsigned long addr, unsigned long len, u32 __user *map)</span>
 		spt-&gt;maxaddr = limit;
 	err = 0;
  out:
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(mm, &amp;range);</span>
 	return err;
 }
<span class="p_header">diff --git a/arch/powerpc/oprofile/cell/spu_task_sync.c b/arch/powerpc/oprofile/cell/spu_task_sync.c</span>
<span class="p_header">index 44d67b167e0b..0fdc92a30f9d 100644</span>
<span class="p_header">--- a/arch/powerpc/oprofile/cell/spu_task_sync.c</span>
<span class="p_header">+++ b/arch/powerpc/oprofile/cell/spu_task_sync.c</span>
<span class="p_chunk">@@ -325,6 +325,7 @@</span> <span class="p_context"> get_exec_dcookie_and_offset(struct spu *spu, unsigned int *offsetp,</span>
 	struct vm_area_struct *vma;
 	struct file *exe_file;
 	struct mm_struct *mm = spu-&gt;mm;
<span class="p_add">+	mm_range_define(range);</span>
 
 	if (!mm)
 		goto out;
<span class="p_chunk">@@ -336,7 +337,7 @@</span> <span class="p_context"> get_exec_dcookie_and_offset(struct spu *spu, unsigned int *offsetp,</span>
 		fput(exe_file);
 	}
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_lock(mm, &amp;range);</span>
 	for (vma = mm-&gt;mmap; vma; vma = vma-&gt;vm_next) {
 		if (vma-&gt;vm_start &gt; spu_ref || vma-&gt;vm_end &lt;= spu_ref)
 			continue;
<span class="p_chunk">@@ -353,13 +354,13 @@</span> <span class="p_context"> get_exec_dcookie_and_offset(struct spu *spu, unsigned int *offsetp,</span>
 	*spu_bin_dcookie = fast_get_dcookie(&amp;vma-&gt;vm_file-&gt;f_path);
 	pr_debug(&quot;got dcookie for %pD\n&quot;, vma-&gt;vm_file);
 
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(mm, &amp;range);</span>
 
 out:
 	return app_cookie;
 
 fail_no_image_cookie:
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(mm, &amp;range);</span>
 
 	printk(KERN_ERR &quot;SPU_PROF: &quot;
 		&quot;%s, line %d: Cannot find dcookie for SPU binary\n&quot;,
<span class="p_header">diff --git a/arch/powerpc/platforms/cell/spufs/file.c b/arch/powerpc/platforms/cell/spufs/file.c</span>
<span class="p_header">index ae2f740a82f1..0360d9c7dd9c 100644</span>
<span class="p_header">--- a/arch/powerpc/platforms/cell/spufs/file.c</span>
<span class="p_header">+++ b/arch/powerpc/platforms/cell/spufs/file.c</span>
<span class="p_chunk">@@ -347,11 +347,11 @@</span> <span class="p_context"> static int spufs_ps_fault(struct vm_fault *vmf,</span>
 		goto refault;
 
 	if (ctx-&gt;state == SPU_STATE_SAVED) {
<span class="p_del">-		up_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_unlock(current-&gt;mm-&gt;mmap_sem, vmf-&gt;lockrange);</span>
 		spu_context_nospu_trace(spufs_ps_fault__sleep, ctx);
 		ret = spufs_wait(ctx-&gt;run_wq, ctx-&gt;state == SPU_STATE_RUNNABLE);
 		spu_context_trace(spufs_ps_fault__wake, ctx, ctx-&gt;spu);
<span class="p_del">-		down_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_lock(current-&gt;mm-&gt;mmap_sem, vmf-&gt;lockrange);</span>
 	} else {
 		area = ctx-&gt;spu-&gt;problem_phys + ps_offs;
 		vm_insert_pfn(vmf-&gt;vma, vmf-&gt;address, (area + offset) &gt;&gt; PAGE_SHIFT);
<span class="p_header">diff --git a/arch/x86/entry/vdso/vma.c b/arch/x86/entry/vdso/vma.c</span>
<span class="p_header">index 139ad7726e10..6f754b7675d8 100644</span>
<span class="p_header">--- a/arch/x86/entry/vdso/vma.c</span>
<span class="p_header">+++ b/arch/x86/entry/vdso/vma.c</span>
<span class="p_chunk">@@ -157,8 +157,9 @@</span> <span class="p_context"> static int map_vdso(const struct vdso_image *image, unsigned long addr)</span>
 	struct vm_area_struct *vma;
 	unsigned long text_start;
 	int ret = 0;
<span class="p_add">+	mm_range_define(range);</span>
 
<span class="p_del">-	if (down_write_killable(&amp;mm-&gt;mmap_sem))</span>
<span class="p_add">+	if (mm_write_lock_killable(mm, &amp;range))</span>
 		return -EINTR;
 
 	addr = get_unmapped_area(NULL, addr,
<span class="p_chunk">@@ -201,7 +202,7 @@</span> <span class="p_context"> static int map_vdso(const struct vdso_image *image, unsigned long addr)</span>
 	}
 
 up_fail:
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(mm, &amp;range);</span>
 	return ret;
 }
 
<span class="p_chunk">@@ -262,8 +263,9 @@</span> <span class="p_context"> int map_vdso_once(const struct vdso_image *image, unsigned long addr)</span>
 {
 	struct mm_struct *mm = current-&gt;mm;
 	struct vm_area_struct *vma;
<span class="p_add">+	mm_range_define(range);</span>
 
<span class="p_del">-	down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_lock(mm, &amp;range);</span>
 	/*
 	 * Check if we have already mapped vdso blob - fail to prevent
 	 * abusing from userspace install_speciall_mapping, which may
<span class="p_chunk">@@ -274,11 +276,11 @@</span> <span class="p_context"> int map_vdso_once(const struct vdso_image *image, unsigned long addr)</span>
 	for (vma = mm-&gt;mmap; vma; vma = vma-&gt;vm_next) {
 		if (vma_is_special_mapping(vma, &amp;vdso_mapping) ||
 				vma_is_special_mapping(vma, &amp;vvar_mapping)) {
<span class="p_del">-			up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+			mm_write_unlock(mm, &amp;range);</span>
 			return -EEXIST;
 		}
 	}
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(mm, &amp;range);</span>
 
 	return map_vdso(image, addr);
 }
<span class="p_header">diff --git a/arch/x86/kernel/tboot.c b/arch/x86/kernel/tboot.c</span>
<span class="p_header">index 4b1724059909..4a854f7dc1e9 100644</span>
<span class="p_header">--- a/arch/x86/kernel/tboot.c</span>
<span class="p_header">+++ b/arch/x86/kernel/tboot.c</span>
<span class="p_chunk">@@ -104,7 +104,11 @@</span> <span class="p_context"> static struct mm_struct tboot_mm = {</span>
 	.pgd            = swapper_pg_dir,
 	.mm_users       = ATOMIC_INIT(2),
 	.mm_count       = ATOMIC_INIT(1),
<span class="p_del">-	.mmap_sem       = __RWSEM_INITIALIZER(init_mm.mmap_sem),</span>
<span class="p_add">+#ifdef CONFIG_MEM_RANGE_LOCK</span>
<span class="p_add">+	.mmap_sem   = __RANGE_LOCK_TREE_INITIALIZER(init_mm.mmap_sem),</span>
<span class="p_add">+#else</span>
<span class="p_add">+	.mmap_sem   = __RWSEM_INITIALIZER(init_mm.mmap_sem),</span>
<span class="p_add">+#endif</span>
 	.page_table_lock =  __SPIN_LOCK_UNLOCKED(init_mm.page_table_lock),
 	.mmlist         = LIST_HEAD_INIT(init_mm.mmlist),
 };
<span class="p_header">diff --git a/arch/x86/kernel/vm86_32.c b/arch/x86/kernel/vm86_32.c</span>
<span class="p_header">index 7924a5356c8a..c927e46231eb 100644</span>
<span class="p_header">--- a/arch/x86/kernel/vm86_32.c</span>
<span class="p_header">+++ b/arch/x86/kernel/vm86_32.c</span>
<span class="p_chunk">@@ -169,8 +169,9 @@</span> <span class="p_context"> static void mark_screen_rdonly(struct mm_struct *mm)</span>
 	pmd_t *pmd;
 	pte_t *pte;
 	int i;
<span class="p_add">+	mm_range_define(range);</span>
 
<span class="p_del">-	down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_lock(mm, &amp;range);</span>
 	pgd = pgd_offset(mm, 0xA0000);
 	if (pgd_none_or_clear_bad(pgd))
 		goto out;
<span class="p_chunk">@@ -196,7 +197,7 @@</span> <span class="p_context"> static void mark_screen_rdonly(struct mm_struct *mm)</span>
 	}
 	pte_unmap_unlock(pte, ptl);
 out:
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(mm, &amp;range);</span>
 	flush_tlb_mm_range(mm, 0xA0000, 0xA0000 + 32*PAGE_SIZE, 0UL);
 }
 
<span class="p_header">diff --git a/arch/x86/mm/fault.c b/arch/x86/mm/fault.c</span>
<span class="p_header">index f078bc9458b0..4ecdec2bd264 100644</span>
<span class="p_header">--- a/arch/x86/mm/fault.c</span>
<span class="p_header">+++ b/arch/x86/mm/fault.c</span>
<span class="p_chunk">@@ -962,7 +962,11 @@</span> <span class="p_context"> bad_area_nosemaphore(struct pt_regs *regs, unsigned long error_code,</span>
 
 static void
 __bad_area(struct pt_regs *regs, unsigned long error_code,
<span class="p_del">-	   unsigned long address,  struct vm_area_struct *vma, int si_code)</span>
<span class="p_add">+	   unsigned long address,  struct vm_area_struct *vma, int si_code</span>
<span class="p_add">+#ifdef CONFIG_MEM_RANGE_LOCK</span>
<span class="p_add">+	   , struct range_lock *range</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	)</span>
 {
 	struct mm_struct *mm = current-&gt;mm;
 
<span class="p_chunk">@@ -970,17 +974,31 @@</span> <span class="p_context"> __bad_area(struct pt_regs *regs, unsigned long error_code,</span>
 	 * Something tried to access memory that isn&#39;t in our memory map..
 	 * Fix it, but check if it&#39;s kernel or user first..
 	 */
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(mm, range);</span>
 
 	__bad_area_nosemaphore(regs, error_code, address, vma, si_code);
 }
 
 static noinline void
<span class="p_del">-bad_area(struct pt_regs *regs, unsigned long error_code, unsigned long address)</span>
<span class="p_add">+_bad_area(struct pt_regs *regs, unsigned long error_code, unsigned long address</span>
<span class="p_add">+#ifdef CONFIG_MEM_RANGE_LOCK</span>
<span class="p_add">+	 , struct range_lock *range</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	)</span>
 {
<span class="p_del">-	__bad_area(regs, error_code, address, NULL, SEGV_MAPERR);</span>
<span class="p_add">+	__bad_area(regs, error_code, address, NULL, SEGV_MAPERR</span>
<span class="p_add">+#ifdef CONFIG_MEM_RANGE_LOCK</span>
<span class="p_add">+		   , range</span>
<span class="p_add">+#endif</span>
<span class="p_add">+		);</span>
 }
 
<span class="p_add">+#ifdef CONFIG_MEM_RANGE_LOCK</span>
<span class="p_add">+#define bad_area _bad_area</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define bad_area(r, e, a, _r) _bad_area(r, e, a)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 static inline bool bad_area_access_from_pkeys(unsigned long error_code,
 		struct vm_area_struct *vma)
 {
<span class="p_chunk">@@ -1000,7 +1018,11 @@</span> <span class="p_context"> static inline bool bad_area_access_from_pkeys(unsigned long error_code,</span>
 
 static noinline void
 bad_area_access_error(struct pt_regs *regs, unsigned long error_code,
<span class="p_del">-		      unsigned long address, struct vm_area_struct *vma)</span>
<span class="p_add">+		      unsigned long address, struct vm_area_struct *vma</span>
<span class="p_add">+#ifdef CONFIG_MEM_RANGE_LOCK</span>
<span class="p_add">+		      , struct range_lock *range</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	)</span>
 {
 	/*
 	 * This OSPKE check is not strictly necessary at runtime.
<span class="p_chunk">@@ -1008,9 +1030,17 @@</span> <span class="p_context"> bad_area_access_error(struct pt_regs *regs, unsigned long error_code,</span>
 	 * if pkeys are compiled out.
 	 */
 	if (bad_area_access_from_pkeys(error_code, vma))
<span class="p_del">-		__bad_area(regs, error_code, address, vma, SEGV_PKUERR);</span>
<span class="p_add">+		__bad_area(regs, error_code, address, vma, SEGV_PKUERR</span>
<span class="p_add">+#ifdef CONFIG_MEM_RANGE_LOCK</span>
<span class="p_add">+			   , range</span>
<span class="p_add">+#endif</span>
<span class="p_add">+			);</span>
 	else
<span class="p_del">-		__bad_area(regs, error_code, address, vma, SEGV_ACCERR);</span>
<span class="p_add">+		__bad_area(regs, error_code, address, vma, SEGV_ACCERR</span>
<span class="p_add">+#ifdef CONFIG_MEM_RANGE_LOCK</span>
<span class="p_add">+			   , range</span>
<span class="p_add">+#endif</span>
<span class="p_add">+			);</span>
 }
 
 static void
<span class="p_chunk">@@ -1268,6 +1298,7 @@</span> <span class="p_context"> __do_page_fault(struct pt_regs *regs, unsigned long error_code,</span>
 	struct mm_struct *mm;
 	int fault, major = 0;
 	unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;
<span class="p_add">+	mm_range_define(range);</span>
 
 	tsk = current;
 	mm = tsk-&gt;mm;
<span class="p_chunk">@@ -1381,14 +1412,14 @@</span> <span class="p_context"> __do_page_fault(struct pt_regs *regs, unsigned long error_code,</span>
 	 * validate the source. If this is invalid we can skip the address
 	 * space check, thus avoiding the deadlock:
 	 */
<span class="p_del">-	if (unlikely(!down_read_trylock(&amp;mm-&gt;mmap_sem))) {</span>
<span class="p_add">+	if (unlikely(!mm_read_trylock(mm, &amp;range))) {</span>
 		if ((error_code &amp; PF_USER) == 0 &amp;&amp;
 		    !search_exception_tables(regs-&gt;ip)) {
 			bad_area_nosemaphore(regs, error_code, address, NULL);
 			return;
 		}
 retry:
<span class="p_del">-		down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_lock(mm, &amp;range);</span>
 	} else {
 		/*
 		 * The above down_read_trylock() might have succeeded in
<span class="p_chunk">@@ -1400,13 +1431,13 @@</span> <span class="p_context"> __do_page_fault(struct pt_regs *regs, unsigned long error_code,</span>
 
 	vma = find_vma(mm, address);
 	if (unlikely(!vma)) {
<span class="p_del">-		bad_area(regs, error_code, address);</span>
<span class="p_add">+		bad_area(regs, error_code, address, &amp;range);</span>
 		return;
 	}
 	if (likely(vma-&gt;vm_start &lt;= address))
 		goto good_area;
 	if (unlikely(!(vma-&gt;vm_flags &amp; VM_GROWSDOWN))) {
<span class="p_del">-		bad_area(regs, error_code, address);</span>
<span class="p_add">+		bad_area(regs, error_code, address, &amp;range);</span>
 		return;
 	}
 	if (error_code &amp; PF_USER) {
<span class="p_chunk">@@ -1417,12 +1448,12 @@</span> <span class="p_context"> __do_page_fault(struct pt_regs *regs, unsigned long error_code,</span>
 		 * 32 pointers and then decrements %sp by 65535.)
 		 */
 		if (unlikely(address + 65536 + 32 * sizeof(unsigned long) &lt; regs-&gt;sp)) {
<span class="p_del">-			bad_area(regs, error_code, address);</span>
<span class="p_add">+			bad_area(regs, error_code, address, &amp;range);</span>
 			return;
 		}
 	}
 	if (unlikely(expand_stack(vma, address))) {
<span class="p_del">-		bad_area(regs, error_code, address);</span>
<span class="p_add">+		bad_area(regs, error_code, address, &amp;range);</span>
 		return;
 	}
 
<span class="p_chunk">@@ -1432,7 +1463,11 @@</span> <span class="p_context"> __do_page_fault(struct pt_regs *regs, unsigned long error_code,</span>
 	 */
 good_area:
 	if (unlikely(access_error(error_code, vma))) {
<span class="p_del">-		bad_area_access_error(regs, error_code, address, vma);</span>
<span class="p_add">+		bad_area_access_error(regs, error_code, address, vma</span>
<span class="p_add">+#ifdef CONFIG_MEM_RANGE_LOCK</span>
<span class="p_add">+				      , &amp;range</span>
<span class="p_add">+#endif</span>
<span class="p_add">+			);</span>
 		return;
 	}
 
<span class="p_chunk">@@ -1442,7 +1477,7 @@</span> <span class="p_context"> __do_page_fault(struct pt_regs *regs, unsigned long error_code,</span>
 	 * the fault.  Since we never set FAULT_FLAG_RETRY_NOWAIT, if
 	 * we get VM_FAULT_RETRY back, the mmap_sem has been unlocked.
 	 */
<span class="p_del">-	fault = handle_mm_fault(vma, address, flags, NULL);</span>
<span class="p_add">+	fault = handle_mm_fault(vma, address, flags, &amp;range);</span>
 	major |= fault &amp; VM_FAULT_MAJOR;
 
 	/*
<span class="p_chunk">@@ -1468,7 +1503,7 @@</span> <span class="p_context"> __do_page_fault(struct pt_regs *regs, unsigned long error_code,</span>
 		return;
 	}
 
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(mm, &amp;range);</span>
 	if (unlikely(fault &amp; VM_FAULT_ERROR)) {
 		mm_fault_error(regs, error_code, address, vma, fault);
 		return;
<span class="p_header">diff --git a/arch/x86/mm/mpx.c b/arch/x86/mm/mpx.c</span>
<span class="p_header">index 313e6fcb550e..0c16c4b37b29 100644</span>
<span class="p_header">--- a/arch/x86/mm/mpx.c</span>
<span class="p_header">+++ b/arch/x86/mm/mpx.c</span>
<span class="p_chunk">@@ -45,15 +45,16 @@</span> <span class="p_context"> static unsigned long mpx_mmap(unsigned long len)</span>
 {
 	struct mm_struct *mm = current-&gt;mm;
 	unsigned long addr, populate;
<span class="p_add">+	mm_range_define(range);</span>
 
 	/* Only bounds table can be allocated here */
 	if (len != mpx_bt_size_bytes(mm))
 		return -EINVAL;
 
<span class="p_del">-	down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_lock(mm, &amp;range);</span>
 	addr = do_mmap(NULL, 0, len, PROT_READ | PROT_WRITE,
 		       MAP_ANONYMOUS | MAP_PRIVATE, VM_MPX, 0, &amp;populate, NULL);
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(mm, &amp;range);</span>
 	if (populate)
 		mm_populate(addr, populate);
 
<span class="p_chunk">@@ -341,6 +342,7 @@</span> <span class="p_context"> int mpx_enable_management(void)</span>
 	void __user *bd_base = MPX_INVALID_BOUNDS_DIR;
 	struct mm_struct *mm = current-&gt;mm;
 	int ret = 0;
<span class="p_add">+	mm_range_define(range);</span>
 
 	/*
 	 * runtime in the userspace will be responsible for allocation of
<span class="p_chunk">@@ -354,25 +356,26 @@</span> <span class="p_context"> int mpx_enable_management(void)</span>
 	 * unmap path; we can just use mm-&gt;context.bd_addr instead.
 	 */
 	bd_base = mpx_get_bounds_dir();
<span class="p_del">-	down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_lock(mm, &amp;range);</span>
 	mm-&gt;context.bd_addr = bd_base;
 	if (mm-&gt;context.bd_addr == MPX_INVALID_BOUNDS_DIR)
 		ret = -ENXIO;
 
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(mm, &amp;range);</span>
 	return ret;
 }
 
 int mpx_disable_management(void)
 {
 	struct mm_struct *mm = current-&gt;mm;
<span class="p_add">+	mm_range_define(range);</span>
 
 	if (!cpu_feature_enabled(X86_FEATURE_MPX))
 		return -ENXIO;
 
<span class="p_del">-	down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_lock(mm, &amp;range);</span>
 	mm-&gt;context.bd_addr = MPX_INVALID_BOUNDS_DIR;
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(mm, &amp;range);</span>
 	return 0;
 }
 
<span class="p_header">diff --git a/drivers/android/binder.c b/drivers/android/binder.c</span>
<span class="p_header">index aae4d8d4be36..ebdd5864ae6e 100644</span>
<span class="p_header">--- a/drivers/android/binder.c</span>
<span class="p_header">+++ b/drivers/android/binder.c</span>
<span class="p_chunk">@@ -581,6 +581,7 @@</span> <span class="p_context"> static int binder_update_page_range(struct binder_proc *proc, int allocate,</span>
 	unsigned long user_page_addr;
 	struct page **page;
 	struct mm_struct *mm;
<span class="p_add">+	mm_range_define(range);</span>
 
 	binder_debug(BINDER_DEBUG_BUFFER_ALLOC,
 		     &quot;%d: %s pages %p-%p\n&quot;, proc-&gt;pid,
<span class="p_chunk">@@ -597,7 +598,7 @@</span> <span class="p_context"> static int binder_update_page_range(struct binder_proc *proc, int allocate,</span>
 		mm = get_task_mm(proc-&gt;tsk);
 
 	if (mm) {
<span class="p_del">-		down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_write_lock(mm, &amp;range);</span>
 		vma = proc-&gt;vma;
 		if (vma &amp;&amp; mm != proc-&gt;vma_vm_mm) {
 			pr_err(&quot;%d: vma mm and task mm mismatch\n&quot;,
<span class="p_chunk">@@ -647,7 +648,7 @@</span> <span class="p_context"> static int binder_update_page_range(struct binder_proc *proc, int allocate,</span>
 		/* vm_insert_page does not seem to increment the refcount */
 	}
 	if (mm) {
<span class="p_del">-		up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_write_unlock(mm, &amp;range);</span>
 		mmput(mm);
 	}
 	return 0;
<span class="p_chunk">@@ -669,7 +670,7 @@</span> <span class="p_context"> static int binder_update_page_range(struct binder_proc *proc, int allocate,</span>
 	}
 err_no_vma:
 	if (mm) {
<span class="p_del">-		up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_write_unlock(mm, &amp;range);</span>
 		mmput(mm);
 	}
 	return -ENOMEM;
<span class="p_header">diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c</span>
<span class="p_header">index 4e6b9501ab0a..3ddba04cedc4 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c</span>
<span class="p_chunk">@@ -521,6 +521,7 @@</span> <span class="p_context"> static int amdgpu_cs_parser_bos(struct amdgpu_cs_parser *p,</span>
 	bool need_mmap_lock = false;
 	unsigned i, tries = 10;
 	int r;
<span class="p_add">+	mm_range_define(range);</span>
 
 	INIT_LIST_HEAD(&amp;p-&gt;validated);
 
<span class="p_chunk">@@ -538,7 +539,7 @@</span> <span class="p_context"> static int amdgpu_cs_parser_bos(struct amdgpu_cs_parser *p,</span>
 		list_add(&amp;p-&gt;uf_entry.tv.head, &amp;p-&gt;validated);
 
 	if (need_mmap_lock)
<span class="p_del">-		down_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_lock(current-&gt;mm, &amp;range);</span>
 
 	while (1) {
 		struct list_head need_pages;
<span class="p_chunk">@@ -695,7 +696,7 @@</span> <span class="p_context"> static int amdgpu_cs_parser_bos(struct amdgpu_cs_parser *p,</span>
 error_free_pages:
 
 	if (need_mmap_lock)
<span class="p_del">-		up_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_unlock(current-&gt;mm, &amp;range);</span>
 
 	if (p-&gt;bo_list) {
 		for (i = p-&gt;bo_list-&gt;first_userptr;
<span class="p_header">diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c</span>
<span class="p_header">index 94cb91cf93eb..712f26f3a7fc 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c</span>
<span class="p_chunk">@@ -312,6 +312,7 @@</span> <span class="p_context"> int amdgpu_gem_userptr_ioctl(struct drm_device *dev, void *data,</span>
 	struct amdgpu_bo *bo;
 	uint32_t handle;
 	int r;
<span class="p_add">+	mm_range_define(range);</span>
 
 	if (offset_in_page(args-&gt;addr | args-&gt;size))
 		return -EINVAL;
<span class="p_chunk">@@ -350,7 +351,7 @@</span> <span class="p_context"> int amdgpu_gem_userptr_ioctl(struct drm_device *dev, void *data,</span>
 	}
 
 	if (args-&gt;flags &amp; AMDGPU_GEM_USERPTR_VALIDATE) {
<span class="p_del">-		down_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_lock(current-&gt;mm, &amp;range);</span>
 
 		r = amdgpu_ttm_tt_get_user_pages(bo-&gt;tbo.ttm,
 						 bo-&gt;tbo.ttm-&gt;pages);
<span class="p_chunk">@@ -367,7 +368,7 @@</span> <span class="p_context"> int amdgpu_gem_userptr_ioctl(struct drm_device *dev, void *data,</span>
 		if (r)
 			goto free_pages;
 
<span class="p_del">-		up_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_unlock(current-&gt;mm, &amp;range);</span>
 	}
 
 	r = drm_gem_handle_create(filp, gobj, &amp;handle);
<span class="p_chunk">@@ -383,7 +384,7 @@</span> <span class="p_context"> int amdgpu_gem_userptr_ioctl(struct drm_device *dev, void *data,</span>
 	release_pages(bo-&gt;tbo.ttm-&gt;pages, bo-&gt;tbo.ttm-&gt;num_pages, false);
 
 unlock_mmap_sem:
<span class="p_del">-	up_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(current-&gt;mm, &amp;range);</span>
 
 release_object:
 	drm_gem_object_unreference_unlocked(gobj);
<span class="p_header">diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_mn.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_mn.c</span>
<span class="p_header">index 38f739fb727b..8787a750fbae 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_mn.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_mn.c</span>
<span class="p_chunk">@@ -231,9 +231,10 @@</span> <span class="p_context"> static struct amdgpu_mn *amdgpu_mn_get(struct amdgpu_device *adev)</span>
 	struct mm_struct *mm = current-&gt;mm;
 	struct amdgpu_mn *rmn;
 	int r;
<span class="p_add">+	mm_range_define(range);</span>
 
 	mutex_lock(&amp;adev-&gt;mn_lock);
<span class="p_del">-	if (down_write_killable(&amp;mm-&gt;mmap_sem)) {</span>
<span class="p_add">+	if (mm_write_lock_killable(mm, &amp;range)) {</span>
 		mutex_unlock(&amp;adev-&gt;mn_lock);
 		return ERR_PTR(-EINTR);
 	}
<span class="p_chunk">@@ -261,13 +262,13 @@</span> <span class="p_context"> static struct amdgpu_mn *amdgpu_mn_get(struct amdgpu_device *adev)</span>
 	hash_add(adev-&gt;mn_hash, &amp;rmn-&gt;node, (unsigned long)mm);
 
 release_locks:
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(mm, &amp;range);</span>
 	mutex_unlock(&amp;adev-&gt;mn_lock);
 
 	return rmn;
 
 free_rmn:
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(mm, &amp;range);</span>
 	mutex_unlock(&amp;adev-&gt;mn_lock);
 	kfree(rmn);
 
<span class="p_header">diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_events.c b/drivers/gpu/drm/amd/amdkfd/kfd_events.c</span>
<span class="p_header">index d1ce83d73a87..92ae40f3433f 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/amd/amdkfd/kfd_events.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/amd/amdkfd/kfd_events.c</span>
<span class="p_chunk">@@ -897,6 +897,7 @@</span> <span class="p_context"> void kfd_signal_iommu_event(struct kfd_dev *dev, unsigned int pasid,</span>
 {
 	struct kfd_hsa_memory_exception_data memory_exception_data;
 	struct vm_area_struct *vma;
<span class="p_add">+	mm_range_define(range);</span>
 
 	/*
 	 * Because we are called from arbitrary context (workqueue) as opposed
<span class="p_chunk">@@ -910,7 +911,7 @@</span> <span class="p_context"> void kfd_signal_iommu_event(struct kfd_dev *dev, unsigned int pasid,</span>
 
 	memset(&amp;memory_exception_data, 0, sizeof(memory_exception_data));
 
<span class="p_del">-	down_read(&amp;p-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_lock(p-&gt;mm-&gt;mmap_sem, &amp;range);</span>
 	vma = find_vma(p-&gt;mm, address);
 
 	memory_exception_data.gpu_id = dev-&gt;id;
<span class="p_chunk">@@ -937,7 +938,7 @@</span> <span class="p_context"> void kfd_signal_iommu_event(struct kfd_dev *dev, unsigned int pasid,</span>
 		}
 	}
 
<span class="p_del">-	up_read(&amp;p-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(p-&gt;mm-&gt;mmap_sem, &amp;range);</span>
 
 	mutex_lock(&amp;p-&gt;event_mutex);
 
<span class="p_header">diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_process.c b/drivers/gpu/drm/amd/amdkfd/kfd_process.c</span>
<span class="p_header">index 84d1ffd1eef9..f421eaead2e6 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/amd/amdkfd/kfd_process.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/amd/amdkfd/kfd_process.c</span>
<span class="p_chunk">@@ -78,6 +78,7 @@</span> <span class="p_context"> void kfd_process_destroy_wq(void)</span>
 struct kfd_process *kfd_create_process(const struct task_struct *thread)
 {
 	struct kfd_process *process;
<span class="p_add">+	mm_range_define(range);</span>
 
 	BUG_ON(!kfd_process_wq);
 
<span class="p_chunk">@@ -89,7 +90,7 @@</span> <span class="p_context"> struct kfd_process *kfd_create_process(const struct task_struct *thread)</span>
 		return ERR_PTR(-EINVAL);
 
 	/* Take mmap_sem because we call __mmu_notifier_register inside */
<span class="p_del">-	down_write(&amp;thread-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_lock(thread-&gt;mm-&gt;mmap_sem, &amp;range);</span>
 
 	/*
 	 * take kfd processes mutex before starting of process creation
<span class="p_chunk">@@ -108,7 +109,7 @@</span> <span class="p_context"> struct kfd_process *kfd_create_process(const struct task_struct *thread)</span>
 
 	mutex_unlock(&amp;kfd_processes_mutex);
 
<span class="p_del">-	up_write(&amp;thread-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(thread-&gt;mm-&gt;mmap_sem, &amp;range);</span>
 
 	return process;
 }
<span class="p_header">diff --git a/drivers/gpu/drm/etnaviv/etnaviv_gem.c b/drivers/gpu/drm/etnaviv/etnaviv_gem.c</span>
<span class="p_header">index 75ca18aaa34e..40d1ce202cf9 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/etnaviv/etnaviv_gem.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/etnaviv/etnaviv_gem.c</span>
<span class="p_chunk">@@ -747,6 +747,7 @@</span> <span class="p_context"> static struct page **etnaviv_gem_userptr_do_get_pages(</span>
 	struct page **pvec;
 	uintptr_t ptr;
 	unsigned int flags = 0;
<span class="p_add">+	mm_range_define(range);</span>
 
 	pvec = drm_malloc_ab(npages, sizeof(struct page *));
 	if (!pvec)
<span class="p_chunk">@@ -758,7 +759,7 @@</span> <span class="p_context"> static struct page **etnaviv_gem_userptr_do_get_pages(</span>
 	pinned = 0;
 	ptr = etnaviv_obj-&gt;userptr.ptr;
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_lock(mm, &amp;range);</span>
 	while (pinned &lt; npages) {
 		ret = get_user_pages_remote(task, mm, ptr, npages - pinned,
 					    flags, pvec + pinned, NULL, NULL,
<span class="p_chunk">@@ -769,7 +770,7 @@</span> <span class="p_context"> static struct page **etnaviv_gem_userptr_do_get_pages(</span>
 		ptr += ret * PAGE_SIZE;
 		pinned += ret;
 	}
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(mm, &amp;range);</span>
 
 	if (ret &lt; 0) {
 		release_pages(pvec, pinned, 0);
<span class="p_header">diff --git a/drivers/gpu/drm/i915/i915_gem.c b/drivers/gpu/drm/i915/i915_gem.c</span>
<span class="p_header">index b6ac3df18b58..b5f63bacdaa6 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/i915/i915_gem.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/i915/i915_gem.c</span>
<span class="p_chunk">@@ -1687,8 +1687,9 @@</span> <span class="p_context"> i915_gem_mmap_ioctl(struct drm_device *dev, void *data,</span>
 	if (args-&gt;flags &amp; I915_MMAP_WC) {
 		struct mm_struct *mm = current-&gt;mm;
 		struct vm_area_struct *vma;
<span class="p_add">+		mm_range_define(range);</span>
 
<span class="p_del">-		if (down_write_killable(&amp;mm-&gt;mmap_sem)) {</span>
<span class="p_add">+		if (mm_write_lock_killable(mm, &amp;range)) {</span>
 			i915_gem_object_put(obj);
 			return -EINTR;
 		}
<span class="p_chunk">@@ -1698,7 +1699,7 @@</span> <span class="p_context"> i915_gem_mmap_ioctl(struct drm_device *dev, void *data,</span>
 				pgprot_writecombine(vm_get_page_prot(vma-&gt;vm_flags));
 		else
 			addr = -ENOMEM;
<span class="p_del">-		up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_write_unlock(mm, &amp;range);</span>
 
 		/* This may race, but that&#39;s ok, it only gets set */
 		WRITE_ONCE(obj-&gt;frontbuffer_ggtt_origin, ORIGIN_CPU);
<span class="p_header">diff --git a/drivers/gpu/drm/i915/i915_gem_userptr.c b/drivers/gpu/drm/i915/i915_gem_userptr.c</span>
<span class="p_header">index 491bb58cab09..2e852f987382 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/i915/i915_gem_userptr.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/i915/i915_gem_userptr.c</span>
<span class="p_chunk">@@ -211,12 +211,13 @@</span> <span class="p_context"> static struct i915_mmu_notifier *</span>
 i915_mmu_notifier_find(struct i915_mm_struct *mm)
 {
 	struct i915_mmu_notifier *mn = mm-&gt;mn;
<span class="p_add">+	mm_range_define(range);</span>
 
 	mn = mm-&gt;mn;
 	if (mn)
 		return mn;
 
<span class="p_del">-	down_write(&amp;mm-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_lock(mm-&gt;mm, &amp;range);</span>
 	mutex_lock(&amp;mm-&gt;i915-&gt;mm_lock);
 	if ((mn = mm-&gt;mn) == NULL) {
 		mn = i915_mmu_notifier_create(mm-&gt;mm);
<span class="p_chunk">@@ -224,7 +225,7 @@</span> <span class="p_context"> i915_mmu_notifier_find(struct i915_mm_struct *mm)</span>
 			mm-&gt;mn = mn;
 	}
 	mutex_unlock(&amp;mm-&gt;i915-&gt;mm_lock);
<span class="p_del">-	up_write(&amp;mm-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(mm-&gt;mm, &amp;range);</span>
 
 	return mn;
 }
<span class="p_chunk">@@ -511,13 +512,14 @@</span> <span class="p_context"> __i915_gem_userptr_get_pages_worker(struct work_struct *_work)</span>
 	if (pvec != NULL) {
 		struct mm_struct *mm = obj-&gt;userptr.mm-&gt;mm;
 		unsigned int flags = 0;
<span class="p_add">+		mm_range_define(range);</span>
 
 		if (!obj-&gt;userptr.read_only)
 			flags |= FOLL_WRITE;
 
 		ret = -EFAULT;
 		if (mmget_not_zero(mm)) {
<span class="p_del">-			down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+			mm_read_lock(mm, &amp;range);</span>
 			while (pinned &lt; npages) {
 				ret = get_user_pages_remote
 					(work-&gt;task, mm,
<span class="p_chunk">@@ -530,7 +532,7 @@</span> <span class="p_context"> __i915_gem_userptr_get_pages_worker(struct work_struct *_work)</span>
 
 				pinned += ret;
 			}
<span class="p_del">-			up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+			mm_read_unlock(mm, &amp;range);</span>
 			mmput(mm);
 		}
 	}
<span class="p_header">diff --git a/drivers/gpu/drm/radeon/radeon_cs.c b/drivers/gpu/drm/radeon/radeon_cs.c</span>
<span class="p_header">index 3ac671f6c8e1..d720ee7239bd 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/radeon/radeon_cs.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/radeon/radeon_cs.c</span>
<span class="p_chunk">@@ -79,6 +79,7 @@</span> <span class="p_context"> static int radeon_cs_parser_relocs(struct radeon_cs_parser *p)</span>
 	unsigned i;
 	bool need_mmap_lock = false;
 	int r;
<span class="p_add">+	mm_range_define(range);</span>
 
 	if (p-&gt;chunk_relocs == NULL) {
 		return 0;
<span class="p_chunk">@@ -189,12 +190,12 @@</span> <span class="p_context"> static int radeon_cs_parser_relocs(struct radeon_cs_parser *p)</span>
 		p-&gt;vm_bos = radeon_vm_get_bos(p-&gt;rdev, p-&gt;ib.vm,
 					      &amp;p-&gt;validated);
 	if (need_mmap_lock)
<span class="p_del">-		down_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_lock(current-&gt;mm, &amp;range);</span>
 
 	r = radeon_bo_list_validate(p-&gt;rdev, &amp;p-&gt;ticket, &amp;p-&gt;validated, p-&gt;ring);
 
 	if (need_mmap_lock)
<span class="p_del">-		up_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_unlock(current-&gt;mm, &amp;range);</span>
 
 	return r;
 }
<span class="p_header">diff --git a/drivers/gpu/drm/radeon/radeon_gem.c b/drivers/gpu/drm/radeon/radeon_gem.c</span>
<span class="p_header">index dddb372de2b9..38864c2c32de 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/radeon/radeon_gem.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/radeon/radeon_gem.c</span>
<span class="p_chunk">@@ -335,17 +335,19 @@</span> <span class="p_context"> int radeon_gem_userptr_ioctl(struct drm_device *dev, void *data,</span>
 	}
 
 	if (args-&gt;flags &amp; RADEON_GEM_USERPTR_VALIDATE) {
<span class="p_del">-		down_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_range_define(range);</span>
<span class="p_add">+</span>
<span class="p_add">+		mm_read_lock(current-&gt;mm, &amp;range);</span>
 		r = radeon_bo_reserve(bo, true);
 		if (r) {
<span class="p_del">-			up_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+			mm_read_unlock(current-&gt;mm, &amp;range);</span>
 			goto release_object;
 		}
 
 		radeon_ttm_placement_from_domain(bo, RADEON_GEM_DOMAIN_GTT);
 		r = ttm_bo_validate(&amp;bo-&gt;tbo, &amp;bo-&gt;placement, true, false);
 		radeon_bo_unreserve(bo);
<span class="p_del">-		up_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_unlock(current-&gt;mm, &amp;range);</span>
 		if (r)
 			goto release_object;
 	}
<span class="p_header">diff --git a/drivers/gpu/drm/radeon/radeon_mn.c b/drivers/gpu/drm/radeon/radeon_mn.c</span>
<span class="p_header">index 896f2cf51e4e..f40703772c53 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/radeon/radeon_mn.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/radeon/radeon_mn.c</span>
<span class="p_chunk">@@ -185,8 +185,9 @@</span> <span class="p_context"> static struct radeon_mn *radeon_mn_get(struct radeon_device *rdev)</span>
 	struct mm_struct *mm = current-&gt;mm;
 	struct radeon_mn *rmn;
 	int r;
<span class="p_add">+	mm_range_define(range);</span>
 
<span class="p_del">-	if (down_write_killable(&amp;mm-&gt;mmap_sem))</span>
<span class="p_add">+	if (mm_write_lock_killable(mm, &amp;range))</span>
 		return ERR_PTR(-EINTR);
 
 	mutex_lock(&amp;rdev-&gt;mn_lock);
<span class="p_chunk">@@ -215,13 +216,13 @@</span> <span class="p_context"> static struct radeon_mn *radeon_mn_get(struct radeon_device *rdev)</span>
 
 release_locks:
 	mutex_unlock(&amp;rdev-&gt;mn_lock);
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(mm, &amp;range);</span>
 
 	return rmn;
 
 free_rmn:
 	mutex_unlock(&amp;rdev-&gt;mn_lock);
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(mm, &amp;range);</span>
 	kfree(rmn);
 
 	return ERR_PTR(r);
<span class="p_header">diff --git a/drivers/gpu/drm/ttm/ttm_bo_vm.c b/drivers/gpu/drm/ttm/ttm_bo_vm.c</span>
<span class="p_header">index 9f53df95f35c..5355b17ea8fa 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/ttm/ttm_bo_vm.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/ttm/ttm_bo_vm.c</span>
<span class="p_chunk">@@ -66,7 +66,7 @@</span> <span class="p_context"> static int ttm_bo_vm_fault_idle(struct ttm_buffer_object *bo,</span>
 			goto out_unlock;
 
 		ttm_bo_reference(bo);
<span class="p_del">-		up_read(&amp;vmf-&gt;vma-&gt;vm_mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_unlock(vmf-&gt;vma-&gt;vm_mm, vmf-&gt;lockrange);</span>
 		(void) dma_fence_wait(bo-&gt;moving, true);
 		ttm_bo_unreserve(bo);
 		ttm_bo_unref(&amp;bo);
<span class="p_chunk">@@ -124,7 +124,7 @@</span> <span class="p_context"> static int ttm_bo_vm_fault(struct vm_fault *vmf)</span>
 		if (vmf-&gt;flags &amp; FAULT_FLAG_ALLOW_RETRY) {
 			if (!(vmf-&gt;flags &amp; FAULT_FLAG_RETRY_NOWAIT)) {
 				ttm_bo_reference(bo);
<span class="p_del">-				up_read(&amp;vmf-&gt;vma-&gt;vm_mm-&gt;mmap_sem);</span>
<span class="p_add">+				mm_read_unlock(vmf-&gt;vma-&gt;vm_mm, vmf-&gt;lockrange);</span>
 				(void) ttm_bo_wait_unreserved(bo);
 				ttm_bo_unref(&amp;bo);
 			}
<span class="p_header">diff --git a/drivers/infiniband/core/umem.c b/drivers/infiniband/core/umem.c</span>
<span class="p_header">index 73749d6d18f1..9fe753b5cc32 100644</span>
<span class="p_header">--- a/drivers/infiniband/core/umem.c</span>
<span class="p_header">+++ b/drivers/infiniband/core/umem.c</span>
<span class="p_chunk">@@ -96,6 +96,7 @@</span> <span class="p_context"> struct ib_umem *ib_umem_get(struct ib_ucontext *context, unsigned long addr,</span>
 	struct scatterlist *sg, *sg_list_start;
 	int need_release = 0;
 	unsigned int gup_flags = FOLL_WRITE;
<span class="p_add">+	mm_range_define(range);</span>
 
 	if (dmasync)
 		dma_attrs |= DMA_ATTR_WRITE_BARRIER;
<span class="p_chunk">@@ -163,7 +164,7 @@</span> <span class="p_context"> struct ib_umem *ib_umem_get(struct ib_ucontext *context, unsigned long addr,</span>
 
 	npages = ib_umem_num_pages(umem);
 
<span class="p_del">-	down_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_lock(current-&gt;mm, &amp;range);</span>
 
 	locked     = npages + current-&gt;mm-&gt;pinned_vm;
 	lock_limit = rlimit(RLIMIT_MEMLOCK) &gt;&gt; PAGE_SHIFT;
<span class="p_chunk">@@ -236,7 +237,7 @@</span> <span class="p_context"> struct ib_umem *ib_umem_get(struct ib_ucontext *context, unsigned long addr,</span>
 	} else
 		current-&gt;mm-&gt;pinned_vm = locked;
 
<span class="p_del">-	up_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(current-&gt;mm, &amp;range);</span>
 	if (vma_list)
 		free_page((unsigned long) vma_list);
 	free_page((unsigned long) page_list);
<span class="p_chunk">@@ -248,10 +249,11 @@</span> <span class="p_context"> EXPORT_SYMBOL(ib_umem_get);</span>
 static void ib_umem_account(struct work_struct *work)
 {
 	struct ib_umem *umem = container_of(work, struct ib_umem, work);
<span class="p_add">+	mm_range_define(range);</span>
 
<span class="p_del">-	down_write(&amp;umem-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_lock(umem-&gt;mm, &amp;range);</span>
 	umem-&gt;mm-&gt;pinned_vm -= umem-&gt;diff;
<span class="p_del">-	up_write(&amp;umem-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(umem-&gt;mm, &amp;range);</span>
 	mmput(umem-&gt;mm);
 	kfree(umem);
 }
<span class="p_chunk">@@ -266,6 +268,7 @@</span> <span class="p_context"> void ib_umem_release(struct ib_umem *umem)</span>
 	struct mm_struct *mm;
 	struct task_struct *task;
 	unsigned long diff;
<span class="p_add">+	mm_range_define(range);</span>
 
 	if (umem-&gt;odp_data) {
 		ib_umem_odp_release(umem);
<span class="p_chunk">@@ -294,7 +297,7 @@</span> <span class="p_context"> void ib_umem_release(struct ib_umem *umem)</span>
 	 * we defer the vm_locked accounting to the system workqueue.
 	 */
 	if (context-&gt;closing) {
<span class="p_del">-		if (!down_write_trylock(&amp;mm-&gt;mmap_sem)) {</span>
<span class="p_add">+		if (!mm_write_trylock(mm, &amp;range)) {</span>
 			INIT_WORK(&amp;umem-&gt;work, ib_umem_account);
 			umem-&gt;mm   = mm;
 			umem-&gt;diff = diff;
<span class="p_chunk">@@ -303,10 +306,10 @@</span> <span class="p_context"> void ib_umem_release(struct ib_umem *umem)</span>
 			return;
 		}
 	} else
<span class="p_del">-		down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_write_lock(mm, &amp;range);</span>
 
 	mm-&gt;pinned_vm -= diff;
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(mm, &amp;range);</span>
 	mmput(mm);
 out:
 	kfree(umem);
<span class="p_header">diff --git a/drivers/infiniband/core/umem_odp.c b/drivers/infiniband/core/umem_odp.c</span>
<span class="p_header">index 6e1e574db5d3..1dec59a4f070 100644</span>
<span class="p_header">--- a/drivers/infiniband/core/umem_odp.c</span>
<span class="p_header">+++ b/drivers/infiniband/core/umem_odp.c</span>
<span class="p_chunk">@@ -654,8 +654,9 @@</span> <span class="p_context"> int ib_umem_odp_map_dma_pages(struct ib_umem *umem, u64 user_virt, u64 bcnt,</span>
 		const size_t gup_num_pages = min_t(size_t,
 				(bcnt + BIT(page_shift) - 1) &gt;&gt; page_shift,
 				PAGE_SIZE / sizeof(struct page *));
<span class="p_add">+		mm_range_define(range);</span>
 
<span class="p_del">-		down_read(&amp;owning_mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_lock(owning_mm, &amp;range);</span>
 		/*
 		 * Note: this might result in redundent page getting. We can
 		 * avoid this by checking dma_list to be 0 before calling
<span class="p_chunk">@@ -666,7 +667,7 @@</span> <span class="p_context"> int ib_umem_odp_map_dma_pages(struct ib_umem *umem, u64 user_virt, u64 bcnt,</span>
 		npages = get_user_pages_remote(owning_process, owning_mm,
 				user_virt, gup_num_pages,
 				flags, local_page_list, NULL, NULL, NULL);
<span class="p_del">-		up_read(&amp;owning_mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_unlock(owning_mm, &amp;range);</span>
 
 		if (npages &lt; 0)
 			break;
<span class="p_header">diff --git a/drivers/infiniband/hw/hfi1/user_pages.c b/drivers/infiniband/hw/hfi1/user_pages.c</span>
<span class="p_header">index e341e6dcc388..7f359e6fd23d 100644</span>
<span class="p_header">--- a/drivers/infiniband/hw/hfi1/user_pages.c</span>
<span class="p_header">+++ b/drivers/infiniband/hw/hfi1/user_pages.c</span>
<span class="p_chunk">@@ -76,6 +76,7 @@</span> <span class="p_context"> bool hfi1_can_pin_pages(struct hfi1_devdata *dd, struct mm_struct *mm,</span>
 	unsigned int usr_ctxts =
 			dd-&gt;num_rcv_contexts - dd-&gt;first_dyn_alloc_ctxt;
 	bool can_lock = capable(CAP_IPC_LOCK);
<span class="p_add">+	mm_range_define(range);</span>
 
 	/*
 	 * Calculate per-cache size. The calculation below uses only a quarter
<span class="p_chunk">@@ -91,9 +92,9 @@</span> <span class="p_context"> bool hfi1_can_pin_pages(struct hfi1_devdata *dd, struct mm_struct *mm,</span>
 	/* Convert to number of pages */
 	size = DIV_ROUND_UP(size, PAGE_SIZE);
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_lock(mm, &amp;range);</span>
 	pinned = mm-&gt;pinned_vm;
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(mm, &amp;range);</span>
 
 	/* First, check the absolute limit against all pinned pages. */
 	if (pinned + npages &gt;= ulimit &amp;&amp; !can_lock)
<span class="p_chunk">@@ -106,14 +107,15 @@</span> <span class="p_context"> int hfi1_acquire_user_pages(struct mm_struct *mm, unsigned long vaddr, size_t np</span>
 			    bool writable, struct page **pages)
 {
 	int ret;
<span class="p_add">+	mm_range_define(range);</span>
 
 	ret = get_user_pages_fast(vaddr, npages, writable, pages);
 	if (ret &lt; 0)
 		return ret;
 
<span class="p_del">-	down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_lock(mm, &amp;range);</span>
 	mm-&gt;pinned_vm += ret;
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(mm, &amp;range);</span>
 
 	return ret;
 }
<span class="p_chunk">@@ -130,8 +132,10 @@</span> <span class="p_context"> void hfi1_release_user_pages(struct mm_struct *mm, struct page **p,</span>
 	}
 
 	if (mm) { /* during close after signal, mm can be NULL */
<span class="p_del">-		down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_range_define(range);</span>
<span class="p_add">+</span>
<span class="p_add">+		mm_write_lock(mm, &amp;range);</span>
 		mm-&gt;pinned_vm -= npages;
<span class="p_del">-		up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_write_unlock(mm, &amp;range);</span>
 	}
 }
<span class="p_header">diff --git a/drivers/infiniband/hw/mlx4/main.c b/drivers/infiniband/hw/mlx4/main.c</span>
<span class="p_header">index 521d0def2d9e..2cb24b8c43e5 100644</span>
<span class="p_header">--- a/drivers/infiniband/hw/mlx4/main.c</span>
<span class="p_header">+++ b/drivers/infiniband/hw/mlx4/main.c</span>
<span class="p_chunk">@@ -1142,6 +1142,7 @@</span> <span class="p_context"> static void mlx4_ib_disassociate_ucontext(struct ib_ucontext *ibcontext)</span>
 	struct mlx4_ib_ucontext *context = to_mucontext(ibcontext);
 	struct task_struct *owning_process  = NULL;
 	struct mm_struct   *owning_mm       = NULL;
<span class="p_add">+	mm_range_define(range);</span>
 
 	owning_process = get_pid_task(ibcontext-&gt;tgid, PIDTYPE_PID);
 	if (!owning_process)
<span class="p_chunk">@@ -1173,7 +1174,7 @@</span> <span class="p_context"> static void mlx4_ib_disassociate_ucontext(struct ib_ucontext *ibcontext)</span>
 	/* need to protect from a race on closing the vma as part of
 	 * mlx4_ib_vma_close().
 	 */
<span class="p_del">-	down_write(&amp;owning_mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_lock(owning_mm, &amp;range);</span>
 	for (i = 0; i &lt; HW_BAR_COUNT; i++) {
 		vma = context-&gt;hw_bar_info[i].vma;
 		if (!vma)
<span class="p_chunk">@@ -1193,7 +1194,7 @@</span> <span class="p_context"> static void mlx4_ib_disassociate_ucontext(struct ib_ucontext *ibcontext)</span>
 		context-&gt;hw_bar_info[i].vma-&gt;vm_ops = NULL;
 	}
 
<span class="p_del">-	up_write(&amp;owning_mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(owning_mm, &amp;range);</span>
 	mmput(owning_mm);
 	put_task_struct(owning_process);
 }
<span class="p_header">diff --git a/drivers/infiniband/hw/mlx5/main.c b/drivers/infiniband/hw/mlx5/main.c</span>
<span class="p_header">index d45772da0963..417603dfd044 100644</span>
<span class="p_header">--- a/drivers/infiniband/hw/mlx5/main.c</span>
<span class="p_header">+++ b/drivers/infiniband/hw/mlx5/main.c</span>
<span class="p_chunk">@@ -1513,6 +1513,7 @@</span> <span class="p_context"> static void mlx5_ib_disassociate_ucontext(struct ib_ucontext *ibcontext)</span>
 	struct mlx5_ib_ucontext *context = to_mucontext(ibcontext);
 	struct task_struct *owning_process  = NULL;
 	struct mm_struct   *owning_mm       = NULL;
<span class="p_add">+	mm_range_define(range);</span>
 
 	owning_process = get_pid_task(ibcontext-&gt;tgid, PIDTYPE_PID);
 	if (!owning_process)
<span class="p_chunk">@@ -1542,7 +1543,7 @@</span> <span class="p_context"> static void mlx5_ib_disassociate_ucontext(struct ib_ucontext *ibcontext)</span>
 	/* need to protect from a race on closing the vma as part of
 	 * mlx5_ib_vma_close.
 	 */
<span class="p_del">-	down_write(&amp;owning_mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_lock(owning_mm-&gt;mmap_sem, &amp;range);</span>
 	list_for_each_entry_safe(vma_private, n, &amp;context-&gt;vma_private_list,
 				 list) {
 		vma = vma_private-&gt;vma;
<span class="p_chunk">@@ -1557,7 +1558,7 @@</span> <span class="p_context"> static void mlx5_ib_disassociate_ucontext(struct ib_ucontext *ibcontext)</span>
 		list_del(&amp;vma_private-&gt;list);
 		kfree(vma_private);
 	}
<span class="p_del">-	up_write(&amp;owning_mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(owning_mm-&gt;mmap_sem, &amp;range);</span>
 	mmput(owning_mm);
 	put_task_struct(owning_process);
 }
<span class="p_header">diff --git a/drivers/infiniband/hw/qib/qib_user_pages.c b/drivers/infiniband/hw/qib/qib_user_pages.c</span>
<span class="p_header">index c1cf13f2722a..6bcd396a09b1 100644</span>
<span class="p_header">--- a/drivers/infiniband/hw/qib/qib_user_pages.c</span>
<span class="p_header">+++ b/drivers/infiniband/hw/qib/qib_user_pages.c</span>
<span class="p_chunk">@@ -134,25 +134,28 @@</span> <span class="p_context"> int qib_get_user_pages(unsigned long start_page, size_t num_pages,</span>
 		       struct page **p)
 {
 	int ret;
<span class="p_add">+	mm_range_define(range);</span>
 
<span class="p_del">-	down_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_lock(current-&gt;mm, &amp;range);</span>
 
 	ret = __qib_get_user_pages(start_page, num_pages, p);
 
<span class="p_del">-	up_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(current-&gt;mm, &amp;range);</span>
 
 	return ret;
 }
 
 void qib_release_user_pages(struct page **p, size_t num_pages)
 {
<span class="p_add">+	mm_range_define(range);</span>
<span class="p_add">+</span>
 	if (current-&gt;mm) /* during close after signal, mm can be NULL */
<span class="p_del">-		down_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_write_lock(current-&gt;mm, &amp;range);</span>
 
 	__qib_release_user_pages(p, num_pages, 1);
 
 	if (current-&gt;mm) {
 		current-&gt;mm-&gt;pinned_vm -= num_pages;
<span class="p_del">-		up_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_write_unlock(current-&gt;mm, &amp;range);</span>
 	}
 }
<span class="p_header">diff --git a/drivers/infiniband/hw/usnic/usnic_uiom.c b/drivers/infiniband/hw/usnic/usnic_uiom.c</span>
<span class="p_header">index 1591d0e78bfa..62244bed96db 100644</span>
<span class="p_header">--- a/drivers/infiniband/hw/usnic/usnic_uiom.c</span>
<span class="p_header">+++ b/drivers/infiniband/hw/usnic/usnic_uiom.c</span>
<span class="p_chunk">@@ -57,10 +57,11 @@</span> <span class="p_context"> static void usnic_uiom_reg_account(struct work_struct *work)</span>
 {
 	struct usnic_uiom_reg *umem = container_of(work,
 						struct usnic_uiom_reg, work);
<span class="p_add">+	mm_range_define(range);</span>
 
<span class="p_del">-	down_write(&amp;umem-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_lock(umem-&gt;mm-&gt;mmap_sem, &amp;range);</span>
 	umem-&gt;mm-&gt;locked_vm -= umem-&gt;diff;
<span class="p_del">-	up_write(&amp;umem-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(umem-&gt;mm-&gt;mmap_sem, &amp;range);</span>
 	mmput(umem-&gt;mm);
 	kfree(umem);
 }
<span class="p_chunk">@@ -113,6 +114,7 @@</span> <span class="p_context"> static int usnic_uiom_get_pages(unsigned long addr, size_t size, int writable,</span>
 	int flags;
 	dma_addr_t pa;
 	unsigned int gup_flags;
<span class="p_add">+	mm_range_define(range);</span>
 
 	if (!can_do_mlock())
 		return -EPERM;
<span class="p_chunk">@@ -125,7 +127,7 @@</span> <span class="p_context"> static int usnic_uiom_get_pages(unsigned long addr, size_t size, int writable,</span>
 
 	npages = PAGE_ALIGN(size + (addr &amp; ~PAGE_MASK)) &gt;&gt; PAGE_SHIFT;
 
<span class="p_del">-	down_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_lock(current-&gt;mm, &amp;range);</span>
 
 	locked = npages + current-&gt;mm-&gt;locked_vm;
 	lock_limit = rlimit(RLIMIT_MEMLOCK) &gt;&gt; PAGE_SHIFT;
<span class="p_chunk">@@ -188,7 +190,7 @@</span> <span class="p_context"> static int usnic_uiom_get_pages(unsigned long addr, size_t size, int writable,</span>
 	else
 		current-&gt;mm-&gt;locked_vm = locked;
 
<span class="p_del">-	up_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(current-&gt;mm, &amp;range);</span>
 	free_page((unsigned long) page_list);
 	return ret;
 }
<span class="p_chunk">@@ -424,6 +426,7 @@</span> <span class="p_context"> void usnic_uiom_reg_release(struct usnic_uiom_reg *uiomr, int closing)</span>
 {
 	struct mm_struct *mm;
 	unsigned long diff;
<span class="p_add">+	mm_range_define(range);</span>
 
 	__usnic_uiom_reg_release(uiomr-&gt;pd, uiomr, 1);
 
<span class="p_chunk">@@ -444,7 +447,7 @@</span> <span class="p_context"> void usnic_uiom_reg_release(struct usnic_uiom_reg *uiomr, int closing)</span>
 	 * we defer the vm_locked accounting to the system workqueue.
 	 */
 	if (closing) {
<span class="p_del">-		if (!down_write_trylock(&amp;mm-&gt;mmap_sem)) {</span>
<span class="p_add">+		if (!range_write_trylock(&amp;mm-&gt;mmap_sem, &amp;range)) {</span>
 			INIT_WORK(&amp;uiomr-&gt;work, usnic_uiom_reg_account);
 			uiomr-&gt;mm = mm;
 			uiomr-&gt;diff = diff;
<span class="p_chunk">@@ -453,10 +456,10 @@</span> <span class="p_context"> void usnic_uiom_reg_release(struct usnic_uiom_reg *uiomr, int closing)</span>
 			return;
 		}
 	} else
<span class="p_del">-		down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_write_lock(mm, &amp;range);</span>
 
 	current-&gt;mm-&gt;locked_vm -= diff;
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(mm, &amp;range);</span>
 	mmput(mm);
 	kfree(uiomr);
 }
<span class="p_header">diff --git a/drivers/iommu/amd_iommu_v2.c b/drivers/iommu/amd_iommu_v2.c</span>
<span class="p_header">index 6629c472eafd..de4ef49e21d8 100644</span>
<span class="p_header">--- a/drivers/iommu/amd_iommu_v2.c</span>
<span class="p_header">+++ b/drivers/iommu/amd_iommu_v2.c</span>
<span class="p_chunk">@@ -519,6 +519,7 @@</span> <span class="p_context"> static void do_fault(struct work_struct *work)</span>
 	unsigned int flags = 0;
 	struct mm_struct *mm;
 	u64 address;
<span class="p_add">+	mm_range_define(range);</span>
 
 	mm = fault-&gt;state-&gt;mm;
 	address = fault-&gt;address;
<span class="p_chunk">@@ -529,7 +530,7 @@</span> <span class="p_context"> static void do_fault(struct work_struct *work)</span>
 		flags |= FAULT_FLAG_WRITE;
 	flags |= FAULT_FLAG_REMOTE;
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_lock(mm, &amp;range);</span>
 	vma = find_extend_vma(mm, address);
 	if (!vma || address &lt; vma-&gt;vm_start)
 		/* failed to get a vma in the right range */
<span class="p_chunk">@@ -539,9 +540,9 @@</span> <span class="p_context"> static void do_fault(struct work_struct *work)</span>
 	if (access_error(vma, fault))
 		goto out;
 
<span class="p_del">-	ret = handle_mm_fault(vma, address, flags);</span>
<span class="p_add">+	ret = handle_mm_fault(vma, address, flags, NULL);</span>
 out:
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(mm, &amp;range);</span>
 
 	if (ret &amp; VM_FAULT_ERROR)
 		/* failed to service fault */
<span class="p_header">diff --git a/drivers/iommu/intel-svm.c b/drivers/iommu/intel-svm.c</span>
<span class="p_header">index 4ba770b9cfbb..74927e17c8d2 100644</span>
<span class="p_header">--- a/drivers/iommu/intel-svm.c</span>
<span class="p_header">+++ b/drivers/iommu/intel-svm.c</span>
<span class="p_chunk">@@ -544,6 +544,7 @@</span> <span class="p_context"> static irqreturn_t prq_event_thread(int irq, void *d)</span>
 		struct qi_desc resp;
 		int ret, result;
 		u64 address;
<span class="p_add">+		mm_range_define(range);</span>
 
 		handled = 1;
 
<span class="p_chunk">@@ -582,7 +583,7 @@</span> <span class="p_context"> static irqreturn_t prq_event_thread(int irq, void *d)</span>
 		/* If the mm is already defunct, don&#39;t handle faults. */
 		if (!mmget_not_zero(svm-&gt;mm))
 			goto bad_req;
<span class="p_del">-		down_read(&amp;svm-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_lock(svm-&gt;mm, &amp;range);</span>
 		vma = find_extend_vma(svm-&gt;mm, address);
 		if (!vma || address &lt; vma-&gt;vm_start)
 			goto invalid;
<span class="p_chunk">@@ -597,7 +598,7 @@</span> <span class="p_context"> static irqreturn_t prq_event_thread(int irq, void *d)</span>
 
 		result = QI_RESP_SUCCESS;
 	invalid:
<span class="p_del">-		up_read(&amp;svm-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_unlock(svm-&gt;mm, &amp;range);</span>
 		mmput(svm-&gt;mm);
 	bad_req:
 		/* Accounting for major/minor faults? */
<span class="p_header">diff --git a/drivers/media/v4l2-core/videobuf-core.c b/drivers/media/v4l2-core/videobuf-core.c</span>
<span class="p_header">index 1dbf6f7785bb..ec9eab28e531 100644</span>
<span class="p_header">--- a/drivers/media/v4l2-core/videobuf-core.c</span>
<span class="p_header">+++ b/drivers/media/v4l2-core/videobuf-core.c</span>
<span class="p_chunk">@@ -533,11 +533,12 @@</span> <span class="p_context"> int videobuf_qbuf(struct videobuf_queue *q, struct v4l2_buffer *b)</span>
 	enum v4l2_field field;
 	unsigned long flags = 0;
 	int retval;
<span class="p_add">+	mm_range_define(range);</span>
 
 	MAGIC_CHECK(q-&gt;int_ops-&gt;magic, MAGIC_QTYPE_OPS);
 
 	if (b-&gt;memory == V4L2_MEMORY_MMAP)
<span class="p_del">-		down_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_lock(current-&gt;mm, &amp;range);</span>
 
 	videobuf_queue_lock(q);
 	retval = -EBUSY;
<span class="p_chunk">@@ -624,7 +625,7 @@</span> <span class="p_context"> int videobuf_qbuf(struct videobuf_queue *q, struct v4l2_buffer *b)</span>
 	videobuf_queue_unlock(q);
 
 	if (b-&gt;memory == V4L2_MEMORY_MMAP)
<span class="p_del">-		up_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_unlock(current-&gt;mm, &amp;range);</span>
 
 	return retval;
 }
<span class="p_header">diff --git a/drivers/media/v4l2-core/videobuf-dma-contig.c b/drivers/media/v4l2-core/videobuf-dma-contig.c</span>
<span class="p_header">index e02353e340dd..682b70a69753 100644</span>
<span class="p_header">--- a/drivers/media/v4l2-core/videobuf-dma-contig.c</span>
<span class="p_header">+++ b/drivers/media/v4l2-core/videobuf-dma-contig.c</span>
<span class="p_chunk">@@ -166,12 +166,13 @@</span> <span class="p_context"> static int videobuf_dma_contig_user_get(struct videobuf_dma_contig_memory *mem,</span>
 	unsigned long pages_done, user_address;
 	unsigned int offset;
 	int ret;
<span class="p_add">+	mm_range_define(range);</span>
 
 	offset = vb-&gt;baddr &amp; ~PAGE_MASK;
 	mem-&gt;size = PAGE_ALIGN(vb-&gt;size + offset);
 	ret = -EINVAL;
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_lock(mm, &amp;range);</span>
 
 	vma = find_vma(mm, vb-&gt;baddr);
 	if (!vma)
<span class="p_chunk">@@ -203,7 +204,7 @@</span> <span class="p_context"> static int videobuf_dma_contig_user_get(struct videobuf_dma_contig_memory *mem,</span>
 	}
 
 out_up:
<span class="p_del">-	up_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(current-&gt;mm, &amp;range);</span>
 
 	return ret;
 }
<span class="p_header">diff --git a/drivers/media/v4l2-core/videobuf-dma-sg.c b/drivers/media/v4l2-core/videobuf-dma-sg.c</span>
<span class="p_header">index b789070047df..32e73381b9b7 100644</span>
<span class="p_header">--- a/drivers/media/v4l2-core/videobuf-dma-sg.c</span>
<span class="p_header">+++ b/drivers/media/v4l2-core/videobuf-dma-sg.c</span>
<span class="p_chunk">@@ -200,10 +200,11 @@</span> <span class="p_context"> static int videobuf_dma_init_user(struct videobuf_dmabuf *dma, int direction,</span>
 			   unsigned long data, unsigned long size)
 {
 	int ret;
<span class="p_add">+	mm_range_define(range);</span>
 
<span class="p_del">-	down_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_lock(current-&gt;mm, &amp;range);</span>
 	ret = videobuf_dma_init_user_locked(dma, direction, data, size);
<span class="p_del">-	up_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(current-&gt;mm, &amp;range);</span>
 
 	return ret;
 }
<span class="p_header">diff --git a/drivers/misc/cxl/fault.c b/drivers/misc/cxl/fault.c</span>
<span class="p_header">index 5344448f514e..96e4f9327c1e 100644</span>
<span class="p_header">--- a/drivers/misc/cxl/fault.c</span>
<span class="p_header">+++ b/drivers/misc/cxl/fault.c</span>
<span class="p_chunk">@@ -296,6 +296,7 @@</span> <span class="p_context"> static void cxl_prefault_vma(struct cxl_context *ctx)</span>
 	struct vm_area_struct *vma;
 	int rc;
 	struct mm_struct *mm;
<span class="p_add">+	mm_range_define(range);</span>
 
 	mm = get_mem_context(ctx);
 	if (mm == NULL) {
<span class="p_chunk">@@ -304,7 +305,7 @@</span> <span class="p_context"> static void cxl_prefault_vma(struct cxl_context *ctx)</span>
 		return;
 	}
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_lock(mm, &amp;range);</span>
 	for (vma = mm-&gt;mmap; vma; vma = vma-&gt;vm_next) {
 		for (ea = vma-&gt;vm_start; ea &lt; vma-&gt;vm_end;
 				ea = next_segment(ea, slb.vsid)) {
<span class="p_chunk">@@ -319,7 +320,7 @@</span> <span class="p_context"> static void cxl_prefault_vma(struct cxl_context *ctx)</span>
 			last_esid = slb.esid;
 		}
 	}
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(mm, &amp;range);</span>
 
 	mmput(mm);
 }
<span class="p_header">diff --git a/drivers/misc/mic/scif/scif_rma.c b/drivers/misc/mic/scif/scif_rma.c</span>
<span class="p_header">index 30e3c524216d..b446bfff42e7 100644</span>
<span class="p_header">--- a/drivers/misc/mic/scif/scif_rma.c</span>
<span class="p_header">+++ b/drivers/misc/mic/scif/scif_rma.c</span>
<span class="p_chunk">@@ -275,19 +275,21 @@</span> <span class="p_context"> static inline int</span>
 __scif_dec_pinned_vm_lock(struct mm_struct *mm,
 			  int nr_pages, bool try_lock)
 {
<span class="p_add">+	mm_range_define(range);</span>
<span class="p_add">+</span>
 	if (!mm || !nr_pages || !scif_ulimit_check)
 		return 0;
 	if (try_lock) {
<span class="p_del">-		if (!down_write_trylock(&amp;mm-&gt;mmap_sem)) {</span>
<span class="p_add">+		if (!range_write_trylock(&amp;mm-&gt;mmap_sem, &amp;range)) {</span>
 			dev_err(scif_info.mdev.this_device,
 				&quot;%s %d err\n&quot;, __func__, __LINE__);
 			return -1;
 		}
 	} else {
<span class="p_del">-		down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_write_lock(mm, &amp;range);</span>
 	}
 	mm-&gt;pinned_vm -= nr_pages;
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(mm, &amp;range);</span>
 	return 0;
 }
 
<span class="p_chunk">@@ -1333,6 +1335,7 @@</span> <span class="p_context"> int __scif_pin_pages(void *addr, size_t len, int *out_prot,</span>
 	int prot = *out_prot;
 	int ulimit = 0;
 	struct mm_struct *mm = NULL;
<span class="p_add">+	mm_range_define(range);</span>
 
 	/* Unsupported flags */
 	if (map_flags &amp; ~(SCIF_MAP_KERNEL | SCIF_MAP_ULIMIT))
<span class="p_chunk">@@ -1386,11 +1389,12 @@</span> <span class="p_context"> int __scif_pin_pages(void *addr, size_t len, int *out_prot,</span>
 		prot |= SCIF_PROT_WRITE;
 retry:
 		mm = current-&gt;mm;
<span class="p_del">-		down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+</span>
<span class="p_add">+		mm_write_lock(mm, &amp;range);</span>
 		if (ulimit) {
 			err = __scif_check_inc_pinned_vm(mm, nr_pages);
 			if (err) {
<span class="p_del">-				up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+				mm_write_unlock(mm, &amp;range);</span>
 				pinned_pages-&gt;nr_pages = 0;
 				goto error_unmap;
 			}
<span class="p_chunk">@@ -1402,7 +1406,7 @@</span> <span class="p_context"> int __scif_pin_pages(void *addr, size_t len, int *out_prot,</span>
 				(prot &amp; SCIF_PROT_WRITE) ? FOLL_WRITE : 0,
 				pinned_pages-&gt;pages,
 				NULL, NULL);
<span class="p_del">-		up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_write_unlock(mm, &amp;range);</span>
 		if (nr_pages != pinned_pages-&gt;nr_pages) {
 			if (try_upgrade) {
 				if (ulimit)
<span class="p_header">diff --git a/drivers/oprofile/buffer_sync.c b/drivers/oprofile/buffer_sync.c</span>
<span class="p_header">index ac27f3d3fbb4..f25d7bb1ea0d 100644</span>
<span class="p_header">--- a/drivers/oprofile/buffer_sync.c</span>
<span class="p_header">+++ b/drivers/oprofile/buffer_sync.c</span>
<span class="p_chunk">@@ -90,12 +90,13 @@</span> <span class="p_context"> munmap_notify(struct notifier_block *self, unsigned long val, void *data)</span>
 	unsigned long addr = (unsigned long)data;
 	struct mm_struct *mm = current-&gt;mm;
 	struct vm_area_struct *mpnt;
<span class="p_add">+	mm_range_define(range);</span>
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_lock(mm, &amp;range);</span>
 
 	mpnt = find_vma(mm, addr);
 	if (mpnt &amp;&amp; mpnt-&gt;vm_file &amp;&amp; (mpnt-&gt;vm_flags &amp; VM_EXEC)) {
<span class="p_del">-		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_unlock(mm, &amp;range);</span>
 		/* To avoid latency problems, we only process the current CPU,
 		 * hoping that most samples for the task are on this CPU
 		 */
<span class="p_chunk">@@ -103,7 +104,7 @@</span> <span class="p_context"> munmap_notify(struct notifier_block *self, unsigned long val, void *data)</span>
 		return 0;
 	}
 
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(mm, &amp;range);</span>
 	return 0;
 }
 
<span class="p_chunk">@@ -255,8 +256,9 @@</span> <span class="p_context"> lookup_dcookie(struct mm_struct *mm, unsigned long addr, off_t *offset)</span>
 {
 	unsigned long cookie = NO_COOKIE;
 	struct vm_area_struct *vma;
<span class="p_add">+	mm_range_define(range);</span>
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_lock(mm, &amp;range);</span>
 	for (vma = find_vma(mm, addr); vma; vma = vma-&gt;vm_next) {
 
 		if (addr &lt; vma-&gt;vm_start || addr &gt;= vma-&gt;vm_end)
<span class="p_chunk">@@ -276,7 +278,7 @@</span> <span class="p_context"> lookup_dcookie(struct mm_struct *mm, unsigned long addr, off_t *offset)</span>
 
 	if (!vma)
 		cookie = INVALID_COOKIE;
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(mm, &amp;range);</span>
 
 	return cookie;
 }
<span class="p_header">diff --git a/drivers/staging/lustre/lustre/llite/llite_mmap.c b/drivers/staging/lustre/lustre/llite/llite_mmap.c</span>
<span class="p_header">index cbbfdaf127a7..9853b6c4cd4d 100644</span>
<span class="p_header">--- a/drivers/staging/lustre/lustre/llite/llite_mmap.c</span>
<span class="p_header">+++ b/drivers/staging/lustre/lustre/llite/llite_mmap.c</span>
<span class="p_chunk">@@ -61,9 +61,10 @@</span> <span class="p_context"> struct vm_area_struct *our_vma(struct mm_struct *mm, unsigned long addr,</span>
 			       size_t count)
 {
 	struct vm_area_struct *vma, *ret = NULL;
<span class="p_add">+	mm_range_define(range);</span>
 
 	/* mmap_sem must have been held by caller. */
<span class="p_del">-	LASSERT(!down_write_trylock(&amp;mm-&gt;mmap_sem));</span>
<span class="p_add">+	LASSERT(!range_write_trylock(&amp;mm-&gt;mmap_sem, &amp;range));</span>
 
 	for (vma = find_vma(mm, addr);
 	    vma &amp;&amp; vma-&gt;vm_start &lt; (addr + count); vma = vma-&gt;vm_next) {
<span class="p_header">diff --git a/drivers/staging/lustre/lustre/llite/vvp_io.c b/drivers/staging/lustre/lustre/llite/vvp_io.c</span>
<span class="p_header">index aa31bc0a58a6..ce0bd479a1c5 100644</span>
<span class="p_header">--- a/drivers/staging/lustre/lustre/llite/vvp_io.c</span>
<span class="p_header">+++ b/drivers/staging/lustre/lustre/llite/vvp_io.c</span>
<span class="p_chunk">@@ -377,6 +377,7 @@</span> <span class="p_context"> static int vvp_mmap_locks(const struct lu_env *env,</span>
 	int		 result = 0;
 	struct iov_iter i;
 	struct iovec iov;
<span class="p_add">+	mm_range_define(range);</span>
 
 	LASSERT(io-&gt;ci_type == CIT_READ || io-&gt;ci_type == CIT_WRITE);
 
<span class="p_chunk">@@ -396,7 +397,7 @@</span> <span class="p_context"> static int vvp_mmap_locks(const struct lu_env *env,</span>
 		count += addr &amp; (~PAGE_MASK);
 		addr &amp;= PAGE_MASK;
 
<span class="p_del">-		down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_lock(mm, &amp;range);</span>
 		while ((vma = our_vma(mm, addr, count)) != NULL) {
 			struct inode *inode = file_inode(vma-&gt;vm_file);
 			int flags = CEF_MUST;
<span class="p_chunk">@@ -437,7 +438,7 @@</span> <span class="p_context"> static int vvp_mmap_locks(const struct lu_env *env,</span>
 			count -= vma-&gt;vm_end - addr;
 			addr = vma-&gt;vm_end;
 		}
<span class="p_del">-		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_unlock(mm, &amp;range);</span>
 		if (result &lt; 0)
 			break;
 	}
<span class="p_header">diff --git a/drivers/staging/vc04_services/interface/vchiq_arm/vchiq_2835_arm.c b/drivers/staging/vc04_services/interface/vchiq_arm/vchiq_2835_arm.c</span>
<span class="p_header">index d04db3f55519..bf70914e2bea 100644</span>
<span class="p_header">--- a/drivers/staging/vc04_services/interface/vchiq_arm/vchiq_2835_arm.c</span>
<span class="p_header">+++ b/drivers/staging/vc04_services/interface/vchiq_arm/vchiq_2835_arm.c</span>
<span class="p_chunk">@@ -468,14 +468,16 @@</span> <span class="p_context"> create_pagelist(char __user *buf, size_t count, unsigned short type,</span>
 		}
 		/* do not try and release vmalloc pages */
 	} else {
<span class="p_del">-		down_read(&amp;task-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_range_define(range);</span>
<span class="p_add">+</span>
<span class="p_add">+		mm_read_lock(task-&gt;mm-&gt;mmap_sem, &amp;range);</span>
 		actual_pages = get_user_pages(
 				          (unsigned long)buf &amp; ~(PAGE_SIZE - 1),
 					  num_pages,
 					  (type == PAGELIST_READ) ? FOLL_WRITE : 0,
 					  pages,
 					  NULL /*vmas */);
<span class="p_del">-		up_read(&amp;task-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_unlock(task-&gt;mm-&gt;mmap_sem, &amp;range);</span>
 
 		if (actual_pages != num_pages) {
 			vchiq_log_info(vchiq_arm_log_level,
<span class="p_header">diff --git a/drivers/staging/vc04_services/interface/vchiq_arm/vchiq_arm.c b/drivers/staging/vc04_services/interface/vchiq_arm/vchiq_arm.c</span>
<span class="p_header">index e823f1d5d177..2177f7852e68 100644</span>
<span class="p_header">--- a/drivers/staging/vc04_services/interface/vchiq_arm/vchiq_arm.c</span>
<span class="p_header">+++ b/drivers/staging/vc04_services/interface/vchiq_arm/vchiq_arm.c</span>
<span class="p_chunk">@@ -2069,6 +2069,7 @@</span> <span class="p_context"> dump_phys_mem(void *virt_addr, u32 num_bytes)</span>
 	struct page   *page;
 	struct page  **pages;
 	u8            *kmapped_virt_ptr;
<span class="p_add">+	mm_range_define(range);</span>
 
 	/* Align virtAddr and endVirtAddr to 16 byte boundaries. */
 
<span class="p_chunk">@@ -2089,14 +2090,14 @@</span> <span class="p_context"> dump_phys_mem(void *virt_addr, u32 num_bytes)</span>
 		return;
 	}
 
<span class="p_del">-	down_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_lock(current-&gt;mm, &amp;range);</span>
 	rc = get_user_pages(
 		(unsigned long)virt_addr, /* start */
 		num_pages,                /* len */
 		0,                        /* gup_flags */
 		pages,                    /* pages (array of page pointers) */
 		NULL);                    /* vmas */
<span class="p_del">-	up_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(current-&gt;mm, &amp;range);</span>
 
 	prev_idx = -1;
 	page = NULL;
<span class="p_header">diff --git a/drivers/vfio/vfio_iommu_spapr_tce.c b/drivers/vfio/vfio_iommu_spapr_tce.c</span>
<span class="p_header">index 63112c36ab2d..a8af18ac0e3b 100644</span>
<span class="p_header">--- a/drivers/vfio/vfio_iommu_spapr_tce.c</span>
<span class="p_header">+++ b/drivers/vfio/vfio_iommu_spapr_tce.c</span>
<span class="p_chunk">@@ -37,6 +37,7 @@</span> <span class="p_context"> static void tce_iommu_detach_group(void *iommu_data,</span>
 static long try_increment_locked_vm(struct mm_struct *mm, long npages)
 {
 	long ret = 0, locked, lock_limit;
<span class="p_add">+	mm_range_define(range);</span>
 
 	if (WARN_ON_ONCE(!mm))
 		return -EPERM;
<span class="p_chunk">@@ -44,7 +45,7 @@</span> <span class="p_context"> static long try_increment_locked_vm(struct mm_struct *mm, long npages)</span>
 	if (!npages)
 		return 0;
 
<span class="p_del">-	down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_lock(mm, &amp;range);</span>
 	locked = mm-&gt;locked_vm + npages;
 	lock_limit = rlimit(RLIMIT_MEMLOCK) &gt;&gt; PAGE_SHIFT;
 	if (locked &gt; lock_limit &amp;&amp; !capable(CAP_IPC_LOCK))
<span class="p_chunk">@@ -58,17 +59,19 @@</span> <span class="p_context"> static long try_increment_locked_vm(struct mm_struct *mm, long npages)</span>
 			rlimit(RLIMIT_MEMLOCK),
 			ret ? &quot; - exceeded&quot; : &quot;&quot;);
 
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(mm, &amp;range);</span>
 
 	return ret;
 }
 
 static void decrement_locked_vm(struct mm_struct *mm, long npages)
 {
<span class="p_add">+	mm_range_define(range);</span>
<span class="p_add">+</span>
 	if (!mm || !npages)
 		return;
 
<span class="p_del">-	down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_lock(mm, &amp;range);</span>
 	if (WARN_ON_ONCE(npages &gt; mm-&gt;locked_vm))
 		npages = mm-&gt;locked_vm;
 	mm-&gt;locked_vm -= npages;
<span class="p_chunk">@@ -76,7 +79,7 @@</span> <span class="p_context"> static void decrement_locked_vm(struct mm_struct *mm, long npages)</span>
 			npages &lt;&lt; PAGE_SHIFT,
 			mm-&gt;locked_vm &lt;&lt; PAGE_SHIFT,
 			rlimit(RLIMIT_MEMLOCK));
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(mm, &amp;range);</span>
 }
 
 /*
<span class="p_header">diff --git a/drivers/vfio/vfio_iommu_type1.c b/drivers/vfio/vfio_iommu_type1.c</span>
<span class="p_header">index 8549cb111627..ac6c86a5fa75 100644</span>
<span class="p_header">--- a/drivers/vfio/vfio_iommu_type1.c</span>
<span class="p_header">+++ b/drivers/vfio/vfio_iommu_type1.c</span>
<span class="p_chunk">@@ -251,6 +251,7 @@</span> <span class="p_context"> static int vfio_lock_acct(struct task_struct *task, long npage, bool *lock_cap)</span>
 	struct mm_struct *mm;
 	bool is_current;
 	int ret;
<span class="p_add">+	mm_range_define(range);</span>
 
 	if (!npage)
 		return 0;
<span class="p_chunk">@@ -261,7 +262,7 @@</span> <span class="p_context"> static int vfio_lock_acct(struct task_struct *task, long npage, bool *lock_cap)</span>
 	if (!mm)
 		return -ESRCH; /* process exited */
 
<span class="p_del">-	ret = down_write_killable(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	ret = mm_write_trylock(mm, &amp;range);</span>
 	if (!ret) {
 		if (npage &gt; 0) {
 			if (lock_cap ? !*lock_cap :
<span class="p_chunk">@@ -279,7 +280,7 @@</span> <span class="p_context"> static int vfio_lock_acct(struct task_struct *task, long npage, bool *lock_cap)</span>
 		if (!ret)
 			mm-&gt;locked_vm += npage;
 
<span class="p_del">-		up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_write_unlock(mm);</span>
 	}
 
 	if (!is_current)
<span class="p_chunk">@@ -339,6 +340,7 @@</span> <span class="p_context"> static int vaddr_get_pfn(struct mm_struct *mm, unsigned long vaddr,</span>
 	struct page *page[1];
 	struct vm_area_struct *vma;
 	int ret;
<span class="p_add">+	mm_range_define(range);</span>
 
 	if (mm == current-&gt;mm) {
 		ret = get_user_pages_fast(vaddr, 1, !!(prot &amp; IOMMU_WRITE),
<span class="p_chunk">@@ -349,10 +351,10 @@</span> <span class="p_context"> static int vaddr_get_pfn(struct mm_struct *mm, unsigned long vaddr,</span>
 		if (prot &amp; IOMMU_WRITE)
 			flags |= FOLL_WRITE;
 
<span class="p_del">-		down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_lock(mm, &amp;range);</span>
 		ret = get_user_pages_remote(NULL, mm, vaddr, 1, flags, page,
<span class="p_del">-					    NULL, NULL);</span>
<span class="p_del">-		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+					    NULL, NULL, NULL);</span>
<span class="p_add">+		mm_read_unlock(mm, &amp;range);</span>
 	}
 
 	if (ret == 1) {
<span class="p_chunk">@@ -360,7 +362,7 @@</span> <span class="p_context"> static int vaddr_get_pfn(struct mm_struct *mm, unsigned long vaddr,</span>
 		return 0;
 	}
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_lock(mm, &amp;range);</span>
 
 	vma = find_vma_intersection(mm, vaddr, vaddr + 1);
 
<span class="p_chunk">@@ -370,7 +372,7 @@</span> <span class="p_context"> static int vaddr_get_pfn(struct mm_struct *mm, unsigned long vaddr,</span>
 			ret = 0;
 	}
 
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(mm, &amp;range);</span>
 	return ret;
 }
 
<span class="p_header">diff --git a/drivers/xen/gntdev.c b/drivers/xen/gntdev.c</span>
<span class="p_header">index f3bf8f4e2d6c..b0be7f3b48ec 100644</span>
<span class="p_header">--- a/drivers/xen/gntdev.c</span>
<span class="p_header">+++ b/drivers/xen/gntdev.c</span>
<span class="p_chunk">@@ -658,12 +658,13 @@</span> <span class="p_context"> static long gntdev_ioctl_get_offset_for_vaddr(struct gntdev_priv *priv,</span>
 	struct vm_area_struct *vma;
 	struct grant_map *map;
 	int rv = -EINVAL;
<span class="p_add">+	mm_range_define(range);</span>
 
 	if (copy_from_user(&amp;op, u, sizeof(op)) != 0)
 		return -EFAULT;
 	pr_debug(&quot;priv %p, offset for vaddr %lx\n&quot;, priv, (unsigned long)op.vaddr);
 
<span class="p_del">-	down_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_lock(current-&gt;mm, &amp;range);</span>
 	vma = find_vma(current-&gt;mm, op.vaddr);
 	if (!vma || vma-&gt;vm_ops != &amp;gntdev_vmops)
 		goto out_unlock;
<span class="p_chunk">@@ -677,7 +678,7 @@</span> <span class="p_context"> static long gntdev_ioctl_get_offset_for_vaddr(struct gntdev_priv *priv,</span>
 	rv = 0;
 
  out_unlock:
<span class="p_del">-	up_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(current-&gt;mm, &amp;range);</span>
 
 	if (rv == 0 &amp;&amp; copy_to_user(u, &amp;op, sizeof(op)) != 0)
 		return -EFAULT;
<span class="p_header">diff --git a/drivers/xen/privcmd.c b/drivers/xen/privcmd.c</span>
<span class="p_header">index 7a92a5e1d40c..156a708bfff4 100644</span>
<span class="p_header">--- a/drivers/xen/privcmd.c</span>
<span class="p_header">+++ b/drivers/xen/privcmd.c</span>
<span class="p_chunk">@@ -260,6 +260,7 @@</span> <span class="p_context"> static long privcmd_ioctl_mmap(struct file *file, void __user *udata)</span>
 	int rc;
 	LIST_HEAD(pagelist);
 	struct mmap_gfn_state state;
<span class="p_add">+	mm_range_define(range);</span>
 
 	/* We only support privcmd_ioctl_mmap_batch for auto translated. */
 	if (xen_feature(XENFEAT_auto_translated_physmap))
<span class="p_chunk">@@ -279,7 +280,7 @@</span> <span class="p_context"> static long privcmd_ioctl_mmap(struct file *file, void __user *udata)</span>
 	if (rc || list_empty(&amp;pagelist))
 		goto out;
 
<span class="p_del">-	down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_lock(mm, &amp;range);</span>
 
 	{
 		struct page *page = list_first_entry(&amp;pagelist,
<span class="p_chunk">@@ -304,7 +305,7 @@</span> <span class="p_context"> static long privcmd_ioctl_mmap(struct file *file, void __user *udata)</span>
 
 
 out_up:
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(mm, &amp;range);</span>
 
 out:
 	free_page_list(&amp;pagelist);
<span class="p_chunk">@@ -454,6 +455,7 @@</span> <span class="p_context"> static long privcmd_ioctl_mmap_batch(</span>
 	unsigned long nr_pages;
 	LIST_HEAD(pagelist);
 	struct mmap_batch_state state;
<span class="p_add">+	mm_range_define(range);</span>
 
 	switch (version) {
 	case 1:
<span class="p_chunk">@@ -500,7 +502,7 @@</span> <span class="p_context"> static long privcmd_ioctl_mmap_batch(</span>
 		}
 	}
 
<span class="p_del">-	down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_lock(mm, &amp;range);</span>
 
 	vma = find_vma(mm, m.addr);
 	if (!vma ||
<span class="p_chunk">@@ -556,7 +558,7 @@</span> <span class="p_context"> static long privcmd_ioctl_mmap_batch(</span>
 	BUG_ON(traverse_pages_block(m.num, sizeof(xen_pfn_t),
 				    &amp;pagelist, mmap_batch_fn, &amp;state));
 
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(mm, &amp;range);</span>
 
 	if (state.global_error) {
 		/* Write back errors in second pass. */
<span class="p_chunk">@@ -577,7 +579,7 @@</span> <span class="p_context"> static long privcmd_ioctl_mmap_batch(</span>
 	return ret;
 
 out_unlock:
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(mm, &amp;range);</span>
 	goto out;
 }
 
<span class="p_header">diff --git a/fs/aio.c b/fs/aio.c</span>
<span class="p_header">index f52d925ee259..5c3057c0b85f 100644</span>
<span class="p_header">--- a/fs/aio.c</span>
<span class="p_header">+++ b/fs/aio.c</span>
<span class="p_chunk">@@ -450,6 +450,7 @@</span> <span class="p_context"> static int aio_setup_ring(struct kioctx *ctx)</span>
 	int nr_pages;
 	int i;
 	struct file *file;
<span class="p_add">+	mm_range_define(range);</span>
 
 	/* Compensate for the ring buffer&#39;s head/tail overlap entry */
 	nr_events += 2;	/* 1 is required, 2 for good luck */
<span class="p_chunk">@@ -504,7 +505,7 @@</span> <span class="p_context"> static int aio_setup_ring(struct kioctx *ctx)</span>
 	ctx-&gt;mmap_size = nr_pages * PAGE_SIZE;
 	pr_debug(&quot;attempting mmap of %lu bytes\n&quot;, ctx-&gt;mmap_size);
 
<span class="p_del">-	if (down_write_killable(&amp;mm-&gt;mmap_sem)) {</span>
<span class="p_add">+	if (mm_write_lock_killable(mm, &amp;range)) {</span>
 		ctx-&gt;mmap_size = 0;
 		aio_free_ring(ctx);
 		return -EINTR;
<span class="p_chunk">@@ -513,7 +514,7 @@</span> <span class="p_context"> static int aio_setup_ring(struct kioctx *ctx)</span>
 	ctx-&gt;mmap_base = do_mmap_pgoff(ctx-&gt;aio_ring_file, 0, ctx-&gt;mmap_size,
 				       PROT_READ | PROT_WRITE,
 				       MAP_SHARED, 0, &amp;unused, NULL);
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(mm, &amp;range);</span>
 	if (IS_ERR((void *)ctx-&gt;mmap_base)) {
 		ctx-&gt;mmap_size = 0;
 		aio_free_ring(ctx);
<span class="p_header">diff --git a/fs/coredump.c b/fs/coredump.c</span>
<span class="p_header">index 592683711c64..9a08dcc78bcb 100644</span>
<span class="p_header">--- a/fs/coredump.c</span>
<span class="p_header">+++ b/fs/coredump.c</span>
<span class="p_chunk">@@ -411,17 +411,18 @@</span> <span class="p_context"> static int coredump_wait(int exit_code, struct core_state *core_state)</span>
 	struct task_struct *tsk = current;
 	struct mm_struct *mm = tsk-&gt;mm;
 	int core_waiters = -EBUSY;
<span class="p_add">+	mm_range_define(range);</span>
 
 	init_completion(&amp;core_state-&gt;startup);
 	core_state-&gt;dumper.task = tsk;
 	core_state-&gt;dumper.next = NULL;
 
<span class="p_del">-	if (down_write_killable(&amp;mm-&gt;mmap_sem))</span>
<span class="p_add">+	if (mm_write_lock_killable(mm, &amp;range))</span>
 		return -EINTR;
 
 	if (!mm-&gt;core_state)
 		core_waiters = zap_threads(tsk, mm, core_state, exit_code);
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(mm, &amp;range);</span>
 
 	if (core_waiters &gt; 0) {
 		struct core_thread *ptr;
<span class="p_header">diff --git a/fs/exec.c b/fs/exec.c</span>
<span class="p_header">index ef44ce8302b6..32b06728580b 100644</span>
<span class="p_header">--- a/fs/exec.c</span>
<span class="p_header">+++ b/fs/exec.c</span>
<span class="p_chunk">@@ -268,12 +268,13 @@</span> <span class="p_context"> static int __bprm_mm_init(struct linux_binprm *bprm)</span>
 	int err;
 	struct vm_area_struct *vma = NULL;
 	struct mm_struct *mm = bprm-&gt;mm;
<span class="p_add">+	mm_range_define(range);</span>
 
 	bprm-&gt;vma = vma = kmem_cache_zalloc(vm_area_cachep, GFP_KERNEL);
 	if (!vma)
 		return -ENOMEM;
 
<span class="p_del">-	if (down_write_killable(&amp;mm-&gt;mmap_sem)) {</span>
<span class="p_add">+	if (mm_write_lock_killable(mm, &amp;range)) {</span>
 		err = -EINTR;
 		goto err_free;
 	}
<span class="p_chunk">@@ -298,11 +299,11 @@</span> <span class="p_context"> static int __bprm_mm_init(struct linux_binprm *bprm)</span>
 
 	mm-&gt;stack_vm = mm-&gt;total_vm = 1;
 	arch_bprm_mm_init(mm, vma);
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(mm, &amp;range);</span>
 	bprm-&gt;p = vma-&gt;vm_end - sizeof(void *);
 	return 0;
 err:
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(mm, &amp;range);</span>
 err_free:
 	bprm-&gt;vma = NULL;
 	kmem_cache_free(vm_area_cachep, vma);
<span class="p_chunk">@@ -673,6 +674,7 @@</span> <span class="p_context"> int setup_arg_pages(struct linux_binprm *bprm,</span>
 	unsigned long stack_size;
 	unsigned long stack_expand;
 	unsigned long rlim_stack;
<span class="p_add">+	mm_range_define(range);</span>
 
 #ifdef CONFIG_STACK_GROWSUP
 	/* Limit stack size */
<span class="p_chunk">@@ -710,7 +712,7 @@</span> <span class="p_context"> int setup_arg_pages(struct linux_binprm *bprm,</span>
 		bprm-&gt;loader -= stack_shift;
 	bprm-&gt;exec -= stack_shift;
 
<span class="p_del">-	if (down_write_killable(&amp;mm-&gt;mmap_sem))</span>
<span class="p_add">+	if (mm_write_lock_killable(mm, &amp;range))</span>
 		return -EINTR;
 
 	vm_flags = VM_STACK_FLAGS;
<span class="p_chunk">@@ -767,7 +769,7 @@</span> <span class="p_context"> int setup_arg_pages(struct linux_binprm *bprm,</span>
 		ret = -EFAULT;
 
 out_unlock:
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(mm, &amp;range);</span>
 	return ret;
 }
 EXPORT_SYMBOL(setup_arg_pages);
<span class="p_chunk">@@ -1001,6 +1003,7 @@</span> <span class="p_context"> static int exec_mmap(struct mm_struct *mm)</span>
 {
 	struct task_struct *tsk;
 	struct mm_struct *old_mm, *active_mm;
<span class="p_add">+	mm_range_define(range);</span>
 
 	/* Notify parent that we&#39;re no longer interested in the old VM */
 	tsk = current;
<span class="p_chunk">@@ -1015,9 +1018,10 @@</span> <span class="p_context"> static int exec_mmap(struct mm_struct *mm)</span>
 		 * through with the exec.  We must hold mmap_sem around
 		 * checking core_state and changing tsk-&gt;mm.
 		 */
<span class="p_del">-		down_read(&amp;old_mm-&gt;mmap_sem);</span>
<span class="p_add">+</span>
<span class="p_add">+		mm_read_lock(old_mm, &amp;range);</span>
 		if (unlikely(old_mm-&gt;core_state)) {
<span class="p_del">-			up_read(&amp;old_mm-&gt;mmap_sem);</span>
<span class="p_add">+			mm_read_unlock(old_mm, &amp;range);</span>
 			return -EINTR;
 		}
 	}
<span class="p_chunk">@@ -1030,7 +1034,7 @@</span> <span class="p_context"> static int exec_mmap(struct mm_struct *mm)</span>
 	vmacache_flush(tsk);
 	task_unlock(tsk);
 	if (old_mm) {
<span class="p_del">-		up_read(&amp;old_mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_unlock(old_mm, &amp;range);</span>
 		BUG_ON(active_mm != old_mm);
 		setmax_mm_hiwater_rss(&amp;tsk-&gt;signal-&gt;maxrss, old_mm);
 		mm_update_next_owner(old_mm);
<span class="p_header">diff --git a/fs/proc/base.c b/fs/proc/base.c</span>
<span class="p_header">index 45f6bf68fff3..39b80fd96c77 100644</span>
<span class="p_header">--- a/fs/proc/base.c</span>
<span class="p_header">+++ b/fs/proc/base.c</span>
<span class="p_chunk">@@ -216,6 +216,7 @@</span> <span class="p_context"> static ssize_t proc_pid_cmdline_read(struct file *file, char __user *buf,</span>
 	unsigned long p;
 	char c;
 	ssize_t rv;
<span class="p_add">+	mm_range_define(range);</span>
 
 	BUG_ON(*pos &lt; 0);
 
<span class="p_chunk">@@ -238,12 +239,12 @@</span> <span class="p_context"> static ssize_t proc_pid_cmdline_read(struct file *file, char __user *buf,</span>
 		goto out_mmput;
 	}
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_lock(mm, &amp;range);</span>
 	arg_start = mm-&gt;arg_start;
 	arg_end = mm-&gt;arg_end;
 	env_start = mm-&gt;env_start;
 	env_end = mm-&gt;env_end;
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(mm, &amp;range);</span>
 
 	BUG_ON(arg_start &gt; arg_end);
 	BUG_ON(env_start &gt; env_end);
<span class="p_chunk">@@ -913,6 +914,7 @@</span> <span class="p_context"> static ssize_t environ_read(struct file *file, char __user *buf,</span>
 	int ret = 0;
 	struct mm_struct *mm = file-&gt;private_data;
 	unsigned long env_start, env_end;
<span class="p_add">+	mm_range_define(range);</span>
 
 	/* Ensure the process spawned far enough to have an environment. */
 	if (!mm || !mm-&gt;env_end)
<span class="p_chunk">@@ -926,10 +928,10 @@</span> <span class="p_context"> static ssize_t environ_read(struct file *file, char __user *buf,</span>
 	if (!mmget_not_zero(mm))
 		goto free;
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_lock(mm, &amp;range);</span>
 	env_start = mm-&gt;env_start;
 	env_end = mm-&gt;env_end;
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(mm, &amp;range);</span>
 
 	while (count &gt; 0) {
 		size_t this_len, max_len;
<span class="p_chunk">@@ -1877,6 +1879,7 @@</span> <span class="p_context"> static int map_files_d_revalidate(struct dentry *dentry, unsigned int flags)</span>
 	struct task_struct *task;
 	struct inode *inode;
 	int status = 0;
<span class="p_add">+	mm_range_define(range);</span>
 
 	if (flags &amp; LOOKUP_RCU)
 		return -ECHILD;
<span class="p_chunk">@@ -1891,9 +1894,9 @@</span> <span class="p_context"> static int map_files_d_revalidate(struct dentry *dentry, unsigned int flags)</span>
 		goto out;
 
 	if (!dname_to_vma_addr(dentry, &amp;vm_start, &amp;vm_end)) {
<span class="p_del">-		down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_lock(mm, &amp;range);</span>
 		exact_vma_exists = !!find_exact_vma(mm, vm_start, vm_end);
<span class="p_del">-		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_unlock(mm, &amp;range);</span>
 	}
 
 	mmput(mm);
<span class="p_chunk">@@ -1924,6 +1927,7 @@</span> <span class="p_context"> static int map_files_get_link(struct dentry *dentry, struct path *path)</span>
 	struct task_struct *task;
 	struct mm_struct *mm;
 	int rc;
<span class="p_add">+	mm_range_define(range);</span>
 
 	rc = -ENOENT;
 	task = get_proc_task(d_inode(dentry));
<span class="p_chunk">@@ -1940,14 +1944,14 @@</span> <span class="p_context"> static int map_files_get_link(struct dentry *dentry, struct path *path)</span>
 		goto out_mmput;
 
 	rc = -ENOENT;
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_lock(mm, &amp;range);</span>
 	vma = find_exact_vma(mm, vm_start, vm_end);
 	if (vma &amp;&amp; vma-&gt;vm_file) {
 		*path = vma-&gt;vm_file-&gt;f_path;
 		path_get(path);
 		rc = 0;
 	}
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(mm, &amp;range);</span>
 
 out_mmput:
 	mmput(mm);
<span class="p_chunk">@@ -2020,6 +2024,7 @@</span> <span class="p_context"> static struct dentry *proc_map_files_lookup(struct inode *dir,</span>
 	struct task_struct *task;
 	int result;
 	struct mm_struct *mm;
<span class="p_add">+	mm_range_define(range);</span>
 
 	result = -ENOENT;
 	task = get_proc_task(dir);
<span class="p_chunk">@@ -2038,7 +2043,7 @@</span> <span class="p_context"> static struct dentry *proc_map_files_lookup(struct inode *dir,</span>
 	if (!mm)
 		goto out_put_task;
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_lock(mm, &amp;range);</span>
 	vma = find_exact_vma(mm, vm_start, vm_end);
 	if (!vma)
 		goto out_no_vma;
<span class="p_chunk">@@ -2048,7 +2053,7 @@</span> <span class="p_context"> static struct dentry *proc_map_files_lookup(struct inode *dir,</span>
 				(void *)(unsigned long)vma-&gt;vm_file-&gt;f_mode);
 
 out_no_vma:
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(mm, &amp;range);</span>
 	mmput(mm);
 out_put_task:
 	put_task_struct(task);
<span class="p_chunk">@@ -2073,6 +2078,7 @@</span> <span class="p_context"> proc_map_files_readdir(struct file *file, struct dir_context *ctx)</span>
 	struct map_files_info info;
 	struct map_files_info *p;
 	int ret;
<span class="p_add">+	mm_range_define(range);</span>
 
 	ret = -ENOENT;
 	task = get_proc_task(file_inode(file));
<span class="p_chunk">@@ -2090,7 +2096,7 @@</span> <span class="p_context"> proc_map_files_readdir(struct file *file, struct dir_context *ctx)</span>
 	mm = get_task_mm(task);
 	if (!mm)
 		goto out_put_task;
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_lock(mm, &amp;range);</span>
 
 	nr_files = 0;
 
<span class="p_chunk">@@ -2117,7 +2123,7 @@</span> <span class="p_context"> proc_map_files_readdir(struct file *file, struct dir_context *ctx)</span>
 			ret = -ENOMEM;
 			if (fa)
 				flex_array_free(fa);
<span class="p_del">-			up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+			mm_read_unlock(mm, &amp;range);</span>
 			mmput(mm);
 			goto out_put_task;
 		}
<span class="p_chunk">@@ -2136,7 +2142,7 @@</span> <span class="p_context"> proc_map_files_readdir(struct file *file, struct dir_context *ctx)</span>
 				BUG();
 		}
 	}
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(mm, &amp;range);</span>
 
 	for (i = 0; i &lt; nr_files; i++) {
 		p = flex_array_get(fa, i);
<span class="p_header">diff --git a/fs/proc/internal.h b/fs/proc/internal.h</span>
<span class="p_header">index c5ae09b6c726..26f402a02ebc 100644</span>
<span class="p_header">--- a/fs/proc/internal.h</span>
<span class="p_header">+++ b/fs/proc/internal.h</span>
<span class="p_chunk">@@ -279,6 +279,9 @@</span> <span class="p_context"> struct proc_maps_private {</span>
 #ifdef CONFIG_NUMA
 	struct mempolicy *task_mempolicy;
 #endif
<span class="p_add">+#ifdef CONFIG_MEM_RANGE_LOCK</span>
<span class="p_add">+	struct range_lock range;</span>
<span class="p_add">+#endif</span>
 };
 
 struct mm_struct *proc_mem_open(struct inode *inode, unsigned int mode);
<span class="p_header">diff --git a/fs/proc/task_mmu.c b/fs/proc/task_mmu.c</span>
<span class="p_header">index f0c8b33d99b1..9a0137c287db 100644</span>
<span class="p_header">--- a/fs/proc/task_mmu.c</span>
<span class="p_header">+++ b/fs/proc/task_mmu.c</span>
<span class="p_chunk">@@ -133,7 +133,7 @@</span> <span class="p_context"> static void vma_stop(struct proc_maps_private *priv)</span>
 	struct mm_struct *mm = priv-&gt;mm;
 
 	release_task_mempolicy(priv);
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(mm, &amp;priv-&gt;range);</span>
 	mmput(mm);
 }
 
<span class="p_chunk">@@ -171,7 +171,7 @@</span> <span class="p_context"> static void *m_start(struct seq_file *m, loff_t *ppos)</span>
 	if (!mm || !mmget_not_zero(mm))
 		return NULL;
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_lock(mm, &amp;priv-&gt;range);</span>
 	hold_task_mempolicy(priv);
 	priv-&gt;tail_vma = get_gate_vma(mm);
 
<span class="p_chunk">@@ -1015,6 +1015,7 @@</span> <span class="p_context"> static ssize_t clear_refs_write(struct file *file, const char __user *buf,</span>
 	enum clear_refs_types type;
 	int itype;
 	int rv;
<span class="p_add">+	mm_range_define(range);</span>
 
 	memset(buffer, 0, sizeof(buffer));
 	if (count &gt; sizeof(buffer) - 1)
<span class="p_chunk">@@ -1044,7 +1045,7 @@</span> <span class="p_context"> static ssize_t clear_refs_write(struct file *file, const char __user *buf,</span>
 		};
 
 		if (type == CLEAR_REFS_MM_HIWATER_RSS) {
<span class="p_del">-			if (down_write_killable(&amp;mm-&gt;mmap_sem)) {</span>
<span class="p_add">+			if (mm_write_lock_killable(mm, &amp;range)) {</span>
 				count = -EINTR;
 				goto out_mm;
 			}
<span class="p_chunk">@@ -1054,17 +1055,17 @@</span> <span class="p_context"> static ssize_t clear_refs_write(struct file *file, const char __user *buf,</span>
 			 * resident set size to this mm&#39;s current rss value.
 			 */
 			reset_mm_hiwater_rss(mm);
<span class="p_del">-			up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+			mm_write_unlock(mm, &amp;range);</span>
 			goto out_mm;
 		}
 
<span class="p_del">-		down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_lock(mm, &amp;range);</span>
 		if (type == CLEAR_REFS_SOFT_DIRTY) {
 			for (vma = mm-&gt;mmap; vma; vma = vma-&gt;vm_next) {
 				if (!(vma-&gt;vm_flags &amp; VM_SOFTDIRTY))
 					continue;
<span class="p_del">-				up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_del">-				if (down_write_killable(&amp;mm-&gt;mmap_sem)) {</span>
<span class="p_add">+				mm_read_unlock(mm, &amp;range);</span>
<span class="p_add">+				if (mm_write_lock_killable(mm, &amp;range)) {</span>
 					count = -EINTR;
 					goto out_mm;
 				}
<span class="p_chunk">@@ -1072,7 +1073,7 @@</span> <span class="p_context"> static ssize_t clear_refs_write(struct file *file, const char __user *buf,</span>
 					vma-&gt;vm_flags &amp;= ~VM_SOFTDIRTY;
 					vma_set_page_prot(vma);
 				}
<span class="p_del">-				downgrade_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+				mm_downgrade_write(mm, &amp;range);</span>
 				break;
 			}
 			mmu_notifier_invalidate_range_start(mm, 0, -1);
<span class="p_chunk">@@ -1081,7 +1082,7 @@</span> <span class="p_context"> static ssize_t clear_refs_write(struct file *file, const char __user *buf,</span>
 		if (type == CLEAR_REFS_SOFT_DIRTY)
 			mmu_notifier_invalidate_range_end(mm, 0, -1);
 		flush_tlb_mm(mm);
<span class="p_del">-		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_unlock(mm, &amp;range);</span>
 out_mm:
 		mmput(mm);
 	}
<span class="p_chunk">@@ -1365,6 +1366,7 @@</span> <span class="p_context"> static ssize_t pagemap_read(struct file *file, char __user *buf,</span>
 	unsigned long start_vaddr;
 	unsigned long end_vaddr;
 	int ret = 0, copied = 0;
<span class="p_add">+	mm_range_define(range);</span>
 
 	if (!mm || !mmget_not_zero(mm))
 		goto out;
<span class="p_chunk">@@ -1420,9 +1422,9 @@</span> <span class="p_context"> static ssize_t pagemap_read(struct file *file, char __user *buf,</span>
 		/* overflow ? */
 		if (end &lt; start_vaddr || end &gt; end_vaddr)
 			end = end_vaddr;
<span class="p_del">-		down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_lock(mm, &amp;range);</span>
 		ret = walk_page_range(start_vaddr, end, &amp;pagemap_walk);
<span class="p_del">-		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_unlock(mm, &amp;range);</span>
 		start_vaddr = end;
 
 		len = min(count, PM_ENTRY_BYTES * pm.pos);
<span class="p_header">diff --git a/fs/proc/task_nommu.c b/fs/proc/task_nommu.c</span>
<span class="p_header">index 23266694db11..7ef4db48636e 100644</span>
<span class="p_header">--- a/fs/proc/task_nommu.c</span>
<span class="p_header">+++ b/fs/proc/task_nommu.c</span>
<span class="p_chunk">@@ -23,8 +23,9 @@</span> <span class="p_context"> void task_mem(struct seq_file *m, struct mm_struct *mm)</span>
 	struct vm_region *region;
 	struct rb_node *p;
 	unsigned long bytes = 0, sbytes = 0, slack = 0, size;
<span class="p_del">-        </span>
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_range_define(range);</span>
<span class="p_add">+</span>
<span class="p_add">+	mm_read_lock(mm, &amp;range);</span>
 	for (p = rb_first(&amp;mm-&gt;mm_rb); p; p = rb_next(p)) {
 		vma = rb_entry(p, struct vm_area_struct, vm_rb);
 
<span class="p_chunk">@@ -76,7 +77,7 @@</span> <span class="p_context"> void task_mem(struct seq_file *m, struct mm_struct *mm)</span>
 		&quot;Shared:\t%8lu bytes\n&quot;,
 		bytes, slack, sbytes);
 
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(mm, &amp;range);</span>
 }
 
 unsigned long task_vsize(struct mm_struct *mm)
<span class="p_chunk">@@ -84,13 +85,14 @@</span> <span class="p_context"> unsigned long task_vsize(struct mm_struct *mm)</span>
 	struct vm_area_struct *vma;
 	struct rb_node *p;
 	unsigned long vsize = 0;
<span class="p_add">+	mm_range_define(range);</span>
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_lock(mm, &amp;range);</span>
 	for (p = rb_first(&amp;mm-&gt;mm_rb); p; p = rb_next(p)) {
 		vma = rb_entry(p, struct vm_area_struct, vm_rb);
 		vsize += vma-&gt;vm_end - vma-&gt;vm_start;
 	}
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(mm, &amp;range);</span>
 	return vsize;
 }
 
<span class="p_chunk">@@ -102,8 +104,9 @@</span> <span class="p_context"> unsigned long task_statm(struct mm_struct *mm,</span>
 	struct vm_region *region;
 	struct rb_node *p;
 	unsigned long size = kobjsize(mm);
<span class="p_add">+	mm_range_define(range);</span>
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_lock(mm, &amp;range);</span>
 	for (p = rb_first(&amp;mm-&gt;mm_rb); p; p = rb_next(p)) {
 		vma = rb_entry(p, struct vm_area_struct, vm_rb);
 		size += kobjsize(vma);
<span class="p_chunk">@@ -118,7 +121,7 @@</span> <span class="p_context"> unsigned long task_statm(struct mm_struct *mm,</span>
 		&gt;&gt; PAGE_SHIFT;
 	*data = (PAGE_ALIGN(mm-&gt;start_stack) - (mm-&gt;start_data &amp; PAGE_MASK))
 		&gt;&gt; PAGE_SHIFT;
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(mm, &amp;range);</span>
 	size &gt;&gt;= PAGE_SHIFT;
 	size += *text + *data;
 	*resident = size;
<span class="p_chunk">@@ -224,13 +227,14 @@</span> <span class="p_context"> static void *m_start(struct seq_file *m, loff_t *pos)</span>
 	if (!mm || !mmget_not_zero(mm))
 		return NULL;
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_lock_init_full(&amp;priv-&gt;range);</span>
<span class="p_add">+	mm_read_lock(mm-&gt;mmap_sem, &amp;priv-&gt;range);</span>
 	/* start from the Nth VMA */
 	for (p = rb_first(&amp;mm-&gt;mm_rb); p; p = rb_next(p))
 		if (n-- == 0)
 			return p;
 
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(mm-&gt;mmap_sem, &amp;priv-&gt;range);</span>
 	mmput(mm);
 	return NULL;
 }
<span class="p_chunk">@@ -240,7 +244,7 @@</span> <span class="p_context"> static void m_stop(struct seq_file *m, void *_vml)</span>
 	struct proc_maps_private *priv = m-&gt;private;
 
 	if (!IS_ERR_OR_NULL(_vml)) {
<span class="p_del">-		up_read(&amp;priv-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_unlock(priv-&gt;mm-&gt;mmap_sem, &amp;priv-&gt;range);</span>
 		mmput(priv-&gt;mm);
 	}
 	if (priv-&gt;task) {
<span class="p_header">diff --git a/fs/userfaultfd.c b/fs/userfaultfd.c</span>
<span class="p_header">index 7d56c21ef65d..2d769640b9db 100644</span>
<span class="p_header">--- a/fs/userfaultfd.c</span>
<span class="p_header">+++ b/fs/userfaultfd.c</span>
<span class="p_chunk">@@ -443,7 +443,7 @@</span> <span class="p_context"> int handle_userfault(struct vm_fault *vmf, unsigned long reason)</span>
 	else
 		must_wait = userfaultfd_huge_must_wait(ctx, vmf-&gt;address,
 						       vmf-&gt;flags, reason);
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(mm, vmf-&gt;lockrange);</span>
 
 	if (likely(must_wait &amp;&amp; !ACCESS_ONCE(ctx-&gt;released) &amp;&amp;
 		   (return_to_userland ? !signal_pending(current) :
<span class="p_chunk">@@ -497,7 +497,7 @@</span> <span class="p_context"> int handle_userfault(struct vm_fault *vmf, unsigned long reason)</span>
 			 * and there&#39;s no need to retake the mmap_sem
 			 * in such case.
 			 */
<span class="p_del">-			down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+			mm_read_lock(mm, vmf-&gt;lockrange);</span>
 			ret = VM_FAULT_NOPAGE;
 		}
 	}
<span class="p_chunk">@@ -719,7 +719,7 @@</span> <span class="p_context"> bool _userfaultfd_remove(struct vm_area_struct *vma,</span>
 		return true;
 
 	userfaultfd_ctx_get(ctx);
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(mm, range);</span>
 
 	msg_init(&amp;ewq.msg);
 
<span class="p_chunk">@@ -798,6 +798,7 @@</span> <span class="p_context"> static int userfaultfd_release(struct inode *inode, struct file *file)</span>
 	/* len == 0 means wake all */
 	struct userfaultfd_wake_range range = { .len = 0, };
 	unsigned long new_flags;
<span class="p_add">+	mm_range_define(lockrange);</span>
 
 	ACCESS_ONCE(ctx-&gt;released) = true;
 
<span class="p_chunk">@@ -812,7 +813,7 @@</span> <span class="p_context"> static int userfaultfd_release(struct inode *inode, struct file *file)</span>
 	 * it&#39;s critical that released is set to true (above), before
 	 * taking the mmap_sem for writing.
 	 */
<span class="p_del">-	down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_lock(mm, &amp;lockrange);</span>
 	prev = NULL;
 	for (vma = mm-&gt;mmap; vma; vma = vma-&gt;vm_next) {
 		cond_resched();
<span class="p_chunk">@@ -835,7 +836,7 @@</span> <span class="p_context"> static int userfaultfd_release(struct inode *inode, struct file *file)</span>
 		vma-&gt;vm_flags = new_flags;
 		vma-&gt;vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;
 	}
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(mm, &amp;lockrange);</span>
 	mmput(mm);
 wakeup:
 	/*
<span class="p_chunk">@@ -1195,6 +1196,7 @@</span> <span class="p_context"> static int userfaultfd_register(struct userfaultfd_ctx *ctx,</span>
 	bool found;
 	bool non_anon_pages;
 	unsigned long start, end, vma_end;
<span class="p_add">+	mm_range_define(range);</span>
 
 	user_uffdio_register = (struct uffdio_register __user *) arg;
 
<span class="p_chunk">@@ -1234,7 +1236,7 @@</span> <span class="p_context"> static int userfaultfd_register(struct userfaultfd_ctx *ctx,</span>
 	if (!mmget_not_zero(mm))
 		goto out;
 
<span class="p_del">-	down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_lock(mm, &amp;range);</span>
 	vma = find_vma_prev(mm, start, &amp;prev);
 	if (!vma)
 		goto out_unlock;
<span class="p_chunk">@@ -1362,7 +1364,7 @@</span> <span class="p_context"> static int userfaultfd_register(struct userfaultfd_ctx *ctx,</span>
 		vma = vma-&gt;vm_next;
 	} while (vma &amp;&amp; vma-&gt;vm_start &lt; end);
 out_unlock:
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(mm, &amp;range);</span>
 	mmput(mm);
 	if (!ret) {
 		/*
<span class="p_chunk">@@ -1390,6 +1392,7 @@</span> <span class="p_context"> static int userfaultfd_unregister(struct userfaultfd_ctx *ctx,</span>
 	bool found;
 	unsigned long start, end, vma_end;
 	const void __user *buf = (void __user *)arg;
<span class="p_add">+	mm_range_define(range);</span>
 
 	ret = -EFAULT;
 	if (copy_from_user(&amp;uffdio_unregister, buf, sizeof(uffdio_unregister)))
<span class="p_chunk">@@ -1407,7 +1410,7 @@</span> <span class="p_context"> static int userfaultfd_unregister(struct userfaultfd_ctx *ctx,</span>
 	if (!mmget_not_zero(mm))
 		goto out;
 
<span class="p_del">-	down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_lock(mm, &amp;range);</span>
 	vma = find_vma_prev(mm, start, &amp;prev);
 	if (!vma)
 		goto out_unlock;
<span class="p_chunk">@@ -1520,7 +1523,7 @@</span> <span class="p_context"> static int userfaultfd_unregister(struct userfaultfd_ctx *ctx,</span>
 		vma = vma-&gt;vm_next;
 	} while (vma &amp;&amp; vma-&gt;vm_start &lt; end);
 out_unlock:
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(mm, &amp;range);</span>
 	mmput(mm);
 out:
 	return ret;
<span class="p_header">diff --git a/ipc/shm.c b/ipc/shm.c</span>
<span class="p_header">index 34c4344e8d4b..29806da63b85 100644</span>
<span class="p_header">--- a/ipc/shm.c</span>
<span class="p_header">+++ b/ipc/shm.c</span>
<span class="p_chunk">@@ -1107,6 +1107,7 @@</span> <span class="p_context"> long do_shmat(int shmid, char __user *shmaddr, int shmflg,</span>
 	struct path path;
 	fmode_t f_mode;
 	unsigned long populate = 0;
<span class="p_add">+	mm_range_define(range);</span>
 
 	err = -EINVAL;
 	if (shmid &lt; 0)
<span class="p_chunk">@@ -1211,7 +1212,7 @@</span> <span class="p_context"> long do_shmat(int shmid, char __user *shmaddr, int shmflg,</span>
 	if (err)
 		goto out_fput;
 
<span class="p_del">-	if (down_write_killable(&amp;current-&gt;mm-&gt;mmap_sem)) {</span>
<span class="p_add">+	if (mm_write_lock_killable(current-&gt;mm, &amp;range)) {</span>
 		err = -EINTR;
 		goto out_fput;
 	}
<span class="p_chunk">@@ -1231,7 +1232,7 @@</span> <span class="p_context"> long do_shmat(int shmid, char __user *shmaddr, int shmflg,</span>
 	if (IS_ERR_VALUE(addr))
 		err = (long)addr;
 invalid:
<span class="p_del">-	up_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(current-&gt;mm, &amp;range);</span>
 	if (populate)
 		mm_populate(addr, populate);
 
<span class="p_chunk">@@ -1282,11 +1283,12 @@</span> <span class="p_context"> SYSCALL_DEFINE1(shmdt, char __user *, shmaddr)</span>
 	struct file *file;
 	struct vm_area_struct *next;
 #endif
<span class="p_add">+	mm_range_define(range);</span>
 
 	if (addr &amp; ~PAGE_MASK)
 		return retval;
 
<span class="p_del">-	if (down_write_killable(&amp;mm-&gt;mmap_sem))</span>
<span class="p_add">+	if (mm_write_lock_killable(mm, &amp;range))</span>
 		return -EINTR;
 
 	/*
<span class="p_chunk">@@ -1374,7 +1376,7 @@</span> <span class="p_context"> SYSCALL_DEFINE1(shmdt, char __user *, shmaddr)</span>
 
 #endif
 
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(mm, &amp;range);</span>
 	return retval;
 }
 
<span class="p_header">diff --git a/kernel/acct.c b/kernel/acct.c</span>
<span class="p_header">index 5b1284370367..5a3c7fe2ddd1 100644</span>
<span class="p_header">--- a/kernel/acct.c</span>
<span class="p_header">+++ b/kernel/acct.c</span>
<span class="p_chunk">@@ -537,14 +537,15 @@</span> <span class="p_context"> void acct_collect(long exitcode, int group_dead)</span>
 
 	if (group_dead &amp;&amp; current-&gt;mm) {
 		struct vm_area_struct *vma;
<span class="p_add">+		mm_range_define(range);</span>
 
<span class="p_del">-		down_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_lock(current-&gt;mm, &amp;range);</span>
 		vma = current-&gt;mm-&gt;mmap;
 		while (vma) {
 			vsize += vma-&gt;vm_end - vma-&gt;vm_start;
 			vma = vma-&gt;vm_next;
 		}
<span class="p_del">-		up_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_unlock(current-&gt;mm, &amp;range);</span>
 	}
 
 	spin_lock_irq(&amp;current-&gt;sighand-&gt;siglock);
<span class="p_header">diff --git a/kernel/events/core.c b/kernel/events/core.c</span>
<span class="p_header">index 6e75a5c9412d..ec06c764742d 100644</span>
<span class="p_header">--- a/kernel/events/core.c</span>
<span class="p_header">+++ b/kernel/events/core.c</span>
<span class="p_chunk">@@ -8223,6 +8223,7 @@</span> <span class="p_context"> static void perf_event_addr_filters_apply(struct perf_event *event)</span>
 	struct mm_struct *mm = NULL;
 	unsigned int count = 0;
 	unsigned long flags;
<span class="p_add">+	mm_range_define(range);</span>
 
 	/*
 	 * We may observe TASK_TOMBSTONE, which means that the event tear-down
<span class="p_chunk">@@ -8238,7 +8239,7 @@</span> <span class="p_context"> static void perf_event_addr_filters_apply(struct perf_event *event)</span>
 	if (!mm)
 		goto restart;
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_lock(mm, &amp;range);</span>
 
 	raw_spin_lock_irqsave(&amp;ifh-&gt;lock, flags);
 	list_for_each_entry(filter, &amp;ifh-&gt;list, entry) {
<span class="p_chunk">@@ -8258,7 +8259,7 @@</span> <span class="p_context"> static void perf_event_addr_filters_apply(struct perf_event *event)</span>
 	event-&gt;addr_filters_gen++;
 	raw_spin_unlock_irqrestore(&amp;ifh-&gt;lock, flags);
 
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(mm, &amp;range);</span>
 
 	mmput(mm);
 
<span class="p_header">diff --git a/kernel/events/uprobes.c b/kernel/events/uprobes.c</span>
<span class="p_header">index dc2e5f7a8bb8..9cb52b895cb0 100644</span>
<span class="p_header">--- a/kernel/events/uprobes.c</span>
<span class="p_header">+++ b/kernel/events/uprobes.c</span>
<span class="p_chunk">@@ -806,11 +806,12 @@</span> <span class="p_context"> register_for_each_vma(struct uprobe *uprobe, struct uprobe_consumer *new)</span>
 	while (info) {
 		struct mm_struct *mm = info-&gt;mm;
 		struct vm_area_struct *vma;
<span class="p_add">+		mm_range_define(range);</span>
 
 		if (err &amp;&amp; is_register)
 			goto free;
 
<span class="p_del">-		down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_write_lock(mm, &amp;range);</span>
 		vma = find_vma(mm, info-&gt;vaddr);
 		if (!vma || !valid_vma(vma, is_register) ||
 		    file_inode(vma-&gt;vm_file) != uprobe-&gt;inode)
<span class="p_chunk">@@ -832,7 +833,7 @@</span> <span class="p_context"> register_for_each_vma(struct uprobe *uprobe, struct uprobe_consumer *new)</span>
 		}
 
  unlock:
<span class="p_del">-		up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_write_unlock(mm, &amp;range);</span>
  free:
 		mmput(mm);
 		info = free_map_info(info);
<span class="p_chunk">@@ -972,8 +973,9 @@</span> <span class="p_context"> static int unapply_uprobe(struct uprobe *uprobe, struct mm_struct *mm)</span>
 {
 	struct vm_area_struct *vma;
 	int err = 0;
<span class="p_add">+	mm_range_define(range);</span>
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_lock(mm, &amp;range);</span>
 	for (vma = mm-&gt;mmap; vma; vma = vma-&gt;vm_next) {
 		unsigned long vaddr;
 		loff_t offset;
<span class="p_chunk">@@ -990,7 +992,7 @@</span> <span class="p_context"> static int unapply_uprobe(struct uprobe *uprobe, struct mm_struct *mm)</span>
 		vaddr = offset_to_vaddr(vma, uprobe-&gt;offset);
 		err |= remove_breakpoint(uprobe, mm, vaddr);
 	}
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(mm, &amp;range);</span>
 
 	return err;
 }
<span class="p_chunk">@@ -1139,8 +1141,9 @@</span> <span class="p_context"> static int xol_add_vma(struct mm_struct *mm, struct xol_area *area)</span>
 {
 	struct vm_area_struct *vma;
 	int ret;
<span class="p_add">+	mm_range_define(range);</span>
 
<span class="p_del">-	if (down_write_killable(&amp;mm-&gt;mmap_sem))</span>
<span class="p_add">+	if (mm_write_lock_killable(mm, &amp;range))</span>
 		return -EINTR;
 
 	if (mm-&gt;uprobes_state.xol_area) {
<span class="p_chunk">@@ -1170,7 +1173,7 @@</span> <span class="p_context"> static int xol_add_vma(struct mm_struct *mm, struct xol_area *area)</span>
 	smp_wmb();	/* pairs with get_xol_area() */
 	mm-&gt;uprobes_state.xol_area = area;
  fail:
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(mm, &amp;range);</span>
 
 	return ret;
 }
<span class="p_chunk">@@ -1736,8 +1739,9 @@</span> <span class="p_context"> static struct uprobe *find_active_uprobe(unsigned long bp_vaddr, int *is_swbp)</span>
 	struct mm_struct *mm = current-&gt;mm;
 	struct uprobe *uprobe = NULL;
 	struct vm_area_struct *vma;
<span class="p_add">+	mm_range_define(range);</span>
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_lock(mm, &amp;range);</span>
 	vma = find_vma(mm, bp_vaddr);
 	if (vma &amp;&amp; vma-&gt;vm_start &lt;= bp_vaddr) {
 		if (valid_vma(vma, false)) {
<span class="p_chunk">@@ -1755,7 +1759,7 @@</span> <span class="p_context"> static struct uprobe *find_active_uprobe(unsigned long bp_vaddr, int *is_swbp)</span>
 
 	if (!uprobe &amp;&amp; test_and_clear_bit(MMF_RECALC_UPROBES, &amp;mm-&gt;flags))
 		mmf_recalc_uprobes(mm);
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(mm, &amp;range);</span>
 
 	return uprobe;
 }
<span class="p_header">diff --git a/kernel/exit.c b/kernel/exit.c</span>
<span class="p_header">index 516acdb0e0ec..f2f6c99ffd0f 100644</span>
<span class="p_header">--- a/kernel/exit.c</span>
<span class="p_header">+++ b/kernel/exit.c</span>
<span class="p_chunk">@@ -508,6 +508,7 @@</span> <span class="p_context"> static void exit_mm(void)</span>
 {
 	struct mm_struct *mm = current-&gt;mm;
 	struct core_state *core_state;
<span class="p_add">+	mm_range_define(range);</span>
 
 	mm_release(current, mm);
 	if (!mm)
<span class="p_chunk">@@ -520,12 +521,12 @@</span> <span class="p_context"> static void exit_mm(void)</span>
 	 * will increment -&gt;nr_threads for each thread in the
 	 * group with -&gt;mm != NULL.
 	 */
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_lock(mm, &amp;range);</span>
 	core_state = mm-&gt;core_state;
 	if (core_state) {
 		struct core_thread self;
 
<span class="p_del">-		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_unlock(mm, &amp;range);</span>
 
 		self.task = current;
 		self.next = xchg(&amp;core_state-&gt;dumper.next, &amp;self);
<span class="p_chunk">@@ -543,14 +544,14 @@</span> <span class="p_context"> static void exit_mm(void)</span>
 			freezable_schedule();
 		}
 		__set_current_state(TASK_RUNNING);
<span class="p_del">-		down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_lock(mm, &amp;range);</span>
 	}
 	mmgrab(mm);
 	BUG_ON(mm != current-&gt;active_mm);
 	/* more a memory barrier than a real lock */
 	task_lock(current);
 	current-&gt;mm = NULL;
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(mm, &amp;range);</span>
 	enter_lazy_tlb(mm, current);
 	task_unlock(current);
 	mm_update_next_owner(mm);
<span class="p_header">diff --git a/kernel/fork.c b/kernel/fork.c</span>
<span class="p_header">index aa1076c5e4a9..d9696585d125 100644</span>
<span class="p_header">--- a/kernel/fork.c</span>
<span class="p_header">+++ b/kernel/fork.c</span>
<span class="p_chunk">@@ -597,9 +597,11 @@</span> <span class="p_context"> static __latent_entropy int dup_mmap(struct mm_struct *mm,</span>
 	int retval;
 	unsigned long charge;
 	LIST_HEAD(uf);
<span class="p_add">+	mm_range_define(range);</span>
<span class="p_add">+	mm_range_define(oldrange);</span>
 
 	uprobe_start_dup_mmap();
<span class="p_del">-	if (down_write_killable(&amp;oldmm-&gt;mmap_sem)) {</span>
<span class="p_add">+	if (mm_write_lock_killable(oldmm, &amp;oldrange)) {</span>
 		retval = -EINTR;
 		goto fail_uprobe_end;
 	}
<span class="p_chunk">@@ -608,7 +610,7 @@</span> <span class="p_context"> static __latent_entropy int dup_mmap(struct mm_struct *mm,</span>
 	/*
 	 * Not linked in yet - no deadlock potential:
 	 */
<span class="p_del">-	down_write_nested(&amp;mm-&gt;mmap_sem, SINGLE_DEPTH_NESTING);</span>
<span class="p_add">+	mm_write_lock(mm, &amp;range);</span>
 
 	/* No ordering required: file already has been exposed. */
 	RCU_INIT_POINTER(mm-&gt;exe_file, get_mm_exe_file(oldmm));
<span class="p_chunk">@@ -712,9 +714,9 @@</span> <span class="p_context"> static __latent_entropy int dup_mmap(struct mm_struct *mm,</span>
 	arch_dup_mmap(oldmm, mm);
 	retval = 0;
 out:
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(mm, &amp;range);</span>
 	flush_tlb_mm(oldmm);
<span class="p_del">-	up_write(&amp;oldmm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(oldmm, &amp;oldrange);</span>
 	dup_userfaultfd_complete(&amp;uf);
 fail_uprobe_end:
 	uprobe_end_dup_mmap();
<span class="p_chunk">@@ -744,9 +746,11 @@</span> <span class="p_context"> static inline void mm_free_pgd(struct mm_struct *mm)</span>
 #else
 static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 {
<span class="p_del">-	down_write(&amp;oldmm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_range_define(oldrange);</span>
<span class="p_add">+</span>
<span class="p_add">+	mm_write_lock(oldmm, &amp;oldrange);</span>
 	RCU_INIT_POINTER(mm-&gt;exe_file, get_mm_exe_file(oldmm));
<span class="p_del">-	up_write(&amp;oldmm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(oldmm, &amp;oldrange);</span>
 	return 0;
 }
 #define mm_alloc_pgd(mm)	(0)
<span class="p_chunk">@@ -795,7 +799,11 @@</span> <span class="p_context"> static struct mm_struct *mm_init(struct mm_struct *mm, struct task_struct *p,</span>
 	mm-&gt;vmacache_seqnum = 0;
 	atomic_set(&amp;mm-&gt;mm_users, 1);
 	atomic_set(&amp;mm-&gt;mm_count, 1);
<span class="p_add">+#ifdef CONFIG_MEM_RANGE_LOCK</span>
<span class="p_add">+	range_lock_tree_init(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+#else</span>
 	init_rwsem(&amp;mm-&gt;mmap_sem);
<span class="p_add">+#endif</span>
 	INIT_LIST_HEAD(&amp;mm-&gt;mmlist);
 	mm-&gt;core_state = NULL;
 	atomic_long_set(&amp;mm-&gt;nr_ptes, 0);
<span class="p_header">diff --git a/kernel/futex.c b/kernel/futex.c</span>
<span class="p_header">index 531a497eefbd..27d88340d3e4 100644</span>
<span class="p_header">--- a/kernel/futex.c</span>
<span class="p_header">+++ b/kernel/futex.c</span>
<span class="p_chunk">@@ -724,11 +724,12 @@</span> <span class="p_context"> static int fault_in_user_writeable(u32 __user *uaddr)</span>
 {
 	struct mm_struct *mm = current-&gt;mm;
 	int ret;
<span class="p_add">+	mm_range_define(range);</span>
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_lock(mm, &amp;range);</span>
 	ret = fixup_user_fault(current, mm, (unsigned long)uaddr,
<span class="p_del">-			       FAULT_FLAG_WRITE, NULL, NULL);</span>
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+			       FAULT_FLAG_WRITE, NULL, &amp;range);</span>
<span class="p_add">+	mm_read_unlock(mm, &amp;range);</span>
 
 	return ret &lt; 0 ? ret : 0;
 }
<span class="p_header">diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c</span>
<span class="p_header">index d71109321841..e2481d73635c 100644</span>
<span class="p_header">--- a/kernel/sched/fair.c</span>
<span class="p_header">+++ b/kernel/sched/fair.c</span>
<span class="p_chunk">@@ -2419,6 +2419,7 @@</span> <span class="p_context"> void task_numa_work(struct callback_head *work)</span>
 	unsigned long start, end;
 	unsigned long nr_pte_updates = 0;
 	long pages, virtpages;
<span class="p_add">+	mm_range_define(range);</span>
 
 	SCHED_WARN_ON(p != container_of(work, struct task_struct, numa_work));
 
<span class="p_chunk">@@ -2468,8 +2469,7 @@</span> <span class="p_context"> void task_numa_work(struct callback_head *work)</span>
 	if (!pages)
 		return;
 
<span class="p_del">-</span>
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_lock(mm, &amp;range);</span>
 	vma = find_vma(mm, start);
 	if (!vma) {
 		reset_ptenuma_scan(p);
<span class="p_chunk">@@ -2536,7 +2536,7 @@</span> <span class="p_context"> void task_numa_work(struct callback_head *work)</span>
 		mm-&gt;numa_scan_offset = start;
 	else
 		reset_ptenuma_scan(p);
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(mm, &amp;range);</span>
 
 	/*
 	 * Make sure tasks use at least 32x as much time to run other code
<span class="p_header">diff --git a/kernel/sys.c b/kernel/sys.c</span>
<span class="p_header">index 8a94b4eabcaa..da53c7bc50c1 100644</span>
<span class="p_header">--- a/kernel/sys.c</span>
<span class="p_header">+++ b/kernel/sys.c</span>
<span class="p_chunk">@@ -1668,6 +1668,7 @@</span> <span class="p_context"> static int prctl_set_mm_exe_file(struct mm_struct *mm, unsigned int fd)</span>
 	struct file *old_exe, *exe_file;
 	struct inode *inode;
 	int err;
<span class="p_add">+	mm_range_define(range);</span>
 
 	exe = fdget(fd);
 	if (!exe.file)
<span class="p_chunk">@@ -1696,7 +1697,7 @@</span> <span class="p_context"> static int prctl_set_mm_exe_file(struct mm_struct *mm, unsigned int fd)</span>
 	if (exe_file) {
 		struct vm_area_struct *vma;
 
<span class="p_del">-		down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_lock(mm, &amp;range);</span>
 		for (vma = mm-&gt;mmap; vma; vma = vma-&gt;vm_next) {
 			if (!vma-&gt;vm_file)
 				continue;
<span class="p_chunk">@@ -1705,7 +1706,7 @@</span> <span class="p_context"> static int prctl_set_mm_exe_file(struct mm_struct *mm, unsigned int fd)</span>
 				goto exit_err;
 		}
 
<span class="p_del">-		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_unlock(mm, &amp;range);</span>
 		fput(exe_file);
 	}
 
<span class="p_chunk">@@ -1719,7 +1720,7 @@</span> <span class="p_context"> static int prctl_set_mm_exe_file(struct mm_struct *mm, unsigned int fd)</span>
 	fdput(exe);
 	return err;
 exit_err:
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(mm, &amp;range);</span>
 	fput(exe_file);
 	goto exit;
 }
<span class="p_chunk">@@ -1826,6 +1827,7 @@</span> <span class="p_context"> static int prctl_set_mm_map(int opt, const void __user *addr, unsigned long data</span>
 	unsigned long user_auxv[AT_VECTOR_SIZE];
 	struct mm_struct *mm = current-&gt;mm;
 	int error;
<span class="p_add">+	mm_range_define(range);</span>
 
 	BUILD_BUG_ON(sizeof(user_auxv) != sizeof(mm-&gt;saved_auxv));
 	BUILD_BUG_ON(sizeof(struct prctl_mm_map) &gt; 256);
<span class="p_chunk">@@ -1862,7 +1864,7 @@</span> <span class="p_context"> static int prctl_set_mm_map(int opt, const void __user *addr, unsigned long data</span>
 			return error;
 	}
 
<span class="p_del">-	down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_lock(mm, &amp;range);</span>
 
 	/*
 	 * We don&#39;t validate if these members are pointing to
<span class="p_chunk">@@ -1899,7 +1901,7 @@</span> <span class="p_context"> static int prctl_set_mm_map(int opt, const void __user *addr, unsigned long data</span>
 	if (prctl_map.auxv_size)
 		memcpy(mm-&gt;saved_auxv, user_auxv, sizeof(user_auxv));
 
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(mm, &amp;range);</span>
 	return 0;
 }
 #endif /* CONFIG_CHECKPOINT_RESTORE */
<span class="p_chunk">@@ -1941,6 +1943,7 @@</span> <span class="p_context"> static int prctl_set_mm(int opt, unsigned long addr,</span>
 	struct prctl_mm_map prctl_map;
 	struct vm_area_struct *vma;
 	int error;
<span class="p_add">+	mm_range_define(range);</span>
 
 	if (arg5 || (arg4 &amp;&amp; (opt != PR_SET_MM_AUXV &amp;&amp;
 			      opt != PR_SET_MM_MAP &amp;&amp;
<span class="p_chunk">@@ -1966,7 +1969,7 @@</span> <span class="p_context"> static int prctl_set_mm(int opt, unsigned long addr,</span>
 
 	error = -EINVAL;
 
<span class="p_del">-	down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_lock(mm, &amp;range);</span>
 	vma = find_vma(mm, addr);
 
 	prctl_map.start_code	= mm-&gt;start_code;
<span class="p_chunk">@@ -2059,7 +2062,7 @@</span> <span class="p_context"> static int prctl_set_mm(int opt, unsigned long addr,</span>
 
 	error = 0;
 out:
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(mm, &amp;range);</span>
 	return error;
 }
 
<span class="p_chunk">@@ -2099,6 +2102,7 @@</span> <span class="p_context"> SYSCALL_DEFINE5(prctl, int, option, unsigned long, arg2, unsigned long, arg3,</span>
 	struct task_struct *me = current;
 	unsigned char comm[sizeof(me-&gt;comm)];
 	long error;
<span class="p_add">+	mm_range_define(range);</span>
 
 	error = security_task_prctl(option, arg2, arg3, arg4, arg5);
 	if (error != -ENOSYS)
<span class="p_chunk">@@ -2271,13 +2275,13 @@</span> <span class="p_context"> SYSCALL_DEFINE5(prctl, int, option, unsigned long, arg2, unsigned long, arg3,</span>
 	case PR_SET_THP_DISABLE:
 		if (arg3 || arg4 || arg5)
 			return -EINVAL;
<span class="p_del">-		if (down_write_killable(&amp;me-&gt;mm-&gt;mmap_sem))</span>
<span class="p_add">+		if (mm_write_lock_killable(me-&gt;mm, &amp;range))</span>
 			return -EINTR;
 		if (arg2)
 			me-&gt;mm-&gt;def_flags |= VM_NOHUGEPAGE;
 		else
 			me-&gt;mm-&gt;def_flags &amp;= ~VM_NOHUGEPAGE;
<span class="p_del">-		up_write(&amp;me-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_write_unlock(me-&gt;mm, &amp;range);</span>
 		break;
 	case PR_MPX_ENABLE_MANAGEMENT:
 		if (arg2 || arg3 || arg4 || arg5)
<span class="p_header">diff --git a/kernel/trace/trace_output.c b/kernel/trace/trace_output.c</span>
<span class="p_header">index 08f9bab8089e..a8ebb73aff25 100644</span>
<span class="p_header">--- a/kernel/trace/trace_output.c</span>
<span class="p_header">+++ b/kernel/trace/trace_output.c</span>
<span class="p_chunk">@@ -379,6 +379,7 @@</span> <span class="p_context"> static int seq_print_user_ip(struct trace_seq *s, struct mm_struct *mm,</span>
 	struct file *file = NULL;
 	unsigned long vmstart = 0;
 	int ret = 1;
<span class="p_add">+	mm_range_define(range);</span>
 
 	if (s-&gt;full)
 		return 0;
<span class="p_chunk">@@ -386,7 +387,7 @@</span> <span class="p_context"> static int seq_print_user_ip(struct trace_seq *s, struct mm_struct *mm,</span>
 	if (mm) {
 		const struct vm_area_struct *vma;
 
<span class="p_del">-		down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_lock(mm, &amp;range);</span>
 		vma = find_vma(mm, ip);
 		if (vma) {
 			file = vma-&gt;vm_file;
<span class="p_chunk">@@ -398,7 +399,7 @@</span> <span class="p_context"> static int seq_print_user_ip(struct trace_seq *s, struct mm_struct *mm,</span>
 				trace_seq_printf(s, &quot;[+0x%lx]&quot;,
 						 ip - vmstart);
 		}
<span class="p_del">-		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_unlock(mm, &amp;range);</span>
 	}
 	if (ret &amp;&amp; ((sym_flags &amp; TRACE_ITER_SYM_ADDR) || !file))
 		trace_seq_printf(s, &quot; &lt;&quot; IP_FMT &quot;&gt;&quot;, ip);
<span class="p_header">diff --git a/mm/filemap.c b/mm/filemap.c</span>
<span class="p_header">index adb7c15b8aa4..e593ebadaf7e 100644</span>
<span class="p_header">--- a/mm/filemap.c</span>
<span class="p_header">+++ b/mm/filemap.c</span>
<span class="p_chunk">@@ -1067,7 +1067,7 @@</span> <span class="p_context"> int __lock_page_or_retry(struct page *page, struct mm_struct *mm,</span>
 		if (flags &amp; FAULT_FLAG_RETRY_NOWAIT)
 			return 0;
 
<span class="p_del">-		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_unlock(mm, range);</span>
 		if (flags &amp; FAULT_FLAG_KILLABLE)
 			wait_on_page_locked_killable(page);
 		else
<span class="p_chunk">@@ -1079,7 +1079,7 @@</span> <span class="p_context"> int __lock_page_or_retry(struct page *page, struct mm_struct *mm,</span>
 
 			ret = __lock_page_killable(page);
 			if (ret) {
<span class="p_del">-				up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+				mm_read_unlock(mm, range);</span>
 				return 0;
 			}
 		} else
<span class="p_header">diff --git a/mm/frame_vector.c b/mm/frame_vector.c</span>
<span class="p_header">index d2c1675ff466..e93dd7675510 100644</span>
<span class="p_header">--- a/mm/frame_vector.c</span>
<span class="p_header">+++ b/mm/frame_vector.c</span>
<span class="p_chunk">@@ -38,6 +38,7 @@</span> <span class="p_context"> int get_vaddr_frames(unsigned long start, unsigned int nr_frames,</span>
 	int ret = 0;
 	int err;
 	int locked;
<span class="p_add">+	mm_range_define(range);</span>
 
 	if (nr_frames == 0)
 		return 0;
<span class="p_chunk">@@ -45,7 +46,7 @@</span> <span class="p_context"> int get_vaddr_frames(unsigned long start, unsigned int nr_frames,</span>
 	if (WARN_ON_ONCE(nr_frames &gt; vec-&gt;nr_allocated))
 		nr_frames = vec-&gt;nr_allocated;
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_lock(mm, &amp;range);</span>
 	locked = 1;
 	vma = find_vma_intersection(mm, start, start + 1);
 	if (!vma) {
<span class="p_chunk">@@ -56,7 +57,8 @@</span> <span class="p_context"> int get_vaddr_frames(unsigned long start, unsigned int nr_frames,</span>
 		vec-&gt;got_ref = true;
 		vec-&gt;is_pfns = false;
 		ret = get_user_pages_locked(start, nr_frames,
<span class="p_del">-			gup_flags, (struct page **)(vec-&gt;ptrs), &amp;locked, NULL);</span>
<span class="p_add">+			gup_flags, (struct page **)(vec-&gt;ptrs),</span>
<span class="p_add">+			&amp;locked, &amp;range);</span>
 		goto out;
 	}
 
<span class="p_chunk">@@ -85,7 +87,7 @@</span> <span class="p_context"> int get_vaddr_frames(unsigned long start, unsigned int nr_frames,</span>
 	} while (vma &amp;&amp; vma-&gt;vm_flags &amp; (VM_IO | VM_PFNMAP));
 out:
 	if (locked)
<span class="p_del">-		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_unlock(mm, &amp;range);</span>
 	if (!ret)
 		ret = -EFAULT;
 	if (ret &gt; 0)
<span class="p_header">diff --git a/mm/gup.c b/mm/gup.c</span>
<span class="p_header">index 3a8ba8cfae3f..d308173af11b 100644</span>
<span class="p_header">--- a/mm/gup.c</span>
<span class="p_header">+++ b/mm/gup.c</span>
<span class="p_chunk">@@ -750,7 +750,7 @@</span> <span class="p_context"> int _fixup_user_fault(struct task_struct *tsk, struct mm_struct *mm,</span>
 	}
 
 	if (ret &amp; VM_FAULT_RETRY) {
<span class="p_del">-		down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_lock(mm, range);</span>
 		if (!(fault_flags &amp; FAULT_FLAG_TRIED)) {
 			*unlocked = true;
 			fault_flags &amp;= ~FAULT_FLAG_ALLOW_RETRY;
<span class="p_chunk">@@ -840,7 +840,7 @@</span> <span class="p_context"> static __always_inline long __get_user_pages_locked(struct task_struct *tsk,</span>
 		 */
 		*locked = 1;
 		lock_dropped = true;
<span class="p_del">-		down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_lock(mm, range);</span>
 		ret = __get_user_pages(tsk, mm, start, 1, flags | FOLL_TRIED,
 				       pages, NULL, NULL
 #ifdef CONFIG_MEM_RANGE_LOCK
<span class="p_chunk">@@ -865,7 +865,7 @@</span> <span class="p_context"> static __always_inline long __get_user_pages_locked(struct task_struct *tsk,</span>
 		 * We must let the caller know we temporarily dropped the lock
 		 * and so the critical section protected by it was lost.
 		 */
<span class="p_del">-		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_unlock(mm, range);</span>
 		*locked = 0;
 	}
 	return pages_done;
<span class="p_chunk">@@ -920,8 +920,9 @@</span> <span class="p_context"> static __always_inline long __get_user_pages_unlocked(struct task_struct *tsk,</span>
 {
 	long ret;
 	int locked = 1;
<span class="p_add">+	mm_range_define(range);</span>
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_lock(mm, &amp;range);</span>
 	ret = __get_user_pages_locked(tsk, mm, start, nr_pages, pages, NULL,
 				      &amp;locked, false,
 #ifdef CONFIG_MEM_RANGE_LOCK
<span class="p_chunk">@@ -929,7 +930,7 @@</span> <span class="p_context"> static __always_inline long __get_user_pages_unlocked(struct task_struct *tsk,</span>
 #endif
 				      gup_flags);
 	if (locked)
<span class="p_del">-		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_unlock(mm, &amp;range);</span>
 	return ret;
 }
 
<span class="p_chunk">@@ -1137,6 +1138,7 @@</span> <span class="p_context"> int __mm_populate(unsigned long start, unsigned long len, int ignore_errors)</span>
 	struct vm_area_struct *vma = NULL;
 	int locked = 0;
 	long ret = 0;
<span class="p_add">+	mm_range_define(range);</span>
 
 	VM_BUG_ON(start &amp; ~PAGE_MASK);
 	VM_BUG_ON(len != PAGE_ALIGN(len));
<span class="p_chunk">@@ -1149,7 +1151,7 @@</span> <span class="p_context"> int __mm_populate(unsigned long start, unsigned long len, int ignore_errors)</span>
 		 */
 		if (!locked) {
 			locked = 1;
<span class="p_del">-			down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+			mm_read_lock(mm, &amp;range);</span>
 			vma = find_vma(mm, nstart);
 		} else if (nstart &gt;= vma-&gt;vm_end)
 			vma = vma-&gt;vm_next;
<span class="p_chunk">@@ -1170,7 +1172,7 @@</span> <span class="p_context"> int __mm_populate(unsigned long start, unsigned long len, int ignore_errors)</span>
 		 * if the vma was already munlocked.
 		 */
 		ret = populate_vma_page_range(vma, nstart, nend, &amp;locked,
<span class="p_del">-					      NULL);</span>
<span class="p_add">+					      &amp;range);</span>
 		if (ret &lt; 0) {
 			if (ignore_errors) {
 				ret = 0;
<span class="p_chunk">@@ -1182,7 +1184,7 @@</span> <span class="p_context"> int __mm_populate(unsigned long start, unsigned long len, int ignore_errors)</span>
 		ret = 0;
 	}
 	if (locked)
<span class="p_del">-		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_unlock(mm, &amp;range);</span>
 	return ret;	/* 0 or negative error code */
 }
 
<span class="p_header">diff --git a/mm/init-mm.c b/mm/init-mm.c</span>
<span class="p_header">index 975e49f00f34..9e8c84a0ee24 100644</span>
<span class="p_header">--- a/mm/init-mm.c</span>
<span class="p_header">+++ b/mm/init-mm.c</span>
<span class="p_chunk">@@ -19,7 +19,11 @@</span> <span class="p_context"> struct mm_struct init_mm = {</span>
 	.pgd		= swapper_pg_dir,
 	.mm_users	= ATOMIC_INIT(2),
 	.mm_count	= ATOMIC_INIT(1),
<span class="p_add">+#ifdef CONFIG_MEM_RANGE_LOCK</span>
<span class="p_add">+	.mmap_sem	= __RANGE_LOCK_TREE_INITIALIZER(init_mm.mmap_sem),</span>
<span class="p_add">+#else</span>
 	.mmap_sem	= __RWSEM_INITIALIZER(init_mm.mmap_sem),
<span class="p_add">+#endif</span>
 	.page_table_lock =  __SPIN_LOCK_UNLOCKED(init_mm.page_table_lock),
 	.mmlist		= LIST_HEAD_INIT(init_mm.mmlist),
 	.user_ns	= &amp;init_user_ns,
<span class="p_header">diff --git a/mm/khugepaged.c b/mm/khugepaged.c</span>
<span class="p_header">index 6357f32608a5..f668f73fa19e 100644</span>
<span class="p_header">--- a/mm/khugepaged.c</span>
<span class="p_header">+++ b/mm/khugepaged.c</span>
<span class="p_chunk">@@ -453,6 +453,7 @@</span> <span class="p_context"> void __khugepaged_exit(struct mm_struct *mm)</span>
 {
 	struct mm_slot *mm_slot;
 	int free = 0;
<span class="p_add">+	mm_range_define(range);</span>
 
 	spin_lock(&amp;khugepaged_mm_lock);
 	mm_slot = get_mm_slot(mm);
<span class="p_chunk">@@ -476,8 +477,8 @@</span> <span class="p_context"> void __khugepaged_exit(struct mm_struct *mm)</span>
 		 * khugepaged has finished working on the pagetables
 		 * under the mmap_sem.
 		 */
<span class="p_del">-		down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_del">-		up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_write_lock(mm, &amp;range);</span>
<span class="p_add">+		mm_write_unlock(mm, &amp;range);</span>
 	}
 }
 
<span class="p_chunk">@@ -906,7 +907,7 @@</span> <span class="p_context"> static bool __collapse_huge_page_swapin(struct mm_struct *mm,</span>
 
 		/* do_swap_page returns VM_FAULT_RETRY with released mmap_sem */
 		if (ret &amp; VM_FAULT_RETRY) {
<span class="p_del">-			down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+			mm_read_lock(mm, range);</span>
 			if (hugepage_vma_revalidate(mm, address, &amp;vmf.vma)) {
 				/* vma is no longer available, don&#39;t continue to swapin */
 				trace_mm_collapse_huge_page_swapin(mm, swapped_in, referenced, 0);
<span class="p_chunk">@@ -963,7 +964,7 @@</span> <span class="p_context"> static void collapse_huge_page(struct mm_struct *mm,</span>
 	 * sync compaction, and we do not need to hold the mmap_sem during
 	 * that. We will recheck the vma after taking it again in write mode.
 	 */
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(mm, range);</span>
 	new_page = khugepaged_alloc_page(hpage, gfp, node);
 	if (!new_page) {
 		result = SCAN_ALLOC_HUGE_PAGE_FAIL;
<span class="p_chunk">@@ -975,11 +976,11 @@</span> <span class="p_context"> static void collapse_huge_page(struct mm_struct *mm,</span>
 		goto out_nolock;
 	}
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_lock(mm, range);</span>
 	result = hugepage_vma_revalidate(mm, address, &amp;vma);
 	if (result) {
 		mem_cgroup_cancel_charge(new_page, memcg, true);
<span class="p_del">-		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_unlock(mm, range);</span>
 		goto out_nolock;
 	}
 
<span class="p_chunk">@@ -987,7 +988,7 @@</span> <span class="p_context"> static void collapse_huge_page(struct mm_struct *mm,</span>
 	if (!pmd) {
 		result = SCAN_PMD_NULL;
 		mem_cgroup_cancel_charge(new_page, memcg, true);
<span class="p_del">-		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_unlock(mm, range);</span>
 		goto out_nolock;
 	}
 
<span class="p_chunk">@@ -1002,17 +1003,17 @@</span> <span class="p_context"> static void collapse_huge_page(struct mm_struct *mm,</span>
 #endif
 		    )) {
 		mem_cgroup_cancel_charge(new_page, memcg, true);
<span class="p_del">-		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_unlock(mm, range);</span>
 		goto out_nolock;
 	}
 
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(mm, range);</span>
 	/*
 	 * Prevent all access to pagetables with the exception of
 	 * gup_fast later handled by the ptep_clear_flush and the VM
 	 * handled by the anon_vma lock + PG_lock.
 	 */
<span class="p_del">-	down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_lock(mm, range);</span>
 	result = hugepage_vma_revalidate(mm, address, &amp;vma);
 	if (result)
 		goto out;
<span class="p_chunk">@@ -1095,7 +1096,7 @@</span> <span class="p_context"> static void collapse_huge_page(struct mm_struct *mm,</span>
 	khugepaged_pages_collapsed++;
 	result = SCAN_SUCCEED;
 out_up_write:
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(mm, range);</span>
 out_nolock:
 	trace_mm_collapse_huge_page(mm, isolated, result);
 	return;
<span class="p_chunk">@@ -1266,6 +1267,7 @@</span> <span class="p_context"> static void retract_page_tables(struct address_space *mapping, pgoff_t pgoff)</span>
 	struct vm_area_struct *vma;
 	unsigned long addr;
 	pmd_t *pmd, _pmd;
<span class="p_add">+	mm_range_define(range);</span>
 
 	i_mmap_lock_write(mapping);
 	vma_interval_tree_foreach(vma, &amp;mapping-&gt;i_mmap, pgoff, pgoff) {
<span class="p_chunk">@@ -1286,12 +1288,12 @@</span> <span class="p_context"> static void retract_page_tables(struct address_space *mapping, pgoff_t pgoff)</span>
 		 * re-fault. Not ideal, but it&#39;s more important to not disturb
 		 * the system too much.
 		 */
<span class="p_del">-		if (down_write_trylock(&amp;vma-&gt;vm_mm-&gt;mmap_sem)) {</span>
<span class="p_add">+		if (mm_write_trylock(vma-&gt;vm_mm, &amp;range)) {</span>
 			spinlock_t *ptl = pmd_lock(vma-&gt;vm_mm, pmd);
 			/* assume page table is clear */
 			_pmd = pmdp_collapse_flush(vma, addr, pmd);
 			spin_unlock(ptl);
<span class="p_del">-			up_write(&amp;vma-&gt;vm_mm-&gt;mmap_sem);</span>
<span class="p_add">+			mm_write_unlock(vma-&gt;vm_mm, &amp;range);</span>
 			atomic_long_dec(&amp;vma-&gt;vm_mm-&gt;nr_ptes);
 			pte_free(vma-&gt;vm_mm, pmd_pgtable(_pmd));
 		}
<span class="p_chunk">@@ -1681,6 +1683,7 @@</span> <span class="p_context"> static unsigned int khugepaged_scan_mm_slot(unsigned int pages,</span>
 	struct mm_struct *mm;
 	struct vm_area_struct *vma;
 	int progress = 0;
<span class="p_add">+	mm_range_define(range);</span>
 
 	VM_BUG_ON(!pages);
 	VM_BUG_ON(NR_CPUS != 1 &amp;&amp; !spin_is_locked(&amp;khugepaged_mm_lock));
<span class="p_chunk">@@ -1696,7 +1699,7 @@</span> <span class="p_context"> static unsigned int khugepaged_scan_mm_slot(unsigned int pages,</span>
 	spin_unlock(&amp;khugepaged_mm_lock);
 
 	mm = mm_slot-&gt;mm;
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_lock(mm, &amp;range);</span>
 	if (unlikely(khugepaged_test_exit(mm)))
 		vma = NULL;
 	else
<span class="p_chunk">@@ -1742,7 +1745,7 @@</span> <span class="p_context"> static unsigned int khugepaged_scan_mm_slot(unsigned int pages,</span>
 				if (!shmem_huge_enabled(vma))
 					goto skip;
 				file = get_file(vma-&gt;vm_file);
<span class="p_del">-				up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+				mm_read_unlock(mm, &amp;range);</span>
 				ret = 1;
 				khugepaged_scan_shmem(mm, file-&gt;f_mapping,
 						pgoff, hpage);
<span class="p_chunk">@@ -1767,7 +1770,7 @@</span> <span class="p_context"> static unsigned int khugepaged_scan_mm_slot(unsigned int pages,</span>
 		}
 	}
 breakouterloop:
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem); /* exit_mmap will destroy ptes after this */</span>
<span class="p_add">+	mm_read_unlock(mm, &amp;range); /* exit_mmap will destroy ptes after this */</span>
 breakouterloop_mmap_sem:
 
 	spin_lock(&amp;khugepaged_mm_lock);
<span class="p_header">diff --git a/mm/ksm.c b/mm/ksm.c</span>
<span class="p_header">index 36a0a12e336d..44a465f99388 100644</span>
<span class="p_header">--- a/mm/ksm.c</span>
<span class="p_header">+++ b/mm/ksm.c</span>
<span class="p_chunk">@@ -447,6 +447,7 @@</span> <span class="p_context"> static void break_cow(struct rmap_item *rmap_item)</span>
 	struct mm_struct *mm = rmap_item-&gt;mm;
 	unsigned long addr = rmap_item-&gt;address;
 	struct vm_area_struct *vma;
<span class="p_add">+	mm_range_define(range);</span>
 
 	/*
 	 * It is not an accident that whenever we want to break COW
<span class="p_chunk">@@ -454,11 +455,11 @@</span> <span class="p_context"> static void break_cow(struct rmap_item *rmap_item)</span>
 	 */
 	put_anon_vma(rmap_item-&gt;anon_vma);
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_lock(mm, &amp;range);</span>
 	vma = find_mergeable_vma(mm, addr);
 	if (vma)
 		break_ksm(vma, addr);
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(mm, &amp;range);</span>
 }
 
 static struct page *get_mergeable_page(struct rmap_item *rmap_item)
<span class="p_chunk">@@ -467,8 +468,9 @@</span> <span class="p_context"> static struct page *get_mergeable_page(struct rmap_item *rmap_item)</span>
 	unsigned long addr = rmap_item-&gt;address;
 	struct vm_area_struct *vma;
 	struct page *page;
<span class="p_add">+	mm_range_define(range);</span>
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_lock(mm, &amp;range);</span>
 	vma = find_mergeable_vma(mm, addr);
 	if (!vma)
 		goto out;
<span class="p_chunk">@@ -484,7 +486,7 @@</span> <span class="p_context"> static struct page *get_mergeable_page(struct rmap_item *rmap_item)</span>
 out:
 		page = NULL;
 	}
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(mm, &amp;range);</span>
 	return page;
 }
 
<span class="p_chunk">@@ -775,6 +777,7 @@</span> <span class="p_context"> static int unmerge_and_remove_all_rmap_items(void)</span>
 	struct mm_struct *mm;
 	struct vm_area_struct *vma;
 	int err = 0;
<span class="p_add">+	mm_range_define(range);</span>
 
 	spin_lock(&amp;ksm_mmlist_lock);
 	ksm_scan.mm_slot = list_entry(ksm_mm_head.mm_list.next,
<span class="p_chunk">@@ -784,7 +787,7 @@</span> <span class="p_context"> static int unmerge_and_remove_all_rmap_items(void)</span>
 	for (mm_slot = ksm_scan.mm_slot;
 			mm_slot != &amp;ksm_mm_head; mm_slot = ksm_scan.mm_slot) {
 		mm = mm_slot-&gt;mm;
<span class="p_del">-		down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_lock(mm, &amp;range);</span>
 		for (vma = mm-&gt;mmap; vma; vma = vma-&gt;vm_next) {
 			if (ksm_test_exit(mm))
 				break;
<span class="p_chunk">@@ -797,7 +800,7 @@</span> <span class="p_context"> static int unmerge_and_remove_all_rmap_items(void)</span>
 		}
 
 		remove_trailing_rmap_items(mm_slot, &amp;mm_slot-&gt;rmap_list);
<span class="p_del">-		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_unlock(mm, &amp;range);</span>
 
 		spin_lock(&amp;ksm_mmlist_lock);
 		ksm_scan.mm_slot = list_entry(mm_slot-&gt;mm_list.next,
<span class="p_chunk">@@ -820,7 +823,7 @@</span> <span class="p_context"> static int unmerge_and_remove_all_rmap_items(void)</span>
 	return 0;
 
 error:
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(mm, &amp;range);</span>
 	spin_lock(&amp;ksm_mmlist_lock);
 	ksm_scan.mm_slot = &amp;ksm_mm_head;
 	spin_unlock(&amp;ksm_mmlist_lock);
<span class="p_chunk">@@ -1088,8 +1091,9 @@</span> <span class="p_context"> static int try_to_merge_with_ksm_page(struct rmap_item *rmap_item,</span>
 	struct mm_struct *mm = rmap_item-&gt;mm;
 	struct vm_area_struct *vma;
 	int err = -EFAULT;
<span class="p_add">+	mm_range_define(range);</span>
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_lock(mm, &amp;range);</span>
 	vma = find_mergeable_vma(mm, rmap_item-&gt;address);
 	if (!vma)
 		goto out;
<span class="p_chunk">@@ -1105,7 +1109,7 @@</span> <span class="p_context"> static int try_to_merge_with_ksm_page(struct rmap_item *rmap_item,</span>
 	rmap_item-&gt;anon_vma = vma-&gt;anon_vma;
 	get_anon_vma(vma-&gt;anon_vma);
 out:
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(mm, &amp;range);</span>
 	return err;
 }
 
<span class="p_chunk">@@ -1579,6 +1583,7 @@</span> <span class="p_context"> static struct rmap_item *scan_get_next_rmap_item(struct page **page)</span>
 	struct vm_area_struct *vma;
 	struct rmap_item *rmap_item;
 	int nid;
<span class="p_add">+	mm_range_define(range);</span>
 
 	if (list_empty(&amp;ksm_mm_head.mm_list))
 		return NULL;
<span class="p_chunk">@@ -1635,7 +1640,7 @@</span> <span class="p_context"> static struct rmap_item *scan_get_next_rmap_item(struct page **page)</span>
 	}
 
 	mm = slot-&gt;mm;
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_lock(mm, &amp;range);</span>
 	if (ksm_test_exit(mm))
 		vma = NULL;
 	else
<span class="p_chunk">@@ -1669,7 +1674,7 @@</span> <span class="p_context"> static struct rmap_item *scan_get_next_rmap_item(struct page **page)</span>
 					ksm_scan.address += PAGE_SIZE;
 				} else
 					put_page(*page);
<span class="p_del">-				up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+				mm_read_unlock(mm, &amp;range);</span>
 				return rmap_item;
 			}
 			put_page(*page);
<span class="p_chunk">@@ -1707,10 +1712,10 @@</span> <span class="p_context"> static struct rmap_item *scan_get_next_rmap_item(struct page **page)</span>
 
 		free_mm_slot(slot);
 		clear_bit(MMF_VM_MERGEABLE, &amp;mm-&gt;flags);
<span class="p_del">-		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_unlock(mm, &amp;range);</span>
 		mmdrop(mm);
 	} else {
<span class="p_del">-		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_unlock(mm, &amp;range);</span>
 		/*
 		 * up_read(&amp;mm-&gt;mmap_sem) first because after
 		 * spin_unlock(&amp;ksm_mmlist_lock) run, the &quot;mm&quot; may
<span class="p_chunk">@@ -1869,6 +1874,7 @@</span> <span class="p_context"> void __ksm_exit(struct mm_struct *mm)</span>
 {
 	struct mm_slot *mm_slot;
 	int easy_to_free = 0;
<span class="p_add">+	mm_range_define(range);</span>
 
 	/*
 	 * This process is exiting: if it&#39;s straightforward (as is the
<span class="p_chunk">@@ -1898,8 +1904,8 @@</span> <span class="p_context"> void __ksm_exit(struct mm_struct *mm)</span>
 		clear_bit(MMF_VM_MERGEABLE, &amp;mm-&gt;flags);
 		mmdrop(mm);
 	} else if (mm_slot) {
<span class="p_del">-		down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_del">-		up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_write_lock(mm, &amp;range);</span>
<span class="p_add">+		mm_write_unlock(mm, &amp;range);</span>
 	}
 }
 
<span class="p_header">diff --git a/mm/madvise.c b/mm/madvise.c</span>
<span class="p_header">index 437f35778f07..bfd048564956 100644</span>
<span class="p_header">--- a/mm/madvise.c</span>
<span class="p_header">+++ b/mm/madvise.c</span>
<span class="p_chunk">@@ -519,7 +519,7 @@</span> <span class="p_context"> static long madvise_dontneed(struct vm_area_struct *vma,</span>
 	if (!userfaultfd_remove(vma, start, end, range)) {
 		*prev = NULL; /* mmap_sem has been dropped, prev is stale */
 
<span class="p_del">-		down_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_lock(current-&gt;mm, range);</span>
 		vma = find_vma(current-&gt;mm, start);
 		if (!vma)
 			return -ENOMEM;
<span class="p_chunk">@@ -597,15 +597,15 @@</span> <span class="p_context"> static long madvise_remove(struct vm_area_struct *vma,</span>
 	 * mmap_sem.
 	 */
 	get_file(f);
<span class="p_del">-	if (userfaultfd_remove(vma, start, end, NULL)) {</span>
<span class="p_add">+	if (userfaultfd_remove(vma, start, end, range)) {</span>
 		/* mmap_sem was not released by userfaultfd_remove() */
<span class="p_del">-		up_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_unlock(current-&gt;mm, range);</span>
 	}
 	error = vfs_fallocate(f,
 				FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE,
 				offset, end - start);
 	fput(f);
<span class="p_del">-	down_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_lock(current-&gt;mm, range);</span>
 	return error;
 }
 
<span class="p_chunk">@@ -783,6 +783,7 @@</span> <span class="p_context"> SYSCALL_DEFINE3(madvise, unsigned long, start, size_t, len_in, int, behavior)</span>
 	int write;
 	size_t len;
 	struct blk_plug plug;
<span class="p_add">+	mm_range_define(range);</span>
 
 	if (!madvise_behavior_valid(behavior))
 		return error;
<span class="p_chunk">@@ -810,10 +811,10 @@</span> <span class="p_context"> SYSCALL_DEFINE3(madvise, unsigned long, start, size_t, len_in, int, behavior)</span>
 
 	write = madvise_need_mmap_write(behavior);
 	if (write) {
<span class="p_del">-		if (down_write_killable(&amp;current-&gt;mm-&gt;mmap_sem))</span>
<span class="p_add">+		if (mm_write_lock_killable(current-&gt;mm, &amp;range))</span>
 			return -EINTR;
 	} else {
<span class="p_del">-		down_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_lock(current-&gt;mm, &amp;range);</span>
 	}
 
 	/*
<span class="p_chunk">@@ -867,9 +868,9 @@</span> <span class="p_context"> SYSCALL_DEFINE3(madvise, unsigned long, start, size_t, len_in, int, behavior)</span>
 out:
 	blk_finish_plug(&amp;plug);
 	if (write)
<span class="p_del">-		up_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_write_unlock(current-&gt;mm, &amp;range);</span>
 	else
<span class="p_del">-		up_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_unlock(current-&gt;mm, &amp;range);</span>
 
 	return error;
 }
<span class="p_header">diff --git a/mm/memcontrol.c b/mm/memcontrol.c</span>
<span class="p_header">index 94172089f52f..ca22fa420ba6 100644</span>
<span class="p_header">--- a/mm/memcontrol.c</span>
<span class="p_header">+++ b/mm/memcontrol.c</span>
<span class="p_chunk">@@ -4681,15 +4681,16 @@</span> <span class="p_context"> static int mem_cgroup_count_precharge_pte_range(pmd_t *pmd,</span>
 static unsigned long mem_cgroup_count_precharge(struct mm_struct *mm)
 {
 	unsigned long precharge;
<span class="p_del">-</span>
 	struct mm_walk mem_cgroup_count_precharge_walk = {
 		.pmd_entry = mem_cgroup_count_precharge_pte_range,
 		.mm = mm,
 	};
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_range_define(range);</span>
<span class="p_add">+</span>
<span class="p_add">+	mm_read_lock(mm, &amp;range);</span>
 	walk_page_range(0, mm-&gt;highest_vm_end,
 			&amp;mem_cgroup_count_precharge_walk);
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(mm, &amp;range);</span>
 
 	precharge = mc.precharge;
 	mc.precharge = 0;
<span class="p_chunk">@@ -4950,6 +4951,7 @@</span> <span class="p_context"> static void mem_cgroup_move_charge(void)</span>
 		.pmd_entry = mem_cgroup_move_charge_pte_range,
 		.mm = mc.mm,
 	};
<span class="p_add">+	mm_range_define(range);</span>
 
 	lru_add_drain_all();
 	/*
<span class="p_chunk">@@ -4960,7 +4962,7 @@</span> <span class="p_context"> static void mem_cgroup_move_charge(void)</span>
 	atomic_inc(&amp;mc.from-&gt;moving_account);
 	synchronize_rcu();
 retry:
<span class="p_del">-	if (unlikely(!down_read_trylock(&amp;mc.mm-&gt;mmap_sem))) {</span>
<span class="p_add">+	if (unlikely(!mm_read_trylock(mc.mm, &amp;range))) {</span>
 		/*
 		 * Someone who are holding the mmap_sem might be waiting in
 		 * waitq. So we cancel all extra charges, wake up all waiters,
<span class="p_chunk">@@ -4978,7 +4980,7 @@</span> <span class="p_context"> static void mem_cgroup_move_charge(void)</span>
 	 */
 	walk_page_range(0, mc.mm-&gt;highest_vm_end, &amp;mem_cgroup_move_charge_walk);
 
<span class="p_del">-	up_read(&amp;mc.mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(mc.mm, &amp;range);</span>
 	atomic_dec(&amp;mc.from-&gt;moving_account);
 }
 
<span class="p_header">diff --git a/mm/memory.c b/mm/memory.c</span>
<span class="p_header">index f98ecbe35e8f..a27ee1c8f07e 100644</span>
<span class="p_header">--- a/mm/memory.c</span>
<span class="p_header">+++ b/mm/memory.c</span>
<span class="p_chunk">@@ -1637,12 +1637,14 @@</span> <span class="p_context"> static int insert_page(struct vm_area_struct *vma, unsigned long addr,</span>
 int vm_insert_page(struct vm_area_struct *vma, unsigned long addr,
 			struct page *page)
 {
<span class="p_add">+	mm_range_define(range);</span>
<span class="p_add">+</span>
 	if (addr &lt; vma-&gt;vm_start || addr &gt;= vma-&gt;vm_end)
 		return -EFAULT;
 	if (!page_count(page))
 		return -EINVAL;
 	if (!(vma-&gt;vm_flags &amp; VM_MIXEDMAP)) {
<span class="p_del">-		BUG_ON(down_read_trylock(&amp;vma-&gt;vm_mm-&gt;mmap_sem));</span>
<span class="p_add">+		BUG_ON(mm_read_trylock(vma-&gt;vm_mm, &amp;range));</span>
 		BUG_ON(vma-&gt;vm_flags &amp; VM_PFNMAP);
 		vma-&gt;vm_flags |= VM_MIXEDMAP;
 	}
<span class="p_chunk">@@ -4181,8 +4183,9 @@</span> <span class="p_context"> int __access_remote_vm(struct task_struct *tsk, struct mm_struct *mm,</span>
 	struct vm_area_struct *vma;
 	void *old_buf = buf;
 	int write = gup_flags &amp; FOLL_WRITE;
<span class="p_add">+	mm_range_define(range);</span>
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_lock(mm, &amp;range);</span>
 	/* ignore errors, just check how much was successfully transferred */
 	while (len) {
 		int bytes, ret, offset;
<span class="p_chunk">@@ -4190,7 +4193,8 @@</span> <span class="p_context"> int __access_remote_vm(struct task_struct *tsk, struct mm_struct *mm,</span>
 		struct page *page = NULL;
 
 		ret = get_user_pages_remote(tsk, mm, addr, 1,
<span class="p_del">-					    gup_flags, &amp;page, &amp;vma, NULL, NULL);</span>
<span class="p_add">+					    gup_flags, &amp;page, &amp;vma, NULL,</span>
<span class="p_add">+					    NULL /* mm range lock untouched */);</span>
 		if (ret &lt;= 0) {
 #ifndef CONFIG_HAVE_IOREMAP_PROT
 			break;
<span class="p_chunk">@@ -4231,7 +4235,7 @@</span> <span class="p_context"> int __access_remote_vm(struct task_struct *tsk, struct mm_struct *mm,</span>
 		buf += bytes;
 		addr += bytes;
 	}
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(mm, &amp;range);</span>
 
 	return buf - old_buf;
 }
<span class="p_chunk">@@ -4282,6 +4286,7 @@</span> <span class="p_context"> void print_vma_addr(char *prefix, unsigned long ip)</span>
 {
 	struct mm_struct *mm = current-&gt;mm;
 	struct vm_area_struct *vma;
<span class="p_add">+	mm_range_define(range);</span>
 
 	/*
 	 * Do not print if we are in atomic
<span class="p_chunk">@@ -4290,7 +4295,7 @@</span> <span class="p_context"> void print_vma_addr(char *prefix, unsigned long ip)</span>
 	if (preempt_count())
 		return;
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_lock(mm, &amp;range);</span>
 	vma = find_vma(mm, ip);
 	if (vma &amp;&amp; vma-&gt;vm_file) {
 		struct file *f = vma-&gt;vm_file;
<span class="p_chunk">@@ -4307,7 +4312,7 @@</span> <span class="p_context"> void print_vma_addr(char *prefix, unsigned long ip)</span>
 			free_page((unsigned long)buf);
 		}
 	}
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(mm, &amp;range);</span>
 }
 
 #if defined(CONFIG_PROVE_LOCKING) || defined(CONFIG_DEBUG_ATOMIC_SLEEP)
<span class="p_header">diff --git a/mm/mempolicy.c b/mm/mempolicy.c</span>
<span class="p_header">index 0658c7240e54..68f1ed522fea 100644</span>
<span class="p_header">--- a/mm/mempolicy.c</span>
<span class="p_header">+++ b/mm/mempolicy.c</span>
<span class="p_chunk">@@ -445,11 +445,12 @@</span> <span class="p_context"> void mpol_rebind_task(struct task_struct *tsk, const nodemask_t *new,</span>
 void mpol_rebind_mm(struct mm_struct *mm, nodemask_t *new)
 {
 	struct vm_area_struct *vma;
<span class="p_add">+	mm_range_define(range);</span>
 
<span class="p_del">-	down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_lock(mm, &amp;range);</span>
 	for (vma = mm-&gt;mmap; vma; vma = vma-&gt;vm_next)
 		mpol_rebind_policy(vma-&gt;vm_policy, new, MPOL_REBIND_ONCE);
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(mm, &amp;range);</span>
 }
 
 static const struct mempolicy_operations mpol_ops[MPOL_MAX] = {
<span class="p_chunk">@@ -871,6 +872,7 @@</span> <span class="p_context"> static long do_get_mempolicy(int *policy, nodemask_t *nmask,</span>
 	struct mm_struct *mm = current-&gt;mm;
 	struct vm_area_struct *vma = NULL;
 	struct mempolicy *pol = current-&gt;mempolicy;
<span class="p_add">+	mm_range_define(range);</span>
 
 	if (flags &amp;
 		~(unsigned long)(MPOL_F_NODE|MPOL_F_ADDR|MPOL_F_MEMS_ALLOWED))
<span class="p_chunk">@@ -892,10 +894,10 @@</span> <span class="p_context"> static long do_get_mempolicy(int *policy, nodemask_t *nmask,</span>
 		 * vma/shared policy at addr is NULL.  We
 		 * want to return MPOL_DEFAULT in this case.
 		 */
<span class="p_del">-		down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_lock(mm, &amp;range);</span>
 		vma = find_vma_intersection(mm, addr, addr+1);
 		if (!vma) {
<span class="p_del">-			up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+			mm_read_unlock(mm, &amp;range);</span>
 			return -EFAULT;
 		}
 		if (vma-&gt;vm_ops &amp;&amp; vma-&gt;vm_ops-&gt;get_policy)
<span class="p_chunk">@@ -932,7 +934,7 @@</span> <span class="p_context"> static long do_get_mempolicy(int *policy, nodemask_t *nmask,</span>
 	}
 
 	if (vma) {
<span class="p_del">-		up_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_unlock(current-&gt;mm, &amp;range);</span>
 		vma = NULL;
 	}
 
<span class="p_chunk">@@ -950,7 +952,7 @@</span> <span class="p_context"> static long do_get_mempolicy(int *policy, nodemask_t *nmask,</span>
  out:
 	mpol_cond_put(pol);
 	if (vma)
<span class="p_del">-		up_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_unlock(current-&gt;mm, &amp;range);</span>
 	return err;
 }
 
<span class="p_chunk">@@ -1028,12 +1030,13 @@</span> <span class="p_context"> int do_migrate_pages(struct mm_struct *mm, const nodemask_t *from,</span>
 	int busy = 0;
 	int err;
 	nodemask_t tmp;
<span class="p_add">+	mm_range_define(range);</span>
 
 	err = migrate_prep();
 	if (err)
 		return err;
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_lock(mm, &amp;range);</span>
 
 	/*
 	 * Find a &#39;source&#39; bit set in &#39;tmp&#39; whose corresponding &#39;dest&#39;
<span class="p_chunk">@@ -1114,7 +1117,7 @@</span> <span class="p_context"> int do_migrate_pages(struct mm_struct *mm, const nodemask_t *from,</span>
 		if (err &lt; 0)
 			break;
 	}
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(mm, &amp;range);</span>
 	if (err &lt; 0)
 		return err;
 	return busy;
<span class="p_chunk">@@ -1178,6 +1181,7 @@</span> <span class="p_context"> static long do_mbind(unsigned long start, unsigned long len,</span>
 	unsigned long end;
 	int err;
 	LIST_HEAD(pagelist);
<span class="p_add">+	mm_range_define(range);</span>
 
 	if (flags &amp; ~(unsigned long)MPOL_MF_VALID)
 		return -EINVAL;
<span class="p_chunk">@@ -1225,12 +1229,12 @@</span> <span class="p_context"> static long do_mbind(unsigned long start, unsigned long len,</span>
 	{
 		NODEMASK_SCRATCH(scratch);
 		if (scratch) {
<span class="p_del">-			down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+			mm_write_lock(mm, &amp;range);</span>
 			task_lock(current);
 			err = mpol_set_nodemask(new, nmask, scratch);
 			task_unlock(current);
 			if (err)
<span class="p_del">-				up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+				mm_write_unlock(mm, &amp;range);</span>
 		} else
 			err = -ENOMEM;
 		NODEMASK_SCRATCH_FREE(scratch);
<span class="p_chunk">@@ -1259,7 +1263,7 @@</span> <span class="p_context"> static long do_mbind(unsigned long start, unsigned long len,</span>
 	} else
 		putback_movable_pages(&amp;pagelist);
 
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(mm, &amp;range);</span>
  mpol_out:
 	mpol_put(new);
 	return err;
<span class="p_header">diff --git a/mm/migrate.c b/mm/migrate.c</span>
<span class="p_header">index 89a0a1707f4c..3726547a2dc9 100644</span>
<span class="p_header">--- a/mm/migrate.c</span>
<span class="p_header">+++ b/mm/migrate.c</span>
<span class="p_chunk">@@ -1405,8 +1405,9 @@</span> <span class="p_context"> static int do_move_page_to_node_array(struct mm_struct *mm,</span>
 	int err;
 	struct page_to_node *pp;
 	LIST_HEAD(pagelist);
<span class="p_add">+	mm_range_define(range);</span>
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_lock(mm, &amp;range);</span>
 
 	/*
 	 * Build a list of pages to migrate
<span class="p_chunk">@@ -1477,7 +1478,7 @@</span> <span class="p_context"> static int do_move_page_to_node_array(struct mm_struct *mm,</span>
 			putback_movable_pages(&amp;pagelist);
 	}
 
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(mm, &amp;range);</span>
 	return err;
 }
 
<span class="p_chunk">@@ -1575,8 +1576,9 @@</span> <span class="p_context"> static void do_pages_stat_array(struct mm_struct *mm, unsigned long nr_pages,</span>
 				const void __user **pages, int *status)
 {
 	unsigned long i;
<span class="p_add">+	mm_range_define(range);</span>
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_lock(mm, &amp;range);</span>
 
 	for (i = 0; i &lt; nr_pages; i++) {
 		unsigned long addr = (unsigned long)(*pages);
<span class="p_chunk">@@ -1603,7 +1605,7 @@</span> <span class="p_context"> static void do_pages_stat_array(struct mm_struct *mm, unsigned long nr_pages,</span>
 		status++;
 	}
 
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(mm, &amp;range);</span>
 }
 
 /*
<span class="p_header">diff --git a/mm/mincore.c b/mm/mincore.c</span>
<span class="p_header">index c5687c45c326..7c2a580cd461 100644</span>
<span class="p_header">--- a/mm/mincore.c</span>
<span class="p_header">+++ b/mm/mincore.c</span>
<span class="p_chunk">@@ -226,6 +226,7 @@</span> <span class="p_context"> SYSCALL_DEFINE3(mincore, unsigned long, start, size_t, len,</span>
 	long retval;
 	unsigned long pages;
 	unsigned char *tmp;
<span class="p_add">+	mm_range_define(range);</span>
 
 	/* Check the start address: needs to be page-aligned.. */
 	if (start &amp; ~PAGE_MASK)
<span class="p_chunk">@@ -252,9 +253,9 @@</span> <span class="p_context"> SYSCALL_DEFINE3(mincore, unsigned long, start, size_t, len,</span>
 		 * Do at most PAGE_SIZE entries per iteration, due to
 		 * the temporary buffer size.
 		 */
<span class="p_del">-		down_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_lock(current-&gt;mm, &amp;range);</span>
 		retval = do_mincore(start, min(pages, PAGE_SIZE), tmp);
<span class="p_del">-		up_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_unlock(current-&gt;mm, &amp;range);</span>
 
 		if (retval &lt;= 0)
 			break;
<span class="p_header">diff --git a/mm/mlock.c b/mm/mlock.c</span>
<span class="p_header">index c483c5c20b4b..9b74ecd70ce0 100644</span>
<span class="p_header">--- a/mm/mlock.c</span>
<span class="p_header">+++ b/mm/mlock.c</span>
<span class="p_chunk">@@ -666,6 +666,7 @@</span> <span class="p_context"> static __must_check int do_mlock(unsigned long start, size_t len, vm_flags_t fla</span>
 	unsigned long locked;
 	unsigned long lock_limit;
 	int error = -ENOMEM;
<span class="p_add">+	mm_range_define(range);</span>
 
 	if (!can_do_mlock())
 		return -EPERM;
<span class="p_chunk">@@ -679,7 +680,7 @@</span> <span class="p_context"> static __must_check int do_mlock(unsigned long start, size_t len, vm_flags_t fla</span>
 	lock_limit &gt;&gt;= PAGE_SHIFT;
 	locked = len &gt;&gt; PAGE_SHIFT;
 
<span class="p_del">-	if (down_write_killable(&amp;current-&gt;mm-&gt;mmap_sem))</span>
<span class="p_add">+	if (mm_write_lock_killable(current-&gt;mm, &amp;range))</span>
 		return -EINTR;
 
 	locked += current-&gt;mm-&gt;locked_vm;
<span class="p_chunk">@@ -698,7 +699,7 @@</span> <span class="p_context"> static __must_check int do_mlock(unsigned long start, size_t len, vm_flags_t fla</span>
 	if ((locked &lt;= lock_limit) || capable(CAP_IPC_LOCK))
 		error = apply_vma_lock_flags(start, len, flags);
 
<span class="p_del">-	up_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(current-&gt;mm, &amp;range);</span>
 	if (error)
 		return error;
 
<span class="p_chunk">@@ -729,14 +730,15 @@</span> <span class="p_context"> SYSCALL_DEFINE3(mlock2, unsigned long, start, size_t, len, int, flags)</span>
 SYSCALL_DEFINE2(munlock, unsigned long, start, size_t, len)
 {
 	int ret;
<span class="p_add">+	mm_range_define(range);</span>
 
 	len = PAGE_ALIGN(len + (offset_in_page(start)));
 	start &amp;= PAGE_MASK;
 
<span class="p_del">-	if (down_write_killable(&amp;current-&gt;mm-&gt;mmap_sem))</span>
<span class="p_add">+	if (mm_write_lock_killable(current-&gt;mm, &amp;range))</span>
 		return -EINTR;
 	ret = apply_vma_lock_flags(start, len, 0);
<span class="p_del">-	up_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(current-&gt;mm, &amp;range);</span>
 
 	return ret;
 }
<span class="p_chunk">@@ -791,6 +793,7 @@</span> <span class="p_context"> SYSCALL_DEFINE1(mlockall, int, flags)</span>
 {
 	unsigned long lock_limit;
 	int ret;
<span class="p_add">+	mm_range_define(range);</span>
 
 	if (!flags || (flags &amp; ~(MCL_CURRENT | MCL_FUTURE | MCL_ONFAULT)))
 		return -EINVAL;
<span class="p_chunk">@@ -804,14 +807,14 @@</span> <span class="p_context"> SYSCALL_DEFINE1(mlockall, int, flags)</span>
 	lock_limit = rlimit(RLIMIT_MEMLOCK);
 	lock_limit &gt;&gt;= PAGE_SHIFT;
 
<span class="p_del">-	if (down_write_killable(&amp;current-&gt;mm-&gt;mmap_sem))</span>
<span class="p_add">+	if (mm_write_lock_killable(current-&gt;mm, &amp;range))</span>
 		return -EINTR;
 
 	ret = -ENOMEM;
 	if (!(flags &amp; MCL_CURRENT) || (current-&gt;mm-&gt;total_vm &lt;= lock_limit) ||
 	    capable(CAP_IPC_LOCK))
 		ret = apply_mlockall_flags(flags);
<span class="p_del">-	up_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(current-&gt;mm, &amp;range);</span>
 	if (!ret &amp;&amp; (flags &amp; MCL_CURRENT))
 		mm_populate(0, TASK_SIZE);
 
<span class="p_chunk">@@ -821,11 +824,12 @@</span> <span class="p_context"> SYSCALL_DEFINE1(mlockall, int, flags)</span>
 SYSCALL_DEFINE0(munlockall)
 {
 	int ret;
<span class="p_add">+	mm_range_define(range);</span>
 
<span class="p_del">-	if (down_write_killable(&amp;current-&gt;mm-&gt;mmap_sem))</span>
<span class="p_add">+	if (mm_write_lock_killable(current-&gt;mm, &amp;range))</span>
 		return -EINTR;
 	ret = apply_mlockall_flags(0);
<span class="p_del">-	up_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(current-&gt;mm, &amp;range);</span>
 	return ret;
 }
 
<span class="p_header">diff --git a/mm/mmap.c b/mm/mmap.c</span>
<span class="p_header">index 1796b9ae540d..e3b84b78917d 100644</span>
<span class="p_header">--- a/mm/mmap.c</span>
<span class="p_header">+++ b/mm/mmap.c</span>
<span class="p_chunk">@@ -186,8 +186,9 @@</span> <span class="p_context"> SYSCALL_DEFINE1(brk, unsigned long, brk)</span>
 	unsigned long min_brk;
 	bool populate;
 	LIST_HEAD(uf);
<span class="p_add">+	mm_range_define(range);</span>
 
<span class="p_del">-	if (down_write_killable(&amp;mm-&gt;mmap_sem))</span>
<span class="p_add">+	if (mm_write_lock_killable(mm, &amp;range))</span>
 		return -EINTR;
 
 #ifdef CONFIG_COMPAT_BRK
<span class="p_chunk">@@ -239,7 +240,7 @@</span> <span class="p_context"> SYSCALL_DEFINE1(brk, unsigned long, brk)</span>
 set_brk:
 	mm-&gt;brk = brk;
 	populate = newbrk &gt; oldbrk &amp;&amp; (mm-&gt;def_flags &amp; VM_LOCKED) != 0;
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(mm, &amp;range);</span>
 	userfaultfd_unmap_complete(mm, &amp;uf);
 	if (populate)
 		mm_populate(oldbrk, newbrk - oldbrk);
<span class="p_chunk">@@ -247,7 +248,7 @@</span> <span class="p_context"> SYSCALL_DEFINE1(brk, unsigned long, brk)</span>
 
 out:
 	retval = mm-&gt;brk;
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(mm, &amp;range);</span>
 	return retval;
 }
 
<span class="p_chunk">@@ -2681,12 +2682,13 @@</span> <span class="p_context"> int vm_munmap(unsigned long start, size_t len)</span>
 	int ret;
 	struct mm_struct *mm = current-&gt;mm;
 	LIST_HEAD(uf);
<span class="p_add">+	mm_range_define(range);</span>
 
<span class="p_del">-	if (down_write_killable(&amp;mm-&gt;mmap_sem))</span>
<span class="p_add">+	if (mm_write_lock_killable(mm, &amp;range))</span>
 		return -EINTR;
 
 	ret = do_munmap(mm, start, len, &amp;uf);
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(mm, &amp;range);</span>
 	userfaultfd_unmap_complete(mm, &amp;uf);
 	return ret;
 }
<span class="p_chunk">@@ -2711,6 +2713,7 @@</span> <span class="p_context"> SYSCALL_DEFINE5(remap_file_pages, unsigned long, start, unsigned long, size,</span>
 	unsigned long populate = 0;
 	unsigned long ret = -EINVAL;
 	struct file *file;
<span class="p_add">+	mm_range_define(range);</span>
 
 	pr_warn_once(&quot;%s (%d) uses deprecated remap_file_pages() syscall. See Documentation/vm/remap_file_pages.txt.\n&quot;,
 		     current-&gt;comm, current-&gt;pid);
<span class="p_chunk">@@ -2727,7 +2730,7 @@</span> <span class="p_context"> SYSCALL_DEFINE5(remap_file_pages, unsigned long, start, unsigned long, size,</span>
 	if (pgoff + (size &gt;&gt; PAGE_SHIFT) &lt; pgoff)
 		return ret;
 
<span class="p_del">-	if (down_write_killable(&amp;mm-&gt;mmap_sem))</span>
<span class="p_add">+	if (mm_write_lock_killable(mm, &amp;range))</span>
 		return -EINTR;
 
 	vma = find_vma(mm, start);
<span class="p_chunk">@@ -2790,7 +2793,7 @@</span> <span class="p_context"> SYSCALL_DEFINE5(remap_file_pages, unsigned long, start, unsigned long, size,</span>
 			prot, flags, pgoff, &amp;populate, NULL);
 	fput(file);
 out:
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(mm, &amp;range);</span>
 	if (populate)
 		mm_populate(ret, populate);
 	if (!IS_ERR_VALUE(ret))
<span class="p_chunk">@@ -2801,9 +2804,11 @@</span> <span class="p_context"> SYSCALL_DEFINE5(remap_file_pages, unsigned long, start, unsigned long, size,</span>
 static inline void verify_mm_writelocked(struct mm_struct *mm)
 {
 #ifdef CONFIG_DEBUG_VM
<span class="p_del">-	if (unlikely(down_read_trylock(&amp;mm-&gt;mmap_sem))) {</span>
<span class="p_add">+	mm_range_define(range);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (unlikely(mm_read_lock_trylock(mm, &amp;range))) {</span>
 		WARN_ON(1);
<span class="p_del">-		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_unlock(mm, &amp;range);</span>
 	}
 #endif
 }
<span class="p_chunk">@@ -2910,13 +2915,14 @@</span> <span class="p_context"> int vm_brk_flags(unsigned long addr, unsigned long len, unsigned long flags)</span>
 	int ret;
 	bool populate;
 	LIST_HEAD(uf);
<span class="p_add">+	mm_range_define(range);</span>
 
<span class="p_del">-	if (down_write_killable(&amp;mm-&gt;mmap_sem))</span>
<span class="p_add">+	if (mm_write_lock_killable(mm, &amp;range))</span>
 		return -EINTR;
 
 	ret = do_brk_flags(addr, len, flags, &amp;uf);
 	populate = ((mm-&gt;def_flags &amp; VM_LOCKED) != 0);
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(mm, &amp;range);</span>
 	userfaultfd_unmap_complete(mm, &amp;uf);
 	if (populate &amp;&amp; !ret)
 		mm_populate(addr, len);
<span class="p_chunk">@@ -3367,8 +3373,9 @@</span> <span class="p_context"> int mm_take_all_locks(struct mm_struct *mm)</span>
 {
 	struct vm_area_struct *vma;
 	struct anon_vma_chain *avc;
<span class="p_add">+	mm_range_define(range);</span>
 
<span class="p_del">-	BUG_ON(down_read_trylock(&amp;mm-&gt;mmap_sem));</span>
<span class="p_add">+	BUG_ON(mm_read_trylock(mm, &amp;range));</span>
 
 	mutex_lock(&amp;mm_all_locks_mutex);
 
<span class="p_chunk">@@ -3447,8 +3454,9 @@</span> <span class="p_context"> void mm_drop_all_locks(struct mm_struct *mm)</span>
 {
 	struct vm_area_struct *vma;
 	struct anon_vma_chain *avc;
<span class="p_add">+	mm_range_define(range);</span>
 
<span class="p_del">-	BUG_ON(down_read_trylock(&amp;mm-&gt;mmap_sem));</span>
<span class="p_add">+	BUG_ON(mm_read_trylock(mm, &amp;range));</span>
 	BUG_ON(!mutex_is_locked(&amp;mm_all_locks_mutex));
 
 	for (vma = mm-&gt;mmap; vma; vma = vma-&gt;vm_next) {
<span class="p_header">diff --git a/mm/mmu_notifier.c b/mm/mmu_notifier.c</span>
<span class="p_header">index 54ca54562928..0d2ab3418afb 100644</span>
<span class="p_header">--- a/mm/mmu_notifier.c</span>
<span class="p_header">+++ b/mm/mmu_notifier.c</span>
<span class="p_chunk">@@ -249,6 +249,7 @@</span> <span class="p_context"> static int do_mmu_notifier_register(struct mmu_notifier *mn,</span>
 {
 	struct mmu_notifier_mm *mmu_notifier_mm;
 	int ret;
<span class="p_add">+	mm_range_define(range);</span>
 
 	BUG_ON(atomic_read(&amp;mm-&gt;mm_users) &lt;= 0);
 
<span class="p_chunk">@@ -258,7 +259,7 @@</span> <span class="p_context"> static int do_mmu_notifier_register(struct mmu_notifier *mn,</span>
 		goto out;
 
 	if (take_mmap_sem)
<span class="p_del">-		down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_write_lock(mm, &amp;range);</span>
 	ret = mm_take_all_locks(mm);
 	if (unlikely(ret))
 		goto out_clean;
<span class="p_chunk">@@ -287,7 +288,7 @@</span> <span class="p_context"> static int do_mmu_notifier_register(struct mmu_notifier *mn,</span>
 	mm_drop_all_locks(mm);
 out_clean:
 	if (take_mmap_sem)
<span class="p_del">-		up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_write_unlock(mm, &amp;range);</span>
 	kfree(mmu_notifier_mm);
 out:
 	BUG_ON(atomic_read(&amp;mm-&gt;mm_users) &lt;= 0);
<span class="p_header">diff --git a/mm/mprotect.c b/mm/mprotect.c</span>
<span class="p_header">index fef798619b06..f14aef5824a7 100644</span>
<span class="p_header">--- a/mm/mprotect.c</span>
<span class="p_header">+++ b/mm/mprotect.c</span>
<span class="p_chunk">@@ -383,6 +383,7 @@</span> <span class="p_context"> static int do_mprotect_pkey(unsigned long start, size_t len,</span>
 	const int grows = prot &amp; (PROT_GROWSDOWN|PROT_GROWSUP);
 	const bool rier = (current-&gt;personality &amp; READ_IMPLIES_EXEC) &amp;&amp;
 				(prot &amp; PROT_READ);
<span class="p_add">+	mm_range_define(range);</span>
 
 	prot &amp;= ~(PROT_GROWSDOWN|PROT_GROWSUP);
 	if (grows == (PROT_GROWSDOWN|PROT_GROWSUP)) /* can&#39;t be both */
<span class="p_chunk">@@ -401,7 +402,7 @@</span> <span class="p_context"> static int do_mprotect_pkey(unsigned long start, size_t len,</span>
 
 	reqprot = prot;
 
<span class="p_del">-	if (down_write_killable(&amp;current-&gt;mm-&gt;mmap_sem))</span>
<span class="p_add">+	if (mm_write_lock_killable(current-&gt;mm, &amp;range))</span>
 		return -EINTR;
 
 	/*
<span class="p_chunk">@@ -491,7 +492,7 @@</span> <span class="p_context"> static int do_mprotect_pkey(unsigned long start, size_t len,</span>
 		prot = reqprot;
 	}
 out:
<span class="p_del">-	up_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(current-&gt;mm, &amp;range);</span>
 	return error;
 }
 
<span class="p_chunk">@@ -513,6 +514,7 @@</span> <span class="p_context"> SYSCALL_DEFINE2(pkey_alloc, unsigned long, flags, unsigned long, init_val)</span>
 {
 	int pkey;
 	int ret;
<span class="p_add">+	mm_range_define(range);</span>
 
 	/* No flags supported yet. */
 	if (flags)
<span class="p_chunk">@@ -521,7 +523,7 @@</span> <span class="p_context"> SYSCALL_DEFINE2(pkey_alloc, unsigned long, flags, unsigned long, init_val)</span>
 	if (init_val &amp; ~PKEY_ACCESS_MASK)
 		return -EINVAL;
 
<span class="p_del">-	down_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_lock(current-&gt;mm, &amp;range);</span>
 	pkey = mm_pkey_alloc(current-&gt;mm);
 
 	ret = -ENOSPC;
<span class="p_chunk">@@ -535,17 +537,18 @@</span> <span class="p_context"> SYSCALL_DEFINE2(pkey_alloc, unsigned long, flags, unsigned long, init_val)</span>
 	}
 	ret = pkey;
 out:
<span class="p_del">-	up_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(current-&gt;mm, &amp;range);</span>
 	return ret;
 }
 
 SYSCALL_DEFINE1(pkey_free, int, pkey)
 {
 	int ret;
<span class="p_add">+	mm_range_define(range);</span>
 
<span class="p_del">-	down_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_lock(current-&gt;mm, &amp;range);</span>
 	ret = mm_pkey_free(current-&gt;mm, pkey);
<span class="p_del">-	up_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(current-&gt;mm, &amp;range);</span>
 
 	/*
 	 * We could provie warnings or errors if any VMA still
<span class="p_header">diff --git a/mm/mremap.c b/mm/mremap.c</span>
<span class="p_header">index cd8a1b199ef9..aa9377fc6db8 100644</span>
<span class="p_header">--- a/mm/mremap.c</span>
<span class="p_header">+++ b/mm/mremap.c</span>
<span class="p_chunk">@@ -515,6 +515,7 @@</span> <span class="p_context"> SYSCALL_DEFINE5(mremap, unsigned long, addr, unsigned long, old_len,</span>
 	bool locked = false;
 	struct vm_userfaultfd_ctx uf = NULL_VM_UFFD_CTX;
 	LIST_HEAD(uf_unmap);
<span class="p_add">+	mm_range_define(range);</span>
 
 	if (flags &amp; ~(MREMAP_FIXED | MREMAP_MAYMOVE))
 		return ret;
<span class="p_chunk">@@ -536,7 +537,7 @@</span> <span class="p_context"> SYSCALL_DEFINE5(mremap, unsigned long, addr, unsigned long, old_len,</span>
 	if (!new_len)
 		return ret;
 
<span class="p_del">-	if (down_write_killable(&amp;current-&gt;mm-&gt;mmap_sem))</span>
<span class="p_add">+	if (mm_write_lock_killable(current-&gt;mm, &amp;range))</span>
 		return -EINTR;
 
 	if (flags &amp; MREMAP_FIXED) {
<span class="p_chunk">@@ -618,7 +619,7 @@</span> <span class="p_context"> SYSCALL_DEFINE5(mremap, unsigned long, addr, unsigned long, old_len,</span>
 		vm_unacct_memory(charged);
 		locked = 0;
 	}
<span class="p_del">-	up_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(current-&gt;mm, &amp;range);</span>
 	if (locked &amp;&amp; new_len &gt; old_len)
 		mm_populate(new_addr + old_len, new_len - old_len);
 	mremap_userfaultfd_complete(&amp;uf, addr, new_addr, old_len);
<span class="p_header">diff --git a/mm/msync.c b/mm/msync.c</span>
<span class="p_header">index 24e612fefa04..8f00ef37e625 100644</span>
<span class="p_header">--- a/mm/msync.c</span>
<span class="p_header">+++ b/mm/msync.c</span>
<span class="p_chunk">@@ -35,6 +35,7 @@</span> <span class="p_context"> SYSCALL_DEFINE3(msync, unsigned long, start, size_t, len, int, flags)</span>
 	struct vm_area_struct *vma;
 	int unmapped_error = 0;
 	int error = -EINVAL;
<span class="p_add">+	mm_range_define(range);</span>
 
 	if (flags &amp; ~(MS_ASYNC | MS_INVALIDATE | MS_SYNC))
 		goto out;
<span class="p_chunk">@@ -54,7 +55,7 @@</span> <span class="p_context"> SYSCALL_DEFINE3(msync, unsigned long, start, size_t, len, int, flags)</span>
 	 * If the interval [start,end) covers some unmapped address ranges,
 	 * just ignore them, but return -ENOMEM at the end.
 	 */
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_lock(mm, &amp;range);</span>
 	vma = find_vma(mm, start);
 	for (;;) {
 		struct file *file;
<span class="p_chunk">@@ -85,12 +86,12 @@</span> <span class="p_context"> SYSCALL_DEFINE3(msync, unsigned long, start, size_t, len, int, flags)</span>
 		if ((flags &amp; MS_SYNC) &amp;&amp; file &amp;&amp;
 				(vma-&gt;vm_flags &amp; VM_SHARED)) {
 			get_file(file);
<span class="p_del">-			up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+			mm_read_unlock(mm, &amp;range);</span>
 			error = vfs_fsync_range(file, fstart, fend, 1);
 			fput(file);
 			if (error || start &gt;= end)
 				goto out;
<span class="p_del">-			down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+			mm_read_lock(mm, &amp;range);</span>
 			vma = find_vma(mm, start);
 		} else {
 			if (start &gt;= end) {
<span class="p_chunk">@@ -101,7 +102,7 @@</span> <span class="p_context"> SYSCALL_DEFINE3(msync, unsigned long, start, size_t, len, int, flags)</span>
 		}
 	}
 out_unlock:
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(mm, &amp;range);</span>
 out:
 	return error ? : unmapped_error;
 }
<span class="p_header">diff --git a/mm/nommu.c b/mm/nommu.c</span>
<span class="p_header">index fc184f597d59..cee0359a8244 100644</span>
<span class="p_header">--- a/mm/nommu.c</span>
<span class="p_header">+++ b/mm/nommu.c</span>
<span class="p_chunk">@@ -183,10 +183,12 @@</span> <span class="p_context"> static long __get_user_pages_unlocked(struct task_struct *tsk,</span>
 			unsigned int gup_flags)
 {
 	long ret;
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_range_define(range);</span>
<span class="p_add">+</span>
<span class="p_add">+	mm_read_lock(mm, &amp;range);</span>
 	ret = __get_user_pages(tsk, mm, start, nr_pages, gup_flags, pages,
 				NULL, NULL);
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(mm, &amp;range);</span>
 	return ret;
 }
 
<span class="p_chunk">@@ -249,12 +251,13 @@</span> <span class="p_context"> void *vmalloc_user(unsigned long size)</span>
 	ret = __vmalloc(size, GFP_KERNEL | __GFP_ZERO, PAGE_KERNEL);
 	if (ret) {
 		struct vm_area_struct *vma;
<span class="p_add">+		mm_range_define(range);</span>
 
<span class="p_del">-		down_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_write_lock(current-&gt;mm, &amp;range);</span>
 		vma = find_vma(current-&gt;mm, (unsigned long)ret);
 		if (vma)
 			vma-&gt;vm_flags |= VM_USERMAP;
<span class="p_del">-		up_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_write_unlock(current-&gt;mm, &amp;range);</span>
 	}
 
 	return ret;
<span class="p_chunk">@@ -1647,10 +1650,11 @@</span> <span class="p_context"> int vm_munmap(unsigned long addr, size_t len)</span>
 {
 	struct mm_struct *mm = current-&gt;mm;
 	int ret;
<span class="p_add">+	mm_range_define(range);</span>
 
<span class="p_del">-	down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_lock(mm, &amp;range);</span>
 	ret = do_munmap(mm, addr, len, NULL);
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(mm, &amp;range);</span>
 	return ret;
 }
 EXPORT_SYMBOL(vm_munmap);
<span class="p_chunk">@@ -1736,10 +1740,11 @@</span> <span class="p_context"> SYSCALL_DEFINE5(mremap, unsigned long, addr, unsigned long, old_len,</span>
 		unsigned long, new_addr)
 {
 	unsigned long ret;
<span class="p_add">+	mm_range_define(range);</span>
 
<span class="p_del">-	down_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_lock(current-&gt;mm, &amp;range);</span>
 	ret = do_mremap(addr, old_len, new_len, flags, new_addr);
<span class="p_del">-	up_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(current-&gt;mm, &amp;range);</span>
 	return ret;
 }
 
<span class="p_chunk">@@ -1819,8 +1824,9 @@</span> <span class="p_context"> int __access_remote_vm(struct task_struct *tsk, struct mm_struct *mm,</span>
 {
 	struct vm_area_struct *vma;
 	int write = gup_flags &amp; FOLL_WRITE;
<span class="p_add">+	mm_range_define(range);</span>
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_lock(mm, &amp;range);</span>
 
 	/* the access must start within one of the target process&#39;s mappings */
 	vma = find_vma(mm, addr);
<span class="p_chunk">@@ -1842,7 +1848,7 @@</span> <span class="p_context"> int __access_remote_vm(struct task_struct *tsk, struct mm_struct *mm,</span>
 		len = 0;
 	}
 
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(mm, &amp;range);</span>
 
 	return len;
 }
<span class="p_header">diff --git a/mm/oom_kill.c b/mm/oom_kill.c</span>
<span class="p_header">index 04c9143a8625..8aaa00aa21bd 100644</span>
<span class="p_header">--- a/mm/oom_kill.c</span>
<span class="p_header">+++ b/mm/oom_kill.c</span>
<span class="p_chunk">@@ -471,6 +471,7 @@</span> <span class="p_context"> static bool __oom_reap_task_mm(struct task_struct *tsk, struct mm_struct *mm)</span>
 	struct mmu_gather tlb;
 	struct vm_area_struct *vma;
 	bool ret = true;
<span class="p_add">+	mm_range_define(range);</span>
 
 	/*
 	 * We have to make sure to not race with the victim exit path
<span class="p_chunk">@@ -488,7 +489,7 @@</span> <span class="p_context"> static bool __oom_reap_task_mm(struct task_struct *tsk, struct mm_struct *mm)</span>
 	 */
 	mutex_lock(&amp;oom_lock);
 
<span class="p_del">-	if (!down_read_trylock(&amp;mm-&gt;mmap_sem)) {</span>
<span class="p_add">+	if (!mm_read_trylock(mm, &amp;range)) {</span>
 		ret = false;
 		goto unlock_oom;
 	}
<span class="p_chunk">@@ -499,7 +500,7 @@</span> <span class="p_context"> static bool __oom_reap_task_mm(struct task_struct *tsk, struct mm_struct *mm)</span>
 	 * and delayed __mmput doesn&#39;t matter that much
 	 */
 	if (!mmget_not_zero(mm)) {
<span class="p_del">-		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_unlock(mm, &amp;range);</span>
 		goto unlock_oom;
 	}
 
<span class="p_chunk">@@ -536,7 +537,7 @@</span> <span class="p_context"> static bool __oom_reap_task_mm(struct task_struct *tsk, struct mm_struct *mm)</span>
 			K(get_mm_counter(mm, MM_ANONPAGES)),
 			K(get_mm_counter(mm, MM_FILEPAGES)),
 			K(get_mm_counter(mm, MM_SHMEMPAGES)));
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(mm, &amp;range);</span>
 
 	/*
 	 * Drop our reference but make sure the mmput slow path is called from a
<span class="p_header">diff --git a/mm/process_vm_access.c b/mm/process_vm_access.c</span>
<span class="p_header">index fb4f2b96d488..27abb4f4ea9f 100644</span>
<span class="p_header">--- a/mm/process_vm_access.c</span>
<span class="p_header">+++ b/mm/process_vm_access.c</span>
<span class="p_chunk">@@ -90,6 +90,7 @@</span> <span class="p_context"> static int process_vm_rw_single_vec(unsigned long addr,</span>
 	unsigned long max_pages_per_loop = PVM_MAX_KMALLOC_PAGES
 		/ sizeof(struct pages *);
 	unsigned int flags = 0;
<span class="p_add">+	mm_range_define(range);</span>
 
 	/* Work out address and page range required */
 	if (len == 0)
<span class="p_chunk">@@ -109,12 +110,12 @@</span> <span class="p_context"> static int process_vm_rw_single_vec(unsigned long addr,</span>
 		 * access remotely because task/mm might not
 		 * current/current-&gt;mm
 		 */
<span class="p_del">-		down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_lock(mm, &amp;range);</span>
 		pages = get_user_pages_remote(task, mm, pa, pages, flags,
 					      process_pages, NULL, &amp;locked,
<span class="p_del">-					      NULL);</span>
<span class="p_add">+					      &amp;range);</span>
 		if (locked)
<span class="p_del">-			up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+			mm_read_unlock(mm, &amp;range);</span>
 		if (pages &lt;= 0)
 			return -EFAULT;
 
<span class="p_header">diff --git a/mm/shmem.c b/mm/shmem.c</span>
<span class="p_header">index e67d6ba4e98e..d7b0658c8596 100644</span>
<span class="p_header">--- a/mm/shmem.c</span>
<span class="p_header">+++ b/mm/shmem.c</span>
<span class="p_chunk">@@ -1951,7 +1951,7 @@</span> <span class="p_context"> static int shmem_fault(struct vm_fault *vmf)</span>
 			if ((vmf-&gt;flags &amp; FAULT_FLAG_ALLOW_RETRY) &amp;&amp;
 			   !(vmf-&gt;flags &amp; FAULT_FLAG_RETRY_NOWAIT)) {
 				/* It&#39;s polite to up mmap_sem if we can */
<span class="p_del">-				up_read(&amp;vma-&gt;vm_mm-&gt;mmap_sem);</span>
<span class="p_add">+				mm_read_unlock(vma-&gt;vm_mm, vmf-&gt;lockrange);</span>
 				ret = VM_FAULT_RETRY;
 			}
 
<span class="p_header">diff --git a/mm/swapfile.c b/mm/swapfile.c</span>
<span class="p_header">index 4f6cba1b6632..18c1645df43d 100644</span>
<span class="p_header">--- a/mm/swapfile.c</span>
<span class="p_header">+++ b/mm/swapfile.c</span>
<span class="p_chunk">@@ -1597,15 +1597,16 @@</span> <span class="p_context"> static int unuse_mm(struct mm_struct *mm,</span>
 {
 	struct vm_area_struct *vma;
 	int ret = 0;
<span class="p_add">+	mm_range_define(range);</span>
 
<span class="p_del">-	if (!down_read_trylock(&amp;mm-&gt;mmap_sem)) {</span>
<span class="p_add">+	if (!mm_read_trylock(mm, &amp;range)) {</span>
 		/*
 		 * Activate page so shrink_inactive_list is unlikely to unmap
 		 * its ptes while lock is dropped, so swapoff can make progress.
 		 */
 		activate_page(page);
 		unlock_page(page);
<span class="p_del">-		down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_lock(mm, &amp;range);</span>
 		lock_page(page);
 	}
 	for (vma = mm-&gt;mmap; vma; vma = vma-&gt;vm_next) {
<span class="p_chunk">@@ -1613,7 +1614,7 @@</span> <span class="p_context"> static int unuse_mm(struct mm_struct *mm,</span>
 			break;
 		cond_resched();
 	}
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(mm, &amp;range);</span>
 	return (ret &lt; 0)? ret: 0;
 }
 
<span class="p_header">diff --git a/mm/userfaultfd.c b/mm/userfaultfd.c</span>
<span class="p_header">index ae2babc46fa5..a8f3b2955eda 100644</span>
<span class="p_header">--- a/mm/userfaultfd.c</span>
<span class="p_header">+++ b/mm/userfaultfd.c</span>
<span class="p_chunk">@@ -182,7 +182,7 @@</span> <span class="p_context"> static __always_inline ssize_t __mcopy_atomic_hugetlb(struct mm_struct *dst_mm,</span>
 	 * feature is not supported.
 	 */
 	if (zeropage) {
<span class="p_del">-		up_read(&amp;dst_mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_unlock(dst_mm, range);</span>
 		return -EINVAL;
 	}
 
<span class="p_chunk">@@ -280,7 +280,7 @@</span> <span class="p_context"> static __always_inline ssize_t __mcopy_atomic_hugetlb(struct mm_struct *dst_mm,</span>
 		cond_resched();
 
 		if (unlikely(err == -EFAULT)) {
<span class="p_del">-			up_read(&amp;dst_mm-&gt;mmap_sem);</span>
<span class="p_add">+			mm_read_unlock(dst_mm, range);</span>
 			BUG_ON(!page);
 
 			err = copy_huge_page_from_user(page,
<span class="p_chunk">@@ -290,7 +290,7 @@</span> <span class="p_context"> static __always_inline ssize_t __mcopy_atomic_hugetlb(struct mm_struct *dst_mm,</span>
 				err = -EFAULT;
 				goto out;
 			}
<span class="p_del">-			down_read(&amp;dst_mm-&gt;mmap_sem);</span>
<span class="p_add">+			mm_read_lock(dst_mm, range);</span>
 
 			dst_vma = NULL;
 			goto retry;
<span class="p_chunk">@@ -310,7 +310,7 @@</span> <span class="p_context"> static __always_inline ssize_t __mcopy_atomic_hugetlb(struct mm_struct *dst_mm,</span>
 	}
 
 out_unlock:
<span class="p_del">-	up_read(&amp;dst_mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(dst_mm, range);</span>
 out:
 	if (page) {
 		/*
<span class="p_chunk">@@ -391,6 +391,7 @@</span> <span class="p_context"> static __always_inline ssize_t __mcopy_atomic(struct mm_struct *dst_mm,</span>
 	unsigned long src_addr, dst_addr;
 	long copied;
 	struct page *page;
<span class="p_add">+	mm_range_define(range);</span>
 
 	/*
 	 * Sanitize the command parameters:
<span class="p_chunk">@@ -407,7 +408,7 @@</span> <span class="p_context"> static __always_inline ssize_t __mcopy_atomic(struct mm_struct *dst_mm,</span>
 	copied = 0;
 	page = NULL;
 retry:
<span class="p_del">-	down_read(&amp;dst_mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_lock(dst_mm, &amp;range);</span>
 
 	/*
 	 * Make sure the vma is not shared, that the dst range is
<span class="p_chunk">@@ -520,7 +521,7 @@</span> <span class="p_context"> static __always_inline ssize_t __mcopy_atomic(struct mm_struct *dst_mm,</span>
 		if (unlikely(err == -EFAULT)) {
 			void *page_kaddr;
 
<span class="p_del">-			up_read(&amp;dst_mm-&gt;mmap_sem);</span>
<span class="p_add">+			mm_read_unlock(dst_mm, &amp;range);</span>
 			BUG_ON(!page);
 
 			page_kaddr = kmap(page);
<span class="p_chunk">@@ -549,7 +550,7 @@</span> <span class="p_context"> static __always_inline ssize_t __mcopy_atomic(struct mm_struct *dst_mm,</span>
 	}
 
 out_unlock:
<span class="p_del">-	up_read(&amp;dst_mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(dst_mm, &amp;range);</span>
 out:
 	if (page)
 		put_page(page);
<span class="p_header">diff --git a/mm/util.c b/mm/util.c</span>
<span class="p_header">index 464df3489903..8c19e81c057a 100644</span>
<span class="p_header">--- a/mm/util.c</span>
<span class="p_header">+++ b/mm/util.c</span>
<span class="p_chunk">@@ -301,14 +301,15 @@</span> <span class="p_context"> unsigned long vm_mmap_pgoff(struct file *file, unsigned long addr,</span>
 	struct mm_struct *mm = current-&gt;mm;
 	unsigned long populate;
 	LIST_HEAD(uf);
<span class="p_add">+	mm_range_define(range);</span>
 
 	ret = security_mmap_file(file, prot, flag);
 	if (!ret) {
<span class="p_del">-		if (down_write_killable(&amp;mm-&gt;mmap_sem))</span>
<span class="p_add">+		if (mm_write_lock_killable(mm, &amp;range))</span>
 			return -EINTR;
 		ret = do_mmap_pgoff(file, addr, len, prot, flag, pgoff,
 				    &amp;populate, &amp;uf);
<span class="p_del">-		up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_write_unlock(mm, &amp;range);</span>
 		userfaultfd_unmap_complete(mm, &amp;uf);
 		if (populate)
 			mm_populate(ret, populate);
<span class="p_chunk">@@ -672,17 +673,19 @@</span> <span class="p_context"> int get_cmdline(struct task_struct *task, char *buffer, int buflen)</span>
 	unsigned int len;
 	struct mm_struct *mm = get_task_mm(task);
 	unsigned long arg_start, arg_end, env_start, env_end;
<span class="p_add">+	mm_range_define(range);</span>
<span class="p_add">+</span>
 	if (!mm)
 		goto out;
 	if (!mm-&gt;arg_end)
 		goto out_mm;	/* Shh! No looking before we&#39;re done */
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_lock(mm, &amp;range);</span>
 	arg_start = mm-&gt;arg_start;
 	arg_end = mm-&gt;arg_end;
 	env_start = mm-&gt;env_start;
 	env_end = mm-&gt;env_end;
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(mm, &amp;range);</span>
 
 	len = arg_end - arg_start;
 
<span class="p_header">diff --git a/virt/kvm/async_pf.c b/virt/kvm/async_pf.c</span>
<span class="p_header">index bb298a200cd3..69820a9828c1 100644</span>
<span class="p_header">--- a/virt/kvm/async_pf.c</span>
<span class="p_header">+++ b/virt/kvm/async_pf.c</span>
<span class="p_chunk">@@ -78,6 +78,7 @@</span> <span class="p_context"> static void async_pf_execute(struct work_struct *work)</span>
 	unsigned long addr = apf-&gt;addr;
 	gva_t gva = apf-&gt;gva;
 	int locked = 1;
<span class="p_add">+	mm_range_define(range);</span>
 
 	might_sleep();
 
<span class="p_chunk">@@ -86,11 +87,11 @@</span> <span class="p_context"> static void async_pf_execute(struct work_struct *work)</span>
 	 * mm and might be done in another context, so we must
 	 * access remotely.
 	 */
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_lock(mm, &amp;range);</span>
 	get_user_pages_remote(NULL, mm, addr, 1, FOLL_WRITE, NULL, NULL,
<span class="p_del">-			&amp;locked);</span>
<span class="p_add">+			      &amp;locked, &amp;range);</span>
 	if (locked)
<span class="p_del">-		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_unlock(mm, &amp;range);</span>
 
 	kvm_async_page_present_sync(vcpu, apf);
 
<span class="p_header">diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c</span>
<span class="p_header">index 9eb9a1998060..5e2c8a3945ce 100644</span>
<span class="p_header">--- a/virt/kvm/kvm_main.c</span>
<span class="p_header">+++ b/virt/kvm/kvm_main.c</span>
<span class="p_chunk">@@ -1242,6 +1242,7 @@</span> <span class="p_context"> unsigned long kvm_host_page_size(struct kvm *kvm, gfn_t gfn)</span>
 {
 	struct vm_area_struct *vma;
 	unsigned long addr, size;
<span class="p_add">+	mm_range_define(range);</span>
 
 	size = PAGE_SIZE;
 
<span class="p_chunk">@@ -1249,7 +1250,7 @@</span> <span class="p_context"> unsigned long kvm_host_page_size(struct kvm *kvm, gfn_t gfn)</span>
 	if (kvm_is_error_hva(addr))
 		return PAGE_SIZE;
 
<span class="p_del">-	down_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_lock(current-&gt;mm, &amp;range);</span>
 	vma = find_vma(current-&gt;mm, addr);
 	if (!vma)
 		goto out;
<span class="p_chunk">@@ -1257,7 +1258,7 @@</span> <span class="p_context"> unsigned long kvm_host_page_size(struct kvm *kvm, gfn_t gfn)</span>
 	size = vma_kernel_pagesize(vma);
 
 out:
<span class="p_del">-	up_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(current-&gt;mm, &amp;range);</span>
 
 	return size;
 }
<span class="p_chunk">@@ -1397,6 +1398,7 @@</span> <span class="p_context"> static int hva_to_pfn_slow(unsigned long addr, bool *async, bool write_fault,</span>
 {
 	struct page *page[1];
 	int npages = 0;
<span class="p_add">+	mm_range_define(range);</span>
 
 	might_sleep();
 
<span class="p_chunk">@@ -1404,9 +1406,9 @@</span> <span class="p_context"> static int hva_to_pfn_slow(unsigned long addr, bool *async, bool write_fault,</span>
 		*writable = write_fault;
 
 	if (async) {
<span class="p_del">-		down_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_lock(current-&gt;mm, &amp;range);</span>
 		npages = get_user_page_nowait(addr, write_fault, page);
<span class="p_del">-		up_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_unlock(current-&gt;mm, &amp;range);</span>
 	} else {
 		unsigned int flags = FOLL_HWPOISON;
 
<span class="p_chunk">@@ -1448,7 +1450,11 @@</span> <span class="p_context"> static bool vma_is_valid(struct vm_area_struct *vma, bool write_fault)</span>
 
 static int hva_to_pfn_remapped(struct vm_area_struct *vma,
 			       unsigned long addr, bool *async,
<span class="p_del">-			       bool write_fault, kvm_pfn_t *p_pfn)</span>
<span class="p_add">+			       bool write_fault, kvm_pfn_t *p_pfn</span>
<span class="p_add">+#ifdef CONFIG_MEM_RANGE_LOCK</span>
<span class="p_add">+			       , struct range_lock *range</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	)</span>
 {
 	unsigned long pfn;
 	int r;
<span class="p_chunk">@@ -1462,7 +1468,7 @@</span> <span class="p_context"> static int hva_to_pfn_remapped(struct vm_area_struct *vma,</span>
 		bool unlocked = false;
 		r = fixup_user_fault(current, current-&gt;mm, addr,
 				     (write_fault ? FAULT_FLAG_WRITE : 0),
<span class="p_del">-				     &amp;unlocked, NULL);</span>
<span class="p_add">+				     &amp;unlocked, range);</span>
 		if (unlocked)
 			return -EAGAIN;
 		if (r)
<span class="p_chunk">@@ -1512,6 +1518,7 @@</span> <span class="p_context"> static kvm_pfn_t hva_to_pfn(unsigned long addr, bool atomic, bool *async,</span>
 	struct vm_area_struct *vma;
 	kvm_pfn_t pfn = 0;
 	int npages, r;
<span class="p_add">+	mm_range_define(range);</span>
 
 	/* we can do it either atomically or asynchronously, not both */
 	BUG_ON(atomic &amp;&amp; async);
<span class="p_chunk">@@ -1526,7 +1533,7 @@</span> <span class="p_context"> static kvm_pfn_t hva_to_pfn(unsigned long addr, bool atomic, bool *async,</span>
 	if (npages == 1)
 		return pfn;
 
<span class="p_del">-	down_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_lock(current-&gt;mm, &amp;range);</span>
 	if (npages == -EHWPOISON ||
 	      (!async &amp;&amp; check_user_page_hwpoison(addr))) {
 		pfn = KVM_PFN_ERR_HWPOISON;
<span class="p_chunk">@@ -1539,7 +1546,11 @@</span> <span class="p_context"> static kvm_pfn_t hva_to_pfn(unsigned long addr, bool atomic, bool *async,</span>
 	if (vma == NULL)
 		pfn = KVM_PFN_ERR_FAULT;
 	else if (vma-&gt;vm_flags &amp; (VM_IO | VM_PFNMAP)) {
<span class="p_del">-		r = hva_to_pfn_remapped(vma, addr, async, write_fault, &amp;pfn);</span>
<span class="p_add">+		r = hva_to_pfn_remapped(vma, addr, async, write_fault, &amp;pfn</span>
<span class="p_add">+#ifdef CONFIG_MEM_RANGE_LOCK</span>
<span class="p_add">+					, &amp;range</span>
<span class="p_add">+#endif</span>
<span class="p_add">+			);</span>
 		if (r == -EAGAIN)
 			goto retry;
 		if (r &lt; 0)
<span class="p_chunk">@@ -1550,7 +1561,7 @@</span> <span class="p_context"> static kvm_pfn_t hva_to_pfn(unsigned long addr, bool atomic, bool *async,</span>
 		pfn = KVM_PFN_ERR_FAULT;
 	}
 exit:
<span class="p_del">-	up_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(current-&gt;mm, &amp;range);</span>
 	return pfn;
 }
 

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



