
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[2/2] xen/swiotlb: Add support for 64KB page granularity - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [2/2] xen/swiotlb: Add support for 64KB page granularity</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=72882">Julien Grall</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Sept. 10, 2015, 1:42 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1441892562-22059-3-git-send-email-julien.grall@citrix.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/7154331/mbox/"
   >mbox</a>
|
   <a href="/patch/7154331/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/7154331/">/patch/7154331/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork2.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork2.web.kernel.org (Postfix) with ESMTP id A6851BEEC1
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 10 Sep 2015 13:44:44 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 6630C2088F
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 10 Sep 2015 13:44:40 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id E9B3D208C4
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 10 Sep 2015 13:44:38 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1754578AbbIJNof (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 10 Sep 2015 09:44:35 -0400
Received: from smtp02.citrix.com ([66.165.176.63]:17446 &quot;EHLO
	SMTP02.CITRIX.COM&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1754135AbbIJNoD (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 10 Sep 2015 09:44:03 -0400
X-IronPort-AV: E=Sophos;i=&quot;5.17,504,1437436800&quot;; d=&quot;scan&#39;208&quot;;a=&quot;302700847&quot;
From: Julien Grall &lt;julien.grall@citrix.com&gt;
To: &lt;xen-devel@lists.xenproject.org&gt;
CC: &lt;linux-arm-kernel@lists.infradead.org&gt;, &lt;ian.campbell@citrix.com&gt;,
	&lt;stefano.stabellini@eu.citrix.com&gt;, &lt;linux-kernel@vger.kernel.org&gt;,
	&quot;Julien Grall&quot; &lt;julien.grall@citrix.com&gt;,
	Russell King &lt;linux@arm.linux.org.uk&gt;,
	Konrad Rzeszutek Wilk &lt;konrad.wilk@oracle.com&gt;,
	Boris Ostrovsky &lt;boris.ostrovsky@oracle.com&gt;,
	David Vrabel &lt;david.vrabel@citrix.com&gt;
Subject: [PATCH 2/2] xen/swiotlb: Add support for 64KB page granularity
Date: Thu, 10 Sep 2015 14:42:42 +0100
Message-ID: &lt;1441892562-22059-3-git-send-email-julien.grall@citrix.com&gt;
X-Mailer: git-send-email 2.1.4
In-Reply-To: &lt;1441892562-22059-1-git-send-email-julien.grall@citrix.com&gt;
References: &lt;1441892562-22059-1-git-send-email-julien.grall@citrix.com&gt;
MIME-Version: 1.0
Content-Type: text/plain
X-DLP: MIA2
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-6.9 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	T_RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=72882">Julien Grall</a> - Sept. 10, 2015, 1:42 p.m.</div>
<pre class="content">
Swiotlb is used on ARM64 to support DMA on platform where devices are
not protected by an SMMU. Furthermore it&#39;s only enabled for DOM0.

While Xen is always using 4KB page granularity in the stage-2 page table,
Linux ARM64 may either use 4KB or 64KB. This means that a Linux page
can be spanned accross multiple Xen page.

The Swiotlb code has to validate that the buffer used for DMA is
physically contiguous in the memory. As a Linux page can&#39;t be shared
between local memory and foreign page by design (the balloon code always
removing entirely a Linux page), the changes in the code are very
minimal because we only need to check the first Xen PFN.

Note that it may be possible to optimize the function
check_page_physically_contiguous to avoid looping over every Xen PFN
for local memory. Although I will let this optimization for a follow-up.
<span class="signed-off-by">
Signed-off-by: Julien Grall &lt;julien.grall@citrix.com&gt;</span>
Cc: Stefano Stabellini &lt;stefano.stabellini@eu.citrix.com&gt;
Cc: Russell King &lt;linux@arm.linux.org.uk&gt;
Cc: Konrad Rzeszutek Wilk &lt;konrad.wilk@oracle.com&gt;
Cc: Boris Ostrovsky &lt;boris.ostrovsky@oracle.com&gt;
Cc: David Vrabel &lt;david.vrabel@citrix.com&gt;
---
 arch/arm/include/asm/xen/page-coherent.h | 26 +++++++++++++--------
 arch/arm/xen/mm.c                        | 38 ++++++++++++++++++++++---------
 drivers/xen/swiotlb-xen.c                | 39 ++++++++++++++++----------------
 3 files changed, 63 insertions(+), 40 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=999">Stefano Stabellini</a> - Sept. 10, 2015, 4:30 p.m.</div>
<pre class="content">
On Thu, 10 Sep 2015, Julien Grall wrote:
<span class="quote">&gt; Swiotlb is used on ARM64 to support DMA on platform where devices are</span>
<span class="quote">&gt; not protected by an SMMU. Furthermore it&#39;s only enabled for DOM0.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; While Xen is always using 4KB page granularity in the stage-2 page table,</span>
<span class="quote">&gt; Linux ARM64 may either use 4KB or 64KB. This means that a Linux page</span>
<span class="quote">&gt; can be spanned accross multiple Xen page.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The Swiotlb code has to validate that the buffer used for DMA is</span>
<span class="quote">&gt; physically contiguous in the memory. As a Linux page can&#39;t be shared</span>
<span class="quote">&gt; between local memory and foreign page by design (the balloon code always</span>
<span class="quote">&gt; removing entirely a Linux page), the changes in the code are very</span>
<span class="quote">&gt; minimal because we only need to check the first Xen PFN.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Note that it may be possible to optimize the function</span>
<span class="quote">&gt; check_page_physically_contiguous to avoid looping over every Xen PFN</span>
<span class="quote">&gt; for local memory. Although I will let this optimization for a follow-up.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Julien Grall &lt;julien.grall@citrix.com&gt;</span>
<span class="quote">&gt; Cc: Stefano Stabellini &lt;stefano.stabellini@eu.citrix.com&gt;</span>
<span class="quote">&gt; Cc: Russell King &lt;linux@arm.linux.org.uk&gt;</span>
<span class="quote">&gt; Cc: Konrad Rzeszutek Wilk &lt;konrad.wilk@oracle.com&gt;</span>
<span class="quote">&gt; Cc: Boris Ostrovsky &lt;boris.ostrovsky@oracle.com&gt;</span>
<span class="quote">&gt; Cc: David Vrabel &lt;david.vrabel@citrix.com&gt;</span>
<span class="reviewed-by">
Reviewed-by: Stefano Stabellini &lt;stefano.stabellini@eu.citrix.com&gt;</span>
<span class="quote">

&gt;  arch/arm/include/asm/xen/page-coherent.h | 26 +++++++++++++--------</span>
<span class="quote">&gt;  arch/arm/xen/mm.c                        | 38 ++++++++++++++++++++++---------</span>
<span class="quote">&gt;  drivers/xen/swiotlb-xen.c                | 39 ++++++++++++++++----------------</span>
<span class="quote">&gt;  3 files changed, 63 insertions(+), 40 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/arch/arm/include/asm/xen/page-coherent.h b/arch/arm/include/asm/xen/page-coherent.h</span>
<span class="quote">&gt; index efd5624..0375c8c 100644</span>
<span class="quote">&gt; --- a/arch/arm/include/asm/xen/page-coherent.h</span>
<span class="quote">&gt; +++ b/arch/arm/include/asm/xen/page-coherent.h</span>
<span class="quote">&gt; @@ -35,11 +35,15 @@ static inline void xen_dma_map_page(struct device *hwdev, struct page *page,</span>
<span class="quote">&gt;  	     dma_addr_t dev_addr, unsigned long offset, size_t size,</span>
<span class="quote">&gt;  	     enum dma_data_direction dir, struct dma_attrs *attrs)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	bool local = PFN_DOWN(dev_addr) == page_to_pfn(page);</span>
<span class="quote">&gt; -	/* Dom0 is mapped 1:1, so if pfn == mfn the page is local otherwise</span>
<span class="quote">&gt; -	 * is a foreign page grant-mapped in dom0. If the page is local we</span>
<span class="quote">&gt; -	 * can safely call the native dma_ops function, otherwise we call</span>
<span class="quote">&gt; -	 * the xen specific function. */</span>
<span class="quote">&gt; +	bool local = XEN_PFN_DOWN(dev_addr) == page_to_xen_pfn(page);</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * Dom0 is mapped 1:1, while the Linux page can be spanned accross</span>
<span class="quote">&gt; +	 * multiple Xen page, it&#39;s not possible to have a mix of local and</span>
<span class="quote">&gt; +	 * foreign Xen page. So if the first xen_pfn == mfn the page is local</span>
<span class="quote">&gt; +	 * otherwise it&#39;s a foreign page grant-mapped in dom0. If the page is</span>
<span class="quote">&gt; +	 * local we can safely call the native dma_ops function, otherwise we</span>
<span class="quote">&gt; +	 * call the xen specific function.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt;  	if (local)</span>
<span class="quote">&gt;  		__generic_dma_ops(hwdev)-&gt;map_page(hwdev, page, offset, size, dir, attrs);</span>
<span class="quote">&gt;  	else</span>
<span class="quote">&gt; @@ -51,10 +55,14 @@ static inline void xen_dma_unmap_page(struct device *hwdev, dma_addr_t handle,</span>
<span class="quote">&gt;  		struct dma_attrs *attrs)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	unsigned long pfn = PFN_DOWN(handle);</span>
<span class="quote">&gt; -	/* Dom0 is mapped 1:1, so calling pfn_valid on a foreign mfn will</span>
<span class="quote">&gt; -	 * always return false. If the page is local we can safely call the</span>
<span class="quote">&gt; -	 * native dma_ops function, otherwise we call the xen specific</span>
<span class="quote">&gt; -	 * function. */</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * Dom0 is mapped 1:1, while the Linux page can be spanned accross</span>
<span class="quote">&gt; +	 * multiple Xen page, it&#39;s not possible to have a mix of local and</span>
<span class="quote">&gt; +	 * foreign Xen page. Dom0 is mapped 1:1, so calling pfn_valid on a</span>
<span class="quote">&gt; +	 * foreign mfn will always return false. If the page is local we can</span>
<span class="quote">&gt; +	 * safely call the native dma_ops function, otherwise we call the xen</span>
<span class="quote">&gt; +	 * specific function.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt;  	if (pfn_valid(pfn)) {</span>
<span class="quote">&gt;  		if (__generic_dma_ops(hwdev)-&gt;unmap_page)</span>
<span class="quote">&gt;  			__generic_dma_ops(hwdev)-&gt;unmap_page(hwdev, handle, size, dir, attrs);</span>
<span class="quote">&gt; diff --git a/arch/arm/xen/mm.c b/arch/arm/xen/mm.c</span>
<span class="quote">&gt; index 7b517e91..7c34f71 100644</span>
<span class="quote">&gt; --- a/arch/arm/xen/mm.c</span>
<span class="quote">&gt; +++ b/arch/arm/xen/mm.c</span>
<span class="quote">&gt; @@ -48,22 +48,22 @@ static void dma_cache_maint(dma_addr_t handle, unsigned long offset,</span>
<span class="quote">&gt;  	size_t size, enum dma_data_direction dir, enum dma_cache_op op)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct gnttab_cache_flush cflush;</span>
<span class="quote">&gt; -	unsigned long pfn;</span>
<span class="quote">&gt; +	unsigned long xen_pfn;</span>
<span class="quote">&gt;  	size_t left = size;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	pfn = (handle &gt;&gt; PAGE_SHIFT) + offset / PAGE_SIZE;</span>
<span class="quote">&gt; -	offset %= PAGE_SIZE;</span>
<span class="quote">&gt; +	xen_pfn = (handle &gt;&gt; XEN_PAGE_SHIFT) + offset / XEN_PAGE_SIZE;</span>
<span class="quote">&gt; +	offset %= XEN_PAGE_SIZE;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	do {</span>
<span class="quote">&gt;  		size_t len = left;</span>
<span class="quote">&gt;  	</span>
<span class="quote">&gt;  		/* buffers in highmem or foreign pages cannot cross page</span>
<span class="quote">&gt;  		 * boundaries */</span>
<span class="quote">&gt; -		if (len + offset &gt; PAGE_SIZE)</span>
<span class="quote">&gt; -			len = PAGE_SIZE - offset;</span>
<span class="quote">&gt; +		if (len + offset &gt; XEN_PAGE_SIZE)</span>
<span class="quote">&gt; +			len = XEN_PAGE_SIZE - offset;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		cflush.op = 0;</span>
<span class="quote">&gt; -		cflush.a.dev_bus_addr = pfn &lt;&lt; PAGE_SHIFT;</span>
<span class="quote">&gt; +		cflush.a.dev_bus_addr = xen_pfn &lt;&lt; XEN_PAGE_SHIFT;</span>
<span class="quote">&gt;  		cflush.offset = offset;</span>
<span class="quote">&gt;  		cflush.length = len;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -79,7 +79,7 @@ static void dma_cache_maint(dma_addr_t handle, unsigned long offset,</span>
<span class="quote">&gt;  			HYPERVISOR_grant_table_op(GNTTABOP_cache_flush, &amp;cflush, 1);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		offset = 0;</span>
<span class="quote">&gt; -		pfn++;</span>
<span class="quote">&gt; +		xen_pfn++;</span>
<span class="quote">&gt;  		left -= len;</span>
<span class="quote">&gt;  	} while (left);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; @@ -141,10 +141,26 @@ bool xen_arch_need_swiotlb(struct device *dev,</span>
<span class="quote">&gt;  			   phys_addr_t phys,</span>
<span class="quote">&gt;  			   dma_addr_t dev_addr)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	unsigned long pfn = PFN_DOWN(phys);</span>
<span class="quote">&gt; -	unsigned long bfn = PFN_DOWN(dev_addr);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	return (!hypercall_cflush &amp;&amp; (pfn != bfn) &amp;&amp; !is_device_dma_coherent(dev));</span>
<span class="quote">&gt; +	unsigned int xen_pfn = XEN_PFN_DOWN(phys);</span>
<span class="quote">&gt; +	unsigned int bfn = XEN_PFN_DOWN(dev_addr);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * The swiotlb buffer should be used if</span>
<span class="quote">&gt; +	 *	- Xen doesn&#39;t have the cache flush hypercall</span>
<span class="quote">&gt; +	 *	- The Linux page refers to foreign memory</span>
<span class="quote">&gt; +	 *	- The device doesn&#39;t support coherent DMA request</span>
<span class="quote">&gt; +	 *</span>
<span class="quote">&gt; +	 * The Linux page may be spanned acrros multiple Xen page, although</span>
<span class="quote">&gt; +	 * it&#39;s not possible to have a mix of local and foreign Xen page.</span>
<span class="quote">&gt; +	 * Furthermore, range_straddles_page_boundary is already checking</span>
<span class="quote">&gt; +	 * if buffer is physically contiguous in the host RAM.</span>
<span class="quote">&gt; +	 *</span>
<span class="quote">&gt; +	 * Therefore we only need to check the first Xen page to know if we</span>
<span class="quote">&gt; +	 * require a bounce buffer because the device doesn&#39;t support coherent</span>
<span class="quote">&gt; +	 * memory and we are not able to flush the cache.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	return (!hypercall_cflush &amp;&amp; (xen_pfn != bfn) &amp;&amp;</span>
<span class="quote">&gt; +		!is_device_dma_coherent(dev));</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  int xen_create_contiguous_region(phys_addr_t pstart, unsigned int order,</span>
<span class="quote">&gt; diff --git a/drivers/xen/swiotlb-xen.c b/drivers/xen/swiotlb-xen.c</span>
<span class="quote">&gt; index cfe755d..5854bf5 100644</span>
<span class="quote">&gt; --- a/drivers/xen/swiotlb-xen.c</span>
<span class="quote">&gt; +++ b/drivers/xen/swiotlb-xen.c</span>
<span class="quote">&gt; @@ -76,27 +76,27 @@ static unsigned long xen_io_tlb_nslabs;</span>
<span class="quote">&gt;  static u64 start_dma_addr;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt; - * Both of these functions should avoid PFN_PHYS because phys_addr_t</span>
<span class="quote">&gt; + * Both of these functions should avoid XEN_PFN_PHYS because phys_addr_t</span>
<span class="quote">&gt;   * can be 32bit when dma_addr_t is 64bit leading to a loss in</span>
<span class="quote">&gt;   * information if the shift is done before casting to 64bit.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  static inline dma_addr_t xen_phys_to_bus(phys_addr_t paddr)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	unsigned long bfn = pfn_to_bfn(PFN_DOWN(paddr));</span>
<span class="quote">&gt; -	dma_addr_t dma = (dma_addr_t)bfn &lt;&lt; PAGE_SHIFT;</span>
<span class="quote">&gt; +	unsigned long bfn = pfn_to_bfn(XEN_PFN_DOWN(paddr));</span>
<span class="quote">&gt; +	dma_addr_t dma = (dma_addr_t)bfn &lt;&lt; XEN_PAGE_SHIFT;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	dma |= paddr &amp; ~PAGE_MASK;</span>
<span class="quote">&gt; +	dma |= paddr &amp; ~XEN_PAGE_MASK;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	return dma;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static inline phys_addr_t xen_bus_to_phys(dma_addr_t baddr)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	unsigned long pfn = bfn_to_pfn(PFN_DOWN(baddr));</span>
<span class="quote">&gt; -	dma_addr_t dma = (dma_addr_t)pfn &lt;&lt; PAGE_SHIFT;</span>
<span class="quote">&gt; +	unsigned long xen_pfn = bfn_to_pfn(XEN_PFN_DOWN(baddr));</span>
<span class="quote">&gt; +	dma_addr_t dma = (dma_addr_t)xen_pfn &lt;&lt; XEN_PAGE_SHIFT;</span>
<span class="quote">&gt;  	phys_addr_t paddr = dma;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	paddr |= baddr &amp; ~PAGE_MASK;</span>
<span class="quote">&gt; +	paddr |= baddr &amp; ~XEN_PAGE_MASK;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	return paddr;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; @@ -106,7 +106,7 @@ static inline dma_addr_t xen_virt_to_bus(void *address)</span>
<span class="quote">&gt;  	return xen_phys_to_bus(virt_to_phys(address));</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static int check_pages_physically_contiguous(unsigned long pfn,</span>
<span class="quote">&gt; +static int check_pages_physically_contiguous(unsigned long xen_pfn,</span>
<span class="quote">&gt;  					     unsigned int offset,</span>
<span class="quote">&gt;  					     size_t length)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; @@ -114,11 +114,11 @@ static int check_pages_physically_contiguous(unsigned long pfn,</span>
<span class="quote">&gt;  	int i;</span>
<span class="quote">&gt;  	int nr_pages;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	next_bfn = pfn_to_bfn(pfn);</span>
<span class="quote">&gt; -	nr_pages = (offset + length + PAGE_SIZE-1) &gt;&gt; PAGE_SHIFT;</span>
<span class="quote">&gt; +	next_bfn = pfn_to_bfn(xen_pfn);</span>
<span class="quote">&gt; +	nr_pages = (offset + length + XEN_PAGE_SIZE-1) &gt;&gt; XEN_PAGE_SHIFT;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	for (i = 1; i &lt; nr_pages; i++) {</span>
<span class="quote">&gt; -		if (pfn_to_bfn(++pfn) != ++next_bfn)</span>
<span class="quote">&gt; +		if (pfn_to_bfn(++xen_pfn) != ++next_bfn)</span>
<span class="quote">&gt;  			return 0;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  	return 1;</span>
<span class="quote">&gt; @@ -126,28 +126,27 @@ static int check_pages_physically_contiguous(unsigned long pfn,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static inline int range_straddles_page_boundary(phys_addr_t p, size_t size)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	unsigned long pfn = PFN_DOWN(p);</span>
<span class="quote">&gt; -	unsigned int offset = p &amp; ~PAGE_MASK;</span>
<span class="quote">&gt; +	unsigned long xen_pfn = XEN_PFN_DOWN(p);</span>
<span class="quote">&gt; +	unsigned int offset = p &amp; ~XEN_PAGE_MASK;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (offset + size &lt;= PAGE_SIZE)</span>
<span class="quote">&gt; +	if (offset + size &lt;= XEN_PAGE_SIZE)</span>
<span class="quote">&gt;  		return 0;</span>
<span class="quote">&gt; -	if (check_pages_physically_contiguous(pfn, offset, size))</span>
<span class="quote">&gt; +	if (check_pages_physically_contiguous(xen_pfn, offset, size))</span>
<span class="quote">&gt;  		return 0;</span>
<span class="quote">&gt;  	return 1;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static int is_xen_swiotlb_buffer(dma_addr_t dma_addr)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	unsigned long bfn = PFN_DOWN(dma_addr);</span>
<span class="quote">&gt; -	unsigned long pfn = bfn_to_local_pfn(bfn);</span>
<span class="quote">&gt; -	phys_addr_t paddr;</span>
<span class="quote">&gt; +	unsigned long bfn = XEN_PFN_DOWN(dma_addr);</span>
<span class="quote">&gt; +	unsigned long xen_pfn = bfn_to_local_pfn(bfn);</span>
<span class="quote">&gt; +	phys_addr_t paddr = XEN_PFN_PHYS(xen_pfn);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/* If the address is outside our domain, it CAN</span>
<span class="quote">&gt;  	 * have the same virtual address as another address</span>
<span class="quote">&gt;  	 * in our domain. Therefore _only_ check address within our domain.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt; -	if (pfn_valid(pfn)) {</span>
<span class="quote">&gt; -		paddr = PFN_PHYS(pfn);</span>
<span class="quote">&gt; +	if (pfn_valid(PFN_DOWN(paddr))) {</span>
<span class="quote">&gt;  		return paddr &gt;= virt_to_phys(xen_io_tlb_start) &amp;&amp;</span>
<span class="quote">&gt;  		       paddr &lt; virt_to_phys(xen_io_tlb_end);</span>
<span class="quote">&gt;  	}</span>
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/arm/include/asm/xen/page-coherent.h b/arch/arm/include/asm/xen/page-coherent.h</span>
<span class="p_header">index efd5624..0375c8c 100644</span>
<span class="p_header">--- a/arch/arm/include/asm/xen/page-coherent.h</span>
<span class="p_header">+++ b/arch/arm/include/asm/xen/page-coherent.h</span>
<span class="p_chunk">@@ -35,11 +35,15 @@</span> <span class="p_context"> static inline void xen_dma_map_page(struct device *hwdev, struct page *page,</span>
 	     dma_addr_t dev_addr, unsigned long offset, size_t size,
 	     enum dma_data_direction dir, struct dma_attrs *attrs)
 {
<span class="p_del">-	bool local = PFN_DOWN(dev_addr) == page_to_pfn(page);</span>
<span class="p_del">-	/* Dom0 is mapped 1:1, so if pfn == mfn the page is local otherwise</span>
<span class="p_del">-	 * is a foreign page grant-mapped in dom0. If the page is local we</span>
<span class="p_del">-	 * can safely call the native dma_ops function, otherwise we call</span>
<span class="p_del">-	 * the xen specific function. */</span>
<span class="p_add">+	bool local = XEN_PFN_DOWN(dev_addr) == page_to_xen_pfn(page);</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Dom0 is mapped 1:1, while the Linux page can be spanned accross</span>
<span class="p_add">+	 * multiple Xen page, it&#39;s not possible to have a mix of local and</span>
<span class="p_add">+	 * foreign Xen page. So if the first xen_pfn == mfn the page is local</span>
<span class="p_add">+	 * otherwise it&#39;s a foreign page grant-mapped in dom0. If the page is</span>
<span class="p_add">+	 * local we can safely call the native dma_ops function, otherwise we</span>
<span class="p_add">+	 * call the xen specific function.</span>
<span class="p_add">+	 */</span>
 	if (local)
 		__generic_dma_ops(hwdev)-&gt;map_page(hwdev, page, offset, size, dir, attrs);
 	else
<span class="p_chunk">@@ -51,10 +55,14 @@</span> <span class="p_context"> static inline void xen_dma_unmap_page(struct device *hwdev, dma_addr_t handle,</span>
 		struct dma_attrs *attrs)
 {
 	unsigned long pfn = PFN_DOWN(handle);
<span class="p_del">-	/* Dom0 is mapped 1:1, so calling pfn_valid on a foreign mfn will</span>
<span class="p_del">-	 * always return false. If the page is local we can safely call the</span>
<span class="p_del">-	 * native dma_ops function, otherwise we call the xen specific</span>
<span class="p_del">-	 * function. */</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Dom0 is mapped 1:1, while the Linux page can be spanned accross</span>
<span class="p_add">+	 * multiple Xen page, it&#39;s not possible to have a mix of local and</span>
<span class="p_add">+	 * foreign Xen page. Dom0 is mapped 1:1, so calling pfn_valid on a</span>
<span class="p_add">+	 * foreign mfn will always return false. If the page is local we can</span>
<span class="p_add">+	 * safely call the native dma_ops function, otherwise we call the xen</span>
<span class="p_add">+	 * specific function.</span>
<span class="p_add">+	 */</span>
 	if (pfn_valid(pfn)) {
 		if (__generic_dma_ops(hwdev)-&gt;unmap_page)
 			__generic_dma_ops(hwdev)-&gt;unmap_page(hwdev, handle, size, dir, attrs);
<span class="p_header">diff --git a/arch/arm/xen/mm.c b/arch/arm/xen/mm.c</span>
<span class="p_header">index 7b517e91..7c34f71 100644</span>
<span class="p_header">--- a/arch/arm/xen/mm.c</span>
<span class="p_header">+++ b/arch/arm/xen/mm.c</span>
<span class="p_chunk">@@ -48,22 +48,22 @@</span> <span class="p_context"> static void dma_cache_maint(dma_addr_t handle, unsigned long offset,</span>
 	size_t size, enum dma_data_direction dir, enum dma_cache_op op)
 {
 	struct gnttab_cache_flush cflush;
<span class="p_del">-	unsigned long pfn;</span>
<span class="p_add">+	unsigned long xen_pfn;</span>
 	size_t left = size;
 
<span class="p_del">-	pfn = (handle &gt;&gt; PAGE_SHIFT) + offset / PAGE_SIZE;</span>
<span class="p_del">-	offset %= PAGE_SIZE;</span>
<span class="p_add">+	xen_pfn = (handle &gt;&gt; XEN_PAGE_SHIFT) + offset / XEN_PAGE_SIZE;</span>
<span class="p_add">+	offset %= XEN_PAGE_SIZE;</span>
 
 	do {
 		size_t len = left;
 	
 		/* buffers in highmem or foreign pages cannot cross page
 		 * boundaries */
<span class="p_del">-		if (len + offset &gt; PAGE_SIZE)</span>
<span class="p_del">-			len = PAGE_SIZE - offset;</span>
<span class="p_add">+		if (len + offset &gt; XEN_PAGE_SIZE)</span>
<span class="p_add">+			len = XEN_PAGE_SIZE - offset;</span>
 
 		cflush.op = 0;
<span class="p_del">-		cflush.a.dev_bus_addr = pfn &lt;&lt; PAGE_SHIFT;</span>
<span class="p_add">+		cflush.a.dev_bus_addr = xen_pfn &lt;&lt; XEN_PAGE_SHIFT;</span>
 		cflush.offset = offset;
 		cflush.length = len;
 
<span class="p_chunk">@@ -79,7 +79,7 @@</span> <span class="p_context"> static void dma_cache_maint(dma_addr_t handle, unsigned long offset,</span>
 			HYPERVISOR_grant_table_op(GNTTABOP_cache_flush, &amp;cflush, 1);
 
 		offset = 0;
<span class="p_del">-		pfn++;</span>
<span class="p_add">+		xen_pfn++;</span>
 		left -= len;
 	} while (left);
 }
<span class="p_chunk">@@ -141,10 +141,26 @@</span> <span class="p_context"> bool xen_arch_need_swiotlb(struct device *dev,</span>
 			   phys_addr_t phys,
 			   dma_addr_t dev_addr)
 {
<span class="p_del">-	unsigned long pfn = PFN_DOWN(phys);</span>
<span class="p_del">-	unsigned long bfn = PFN_DOWN(dev_addr);</span>
<span class="p_del">-</span>
<span class="p_del">-	return (!hypercall_cflush &amp;&amp; (pfn != bfn) &amp;&amp; !is_device_dma_coherent(dev));</span>
<span class="p_add">+	unsigned int xen_pfn = XEN_PFN_DOWN(phys);</span>
<span class="p_add">+	unsigned int bfn = XEN_PFN_DOWN(dev_addr);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The swiotlb buffer should be used if</span>
<span class="p_add">+	 *	- Xen doesn&#39;t have the cache flush hypercall</span>
<span class="p_add">+	 *	- The Linux page refers to foreign memory</span>
<span class="p_add">+	 *	- The device doesn&#39;t support coherent DMA request</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * The Linux page may be spanned acrros multiple Xen page, although</span>
<span class="p_add">+	 * it&#39;s not possible to have a mix of local and foreign Xen page.</span>
<span class="p_add">+	 * Furthermore, range_straddles_page_boundary is already checking</span>
<span class="p_add">+	 * if buffer is physically contiguous in the host RAM.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * Therefore we only need to check the first Xen page to know if we</span>
<span class="p_add">+	 * require a bounce buffer because the device doesn&#39;t support coherent</span>
<span class="p_add">+	 * memory and we are not able to flush the cache.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	return (!hypercall_cflush &amp;&amp; (xen_pfn != bfn) &amp;&amp;</span>
<span class="p_add">+		!is_device_dma_coherent(dev));</span>
 }
 
 int xen_create_contiguous_region(phys_addr_t pstart, unsigned int order,
<span class="p_header">diff --git a/drivers/xen/swiotlb-xen.c b/drivers/xen/swiotlb-xen.c</span>
<span class="p_header">index cfe755d..5854bf5 100644</span>
<span class="p_header">--- a/drivers/xen/swiotlb-xen.c</span>
<span class="p_header">+++ b/drivers/xen/swiotlb-xen.c</span>
<span class="p_chunk">@@ -76,27 +76,27 @@</span> <span class="p_context"> static unsigned long xen_io_tlb_nslabs;</span>
 static u64 start_dma_addr;
 
 /*
<span class="p_del">- * Both of these functions should avoid PFN_PHYS because phys_addr_t</span>
<span class="p_add">+ * Both of these functions should avoid XEN_PFN_PHYS because phys_addr_t</span>
  * can be 32bit when dma_addr_t is 64bit leading to a loss in
  * information if the shift is done before casting to 64bit.
  */
 static inline dma_addr_t xen_phys_to_bus(phys_addr_t paddr)
 {
<span class="p_del">-	unsigned long bfn = pfn_to_bfn(PFN_DOWN(paddr));</span>
<span class="p_del">-	dma_addr_t dma = (dma_addr_t)bfn &lt;&lt; PAGE_SHIFT;</span>
<span class="p_add">+	unsigned long bfn = pfn_to_bfn(XEN_PFN_DOWN(paddr));</span>
<span class="p_add">+	dma_addr_t dma = (dma_addr_t)bfn &lt;&lt; XEN_PAGE_SHIFT;</span>
 
<span class="p_del">-	dma |= paddr &amp; ~PAGE_MASK;</span>
<span class="p_add">+	dma |= paddr &amp; ~XEN_PAGE_MASK;</span>
 
 	return dma;
 }
 
 static inline phys_addr_t xen_bus_to_phys(dma_addr_t baddr)
 {
<span class="p_del">-	unsigned long pfn = bfn_to_pfn(PFN_DOWN(baddr));</span>
<span class="p_del">-	dma_addr_t dma = (dma_addr_t)pfn &lt;&lt; PAGE_SHIFT;</span>
<span class="p_add">+	unsigned long xen_pfn = bfn_to_pfn(XEN_PFN_DOWN(baddr));</span>
<span class="p_add">+	dma_addr_t dma = (dma_addr_t)xen_pfn &lt;&lt; XEN_PAGE_SHIFT;</span>
 	phys_addr_t paddr = dma;
 
<span class="p_del">-	paddr |= baddr &amp; ~PAGE_MASK;</span>
<span class="p_add">+	paddr |= baddr &amp; ~XEN_PAGE_MASK;</span>
 
 	return paddr;
 }
<span class="p_chunk">@@ -106,7 +106,7 @@</span> <span class="p_context"> static inline dma_addr_t xen_virt_to_bus(void *address)</span>
 	return xen_phys_to_bus(virt_to_phys(address));
 }
 
<span class="p_del">-static int check_pages_physically_contiguous(unsigned long pfn,</span>
<span class="p_add">+static int check_pages_physically_contiguous(unsigned long xen_pfn,</span>
 					     unsigned int offset,
 					     size_t length)
 {
<span class="p_chunk">@@ -114,11 +114,11 @@</span> <span class="p_context"> static int check_pages_physically_contiguous(unsigned long pfn,</span>
 	int i;
 	int nr_pages;
 
<span class="p_del">-	next_bfn = pfn_to_bfn(pfn);</span>
<span class="p_del">-	nr_pages = (offset + length + PAGE_SIZE-1) &gt;&gt; PAGE_SHIFT;</span>
<span class="p_add">+	next_bfn = pfn_to_bfn(xen_pfn);</span>
<span class="p_add">+	nr_pages = (offset + length + XEN_PAGE_SIZE-1) &gt;&gt; XEN_PAGE_SHIFT;</span>
 
 	for (i = 1; i &lt; nr_pages; i++) {
<span class="p_del">-		if (pfn_to_bfn(++pfn) != ++next_bfn)</span>
<span class="p_add">+		if (pfn_to_bfn(++xen_pfn) != ++next_bfn)</span>
 			return 0;
 	}
 	return 1;
<span class="p_chunk">@@ -126,28 +126,27 @@</span> <span class="p_context"> static int check_pages_physically_contiguous(unsigned long pfn,</span>
 
 static inline int range_straddles_page_boundary(phys_addr_t p, size_t size)
 {
<span class="p_del">-	unsigned long pfn = PFN_DOWN(p);</span>
<span class="p_del">-	unsigned int offset = p &amp; ~PAGE_MASK;</span>
<span class="p_add">+	unsigned long xen_pfn = XEN_PFN_DOWN(p);</span>
<span class="p_add">+	unsigned int offset = p &amp; ~XEN_PAGE_MASK;</span>
 
<span class="p_del">-	if (offset + size &lt;= PAGE_SIZE)</span>
<span class="p_add">+	if (offset + size &lt;= XEN_PAGE_SIZE)</span>
 		return 0;
<span class="p_del">-	if (check_pages_physically_contiguous(pfn, offset, size))</span>
<span class="p_add">+	if (check_pages_physically_contiguous(xen_pfn, offset, size))</span>
 		return 0;
 	return 1;
 }
 
 static int is_xen_swiotlb_buffer(dma_addr_t dma_addr)
 {
<span class="p_del">-	unsigned long bfn = PFN_DOWN(dma_addr);</span>
<span class="p_del">-	unsigned long pfn = bfn_to_local_pfn(bfn);</span>
<span class="p_del">-	phys_addr_t paddr;</span>
<span class="p_add">+	unsigned long bfn = XEN_PFN_DOWN(dma_addr);</span>
<span class="p_add">+	unsigned long xen_pfn = bfn_to_local_pfn(bfn);</span>
<span class="p_add">+	phys_addr_t paddr = XEN_PFN_PHYS(xen_pfn);</span>
 
 	/* If the address is outside our domain, it CAN
 	 * have the same virtual address as another address
 	 * in our domain. Therefore _only_ check address within our domain.
 	 */
<span class="p_del">-	if (pfn_valid(pfn)) {</span>
<span class="p_del">-		paddr = PFN_PHYS(pfn);</span>
<span class="p_add">+	if (pfn_valid(PFN_DOWN(paddr))) {</span>
 		return paddr &gt;= virt_to_phys(xen_io_tlb_start) &amp;&amp;
 		       paddr &lt; virt_to_phys(xen_io_tlb_end);
 	}

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



