
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v2,01/13] mm: support madvise(MADV_FREE) - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v2,01/13] mm: support madvise(MADV_FREE)</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=36811">Minchan Kim</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Nov. 4, 2015, 1:25 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1446600367-7976-2-git-send-email-minchan@kernel.org&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/7547921/mbox/"
   >mbox</a>
|
   <a href="/patch/7547921/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/7547921/">/patch/7547921/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork1.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork1.web.kernel.org (Postfix) with ESMTP id 159449F36A
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed,  4 Nov 2015 01:30:22 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 9B74F204E3
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed,  4 Nov 2015 01:30:20 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id F35C120532
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed,  4 Nov 2015 01:30:18 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S932769AbbKDBaM (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 3 Nov 2015 20:30:12 -0500
Received: from LGEAMRELO11.lge.com ([156.147.23.51]:41259 &quot;EHLO
	lgeamrelo11.lge.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1755673AbbKDB0H (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 3 Nov 2015 20:26:07 -0500
Received: from unknown (HELO lgemrelse7q.lge.com) (156.147.1.151)
	by 156.147.23.51 with ESMTP; 4 Nov 2015 10:26:04 +0900
X-Original-SENDERIP: 156.147.1.151
X-Original-MAILFROM: minchan@kernel.org
Received: from unknown (HELO localhost.localdomain) (10.177.223.161)
	by 156.147.1.151 with ESMTP; 4 Nov 2015 10:26:04 +0900
X-Original-SENDERIP: 10.177.223.161
X-Original-MAILFROM: minchan@kernel.org
From: Minchan Kim &lt;minchan@kernel.org&gt;
To: Andrew Morton &lt;akpm@linux-foundation.org&gt;
Cc: linux-kernel@vger.kernel.org, linux-mm@kvack.org,
	Michael Kerrisk &lt;mtk.manpages@gmail.com&gt;,
	linux-api@vger.kernel.org, Hugh Dickins &lt;hughd@google.com&gt;,
	Johannes Weiner &lt;hannes@cmpxchg.org&gt;,
	Rik van Riel &lt;riel@redhat.com&gt;, Mel Gorman &lt;mgorman@suse.de&gt;,
	KOSAKI Motohiro &lt;kosaki.motohiro@jp.fujitsu.com&gt;,
	Jason Evans &lt;je@fb.com&gt;, Daniel Micay &lt;danielmicay@gmail.com&gt;,
	&quot;Kirill A. Shutemov&quot; &lt;kirill@shutemov.name&gt;,
	Shaohua Li &lt;shli@kernel.org&gt;, Michal Hocko &lt;mhocko@suse.cz&gt;,
	yalin.wang2010@gmail.com, Minchan Kim &lt;minchan@kernel.org&gt;
Subject: [PATCH v2 01/13] mm: support madvise(MADV_FREE)
Date: Wed,  4 Nov 2015 10:25:55 +0900
Message-Id: &lt;1446600367-7976-2-git-send-email-minchan@kernel.org&gt;
X-Mailer: git-send-email 1.9.1
In-Reply-To: &lt;1446600367-7976-1-git-send-email-minchan@kernel.org&gt;
References: &lt;1446600367-7976-1-git-send-email-minchan@kernel.org&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-6.9 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	T_RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36811">Minchan Kim</a> - Nov. 4, 2015, 1:25 a.m.</div>
<pre class="content">
Linux doesn&#39;t have an ability to free pages lazy while other OS already
have been supported that named by madvise(MADV_FREE).

The gain is clear that kernel can discard freed pages rather than swapping
out or OOM if memory pressure happens.

Without memory pressure, freed pages would be reused by userspace without
another additional overhead(ex, page fault + allocation + zeroing).

Jason Evans said:

: Facebook has been using MAP_UNINITIALIZED
: (https://lkml.org/lkml/2012/1/18/308) in some of its applications for
: several years, but there are operational costs to maintaining this
: out-of-tree in our kernel and in jemalloc, and we are anxious to retire it
: in favor of MADV_FREE.  When we first enabled MAP_UNINITIALIZED it
: increased throughput for much of our workload by ~5%, and although the
: benefit has decreased using newer hardware and kernels, there is still
: enough benefit that we cannot reasonably retire it without a replacement.
:
: Aside from Facebook operations, there are numerous broadly used
: applications that would benefit from MADV_FREE.  The ones that immediately
: come to mind are redis, varnish, and MariaDB.  I don&#39;t have much insight
: into Android internals and development process, but I would hope to see
: MADV_FREE support eventually end up there as well to benefit applications
: linked with the integrated jemalloc.
:
: jemalloc will use MADV_FREE once it becomes available in the Linux kernel.
: In fact, jemalloc already uses MADV_FREE or equivalent everywhere it&#39;s
: available: *BSD, OS X, Windows, and Solaris -- every platform except Linux
: (and AIX, but I&#39;m not sure it even compiles on AIX).  The lack of
: MADV_FREE on Linux forced me down a long series of increasingly
: sophisticated heuristics for madvise() volume reduction, and even so this
: remains a common performance issue for people using jemalloc on Linux.
: Please integrate MADV_FREE; many people will benefit substantially.

How it works:

When madvise syscall is called, VM clears dirty bit of ptes of the range.
If memory pressure happens, VM checks dirty bit of page table and if it
found still &quot;clean&quot;, it means it&#39;s a &quot;lazyfree pages&quot; so VM could discard
the page instead of swapping out.  Once there was store operation for the
page before VM peek a page to reclaim, dirty bit is set so VM can swap out
the page instead of discarding.

Firstly, heavy users would be general allocators(ex, jemalloc, tcmalloc
and hope glibc supports it) and jemalloc/tcmalloc already have supported
the feature for other OS(ex, FreeBSD)

barrios@blaptop:~/benchmark/ebizzy$ lscpu
Architecture:          x86_64
CPU op-mode(s):        32-bit, 64-bit
Byte Order:            Little Endian
CPU(s):                12
On-line CPU(s) list:   0-11
Thread(s) per core:    1
Core(s) per socket:    1
Socket(s):             12
NUMA node(s):          1
Vendor ID:             GenuineIntel
CPU family:            6
Model:                 2
Stepping:              3
CPU MHz:               3200.185
BogoMIPS:              6400.53
Virtualization:        VT-x
Hypervisor vendor:     KVM
Virtualization type:   full
L1d cache:             32K
L1i cache:             32K
L2 cache:              4096K
NUMA node0 CPU(s):     0-11
ebizzy benchmark(./ebizzy -S 10 -n 512)

Higher avg is better.

 vanilla-jemalloc		MADV_free-jemalloc

1 thread
records: 10			    records: 10
avg:	2961.90			    avg:   12069.70
std:	  71.96(2.43%)		    std:     186.68(1.55%)
max:	3070.00			    max:   12385.00
min:	2796.00			    min:   11746.00

2 thread
records: 10			    records: 10
avg:	5020.00			    avg:   17827.00
std:	 264.87(5.28%)		    std:     358.52(2.01%)
max:	5244.00			    max:   18760.00
min:	4251.00			    min:   17382.00

4 thread
records: 10			    records: 10
avg:	8988.80			    avg:   27930.80
std:	1175.33(13.08%)		    std:    3317.33(11.88%)
max:	9508.00			    max:   30879.00
min:	5477.00			    min:   21024.00

8 thread
records: 10			    records: 10
avg:   13036.50			    avg:   33739.40
std:	 170.67(1.31%)		    std:    5146.22(15.25%)
max:   13371.00			    max:   40572.00
min:   12785.00			    min:   24088.00

16 thread
records: 10			    records: 10
avg:   11092.40			    avg:   31424.20
std:	 710.60(6.41%)		    std:    3763.89(11.98%)
max:   12446.00			    max:   36635.00
min:	9949.00			    min:   25669.00

32 thread
records: 10			    records: 10
avg:   11067.00			    avg:   34495.80
std:	 971.06(8.77%)		    std:    2721.36(7.89%)
max:   12010.00			    max:   38598.00
min:	9002.00			    min:   30636.00

In summary, MADV_FREE is about much faster than MADV_DONTNEED.
<span class="acked-by">
Acked-by: Hugh Dickins &lt;hughd@google.com&gt;</span>
<span class="reviewed-by">Reviewed-by: Michal Hocko &lt;mhocko@suse.cz&gt;</span>
<span class="signed-off-by">Signed-off-by: Minchan Kim &lt;minchan@kernel.org&gt;</span>
---
 include/linux/rmap.h                   |   1 +
 include/linux/vm_event_item.h          |   1 +
 include/uapi/asm-generic/mman-common.h |   1 +
 mm/madvise.c                           | 132 +++++++++++++++++++++++++++++++++
 mm/rmap.c                              |   7 ++
 mm/swap_state.c                        |   5 +-
 mm/vmscan.c                            |  10 ++-
 mm/vmstat.c                            |   1 +
 8 files changed, 153 insertions(+), 5 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=121341">Sergey Senozhatsky</a> - Nov. 4, 2015, 2:16 a.m.</div>
<pre class="content">
Hi Minchan,

On (11/04/15 10:25), Minchan Kim wrote:
[..]
<span class="quote">&gt;+static int madvise_free_pte_range(pmd_t *pmd, unsigned long addr,</span>
<span class="quote">&gt;+                               unsigned long end, struct mm_walk *walk)</span>
<span class="quote">&gt;+</span>
...
<span class="quote">&gt; +	if (pmd_trans_unstable(pmd))</span>
<span class="quote">&gt; +		return 0;</span>

I think it makes sense to update pmd_trans_unstable() and
pmd_none_or_trans_huge_or_clear_bad() comments in asm-generic/pgtable.h
Because they explicitly mention MADV_DONTNEED only. Just a thought.
<span class="quote">

&gt; @@ -379,6 +502,14 @@ madvise_vma(struct vm_area_struct *vma, struct vm_area_struct **prev,</span>
<span class="quote">&gt;  		return madvise_remove(vma, prev, start, end);</span>
<span class="quote">&gt;  	case MADV_WILLNEED:</span>
<span class="quote">&gt;  		return madvise_willneed(vma, prev, start, end);</span>
<span class="quote">&gt; +	case MADV_FREE:</span>
<span class="quote">&gt; +		/*</span>
<span class="quote">&gt; +		 * XXX: In this implementation, MADV_FREE works like</span>
		  ^^^^
		XXX
<span class="quote">
&gt; +		 * MADV_DONTNEED on swapless system or full swap.</span>
<span class="quote">&gt; +		 */</span>
<span class="quote">&gt; +		if (get_nr_swap_pages() &gt; 0)</span>
<span class="quote">&gt; +			return madvise_free(vma, prev, start, end);</span>
<span class="quote">&gt; +		/* passthrough */</span>

	-ss
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=121341">Sergey Senozhatsky</a> - Nov. 4, 2015, 2:29 a.m.</div>
<pre class="content">
On (11/04/15 10:25), Minchan Kim wrote:
[..]
<span class="quote">&gt; +static int madvise_free_pte_range(pmd_t *pmd, unsigned long addr,</span>
<span class="quote">&gt; +				unsigned long end, struct mm_walk *walk)</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct mmu_gather *tlb = walk-&gt;private;</span>
<span class="quote">&gt; +	struct mm_struct *mm = tlb-&gt;mm;</span>
<span class="quote">&gt; +	struct vm_area_struct *vma = walk-&gt;vma;</span>
<span class="quote">&gt; +	spinlock_t *ptl;</span>
<span class="quote">&gt; +	pte_t *pte, ptent;</span>
<span class="quote">&gt; +	struct page *page;</span>


I&#39;ll just ask (probably I&#39;m missing something)

	+ pmd_trans_huge_lock() ?
<span class="quote">
&gt; +	split_huge_page_pmd(vma, addr, pmd);</span>
<span class="quote">&gt; +	if (pmd_trans_unstable(pmd))</span>
<span class="quote">&gt; +		return 0;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	pte = pte_offset_map_lock(mm, pmd, addr, &amp;ptl);</span>
<span class="quote">&gt; +	arch_enter_lazy_mmu_mode();</span>
<span class="quote">&gt; +	for (; addr != end; pte++, addr += PAGE_SIZE) {</span>
<span class="quote">&gt; +		ptent = *pte;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		if (!pte_present(ptent))</span>
<span class="quote">&gt; +			continue;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		page = vm_normal_page(vma, addr, ptent);</span>
<span class="quote">&gt; +		if (!page)</span>
<span class="quote">&gt; +			continue;</span>
<span class="quote">&gt; +</span>

	-ss
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=41531">Andy Lutomirski</a> - Nov. 4, 2015, 3:41 a.m.</div>
<pre class="content">
On Nov 3, 2015 5:30 PM, &quot;Minchan Kim&quot; &lt;minchan@kernel.org&gt; wrote:
<span class="quote">&gt;</span>
<span class="quote">&gt; Linux doesn&#39;t have an ability to free pages lazy while other OS already</span>
<span class="quote">&gt; have been supported that named by madvise(MADV_FREE).</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; The gain is clear that kernel can discard freed pages rather than swapping</span>
<span class="quote">&gt; out or OOM if memory pressure happens.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Without memory pressure, freed pages would be reused by userspace without</span>
<span class="quote">&gt; another additional overhead(ex, page fault + allocation + zeroing).</span>
<span class="quote">&gt;</span>

[...]
<span class="quote">
&gt;</span>
<span class="quote">&gt; How it works:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; When madvise syscall is called, VM clears dirty bit of ptes of the range.</span>
<span class="quote">&gt; If memory pressure happens, VM checks dirty bit of page table and if it</span>
<span class="quote">&gt; found still &quot;clean&quot;, it means it&#39;s a &quot;lazyfree pages&quot; so VM could discard</span>
<span class="quote">&gt; the page instead of swapping out.  Once there was store operation for the</span>
<span class="quote">&gt; page before VM peek a page to reclaim, dirty bit is set so VM can swap out</span>
<span class="quote">&gt; the page instead of discarding.</span>

What happens if you MADV_FREE something that&#39;s MAP_SHARED or isn&#39;t
ordinary anonymous memory?  There&#39;s a long history of MADV_DONTNEED on
such mappings causing exploitable problems, and I think it would be
nice if MADV_FREE were obviously safe.

Does this set the write protect bit?

What happens on architectures without hardware dirty tracking?  For
that matter, even on architecture with hardware dirty tracking, what
happens in multithreaded processes that have the dirty TLB state
cached in a different CPU&#39;s TLB?

Using the dirty bit for these semantics scares me.  This API creates a
page that can have visible nonzero contents and then can
asynchronously and magically zero itself thereafter.  That makes me
nervous.  Could we use the accessed bit instead?  Then the observable
semantics would be equivalent to having MADV_FREE either zero the page
or do nothing, except that it doesn&#39;t make up its mind until the next
read.
<span class="quote">
&gt; +                       ptent = pte_mkold(ptent);</span>
<span class="quote">&gt; +                       ptent = pte_mkclean(ptent);</span>
<span class="quote">&gt; +                       set_pte_at(mm, addr, pte, ptent);</span>
<span class="quote">&gt; +                       tlb_remove_tlb_entry(tlb, pte, addr);</span>

It looks like you are flushing the TLB.  In a multithreaded program,
that&#39;s rather expensive.  Potentially silly question: would it be
better to just zero the page immediately in a multithreaded program
and then, when swapping out, check the page is zeroed and, if so, skip
swapping it out?  That could be done without forcing an IPI.
<span class="quote">
&gt; +static int madvise_free_single_vma(struct vm_area_struct *vma,</span>
<span class="quote">&gt; +                       unsigned long start_addr, unsigned long end_addr)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       unsigned long start, end;</span>
<span class="quote">&gt; +       struct mm_struct *mm = vma-&gt;vm_mm;</span>
<span class="quote">&gt; +       struct mmu_gather tlb;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       if (vma-&gt;vm_flags &amp; (VM_LOCKED|VM_HUGETLB|VM_PFNMAP))</span>
<span class="quote">&gt; +               return -EINVAL;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       /* MADV_FREE works for only anon vma at the moment */</span>
<span class="quote">&gt; +       if (!vma_is_anonymous(vma))</span>
<span class="quote">&gt; +               return -EINVAL;</span>

Does anything weird happen if it&#39;s shared?
<span class="quote">
&gt; +               if (!PageDirty(page) &amp;&amp; (flags &amp; TTU_FREE)) {</span>
<span class="quote">&gt; +                       /* It&#39;s a freeable page by MADV_FREE */</span>
<span class="quote">&gt; +                       dec_mm_counter(mm, MM_ANONPAGES);</span>
<span class="quote">&gt; +                       goto discard;</span>
<span class="quote">&gt; +               }</span>

Does something clear TTU_FREE the next time the page gets marked clean?

--Andy
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=110221">Daniel Micay</a> - Nov. 4, 2015, 5:50 a.m.</div>
<pre class="content">
<span class="quote">&gt; Does this set the write protect bit?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; What happens on architectures without hardware dirty tracking?</span>

It&#39;s supposed to avoid needing page faults when the data is accessed
again, but it can just be implemented via page faults on architectures
without a way to check for access or writes. MADV_DONTNEED is also a
valid implementation of MADV_FREE if it comes to that (which is what it
does on swapless systems for now).
<span class="quote">
&gt; Using the dirty bit for these semantics scares me.  This API creates a</span>
<span class="quote">&gt; page that can have visible nonzero contents and then can</span>
<span class="quote">&gt; asynchronously and magically zero itself thereafter.  That makes me</span>
<span class="quote">&gt; nervous.  Could we use the accessed bit instead?  Then the observable</span>
<span class="quote">&gt; semantics would be equivalent to having MADV_FREE either zero the page</span>
<span class="quote">&gt; or do nothing, except that it doesn&#39;t make up its mind until the next</span>
<span class="quote">&gt; read.</span>

FWIW, those are already basically the semantics provided by GCC and LLVM
for data the compiler considers uninitialized (they could be more
aggressive since C just says it&#39;s undefined, but in practice they allow
it but can produce inconsistent results even if it isn&#39;t touched).

http://llvm.org/docs/LangRef.html#undefined-values

It doesn&#39;t seem like there would be an advantage to checking if the data
was written to vs. whether it was accessed if checking for both of those
is comparable in performance. I don&#39;t know enough about that.
<span class="quote">
&gt;&gt; +                       ptent = pte_mkold(ptent);</span>
<span class="quote">&gt;&gt; +                       ptent = pte_mkclean(ptent);</span>
<span class="quote">&gt;&gt; +                       set_pte_at(mm, addr, pte, ptent);</span>
<span class="quote">&gt;&gt; +                       tlb_remove_tlb_entry(tlb, pte, addr);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It looks like you are flushing the TLB.  In a multithreaded program,</span>
<span class="quote">&gt; that&#39;s rather expensive.  Potentially silly question: would it be</span>
<span class="quote">&gt; better to just zero the page immediately in a multithreaded program</span>
<span class="quote">&gt; and then, when swapping out, check the page is zeroed and, if so, skip</span>
<span class="quote">&gt; swapping it out?  That could be done without forcing an IPI.</span>

In the common case it will be passed many pages by the allocator. There
will still be a layer of purging logic on top of MADV_FREE but it can be
much thinner than the current workarounds for MADV_DONTNEED. So the
allocator would still be coalescing dirty ranges and only purging when
the ratio of dirty:clean pages rises above some threshold. It would be
able to weight the largest ranges for purging first rather than logic
based on stuff like aging as is used for MADV_DONTNEED.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=110221">Daniel Micay</a> - Nov. 4, 2015, 5:53 a.m.</div>
<pre class="content">
<span class="quote">&gt; In the common case it will be passed many pages by the allocator. There</span>
<span class="quote">&gt; will still be a layer of purging logic on top of MADV_FREE but it can be</span>
<span class="quote">&gt; much thinner than the current workarounds for MADV_DONTNEED. So the</span>
<span class="quote">&gt; allocator would still be coalescing dirty ranges and only purging when</span>
<span class="quote">&gt; the ratio of dirty:clean pages rises above some threshold. It would be</span>
<span class="quote">&gt; able to weight the largest ranges for purging first rather than logic</span>
<span class="quote">&gt; based on stuff like aging as is used for MADV_DONTNEED.</span>

I would expect that jemalloc would just start putting the dirty ranges
into the usual pair of red-black trees (with coalescing) and then doing
purging starting from the largest spans to get back down below whatever
dirty:clean ratio it&#39;s trying to keep. Right now, it has all lots of
other logic to deal with this since each MADV_DONTNEED call results in
lots of zeroing and then page faults.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=110221">Daniel Micay</a> - Nov. 4, 2015, 6:04 a.m.</div>
<pre class="content">
On 04/11/15 12:53 AM, Daniel Micay wrote:
<span class="quote">&gt;&gt; In the common case it will be passed many pages by the allocator. There</span>
<span class="quote">&gt;&gt; will still be a layer of purging logic on top of MADV_FREE but it can be</span>
<span class="quote">&gt;&gt; much thinner than the current workarounds for MADV_DONTNEED. So the</span>
<span class="quote">&gt;&gt; allocator would still be coalescing dirty ranges and only purging when</span>
<span class="quote">&gt;&gt; the ratio of dirty:clean pages rises above some threshold. It would be</span>
<span class="quote">&gt;&gt; able to weight the largest ranges for purging first rather than logic</span>
<span class="quote">&gt;&gt; based on stuff like aging as is used for MADV_DONTNEED.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I would expect that jemalloc would just start putting the dirty ranges</span>
<span class="quote">&gt; into the usual pair of red-black trees (with coalescing) and then doing</span>
<span class="quote">&gt; purging starting from the largest spans to get back down below whatever</span>
<span class="quote">&gt; dirty:clean ratio it&#39;s trying to keep. Right now, it has all lots of</span>
<span class="quote">&gt; other logic to deal with this since each MADV_DONTNEED call results in</span>
<span class="quote">&gt; lots of zeroing and then page faults.</span>

Er, I mean dirty:active (i.e. ratio of unpurged, dirty pages to ones
that are handed out as allocations, which is kept at something like
1:8). A high constant cost in the madvise call but quick handling of
each page means that allocators need to be more aggressive with purging
more than they strictly need to in one go. For example, it might need to
purge 2M to meet the ratio but it could have a contiguous span of 32M of
dirty pages. If the cost per page is low enough, it could just do the
entire range.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=41531">Andy Lutomirski</a> - Nov. 4, 2015, 6:23 p.m.</div>
<pre class="content">
On Tue, Nov 3, 2015 at 9:50 PM, Daniel Micay &lt;danielmicay@gmail.com&gt; wrote:
<span class="quote">&gt;&gt; Does this set the write protect bit?</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; What happens on architectures without hardware dirty tracking?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; It&#39;s supposed to avoid needing page faults when the data is accessed</span>
<span class="quote">&gt; again, but it can just be implemented via page faults on architectures</span>
<span class="quote">&gt; without a way to check for access or writes. MADV_DONTNEED is also a</span>
<span class="quote">&gt; valid implementation of MADV_FREE if it comes to that (which is what it</span>
<span class="quote">&gt; does on swapless systems for now).</span>

I wonder whether arches without the requisite tracking should just
turn it off.  While it might be faster than MADV_DONTNEED or munmap on
those arches, it doesn&#39;t really deserve to be faster.
<span class="quote">
&gt;</span>
<span class="quote">&gt;&gt; Using the dirty bit for these semantics scares me.  This API creates a</span>
<span class="quote">&gt;&gt; page that can have visible nonzero contents and then can</span>
<span class="quote">&gt;&gt; asynchronously and magically zero itself thereafter.  That makes me</span>
<span class="quote">&gt;&gt; nervous.  Could we use the accessed bit instead?  Then the observable</span>
<span class="quote">&gt;&gt; semantics would be equivalent to having MADV_FREE either zero the page</span>
<span class="quote">&gt;&gt; or do nothing, except that it doesn&#39;t make up its mind until the next</span>
<span class="quote">&gt;&gt; read.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; FWIW, those are already basically the semantics provided by GCC and LLVM</span>
<span class="quote">&gt; for data the compiler considers uninitialized (they could be more</span>
<span class="quote">&gt; aggressive since C just says it&#39;s undefined, but in practice they allow</span>
<span class="quote">&gt; it but can produce inconsistent results even if it isn&#39;t touched).</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; http://llvm.org/docs/LangRef.html#undefined-values</span>

But C isn&#39;t the only thing in the world.  Also, I think that a C
optimizer should be free to turn:

if ([complicated condition])
  *ptr = 1;

into:

if (*ptr != 1 &amp;&amp; [complicated condition])
  *ptr = 1;

as long as [complicated condition] has no side effects.  The MADV_FREE
semantics in this patch set break that.
<span class="quote">
&gt;</span>
<span class="quote">&gt; It doesn&#39;t seem like there would be an advantage to checking if the data</span>
<span class="quote">&gt; was written to vs. whether it was accessed if checking for both of those</span>
<span class="quote">&gt; is comparable in performance. I don&#39;t know enough about that.</span>

I&#39;d imagine that there would be no performance difference whatsoever
on hardware that has a real accessed bit.  The only thing that changes
is the choice of which bit to use.
<span class="quote">
&gt;</span>
<span class="quote">&gt;&gt;&gt; +                       ptent = pte_mkold(ptent);</span>
<span class="quote">&gt;&gt;&gt; +                       ptent = pte_mkclean(ptent);</span>
<span class="quote">&gt;&gt;&gt; +                       set_pte_at(mm, addr, pte, ptent);</span>
<span class="quote">&gt;&gt;&gt; +                       tlb_remove_tlb_entry(tlb, pte, addr);</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; It looks like you are flushing the TLB.  In a multithreaded program,</span>
<span class="quote">&gt;&gt; that&#39;s rather expensive.  Potentially silly question: would it be</span>
<span class="quote">&gt;&gt; better to just zero the page immediately in a multithreaded program</span>
<span class="quote">&gt;&gt; and then, when swapping out, check the page is zeroed and, if so, skip</span>
<span class="quote">&gt;&gt; swapping it out?  That could be done without forcing an IPI.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; In the common case it will be passed many pages by the allocator. There</span>
<span class="quote">&gt; will still be a layer of purging logic on top of MADV_FREE but it can be</span>
<span class="quote">&gt; much thinner than the current workarounds for MADV_DONTNEED. So the</span>
<span class="quote">&gt; allocator would still be coalescing dirty ranges and only purging when</span>
<span class="quote">&gt; the ratio of dirty:clean pages rises above some threshold. It would be</span>
<span class="quote">&gt; able to weight the largest ranges for purging first rather than logic</span>
<span class="quote">&gt; based on stuff like aging as is used for MADV_DONTNEED.</span>
<span class="quote">&gt;</span>

With enough pages at once, though, munmap would be fine, too.

Maybe what&#39;s really needed is a MADV_FREE variant that takes an iovec.
On an all-cores multithreaded mm, the TLB shootdown broadcast takes
thousands of cycles on each core more or less regardless of how much
of the TLB gets zapped.

--Andy
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=19951">Shaohua Li</a> - Nov. 4, 2015, 8 p.m.</div>
<pre class="content">
On Wed, Nov 04, 2015 at 10:25:55AM +0900, Minchan Kim wrote:
<span class="quote">&gt; Linux doesn&#39;t have an ability to free pages lazy while other OS already</span>
<span class="quote">&gt; have been supported that named by madvise(MADV_FREE).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The gain is clear that kernel can discard freed pages rather than swapping</span>
<span class="quote">&gt; out or OOM if memory pressure happens.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Without memory pressure, freed pages would be reused by userspace without</span>
<span class="quote">&gt; another additional overhead(ex, page fault + allocation + zeroing).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Jason Evans said:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; : Facebook has been using MAP_UNINITIALIZED</span>
<span class="quote">&gt; : (https://lkml.org/lkml/2012/1/18/308) in some of its applications for</span>
<span class="quote">&gt; : several years, but there are operational costs to maintaining this</span>
<span class="quote">&gt; : out-of-tree in our kernel and in jemalloc, and we are anxious to retire it</span>
<span class="quote">&gt; : in favor of MADV_FREE.  When we first enabled MAP_UNINITIALIZED it</span>
<span class="quote">&gt; : increased throughput for much of our workload by ~5%, and although the</span>
<span class="quote">&gt; : benefit has decreased using newer hardware and kernels, there is still</span>
<span class="quote">&gt; : enough benefit that we cannot reasonably retire it without a replacement.</span>
<span class="quote">&gt; :</span>
<span class="quote">&gt; : Aside from Facebook operations, there are numerous broadly used</span>
<span class="quote">&gt; : applications that would benefit from MADV_FREE.  The ones that immediately</span>
<span class="quote">&gt; : come to mind are redis, varnish, and MariaDB.  I don&#39;t have much insight</span>
<span class="quote">&gt; : into Android internals and development process, but I would hope to see</span>
<span class="quote">&gt; : MADV_FREE support eventually end up there as well to benefit applications</span>
<span class="quote">&gt; : linked with the integrated jemalloc.</span>
<span class="quote">&gt; :</span>
<span class="quote">&gt; : jemalloc will use MADV_FREE once it becomes available in the Linux kernel.</span>
<span class="quote">&gt; : In fact, jemalloc already uses MADV_FREE or equivalent everywhere it&#39;s</span>
<span class="quote">&gt; : available: *BSD, OS X, Windows, and Solaris -- every platform except Linux</span>
<span class="quote">&gt; : (and AIX, but I&#39;m not sure it even compiles on AIX).  The lack of</span>
<span class="quote">&gt; : MADV_FREE on Linux forced me down a long series of increasingly</span>
<span class="quote">&gt; : sophisticated heuristics for madvise() volume reduction, and even so this</span>
<span class="quote">&gt; : remains a common performance issue for people using jemalloc on Linux.</span>
<span class="quote">&gt; : Please integrate MADV_FREE; many people will benefit substantially.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; How it works:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; When madvise syscall is called, VM clears dirty bit of ptes of the range.</span>
<span class="quote">&gt; If memory pressure happens, VM checks dirty bit of page table and if it</span>
<span class="quote">&gt; found still &quot;clean&quot;, it means it&#39;s a &quot;lazyfree pages&quot; so VM could discard</span>
<span class="quote">&gt; the page instead of swapping out.  Once there was store operation for the</span>
<span class="quote">&gt; page before VM peek a page to reclaim, dirty bit is set so VM can swap out</span>
<span class="quote">&gt; the page instead of discarding.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Firstly, heavy users would be general allocators(ex, jemalloc, tcmalloc</span>
<span class="quote">&gt; and hope glibc supports it) and jemalloc/tcmalloc already have supported</span>
<span class="quote">&gt; the feature for other OS(ex, FreeBSD)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; barrios@blaptop:~/benchmark/ebizzy$ lscpu</span>
<span class="quote">&gt; Architecture:          x86_64</span>
<span class="quote">&gt; CPU op-mode(s):        32-bit, 64-bit</span>
<span class="quote">&gt; Byte Order:            Little Endian</span>
<span class="quote">&gt; CPU(s):                12</span>
<span class="quote">&gt; On-line CPU(s) list:   0-11</span>
<span class="quote">&gt; Thread(s) per core:    1</span>
<span class="quote">&gt; Core(s) per socket:    1</span>
<span class="quote">&gt; Socket(s):             12</span>
<span class="quote">&gt; NUMA node(s):          1</span>
<span class="quote">&gt; Vendor ID:             GenuineIntel</span>
<span class="quote">&gt; CPU family:            6</span>
<span class="quote">&gt; Model:                 2</span>
<span class="quote">&gt; Stepping:              3</span>
<span class="quote">&gt; CPU MHz:               3200.185</span>
<span class="quote">&gt; BogoMIPS:              6400.53</span>
<span class="quote">&gt; Virtualization:        VT-x</span>
<span class="quote">&gt; Hypervisor vendor:     KVM</span>
<span class="quote">&gt; Virtualization type:   full</span>
<span class="quote">&gt; L1d cache:             32K</span>
<span class="quote">&gt; L1i cache:             32K</span>
<span class="quote">&gt; L2 cache:              4096K</span>
<span class="quote">&gt; NUMA node0 CPU(s):     0-11</span>
<span class="quote">&gt; ebizzy benchmark(./ebizzy -S 10 -n 512)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Higher avg is better.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  vanilla-jemalloc		MADV_free-jemalloc</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 1 thread</span>
<span class="quote">&gt; records: 10			    records: 10</span>
<span class="quote">&gt; avg:	2961.90			    avg:   12069.70</span>
<span class="quote">&gt; std:	  71.96(2.43%)		    std:     186.68(1.55%)</span>
<span class="quote">&gt; max:	3070.00			    max:   12385.00</span>
<span class="quote">&gt; min:	2796.00			    min:   11746.00</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 2 thread</span>
<span class="quote">&gt; records: 10			    records: 10</span>
<span class="quote">&gt; avg:	5020.00			    avg:   17827.00</span>
<span class="quote">&gt; std:	 264.87(5.28%)		    std:     358.52(2.01%)</span>
<span class="quote">&gt; max:	5244.00			    max:   18760.00</span>
<span class="quote">&gt; min:	4251.00			    min:   17382.00</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 4 thread</span>
<span class="quote">&gt; records: 10			    records: 10</span>
<span class="quote">&gt; avg:	8988.80			    avg:   27930.80</span>
<span class="quote">&gt; std:	1175.33(13.08%)		    std:    3317.33(11.88%)</span>
<span class="quote">&gt; max:	9508.00			    max:   30879.00</span>
<span class="quote">&gt; min:	5477.00			    min:   21024.00</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 8 thread</span>
<span class="quote">&gt; records: 10			    records: 10</span>
<span class="quote">&gt; avg:   13036.50			    avg:   33739.40</span>
<span class="quote">&gt; std:	 170.67(1.31%)		    std:    5146.22(15.25%)</span>
<span class="quote">&gt; max:   13371.00			    max:   40572.00</span>
<span class="quote">&gt; min:   12785.00			    min:   24088.00</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 16 thread</span>
<span class="quote">&gt; records: 10			    records: 10</span>
<span class="quote">&gt; avg:   11092.40			    avg:   31424.20</span>
<span class="quote">&gt; std:	 710.60(6.41%)		    std:    3763.89(11.98%)</span>
<span class="quote">&gt; max:   12446.00			    max:   36635.00</span>
<span class="quote">&gt; min:	9949.00			    min:   25669.00</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 32 thread</span>
<span class="quote">&gt; records: 10			    records: 10</span>
<span class="quote">&gt; avg:   11067.00			    avg:   34495.80</span>
<span class="quote">&gt; std:	 971.06(8.77%)		    std:    2721.36(7.89%)</span>
<span class="quote">&gt; max:   12010.00			    max:   38598.00</span>
<span class="quote">&gt; min:	9002.00			    min:   30636.00</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; In summary, MADV_FREE is about much faster than MADV_DONTNEED.</span>

The MADV_FREE is discussed for a while, it probably is too late to propose
something new, but we had the new idea (from Ben Maurer, CCed) recently and
think it&#39;s better. Our target is still jemalloc.

Compared to MADV_DONTNEED, MADV_FREE&#39;s lazy memory free is a huge win to reduce
page fault. But there is one issue remaining, the TLB flush. Both MADV_DONTNEED
and MADV_FREE do TLB flush. TLB flush overhead is quite big in contemporary
multi-thread applications. In our production workload, we observed 80% CPU
spending on TLB flush triggered by jemalloc madvise(MADV_DONTNEED) sometimes.
We haven&#39;t tested MADV_FREE yet, but the result should be similar. It&#39;s hard to
avoid the TLB flush issue with MADV_FREE, because it helps avoid data
corruption.

The new proposal tries to fix the TLB issue. We introduce two madvise verbs:

MARK_FREE. Userspace notifies kernel the memory range can be discarded. Kernel
just records the range in current stage. Should memory pressure happen, page
reclaim can free the memory directly regardless the pte state.

MARK_NOFREE. Userspace notifies kernel the memory range will be reused soon.
Kernel deletes the record and prevents page reclaim discards the memory. If the
memory isn&#39;t reclaimed, userspace will access the old memory, otherwise do
normal page fault handling.

The point is to let userspace notify kernel if memory can be discarded, instead
of depending on pte dirty bit used by MADV_FREE. With these, no TLB flush is
required till page reclaim actually frees the memory (page reclaim need do the
TLB flush for MADV_FREE too). It still preserves the lazy memory free merit of
MADV_FREE.

Compared to MADV_FREE, reusing memory with the new proposal isn&#39;t transparent,
eg must call MARK_NOFREE. But it&#39;s easy to utilize the new API in jemalloc.

We don&#39;t have code to backup this yet, sorry. We&#39;d like to discuss it if it
makes sense.

Thanks,
Shaohua
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=110221">Daniel Micay</a> - Nov. 4, 2015, 9:16 p.m.</div>
<pre class="content">
<span class="quote">&gt; Compared to MADV_DONTNEED, MADV_FREE&#39;s lazy memory free is a huge win to reduce</span>
<span class="quote">&gt; page fault. But there is one issue remaining, the TLB flush. Both MADV_DONTNEED</span>
<span class="quote">&gt; and MADV_FREE do TLB flush. TLB flush overhead is quite big in contemporary</span>
<span class="quote">&gt; multi-thread applications. In our production workload, we observed 80% CPU</span>
<span class="quote">&gt; spending on TLB flush triggered by jemalloc madvise(MADV_DONTNEED) sometimes.</span>
<span class="quote">&gt; We haven&#39;t tested MADV_FREE yet, but the result should be similar. It&#39;s hard to</span>
<span class="quote">&gt; avoid the TLB flush issue with MADV_FREE, because it helps avoid data</span>
<span class="quote">&gt; corruption.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The new proposal tries to fix the TLB issue. We introduce two madvise verbs:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; MARK_FREE. Userspace notifies kernel the memory range can be discarded. Kernel</span>
<span class="quote">&gt; just records the range in current stage. Should memory pressure happen, page</span>
<span class="quote">&gt; reclaim can free the memory directly regardless the pte state.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; MARK_NOFREE. Userspace notifies kernel the memory range will be reused soon.</span>
<span class="quote">&gt; Kernel deletes the record and prevents page reclaim discards the memory. If the</span>
<span class="quote">&gt; memory isn&#39;t reclaimed, userspace will access the old memory, otherwise do</span>
<span class="quote">&gt; normal page fault handling.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The point is to let userspace notify kernel if memory can be discarded, instead</span>
<span class="quote">&gt; of depending on pte dirty bit used by MADV_FREE. With these, no TLB flush is</span>
<span class="quote">&gt; required till page reclaim actually frees the memory (page reclaim need do the</span>
<span class="quote">&gt; TLB flush for MADV_FREE too). It still preserves the lazy memory free merit of</span>
<span class="quote">&gt; MADV_FREE.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Compared to MADV_FREE, reusing memory with the new proposal isn&#39;t transparent,</span>
<span class="quote">&gt; eg must call MARK_NOFREE. But it&#39;s easy to utilize the new API in jemalloc.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; We don&#39;t have code to backup this yet, sorry. We&#39;d like to discuss it if it</span>
<span class="quote">&gt; makes sense.</span>

That&#39;s comparable to Android&#39;s pinning / unpinning API for ashmem and I
think it makes sense if it&#39;s faster. It&#39;s different than the MADV_FREE
API though, because the new allocations that are handed out won&#39;t have
the usual lazy commit which MADV_FREE provides. Pages in an allocation
that&#39;s handed out can still be dropped until they are actually written
to. It&#39;s considered active by jemalloc either way, but only a subset of
the active pages are actually committed. There&#39;s probably a use case for
both of these systems.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=110221">Daniel Micay</a> - Nov. 4, 2015, 9:29 p.m.</div>
<pre class="content">
<span class="quote">&gt; That&#39;s comparable to Android&#39;s pinning / unpinning API for ashmem and I</span>
<span class="quote">&gt; think it makes sense if it&#39;s faster. It&#39;s different than the MADV_FREE</span>
<span class="quote">&gt; API though, because the new allocations that are handed out won&#39;t have</span>
<span class="quote">&gt; the usual lazy commit which MADV_FREE provides. Pages in an allocation</span>
<span class="quote">&gt; that&#39;s handed out can still be dropped until they are actually written</span>
<span class="quote">&gt; to. It&#39;s considered active by jemalloc either way, but only a subset of</span>
<span class="quote">&gt; the active pages are actually committed. There&#39;s probably a use case for</span>
<span class="quote">&gt; both of these systems.</span>

Also, consider that MADV_FREE would allow jemalloc to be extremely
aggressive with purging when it actually has to do it. It can start with
the largest span of memory and it can mark more than strictly necessary
to drop below the ratio as there&#39;s no cost to using the memory again
(not even a system call).

Since the main cost is using the system call at all, there&#39;s going to be
pressure to mark the largest possible spans in one go. It will mean
concentration on memory compaction will improve performance. I think
that&#39;s the right direction for the kernel to be guiding userspace. It
will play better with THP than the allocator trying to be very precise
with purging based on aging.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=41531">Andy Lutomirski</a> - Nov. 4, 2015, 9:43 p.m.</div>
<pre class="content">
On Wed, Nov 4, 2015 at 12:00 PM, Shaohua Li &lt;shli@kernel.org&gt; wrote:
<span class="quote">&gt;</span>
<span class="quote">&gt; The new proposal tries to fix the TLB issue. We introduce two madvise verbs:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; MARK_FREE. Userspace notifies kernel the memory range can be discarded. Kernel</span>
<span class="quote">&gt; just records the range in current stage. Should memory pressure happen, page</span>
<span class="quote">&gt; reclaim can free the memory directly regardless the pte state.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; MARK_NOFREE. Userspace notifies kernel the memory range will be reused soon.</span>
<span class="quote">&gt; Kernel deletes the record and prevents page reclaim discards the memory. If the</span>
<span class="quote">&gt; memory isn&#39;t reclaimed, userspace will access the old memory, otherwise do</span>
<span class="quote">&gt; normal page fault handling.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; The point is to let userspace notify kernel if memory can be discarded, instead</span>
<span class="quote">&gt; of depending on pte dirty bit used by MADV_FREE. With these, no TLB flush is</span>
<span class="quote">&gt; required till page reclaim actually frees the memory (page reclaim need do the</span>
<span class="quote">&gt; TLB flush for MADV_FREE too). It still preserves the lazy memory free merit of</span>
<span class="quote">&gt; MADV_FREE.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Compared to MADV_FREE, reusing memory with the new proposal isn&#39;t transparent,</span>
<span class="quote">&gt; eg must call MARK_NOFREE. But it&#39;s easy to utilize the new API in jemalloc.</span>
<span class="quote">&gt;</span>

I can&#39;t speak to the usefulness of this or to other arches, but on x86
(unless you have nohz_full or similar enabled), a pair of syscalls
should be *much* faster than an IPI or a page fault.

I don&#39;t know how expensive it is to write to a clean page or to access
an unaccessed page on x86.  I&#39;m sure it&#39;s not free (there&#39;s memory
bandwidth if nothing else), but it could be very cheap.

--Andy
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=110221">Daniel Micay</a> - Nov. 4, 2015, 10:05 p.m.</div>
<pre class="content">
<span class="quote">&gt; With enough pages at once, though, munmap would be fine, too.</span>

That implies lots of page faults and zeroing though. The zeroing alone
is a major performance issue.

There are separate issues with munmap since it ends up resulting in a
lot more virtual memory fragmentation. It would help if the kernel used
first-best-fit for mmap instead of the current naive algorithm (bonus:
O(log n) worst-case, not O(n)). Since allocators like jemalloc and
PartitionAlloc want 2M aligned spans, mixing them with other allocators
can also accelerate the VM fragmentation caused by the dumb mmap
algorithm (i.e. they make a 2M aligned mapping, some other mmap user
does 4k, now there&#39;s a nearly 2M gap when the next 2M region is made and
the kernel keeps going rather than reusing it). Anyway, that&#39;s a totally
separate issue from this. Just felt like complaining :).
<span class="quote">
&gt; Maybe what&#39;s really needed is a MADV_FREE variant that takes an iovec.</span>
<span class="quote">&gt; On an all-cores multithreaded mm, the TLB shootdown broadcast takes</span>
<span class="quote">&gt; thousands of cycles on each core more or less regardless of how much</span>
<span class="quote">&gt; of the TLB gets zapped.</span>

That would work very well. The allocator ends up having a sequence of
dirty spans that it needs to purge in one go. As long as purging is
fairly spread out, the cost of a single TLB shootdown isn&#39;t that bad. It
is extremely bad if it needs to do it over and over to purge a bunch of
ranges, which can happen if the memory has ended up being very, very
fragmentated despite the efforts to compact it (depends on what the
application ends up doing).
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36811">Minchan Kim</a> - Nov. 4, 2015, 11:39 p.m.</div>
<pre class="content">
Hi Sergey,

On Wed, Nov 04, 2015 at 11:16:24AM +0900, Sergey Senozhatsky wrote:
<span class="quote">&gt; Hi Minchan,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; On (11/04/15 10:25), Minchan Kim wrote:</span>
<span class="quote">&gt; [..]</span>
<span class="quote">&gt; &gt;+static int madvise_free_pte_range(pmd_t *pmd, unsigned long addr,</span>
<span class="quote">&gt; &gt;+                               unsigned long end, struct mm_walk *walk)</span>
<span class="quote">&gt; &gt;+</span>
<span class="quote">&gt; ...</span>
<span class="quote">&gt; &gt; +	if (pmd_trans_unstable(pmd))</span>
<span class="quote">&gt; &gt; +		return 0;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I think it makes sense to update pmd_trans_unstable() and</span>
<span class="quote">&gt; pmd_none_or_trans_huge_or_clear_bad() comments in asm-generic/pgtable.h</span>
<span class="quote">&gt; Because they explicitly mention MADV_DONTNEED only. Just a thought.</span>

Hmm, When I read comments(but actually I don&#39;t understand it 100%), it
says pmd disappearing from MADV_DONTNEED with mmap_sem read-side
lock. But MADV_FREE doesn&#39;t remove the pmd. So, I don&#39;t understand
what I should add comment. Please suggest if I am missing something.
<span class="quote">
&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; @@ -379,6 +502,14 @@ madvise_vma(struct vm_area_struct *vma, struct vm_area_struct **prev,</span>
<span class="quote">&gt; &gt;  		return madvise_remove(vma, prev, start, end);</span>
<span class="quote">&gt; &gt;  	case MADV_WILLNEED:</span>
<span class="quote">&gt; &gt;  		return madvise_willneed(vma, prev, start, end);</span>
<span class="quote">&gt; &gt; +	case MADV_FREE:</span>
<span class="quote">&gt; &gt; +		/*</span>
<span class="quote">&gt; &gt; +		 * XXX: In this implementation, MADV_FREE works like</span>
<span class="quote">&gt; 		  ^^^^</span>
<span class="quote">&gt; 		XXX</span>

What does it mean?
<span class="quote">
&gt; </span>
<span class="quote">&gt; &gt; +		 * MADV_DONTNEED on swapless system or full swap.</span>
<span class="quote">&gt; &gt; +		 */</span>
<span class="quote">&gt; &gt; +		if (get_nr_swap_pages() &gt; 0)</span>
<span class="quote">&gt; &gt; +			return madvise_free(vma, prev, start, end);</span>
<span class="quote">&gt; &gt; +		/* passthrough */</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	-ss</span>
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36811">Minchan Kim</a> - Nov. 4, 2015, 11:40 p.m.</div>
<pre class="content">
On Wed, Nov 04, 2015 at 11:29:51AM +0900, Sergey Senozhatsky wrote:
<span class="quote">&gt; On (11/04/15 10:25), Minchan Kim wrote:</span>
<span class="quote">&gt; [..]</span>
<span class="quote">&gt; &gt; +static int madvise_free_pte_range(pmd_t *pmd, unsigned long addr,</span>
<span class="quote">&gt; &gt; +				unsigned long end, struct mm_walk *walk)</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	struct mmu_gather *tlb = walk-&gt;private;</span>
<span class="quote">&gt; &gt; +	struct mm_struct *mm = tlb-&gt;mm;</span>
<span class="quote">&gt; &gt; +	struct vm_area_struct *vma = walk-&gt;vma;</span>
<span class="quote">&gt; &gt; +	spinlock_t *ptl;</span>
<span class="quote">&gt; &gt; +	pte_t *pte, ptent;</span>
<span class="quote">&gt; &gt; +	struct page *page;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I&#39;ll just ask (probably I&#39;m missing something)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	+ pmd_trans_huge_lock() ?</span>

No. It would be deadlock.
<span class="quote">
&gt; </span>
<span class="quote">&gt; &gt; +	split_huge_page_pmd(vma, addr, pmd);</span>
<span class="quote">&gt; &gt; +	if (pmd_trans_unstable(pmd))</span>
<span class="quote">&gt; &gt; +		return 0;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	pte = pte_offset_map_lock(mm, pmd, addr, &amp;ptl);</span>
<span class="quote">&gt; &gt; +	arch_enter_lazy_mmu_mode();</span>
<span class="quote">&gt; &gt; +	for (; addr != end; pte++, addr += PAGE_SIZE) {</span>
<span class="quote">&gt; &gt; +		ptent = *pte;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		if (!pte_present(ptent))</span>
<span class="quote">&gt; &gt; +			continue;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		page = vm_normal_page(vma, addr, ptent);</span>
<span class="quote">&gt; &gt; +		if (!page)</span>
<span class="quote">&gt; &gt; +			continue;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	-ss</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; --</span>
<span class="quote">&gt; To unsubscribe, send a message with &#39;unsubscribe linux-mm&#39; in</span>
<span class="quote">&gt; the body to majordomo@kvack.org.  For more info on Linux MM,</span>
<span class="quote">&gt; see: http://www.linux-mm.org/ .</span>
<span class="quote">&gt; Don&#39;t email: &lt;a href=mailto:&quot;dont@kvack.org&quot;&gt; email@kvack.org &lt;/a&gt;</span>
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36811">Minchan Kim</a> - Nov. 5, 2015, 12:13 a.m.</div>
<pre class="content">
On Tue, Nov 03, 2015 at 07:41:35PM -0800, Andy Lutomirski wrote:
<span class="quote">&gt; On Nov 3, 2015 5:30 PM, &quot;Minchan Kim&quot; &lt;minchan@kernel.org&gt; wrote:</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Linux doesn&#39;t have an ability to free pages lazy while other OS already</span>
<span class="quote">&gt; &gt; have been supported that named by madvise(MADV_FREE).</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; The gain is clear that kernel can discard freed pages rather than swapping</span>
<span class="quote">&gt; &gt; out or OOM if memory pressure happens.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Without memory pressure, freed pages would be reused by userspace without</span>
<span class="quote">&gt; &gt; another additional overhead(ex, page fault + allocation + zeroing).</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; [...]</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; How it works:</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; When madvise syscall is called, VM clears dirty bit of ptes of the range.</span>
<span class="quote">&gt; &gt; If memory pressure happens, VM checks dirty bit of page table and if it</span>
<span class="quote">&gt; &gt; found still &quot;clean&quot;, it means it&#39;s a &quot;lazyfree pages&quot; so VM could discard</span>
<span class="quote">&gt; &gt; the page instead of swapping out.  Once there was store operation for the</span>
<span class="quote">&gt; &gt; page before VM peek a page to reclaim, dirty bit is set so VM can swap out</span>
<span class="quote">&gt; &gt; the page instead of discarding.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; What happens if you MADV_FREE something that&#39;s MAP_SHARED or isn&#39;t</span>
<span class="quote">&gt; ordinary anonymous memory?  There&#39;s a long history of MADV_DONTNEED on</span>
<span class="quote">&gt; such mappings causing exploitable problems, and I think it would be</span>
<span class="quote">&gt; nice if MADV_FREE were obviously safe.</span>

It filter out VM_LOCKED|VM_HUGETLB|VM_PFNMAP and file-backed vma and MAP_SHARED
with vma_is_anonymous.
<span class="quote">
&gt; </span>
<span class="quote">&gt; Does this set the write protect bit?</span>

No.
<span class="quote">
&gt; </span>
<span class="quote">&gt; What happens on architectures without hardware dirty tracking?  For</span>
<span class="quote">&gt; that matter, even on architecture with hardware dirty tracking, what</span>
<span class="quote">&gt; happens in multithreaded processes that have the dirty TLB state</span>
<span class="quote">&gt; cached in a different CPU&#39;s TLB?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Using the dirty bit for these semantics scares me.  This API creates a</span>
<span class="quote">&gt; page that can have visible nonzero contents and then can</span>
<span class="quote">&gt; asynchronously and magically zero itself thereafter.  That makes me</span>
<span class="quote">&gt; nervous.  Could we use the accessed bit instead?  Then the observable</span>

Access bit is used by aging algorithm for reclaim. In addition,
we have supported clear_refs feacture.
IOW, it could be reset anytime so it&#39;s hard to use marker for
lazy freeing at the moment.
<span class="quote">
&gt; semantics would be equivalent to having MADV_FREE either zero the page</span>
<span class="quote">&gt; or do nothing, except that it doesn&#39;t make up its mind until the next</span>
<span class="quote">&gt; read.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; +                       ptent = pte_mkold(ptent);</span>
<span class="quote">&gt; &gt; +                       ptent = pte_mkclean(ptent);</span>
<span class="quote">&gt; &gt; +                       set_pte_at(mm, addr, pte, ptent);</span>
<span class="quote">&gt; &gt; +                       tlb_remove_tlb_entry(tlb, pte, addr);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It looks like you are flushing the TLB.  In a multithreaded program,</span>
<span class="quote">&gt; that&#39;s rather expensive.  Potentially silly question: would it be</span>
<span class="quote">&gt; better to just zero the page immediately in a multithreaded program</span>
<span class="quote">&gt; and then, when swapping out, check the page is zeroed and, if so, skip</span>
<span class="quote">&gt; swapping it out?  That could be done without forcing an IPI.</span>

So, we should monitor all of pages in reclaim patch whether they are
zero or not? It is fatster for allocation side but much slower in
reclaim side. For avoiding that, we should mark something for lazy
freeing page out of page table.

Anyway, it depends on the TLB flush overehead vs memset overhead.
If the hinted range is pretty big and small system(ie, not many core),
memset overhead would&#39;t not trivial compared to TLB flush.
Even, some of ARM arches doesn&#39;t do IPI to TLB flush so the overhead
would be cheaper.

I don&#39;t want to push more optimization in new syscall from the beginning.
It&#39;s an optimization and might come better idea once we hear from the
voice of userland folks. Then, it&#39;s not too late.
Let&#39;s do step by step.
<span class="quote">
&gt; </span>
<span class="quote">&gt; &gt; +static int madvise_free_single_vma(struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt; +                       unsigned long start_addr, unsigned long end_addr)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +       unsigned long start, end;</span>
<span class="quote">&gt; &gt; +       struct mm_struct *mm = vma-&gt;vm_mm;</span>
<span class="quote">&gt; &gt; +       struct mmu_gather tlb;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +       if (vma-&gt;vm_flags &amp; (VM_LOCKED|VM_HUGETLB|VM_PFNMAP))</span>
<span class="quote">&gt; &gt; +               return -EINVAL;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +       /* MADV_FREE works for only anon vma at the moment */</span>
<span class="quote">&gt; &gt; +       if (!vma_is_anonymous(vma))</span>
<span class="quote">&gt; &gt; +               return -EINVAL;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Does anything weird happen if it&#39;s shared?</span>

Hmm, you mean MAP_SHARED|MAP_ANONYMOUS?
In that case, vma-&gt;vm_ops = &amp;shmem_vm_ops so vma_is anonymous should filter it out.
<span class="quote">
&gt; </span>
<span class="quote">&gt; &gt; +               if (!PageDirty(page) &amp;&amp; (flags &amp; TTU_FREE)) {</span>
<span class="quote">&gt; &gt; +                       /* It&#39;s a freeable page by MADV_FREE */</span>
<span class="quote">&gt; &gt; +                       dec_mm_counter(mm, MM_ANONPAGES);</span>
<span class="quote">&gt; &gt; +                       goto discard;</span>
<span class="quote">&gt; &gt; +               }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Does something clear TTU_FREE the next time the page gets marked clean?</span>

Sorry, I don&#39;t understand. Could you elaborate it more?
<span class="quote">
&gt; </span>
<span class="quote">&gt; --Andy</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; --</span>
<span class="quote">&gt; To unsubscribe, send a message with &#39;unsubscribe linux-mm&#39; in</span>
<span class="quote">&gt; the body to majordomo@kvack.org.  For more info on Linux MM,</span>
<span class="quote">&gt; see: http://www.linux-mm.org/ .</span>
<span class="quote">&gt; Don&#39;t email: &lt;a href=mailto:&quot;dont@kvack.org&quot;&gt; email@kvack.org &lt;/a&gt;</span>
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=41531">Andy Lutomirski</a> - Nov. 5, 2015, 12:42 a.m.</div>
<pre class="content">
On Wed, Nov 4, 2015 at 4:13 PM, Minchan Kim &lt;minchan@kernel.org&gt; wrote:
<span class="quote">&gt; On Tue, Nov 03, 2015 at 07:41:35PM -0800, Andy Lutomirski wrote:</span>
<span class="quote">&gt;&gt; On Nov 3, 2015 5:30 PM, &quot;Minchan Kim&quot; &lt;minchan@kernel.org&gt; wrote:</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; Linux doesn&#39;t have an ability to free pages lazy while other OS already</span>
<span class="quote">&gt;&gt; &gt; have been supported that named by madvise(MADV_FREE).</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; The gain is clear that kernel can discard freed pages rather than swapping</span>
<span class="quote">&gt;&gt; &gt; out or OOM if memory pressure happens.</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; Without memory pressure, freed pages would be reused by userspace without</span>
<span class="quote">&gt;&gt; &gt; another additional overhead(ex, page fault + allocation + zeroing).</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; [...]</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; How it works:</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; When madvise syscall is called, VM clears dirty bit of ptes of the range.</span>
<span class="quote">&gt;&gt; &gt; If memory pressure happens, VM checks dirty bit of page table and if it</span>
<span class="quote">&gt;&gt; &gt; found still &quot;clean&quot;, it means it&#39;s a &quot;lazyfree pages&quot; so VM could discard</span>
<span class="quote">&gt;&gt; &gt; the page instead of swapping out.  Once there was store operation for the</span>
<span class="quote">&gt;&gt; &gt; page before VM peek a page to reclaim, dirty bit is set so VM can swap out</span>
<span class="quote">&gt;&gt; &gt; the page instead of discarding.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; What happens if you MADV_FREE something that&#39;s MAP_SHARED or isn&#39;t</span>
<span class="quote">&gt;&gt; ordinary anonymous memory?  There&#39;s a long history of MADV_DONTNEED on</span>
<span class="quote">&gt;&gt; such mappings causing exploitable problems, and I think it would be</span>
<span class="quote">&gt;&gt; nice if MADV_FREE were obviously safe.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; It filter out VM_LOCKED|VM_HUGETLB|VM_PFNMAP and file-backed vma and MAP_SHARED</span>
<span class="quote">&gt; with vma_is_anonymous.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Does this set the write protect bit?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; No.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; What happens on architectures without hardware dirty tracking?  For</span>
<span class="quote">&gt;&gt; that matter, even on architecture with hardware dirty tracking, what</span>
<span class="quote">&gt;&gt; happens in multithreaded processes that have the dirty TLB state</span>
<span class="quote">&gt;&gt; cached in a different CPU&#39;s TLB?</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Using the dirty bit for these semantics scares me.  This API creates a</span>
<span class="quote">&gt;&gt; page that can have visible nonzero contents and then can</span>
<span class="quote">&gt;&gt; asynchronously and magically zero itself thereafter.  That makes me</span>
<span class="quote">&gt;&gt; nervous.  Could we use the accessed bit instead?  Then the observable</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Access bit is used by aging algorithm for reclaim. In addition,</span>
<span class="quote">&gt; we have supported clear_refs feacture.</span>
<span class="quote">&gt; IOW, it could be reset anytime so it&#39;s hard to use marker for</span>
<span class="quote">&gt; lazy freeing at the moment.</span>
<span class="quote">&gt;</span>

That&#39;s unfortunate.  I think that the ABI would be much nicer if it
used the accessed bit.

In any case, shouldn&#39;t the aging algorithm be irrelevant here?  A
MADV_FREE page that isn&#39;t accessed can be discarded, whereas we could
hopefully just say that a MADV_FREE page that is accessed gets moved
to whatever list holds recently accessed pages and also stops being a
candidate for discarding due to MADV_FREE?
<span class="quote">
&gt;&gt;</span>
<span class="quote">&gt;&gt; &gt; +               if (!PageDirty(page) &amp;&amp; (flags &amp; TTU_FREE)) {</span>
<span class="quote">&gt;&gt; &gt; +                       /* It&#39;s a freeable page by MADV_FREE */</span>
<span class="quote">&gt;&gt; &gt; +                       dec_mm_counter(mm, MM_ANONPAGES);</span>
<span class="quote">&gt;&gt; &gt; +                       goto discard;</span>
<span class="quote">&gt;&gt; &gt; +               }</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Does something clear TTU_FREE the next time the page gets marked clean?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Sorry, I don&#39;t understand. Could you elaborate it more?</span>

I don&#39;t fully understand how TTU_FREE ends up being set here, but, if
the page is dirtied by user code and then cleaned later by the kernel,
what prevents TTU_FREE from being incorrectly set here?


--Andy
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36811">Minchan Kim</a> - Nov. 5, 2015, 12:56 a.m.</div>
<pre class="content">
On Wed, Nov 04, 2015 at 04:42:37PM -0800, Andy Lutomirski wrote:
<span class="quote">&gt; On Wed, Nov 4, 2015 at 4:13 PM, Minchan Kim &lt;minchan@kernel.org&gt; wrote:</span>
<span class="quote">&gt; &gt; On Tue, Nov 03, 2015 at 07:41:35PM -0800, Andy Lutomirski wrote:</span>
<span class="quote">&gt; &gt;&gt; On Nov 3, 2015 5:30 PM, &quot;Minchan Kim&quot; &lt;minchan@kernel.org&gt; wrote:</span>
<span class="quote">&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; &gt; Linux doesn&#39;t have an ability to free pages lazy while other OS already</span>
<span class="quote">&gt; &gt;&gt; &gt; have been supported that named by madvise(MADV_FREE).</span>
<span class="quote">&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; &gt; The gain is clear that kernel can discard freed pages rather than swapping</span>
<span class="quote">&gt; &gt;&gt; &gt; out or OOM if memory pressure happens.</span>
<span class="quote">&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; &gt; Without memory pressure, freed pages would be reused by userspace without</span>
<span class="quote">&gt; &gt;&gt; &gt; another additional overhead(ex, page fault + allocation + zeroing).</span>
<span class="quote">&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; [...]</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; &gt; How it works:</span>
<span class="quote">&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; &gt; When madvise syscall is called, VM clears dirty bit of ptes of the range.</span>
<span class="quote">&gt; &gt;&gt; &gt; If memory pressure happens, VM checks dirty bit of page table and if it</span>
<span class="quote">&gt; &gt;&gt; &gt; found still &quot;clean&quot;, it means it&#39;s a &quot;lazyfree pages&quot; so VM could discard</span>
<span class="quote">&gt; &gt;&gt; &gt; the page instead of swapping out.  Once there was store operation for the</span>
<span class="quote">&gt; &gt;&gt; &gt; page before VM peek a page to reclaim, dirty bit is set so VM can swap out</span>
<span class="quote">&gt; &gt;&gt; &gt; the page instead of discarding.</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; What happens if you MADV_FREE something that&#39;s MAP_SHARED or isn&#39;t</span>
<span class="quote">&gt; &gt;&gt; ordinary anonymous memory?  There&#39;s a long history of MADV_DONTNEED on</span>
<span class="quote">&gt; &gt;&gt; such mappings causing exploitable problems, and I think it would be</span>
<span class="quote">&gt; &gt;&gt; nice if MADV_FREE were obviously safe.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; It filter out VM_LOCKED|VM_HUGETLB|VM_PFNMAP and file-backed vma and MAP_SHARED</span>
<span class="quote">&gt; &gt; with vma_is_anonymous.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; Does this set the write protect bit?</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; No.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; What happens on architectures without hardware dirty tracking?  For</span>
<span class="quote">&gt; &gt;&gt; that matter, even on architecture with hardware dirty tracking, what</span>
<span class="quote">&gt; &gt;&gt; happens in multithreaded processes that have the dirty TLB state</span>
<span class="quote">&gt; &gt;&gt; cached in a different CPU&#39;s TLB?</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; Using the dirty bit for these semantics scares me.  This API creates a</span>
<span class="quote">&gt; &gt;&gt; page that can have visible nonzero contents and then can</span>
<span class="quote">&gt; &gt;&gt; asynchronously and magically zero itself thereafter.  That makes me</span>
<span class="quote">&gt; &gt;&gt; nervous.  Could we use the accessed bit instead?  Then the observable</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Access bit is used by aging algorithm for reclaim. In addition,</span>
<span class="quote">&gt; &gt; we have supported clear_refs feacture.</span>
<span class="quote">&gt; &gt; IOW, it could be reset anytime so it&#39;s hard to use marker for</span>
<span class="quote">&gt; &gt; lazy freeing at the moment.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; That&#39;s unfortunate.  I think that the ABI would be much nicer if it</span>
<span class="quote">&gt; used the accessed bit.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; In any case, shouldn&#39;t the aging algorithm be irrelevant here?  A</span>
<span class="quote">&gt; MADV_FREE page that isn&#39;t accessed can be discarded, whereas we could</span>
<span class="quote">&gt; hopefully just say that a MADV_FREE page that is accessed gets moved</span>
<span class="quote">&gt; to whatever list holds recently accessed pages and also stops being a</span>
<span class="quote">&gt; candidate for discarding due to MADV_FREE?</span>

I meant if we use access bit as indicator for lazy-freeing page,
we could discard valid page which is never hinted by MADV_FREE but
just doesn&#39;t mark access bit in page table by aging algorithm.
<span class="quote">
&gt; </span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; &gt; +               if (!PageDirty(page) &amp;&amp; (flags &amp; TTU_FREE)) {</span>
<span class="quote">&gt; &gt;&gt; &gt; +                       /* It&#39;s a freeable page by MADV_FREE */</span>
<span class="quote">&gt; &gt;&gt; &gt; +                       dec_mm_counter(mm, MM_ANONPAGES);</span>
<span class="quote">&gt; &gt;&gt; &gt; +                       goto discard;</span>
<span class="quote">&gt; &gt;&gt; &gt; +               }</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; Does something clear TTU_FREE the next time the page gets marked clean?</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Sorry, I don&#39;t understand. Could you elaborate it more?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I don&#39;t fully understand how TTU_FREE ends up being set here, but, if</span>
<span class="quote">&gt; the page is dirtied by user code and then cleaned later by the kernel,</span>
<span class="quote">&gt; what prevents TTU_FREE from being incorrectly set here?</span>

Kernel shouldn&#39;t make the page clean without writeback(ie, swapout)
if the page has valid data.
<span class="quote">
&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; --Andy</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; --</span>
<span class="quote">&gt; To unsubscribe, send a message with &#39;unsubscribe linux-mm&#39; in</span>
<span class="quote">&gt; the body to majordomo@kvack.org.  For more info on Linux MM,</span>
<span class="quote">&gt; see: http://www.linux-mm.org/ .</span>
<span class="quote">&gt; Don&#39;t email: &lt;a href=mailto:&quot;dont@kvack.org&quot;&gt; email@kvack.org &lt;/a&gt;</span>
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=41531">Andy Lutomirski</a> - Nov. 5, 2015, 1:29 a.m.</div>
<pre class="content">
On Wed, Nov 4, 2015 at 4:56 PM, Minchan Kim &lt;minchan@kernel.org&gt; wrote:
<span class="quote">&gt; On Wed, Nov 04, 2015 at 04:42:37PM -0800, Andy Lutomirski wrote:</span>
<span class="quote">&gt;&gt; On Wed, Nov 4, 2015 at 4:13 PM, Minchan Kim &lt;minchan@kernel.org&gt; wrote:</span>
<span class="quote">&gt;&gt; &gt; On Tue, Nov 03, 2015 at 07:41:35PM -0800, Andy Lutomirski wrote:</span>
<span class="quote">&gt;&gt; &gt;&gt; On Nov 3, 2015 5:30 PM, &quot;Minchan Kim&quot; &lt;minchan@kernel.org&gt; wrote:</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt; Linux doesn&#39;t have an ability to free pages lazy while other OS already</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt; have been supported that named by madvise(MADV_FREE).</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt; The gain is clear that kernel can discard freed pages rather than swapping</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt; out or OOM if memory pressure happens.</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt; Without memory pressure, freed pages would be reused by userspace without</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt; another additional overhead(ex, page fault + allocation + zeroing).</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt;&gt;</span>
<span class="quote">&gt;&gt; &gt;&gt; [...]</span>
<span class="quote">&gt;&gt; &gt;&gt;</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt; How it works:</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt; When madvise syscall is called, VM clears dirty bit of ptes of the range.</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt; If memory pressure happens, VM checks dirty bit of page table and if it</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt; found still &quot;clean&quot;, it means it&#39;s a &quot;lazyfree pages&quot; so VM could discard</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt; the page instead of swapping out.  Once there was store operation for the</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt; page before VM peek a page to reclaim, dirty bit is set so VM can swap out</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt; the page instead of discarding.</span>
<span class="quote">&gt;&gt; &gt;&gt;</span>
<span class="quote">&gt;&gt; &gt;&gt; What happens if you MADV_FREE something that&#39;s MAP_SHARED or isn&#39;t</span>
<span class="quote">&gt;&gt; &gt;&gt; ordinary anonymous memory?  There&#39;s a long history of MADV_DONTNEED on</span>
<span class="quote">&gt;&gt; &gt;&gt; such mappings causing exploitable problems, and I think it would be</span>
<span class="quote">&gt;&gt; &gt;&gt; nice if MADV_FREE were obviously safe.</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; It filter out VM_LOCKED|VM_HUGETLB|VM_PFNMAP and file-backed vma and MAP_SHARED</span>
<span class="quote">&gt;&gt; &gt; with vma_is_anonymous.</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt;&gt;</span>
<span class="quote">&gt;&gt; &gt;&gt; Does this set the write protect bit?</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; No.</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt;&gt;</span>
<span class="quote">&gt;&gt; &gt;&gt; What happens on architectures without hardware dirty tracking?  For</span>
<span class="quote">&gt;&gt; &gt;&gt; that matter, even on architecture with hardware dirty tracking, what</span>
<span class="quote">&gt;&gt; &gt;&gt; happens in multithreaded processes that have the dirty TLB state</span>
<span class="quote">&gt;&gt; &gt;&gt; cached in a different CPU&#39;s TLB?</span>
<span class="quote">&gt;&gt; &gt;&gt;</span>
<span class="quote">&gt;&gt; &gt;&gt; Using the dirty bit for these semantics scares me.  This API creates a</span>
<span class="quote">&gt;&gt; &gt;&gt; page that can have visible nonzero contents and then can</span>
<span class="quote">&gt;&gt; &gt;&gt; asynchronously and magically zero itself thereafter.  That makes me</span>
<span class="quote">&gt;&gt; &gt;&gt; nervous.  Could we use the accessed bit instead?  Then the observable</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; Access bit is used by aging algorithm for reclaim. In addition,</span>
<span class="quote">&gt;&gt; &gt; we have supported clear_refs feacture.</span>
<span class="quote">&gt;&gt; &gt; IOW, it could be reset anytime so it&#39;s hard to use marker for</span>
<span class="quote">&gt;&gt; &gt; lazy freeing at the moment.</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; That&#39;s unfortunate.  I think that the ABI would be much nicer if it</span>
<span class="quote">&gt;&gt; used the accessed bit.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; In any case, shouldn&#39;t the aging algorithm be irrelevant here?  A</span>
<span class="quote">&gt;&gt; MADV_FREE page that isn&#39;t accessed can be discarded, whereas we could</span>
<span class="quote">&gt;&gt; hopefully just say that a MADV_FREE page that is accessed gets moved</span>
<span class="quote">&gt;&gt; to whatever list holds recently accessed pages and also stops being a</span>
<span class="quote">&gt;&gt; candidate for discarding due to MADV_FREE?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I meant if we use access bit as indicator for lazy-freeing page,</span>
<span class="quote">&gt; we could discard valid page which is never hinted by MADV_FREE but</span>
<span class="quote">&gt; just doesn&#39;t mark access bit in page table by aging algorithm.</span>

Oh, is the rule that the anonymous pages that are clean are discarded
instead of swapped out?  That is, does your patch set detect that an
anonymous page can be discarded if it&#39;s clean and that the lack of a
dirty bit is the only indication that the page has been hit with
MADV_FREE?

If so, that seems potentially error prone -- I had assumed that pages
that were swapped in but not written since swap-in would also be
clean, and I don&#39;t see how you distinguish them.

--Andy
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36811">Minchan Kim</a> - Nov. 5, 2015, 1:33 a.m.</div>
<pre class="content">
On Wed, Nov 04, 2015 at 12:00:06PM -0800, Shaohua Li wrote:
<span class="quote">&gt; On Wed, Nov 04, 2015 at 10:25:55AM +0900, Minchan Kim wrote:</span>
<span class="quote">&gt; &gt; Linux doesn&#39;t have an ability to free pages lazy while other OS already</span>
<span class="quote">&gt; &gt; have been supported that named by madvise(MADV_FREE).</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; The gain is clear that kernel can discard freed pages rather than swapping</span>
<span class="quote">&gt; &gt; out or OOM if memory pressure happens.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Without memory pressure, freed pages would be reused by userspace without</span>
<span class="quote">&gt; &gt; another additional overhead(ex, page fault + allocation + zeroing).</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Jason Evans said:</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; : Facebook has been using MAP_UNINITIALIZED</span>
<span class="quote">&gt; &gt; : (https://lkml.org/lkml/2012/1/18/308) in some of its applications for</span>
<span class="quote">&gt; &gt; : several years, but there are operational costs to maintaining this</span>
<span class="quote">&gt; &gt; : out-of-tree in our kernel and in jemalloc, and we are anxious to retire it</span>
<span class="quote">&gt; &gt; : in favor of MADV_FREE.  When we first enabled MAP_UNINITIALIZED it</span>
<span class="quote">&gt; &gt; : increased throughput for much of our workload by ~5%, and although the</span>
<span class="quote">&gt; &gt; : benefit has decreased using newer hardware and kernels, there is still</span>
<span class="quote">&gt; &gt; : enough benefit that we cannot reasonably retire it without a replacement.</span>
<span class="quote">&gt; &gt; :</span>
<span class="quote">&gt; &gt; : Aside from Facebook operations, there are numerous broadly used</span>
<span class="quote">&gt; &gt; : applications that would benefit from MADV_FREE.  The ones that immediately</span>
<span class="quote">&gt; &gt; : come to mind are redis, varnish, and MariaDB.  I don&#39;t have much insight</span>
<span class="quote">&gt; &gt; : into Android internals and development process, but I would hope to see</span>
<span class="quote">&gt; &gt; : MADV_FREE support eventually end up there as well to benefit applications</span>
<span class="quote">&gt; &gt; : linked with the integrated jemalloc.</span>
<span class="quote">&gt; &gt; :</span>
<span class="quote">&gt; &gt; : jemalloc will use MADV_FREE once it becomes available in the Linux kernel.</span>
<span class="quote">&gt; &gt; : In fact, jemalloc already uses MADV_FREE or equivalent everywhere it&#39;s</span>
<span class="quote">&gt; &gt; : available: *BSD, OS X, Windows, and Solaris -- every platform except Linux</span>
<span class="quote">&gt; &gt; : (and AIX, but I&#39;m not sure it even compiles on AIX).  The lack of</span>
<span class="quote">&gt; &gt; : MADV_FREE on Linux forced me down a long series of increasingly</span>
<span class="quote">&gt; &gt; : sophisticated heuristics for madvise() volume reduction, and even so this</span>
<span class="quote">&gt; &gt; : remains a common performance issue for people using jemalloc on Linux.</span>
<span class="quote">&gt; &gt; : Please integrate MADV_FREE; many people will benefit substantially.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; How it works:</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; When madvise syscall is called, VM clears dirty bit of ptes of the range.</span>
<span class="quote">&gt; &gt; If memory pressure happens, VM checks dirty bit of page table and if it</span>
<span class="quote">&gt; &gt; found still &quot;clean&quot;, it means it&#39;s a &quot;lazyfree pages&quot; so VM could discard</span>
<span class="quote">&gt; &gt; the page instead of swapping out.  Once there was store operation for the</span>
<span class="quote">&gt; &gt; page before VM peek a page to reclaim, dirty bit is set so VM can swap out</span>
<span class="quote">&gt; &gt; the page instead of discarding.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Firstly, heavy users would be general allocators(ex, jemalloc, tcmalloc</span>
<span class="quote">&gt; &gt; and hope glibc supports it) and jemalloc/tcmalloc already have supported</span>
<span class="quote">&gt; &gt; the feature for other OS(ex, FreeBSD)</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; barrios@blaptop:~/benchmark/ebizzy$ lscpu</span>
<span class="quote">&gt; &gt; Architecture:          x86_64</span>
<span class="quote">&gt; &gt; CPU op-mode(s):        32-bit, 64-bit</span>
<span class="quote">&gt; &gt; Byte Order:            Little Endian</span>
<span class="quote">&gt; &gt; CPU(s):                12</span>
<span class="quote">&gt; &gt; On-line CPU(s) list:   0-11</span>
<span class="quote">&gt; &gt; Thread(s) per core:    1</span>
<span class="quote">&gt; &gt; Core(s) per socket:    1</span>
<span class="quote">&gt; &gt; Socket(s):             12</span>
<span class="quote">&gt; &gt; NUMA node(s):          1</span>
<span class="quote">&gt; &gt; Vendor ID:             GenuineIntel</span>
<span class="quote">&gt; &gt; CPU family:            6</span>
<span class="quote">&gt; &gt; Model:                 2</span>
<span class="quote">&gt; &gt; Stepping:              3</span>
<span class="quote">&gt; &gt; CPU MHz:               3200.185</span>
<span class="quote">&gt; &gt; BogoMIPS:              6400.53</span>
<span class="quote">&gt; &gt; Virtualization:        VT-x</span>
<span class="quote">&gt; &gt; Hypervisor vendor:     KVM</span>
<span class="quote">&gt; &gt; Virtualization type:   full</span>
<span class="quote">&gt; &gt; L1d cache:             32K</span>
<span class="quote">&gt; &gt; L1i cache:             32K</span>
<span class="quote">&gt; &gt; L2 cache:              4096K</span>
<span class="quote">&gt; &gt; NUMA node0 CPU(s):     0-11</span>
<span class="quote">&gt; &gt; ebizzy benchmark(./ebizzy -S 10 -n 512)</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Higher avg is better.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt;  vanilla-jemalloc		MADV_free-jemalloc</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; 1 thread</span>
<span class="quote">&gt; &gt; records: 10			    records: 10</span>
<span class="quote">&gt; &gt; avg:	2961.90			    avg:   12069.70</span>
<span class="quote">&gt; &gt; std:	  71.96(2.43%)		    std:     186.68(1.55%)</span>
<span class="quote">&gt; &gt; max:	3070.00			    max:   12385.00</span>
<span class="quote">&gt; &gt; min:	2796.00			    min:   11746.00</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; 2 thread</span>
<span class="quote">&gt; &gt; records: 10			    records: 10</span>
<span class="quote">&gt; &gt; avg:	5020.00			    avg:   17827.00</span>
<span class="quote">&gt; &gt; std:	 264.87(5.28%)		    std:     358.52(2.01%)</span>
<span class="quote">&gt; &gt; max:	5244.00			    max:   18760.00</span>
<span class="quote">&gt; &gt; min:	4251.00			    min:   17382.00</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; 4 thread</span>
<span class="quote">&gt; &gt; records: 10			    records: 10</span>
<span class="quote">&gt; &gt; avg:	8988.80			    avg:   27930.80</span>
<span class="quote">&gt; &gt; std:	1175.33(13.08%)		    std:    3317.33(11.88%)</span>
<span class="quote">&gt; &gt; max:	9508.00			    max:   30879.00</span>
<span class="quote">&gt; &gt; min:	5477.00			    min:   21024.00</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; 8 thread</span>
<span class="quote">&gt; &gt; records: 10			    records: 10</span>
<span class="quote">&gt; &gt; avg:   13036.50			    avg:   33739.40</span>
<span class="quote">&gt; &gt; std:	 170.67(1.31%)		    std:    5146.22(15.25%)</span>
<span class="quote">&gt; &gt; max:   13371.00			    max:   40572.00</span>
<span class="quote">&gt; &gt; min:   12785.00			    min:   24088.00</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; 16 thread</span>
<span class="quote">&gt; &gt; records: 10			    records: 10</span>
<span class="quote">&gt; &gt; avg:   11092.40			    avg:   31424.20</span>
<span class="quote">&gt; &gt; std:	 710.60(6.41%)		    std:    3763.89(11.98%)</span>
<span class="quote">&gt; &gt; max:   12446.00			    max:   36635.00</span>
<span class="quote">&gt; &gt; min:	9949.00			    min:   25669.00</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; 32 thread</span>
<span class="quote">&gt; &gt; records: 10			    records: 10</span>
<span class="quote">&gt; &gt; avg:   11067.00			    avg:   34495.80</span>
<span class="quote">&gt; &gt; std:	 971.06(8.77%)		    std:    2721.36(7.89%)</span>
<span class="quote">&gt; &gt; max:   12010.00			    max:   38598.00</span>
<span class="quote">&gt; &gt; min:	9002.00			    min:   30636.00</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; In summary, MADV_FREE is about much faster than MADV_DONTNEED.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The MADV_FREE is discussed for a while, it probably is too late to propose</span>
<span class="quote">&gt; something new, but we had the new idea (from Ben Maurer, CCed) recently and</span>
<span class="quote">&gt; think it&#39;s better. Our target is still jemalloc.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Compared to MADV_DONTNEED, MADV_FREE&#39;s lazy memory free is a huge win to reduce</span>
<span class="quote">&gt; page fault. But there is one issue remaining, the TLB flush. Both MADV_DONTNEED</span>
<span class="quote">&gt; and MADV_FREE do TLB flush. TLB flush overhead is quite big in contemporary</span>
<span class="quote">&gt; multi-thread applications. In our production workload, we observed 80% CPU</span>
<span class="quote">&gt; spending on TLB flush triggered by jemalloc madvise(MADV_DONTNEED) sometimes.</span>
<span class="quote">&gt; We haven&#39;t tested MADV_FREE yet, but the result should be similar. It&#39;s hard to</span>
<span class="quote">&gt; avoid the TLB flush issue with MADV_FREE, because it helps avoid data</span>
<span class="quote">&gt; corruption.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The new proposal tries to fix the TLB issue. We introduce two madvise verbs:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; MARK_FREE. Userspace notifies kernel the memory range can be discarded. Kernel</span>
<span class="quote">&gt; just records the range in current stage. Should memory pressure happen, page</span>
<span class="quote">&gt; reclaim can free the memory directly regardless the pte state.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; MARK_NOFREE. Userspace notifies kernel the memory range will be reused soon.</span>
<span class="quote">&gt; Kernel deletes the record and prevents page reclaim discards the memory. If the</span>
<span class="quote">&gt; memory isn&#39;t reclaimed, userspace will access the old memory, otherwise do</span>
<span class="quote">&gt; normal page fault handling.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The point is to let userspace notify kernel if memory can be discarded, instead</span>
<span class="quote">&gt; of depending on pte dirty bit used by MADV_FREE. With these, no TLB flush is</span>
<span class="quote">&gt; required till page reclaim actually frees the memory (page reclaim need do the</span>
<span class="quote">&gt; TLB flush for MADV_FREE too). It still preserves the lazy memory free merit of</span>
<span class="quote">&gt; MADV_FREE.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Compared to MADV_FREE, reusing memory with the new proposal isn&#39;t transparent,</span>
<span class="quote">&gt; eg must call MARK_NOFREE. But it&#39;s easy to utilize the new API in jemalloc.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; We don&#39;t have code to backup this yet, sorry. We&#39;d like to discuss it if it</span>
<span class="quote">&gt; makes sense.</span>

It&#39;s really what volatile range did.
John Stultz and me tried it for a *long* time but it had lots of troubles.
It&#39;s really hard to write it down in my time due to really long history
and even I forgot lots of detail(ie, dead brain).
Please search volatile ranges in google.
Finally, people in LSF/MM suggested MADV_FREE to help anonymous page side
rather than stucking hich prevent useful feature. :(
<span class="quote">
&gt; </span>
<span class="quote">&gt; Thanks,</span>
<span class="quote">&gt; Shaohua</span>
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36811">Minchan Kim</a> - Nov. 5, 2015, 1:37 a.m.</div>
<pre class="content">
On Thu, Nov 05, 2015 at 10:33:50AM +0900, Minchan Kim wrote:
<span class="quote">&gt; On Wed, Nov 04, 2015 at 12:00:06PM -0800, Shaohua Li wrote:</span>
<span class="quote">&gt; &gt; On Wed, Nov 04, 2015 at 10:25:55AM +0900, Minchan Kim wrote:</span>
<span class="quote">&gt; &gt; &gt; Linux doesn&#39;t have an ability to free pages lazy while other OS already</span>
<span class="quote">&gt; &gt; &gt; have been supported that named by madvise(MADV_FREE).</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; The gain is clear that kernel can discard freed pages rather than swapping</span>
<span class="quote">&gt; &gt; &gt; out or OOM if memory pressure happens.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Without memory pressure, freed pages would be reused by userspace without</span>
<span class="quote">&gt; &gt; &gt; another additional overhead(ex, page fault + allocation + zeroing).</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Jason Evans said:</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; : Facebook has been using MAP_UNINITIALIZED</span>
<span class="quote">&gt; &gt; &gt; : (https://lkml.org/lkml/2012/1/18/308) in some of its applications for</span>
<span class="quote">&gt; &gt; &gt; : several years, but there are operational costs to maintaining this</span>
<span class="quote">&gt; &gt; &gt; : out-of-tree in our kernel and in jemalloc, and we are anxious to retire it</span>
<span class="quote">&gt; &gt; &gt; : in favor of MADV_FREE.  When we first enabled MAP_UNINITIALIZED it</span>
<span class="quote">&gt; &gt; &gt; : increased throughput for much of our workload by ~5%, and although the</span>
<span class="quote">&gt; &gt; &gt; : benefit has decreased using newer hardware and kernels, there is still</span>
<span class="quote">&gt; &gt; &gt; : enough benefit that we cannot reasonably retire it without a replacement.</span>
<span class="quote">&gt; &gt; &gt; :</span>
<span class="quote">&gt; &gt; &gt; : Aside from Facebook operations, there are numerous broadly used</span>
<span class="quote">&gt; &gt; &gt; : applications that would benefit from MADV_FREE.  The ones that immediately</span>
<span class="quote">&gt; &gt; &gt; : come to mind are redis, varnish, and MariaDB.  I don&#39;t have much insight</span>
<span class="quote">&gt; &gt; &gt; : into Android internals and development process, but I would hope to see</span>
<span class="quote">&gt; &gt; &gt; : MADV_FREE support eventually end up there as well to benefit applications</span>
<span class="quote">&gt; &gt; &gt; : linked with the integrated jemalloc.</span>
<span class="quote">&gt; &gt; &gt; :</span>
<span class="quote">&gt; &gt; &gt; : jemalloc will use MADV_FREE once it becomes available in the Linux kernel.</span>
<span class="quote">&gt; &gt; &gt; : In fact, jemalloc already uses MADV_FREE or equivalent everywhere it&#39;s</span>
<span class="quote">&gt; &gt; &gt; : available: *BSD, OS X, Windows, and Solaris -- every platform except Linux</span>
<span class="quote">&gt; &gt; &gt; : (and AIX, but I&#39;m not sure it even compiles on AIX).  The lack of</span>
<span class="quote">&gt; &gt; &gt; : MADV_FREE on Linux forced me down a long series of increasingly</span>
<span class="quote">&gt; &gt; &gt; : sophisticated heuristics for madvise() volume reduction, and even so this</span>
<span class="quote">&gt; &gt; &gt; : remains a common performance issue for people using jemalloc on Linux.</span>
<span class="quote">&gt; &gt; &gt; : Please integrate MADV_FREE; many people will benefit substantially.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; How it works:</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; When madvise syscall is called, VM clears dirty bit of ptes of the range.</span>
<span class="quote">&gt; &gt; &gt; If memory pressure happens, VM checks dirty bit of page table and if it</span>
<span class="quote">&gt; &gt; &gt; found still &quot;clean&quot;, it means it&#39;s a &quot;lazyfree pages&quot; so VM could discard</span>
<span class="quote">&gt; &gt; &gt; the page instead of swapping out.  Once there was store operation for the</span>
<span class="quote">&gt; &gt; &gt; page before VM peek a page to reclaim, dirty bit is set so VM can swap out</span>
<span class="quote">&gt; &gt; &gt; the page instead of discarding.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Firstly, heavy users would be general allocators(ex, jemalloc, tcmalloc</span>
<span class="quote">&gt; &gt; &gt; and hope glibc supports it) and jemalloc/tcmalloc already have supported</span>
<span class="quote">&gt; &gt; &gt; the feature for other OS(ex, FreeBSD)</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; barrios@blaptop:~/benchmark/ebizzy$ lscpu</span>
<span class="quote">&gt; &gt; &gt; Architecture:          x86_64</span>
<span class="quote">&gt; &gt; &gt; CPU op-mode(s):        32-bit, 64-bit</span>
<span class="quote">&gt; &gt; &gt; Byte Order:            Little Endian</span>
<span class="quote">&gt; &gt; &gt; CPU(s):                12</span>
<span class="quote">&gt; &gt; &gt; On-line CPU(s) list:   0-11</span>
<span class="quote">&gt; &gt; &gt; Thread(s) per core:    1</span>
<span class="quote">&gt; &gt; &gt; Core(s) per socket:    1</span>
<span class="quote">&gt; &gt; &gt; Socket(s):             12</span>
<span class="quote">&gt; &gt; &gt; NUMA node(s):          1</span>
<span class="quote">&gt; &gt; &gt; Vendor ID:             GenuineIntel</span>
<span class="quote">&gt; &gt; &gt; CPU family:            6</span>
<span class="quote">&gt; &gt; &gt; Model:                 2</span>
<span class="quote">&gt; &gt; &gt; Stepping:              3</span>
<span class="quote">&gt; &gt; &gt; CPU MHz:               3200.185</span>
<span class="quote">&gt; &gt; &gt; BogoMIPS:              6400.53</span>
<span class="quote">&gt; &gt; &gt; Virtualization:        VT-x</span>
<span class="quote">&gt; &gt; &gt; Hypervisor vendor:     KVM</span>
<span class="quote">&gt; &gt; &gt; Virtualization type:   full</span>
<span class="quote">&gt; &gt; &gt; L1d cache:             32K</span>
<span class="quote">&gt; &gt; &gt; L1i cache:             32K</span>
<span class="quote">&gt; &gt; &gt; L2 cache:              4096K</span>
<span class="quote">&gt; &gt; &gt; NUMA node0 CPU(s):     0-11</span>
<span class="quote">&gt; &gt; &gt; ebizzy benchmark(./ebizzy -S 10 -n 512)</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Higher avg is better.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt;  vanilla-jemalloc		MADV_free-jemalloc</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; 1 thread</span>
<span class="quote">&gt; &gt; &gt; records: 10			    records: 10</span>
<span class="quote">&gt; &gt; &gt; avg:	2961.90			    avg:   12069.70</span>
<span class="quote">&gt; &gt; &gt; std:	  71.96(2.43%)		    std:     186.68(1.55%)</span>
<span class="quote">&gt; &gt; &gt; max:	3070.00			    max:   12385.00</span>
<span class="quote">&gt; &gt; &gt; min:	2796.00			    min:   11746.00</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; 2 thread</span>
<span class="quote">&gt; &gt; &gt; records: 10			    records: 10</span>
<span class="quote">&gt; &gt; &gt; avg:	5020.00			    avg:   17827.00</span>
<span class="quote">&gt; &gt; &gt; std:	 264.87(5.28%)		    std:     358.52(2.01%)</span>
<span class="quote">&gt; &gt; &gt; max:	5244.00			    max:   18760.00</span>
<span class="quote">&gt; &gt; &gt; min:	4251.00			    min:   17382.00</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; 4 thread</span>
<span class="quote">&gt; &gt; &gt; records: 10			    records: 10</span>
<span class="quote">&gt; &gt; &gt; avg:	8988.80			    avg:   27930.80</span>
<span class="quote">&gt; &gt; &gt; std:	1175.33(13.08%)		    std:    3317.33(11.88%)</span>
<span class="quote">&gt; &gt; &gt; max:	9508.00			    max:   30879.00</span>
<span class="quote">&gt; &gt; &gt; min:	5477.00			    min:   21024.00</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; 8 thread</span>
<span class="quote">&gt; &gt; &gt; records: 10			    records: 10</span>
<span class="quote">&gt; &gt; &gt; avg:   13036.50			    avg:   33739.40</span>
<span class="quote">&gt; &gt; &gt; std:	 170.67(1.31%)		    std:    5146.22(15.25%)</span>
<span class="quote">&gt; &gt; &gt; max:   13371.00			    max:   40572.00</span>
<span class="quote">&gt; &gt; &gt; min:   12785.00			    min:   24088.00</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; 16 thread</span>
<span class="quote">&gt; &gt; &gt; records: 10			    records: 10</span>
<span class="quote">&gt; &gt; &gt; avg:   11092.40			    avg:   31424.20</span>
<span class="quote">&gt; &gt; &gt; std:	 710.60(6.41%)		    std:    3763.89(11.98%)</span>
<span class="quote">&gt; &gt; &gt; max:   12446.00			    max:   36635.00</span>
<span class="quote">&gt; &gt; &gt; min:	9949.00			    min:   25669.00</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; 32 thread</span>
<span class="quote">&gt; &gt; &gt; records: 10			    records: 10</span>
<span class="quote">&gt; &gt; &gt; avg:   11067.00			    avg:   34495.80</span>
<span class="quote">&gt; &gt; &gt; std:	 971.06(8.77%)		    std:    2721.36(7.89%)</span>
<span class="quote">&gt; &gt; &gt; max:   12010.00			    max:   38598.00</span>
<span class="quote">&gt; &gt; &gt; min:	9002.00			    min:   30636.00</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; In summary, MADV_FREE is about much faster than MADV_DONTNEED.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; The MADV_FREE is discussed for a while, it probably is too late to propose</span>
<span class="quote">&gt; &gt; something new, but we had the new idea (from Ben Maurer, CCed) recently and</span>
<span class="quote">&gt; &gt; think it&#39;s better. Our target is still jemalloc.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Compared to MADV_DONTNEED, MADV_FREE&#39;s lazy memory free is a huge win to reduce</span>
<span class="quote">&gt; &gt; page fault. But there is one issue remaining, the TLB flush. Both MADV_DONTNEED</span>
<span class="quote">&gt; &gt; and MADV_FREE do TLB flush. TLB flush overhead is quite big in contemporary</span>
<span class="quote">&gt; &gt; multi-thread applications. In our production workload, we observed 80% CPU</span>
<span class="quote">&gt; &gt; spending on TLB flush triggered by jemalloc madvise(MADV_DONTNEED) sometimes.</span>
<span class="quote">&gt; &gt; We haven&#39;t tested MADV_FREE yet, but the result should be similar. It&#39;s hard to</span>
<span class="quote">&gt; &gt; avoid the TLB flush issue with MADV_FREE, because it helps avoid data</span>
<span class="quote">&gt; &gt; corruption.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; The new proposal tries to fix the TLB issue. We introduce two madvise verbs:</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; MARK_FREE. Userspace notifies kernel the memory range can be discarded. Kernel</span>
<span class="quote">&gt; &gt; just records the range in current stage. Should memory pressure happen, page</span>
<span class="quote">&gt; &gt; reclaim can free the memory directly regardless the pte state.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; MARK_NOFREE. Userspace notifies kernel the memory range will be reused soon.</span>
<span class="quote">&gt; &gt; Kernel deletes the record and prevents page reclaim discards the memory. If the</span>
<span class="quote">&gt; &gt; memory isn&#39;t reclaimed, userspace will access the old memory, otherwise do</span>
<span class="quote">&gt; &gt; normal page fault handling.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; The point is to let userspace notify kernel if memory can be discarded, instead</span>
<span class="quote">&gt; &gt; of depending on pte dirty bit used by MADV_FREE. With these, no TLB flush is</span>
<span class="quote">&gt; &gt; required till page reclaim actually frees the memory (page reclaim need do the</span>
<span class="quote">&gt; &gt; TLB flush for MADV_FREE too). It still preserves the lazy memory free merit of</span>
<span class="quote">&gt; &gt; MADV_FREE.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Compared to MADV_FREE, reusing memory with the new proposal isn&#39;t transparent,</span>
<span class="quote">&gt; &gt; eg must call MARK_NOFREE. But it&#39;s easy to utilize the new API in jemalloc.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; We don&#39;t have code to backup this yet, sorry. We&#39;d like to discuss it if it</span>
<span class="quote">&gt; &gt; makes sense.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It&#39;s really what volatile range did.</span>
<span class="quote">&gt; John Stultz and me tried it for a *long* time but it had lots of troubles.</span>
<span class="quote">&gt; It&#39;s really hard to write it down in my time due to really long history</span>
<span class="quote">&gt; and even I forgot lots of detail(ie, dead brain).</span>
<span class="quote">&gt; Please search volatile ranges in google.</span>
<span class="quote">&gt; Finally, people in LSF/MM suggested MADV_FREE to help anonymous page side</span>
<span class="quote">&gt; rather than stucking hich prevent useful feature. :(</span>

I should have Cced John Stutlz.

He would have good memory than me so he would help but I&#39;m not sure
he has a interest on volatile ranges, still.
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36811">Minchan Kim</a> - Nov. 5, 2015, 1:48 a.m.</div>
<pre class="content">
On Wed, Nov 04, 2015 at 05:29:57PM -0800, Andy Lutomirski wrote:
<span class="quote">&gt; On Wed, Nov 4, 2015 at 4:56 PM, Minchan Kim &lt;minchan@kernel.org&gt; wrote:</span>
<span class="quote">&gt; &gt; On Wed, Nov 04, 2015 at 04:42:37PM -0800, Andy Lutomirski wrote:</span>
<span class="quote">&gt; &gt;&gt; On Wed, Nov 4, 2015 at 4:13 PM, Minchan Kim &lt;minchan@kernel.org&gt; wrote:</span>
<span class="quote">&gt; &gt;&gt; &gt; On Tue, Nov 03, 2015 at 07:41:35PM -0800, Andy Lutomirski wrote:</span>
<span class="quote">&gt; &gt;&gt; &gt;&gt; On Nov 3, 2015 5:30 PM, &quot;Minchan Kim&quot; &lt;minchan@kernel.org&gt; wrote:</span>
<span class="quote">&gt; &gt;&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; &gt;&gt; &gt; Linux doesn&#39;t have an ability to free pages lazy while other OS already</span>
<span class="quote">&gt; &gt;&gt; &gt;&gt; &gt; have been supported that named by madvise(MADV_FREE).</span>
<span class="quote">&gt; &gt;&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; &gt;&gt; &gt; The gain is clear that kernel can discard freed pages rather than swapping</span>
<span class="quote">&gt; &gt;&gt; &gt;&gt; &gt; out or OOM if memory pressure happens.</span>
<span class="quote">&gt; &gt;&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; &gt;&gt; &gt; Without memory pressure, freed pages would be reused by userspace without</span>
<span class="quote">&gt; &gt;&gt; &gt;&gt; &gt; another additional overhead(ex, page fault + allocation + zeroing).</span>
<span class="quote">&gt; &gt;&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; &gt;&gt; [...]</span>
<span class="quote">&gt; &gt;&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; &gt;&gt; &gt; How it works:</span>
<span class="quote">&gt; &gt;&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; &gt;&gt; &gt; When madvise syscall is called, VM clears dirty bit of ptes of the range.</span>
<span class="quote">&gt; &gt;&gt; &gt;&gt; &gt; If memory pressure happens, VM checks dirty bit of page table and if it</span>
<span class="quote">&gt; &gt;&gt; &gt;&gt; &gt; found still &quot;clean&quot;, it means it&#39;s a &quot;lazyfree pages&quot; so VM could discard</span>
<span class="quote">&gt; &gt;&gt; &gt;&gt; &gt; the page instead of swapping out.  Once there was store operation for the</span>
<span class="quote">&gt; &gt;&gt; &gt;&gt; &gt; page before VM peek a page to reclaim, dirty bit is set so VM can swap out</span>
<span class="quote">&gt; &gt;&gt; &gt;&gt; &gt; the page instead of discarding.</span>
<span class="quote">&gt; &gt;&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; &gt;&gt; What happens if you MADV_FREE something that&#39;s MAP_SHARED or isn&#39;t</span>
<span class="quote">&gt; &gt;&gt; &gt;&gt; ordinary anonymous memory?  There&#39;s a long history of MADV_DONTNEED on</span>
<span class="quote">&gt; &gt;&gt; &gt;&gt; such mappings causing exploitable problems, and I think it would be</span>
<span class="quote">&gt; &gt;&gt; &gt;&gt; nice if MADV_FREE were obviously safe.</span>
<span class="quote">&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; &gt; It filter out VM_LOCKED|VM_HUGETLB|VM_PFNMAP and file-backed vma and MAP_SHARED</span>
<span class="quote">&gt; &gt;&gt; &gt; with vma_is_anonymous.</span>
<span class="quote">&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; &gt;&gt; Does this set the write protect bit?</span>
<span class="quote">&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; &gt; No.</span>
<span class="quote">&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; &gt;&gt; What happens on architectures without hardware dirty tracking?  For</span>
<span class="quote">&gt; &gt;&gt; &gt;&gt; that matter, even on architecture with hardware dirty tracking, what</span>
<span class="quote">&gt; &gt;&gt; &gt;&gt; happens in multithreaded processes that have the dirty TLB state</span>
<span class="quote">&gt; &gt;&gt; &gt;&gt; cached in a different CPU&#39;s TLB?</span>
<span class="quote">&gt; &gt;&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; &gt;&gt; Using the dirty bit for these semantics scares me.  This API creates a</span>
<span class="quote">&gt; &gt;&gt; &gt;&gt; page that can have visible nonzero contents and then can</span>
<span class="quote">&gt; &gt;&gt; &gt;&gt; asynchronously and magically zero itself thereafter.  That makes me</span>
<span class="quote">&gt; &gt;&gt; &gt;&gt; nervous.  Could we use the accessed bit instead?  Then the observable</span>
<span class="quote">&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; &gt; Access bit is used by aging algorithm for reclaim. In addition,</span>
<span class="quote">&gt; &gt;&gt; &gt; we have supported clear_refs feacture.</span>
<span class="quote">&gt; &gt;&gt; &gt; IOW, it could be reset anytime so it&#39;s hard to use marker for</span>
<span class="quote">&gt; &gt;&gt; &gt; lazy freeing at the moment.</span>
<span class="quote">&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; That&#39;s unfortunate.  I think that the ABI would be much nicer if it</span>
<span class="quote">&gt; &gt;&gt; used the accessed bit.</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; In any case, shouldn&#39;t the aging algorithm be irrelevant here?  A</span>
<span class="quote">&gt; &gt;&gt; MADV_FREE page that isn&#39;t accessed can be discarded, whereas we could</span>
<span class="quote">&gt; &gt;&gt; hopefully just say that a MADV_FREE page that is accessed gets moved</span>
<span class="quote">&gt; &gt;&gt; to whatever list holds recently accessed pages and also stops being a</span>
<span class="quote">&gt; &gt;&gt; candidate for discarding due to MADV_FREE?</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; I meant if we use access bit as indicator for lazy-freeing page,</span>
<span class="quote">&gt; &gt; we could discard valid page which is never hinted by MADV_FREE but</span>
<span class="quote">&gt; &gt; just doesn&#39;t mark access bit in page table by aging algorithm.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Oh, is the rule that the anonymous pages that are clean are discarded</span>
<span class="quote">&gt; instead of swapped out?  That is, does your patch set detect that an</span>

The page swapped-in after swapped-out has clean pte and swap device
has valid data if the page isn&#39;t touch so VM discards the page rather
than swapout. Of course, pte should point out the swap slot.
If VM decide to remove the page from swap slot, it should be marked
PG_dirty.
<span class="quote">
&gt; anonymous page can be discarded if it&#39;s clean and that the lack of a</span>
<span class="quote">&gt; dirty bit is the only indication that the page has been hit with</span>
<span class="quote">&gt; MADV_FREE?</span>

No dirty bit, exactly speaking, PG_Dirty
because the page I mentioned above has clean pte but will have PG_dirty.
<span class="quote">
&gt; </span>
<span class="quote">&gt; If so, that seems potentially error prone -- I had assumed that pages</span>
<span class="quote">&gt; that were swapped in but not written since swap-in would also be</span>
<span class="quote">&gt; clean, and I don&#39;t see how you distinguish them.</span>

I hope above will answer.
<span class="quote">&gt; </span>
<span class="quote">&gt; --Andy</span>
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=121341">Sergey Senozhatsky</a> - Nov. 5, 2015, 3:41 a.m.</div>
<pre class="content">
Hi Minchan,

On (11/05/15 08:39), Minchan Kim wrote:
[..]
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I think it makes sense to update pmd_trans_unstable() and</span>
<span class="quote">&gt; &gt; pmd_none_or_trans_huge_or_clear_bad() comments in asm-generic/pgtable.h</span>
<span class="quote">&gt; &gt; Because they explicitly mention MADV_DONTNEED only. Just a thought.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Hmm, When I read comments(but actually I don&#39;t understand it 100%), it</span>
<span class="quote">&gt; says pmd disappearing from MADV_DONTNEED with mmap_sem read-side</span>
<span class="quote">&gt; lock. But MADV_FREE doesn&#39;t remove the pmd. So, I don&#39;t understand</span>
<span class="quote">&gt; what I should add comment. Please suggest if I am missing something.</span>
<span class="quote">&gt; </span>

Hm, sorry, I need to think about it more, probably my comment is irrelevant.
Was fantasizing some stupid use cases like doing MADV_DONTNEED and MADV_FREE
on overlapping addresses from different threads, processes that share mem, etc.
<span class="quote">
&gt; &gt; &gt; @@ -379,6 +502,14 @@ madvise_vma(struct vm_area_struct *vma, struct vm_area_struct **prev,</span>
<span class="quote">&gt; &gt; &gt;  		return madvise_remove(vma, prev, start, end);</span>
<span class="quote">&gt; &gt; &gt;  	case MADV_WILLNEED:</span>
<span class="quote">&gt; &gt; &gt;  		return madvise_willneed(vma, prev, start, end);</span>
<span class="quote">&gt; &gt; &gt; +	case MADV_FREE:</span>
<span class="quote">&gt; &gt; &gt; +		/*</span>
<span class="quote">&gt; &gt; &gt; +		 * XXX: In this implementation, MADV_FREE works like</span>
<span class="quote">&gt; &gt; 		  ^^^^</span>
<span class="quote">&gt; &gt; 		XXX</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; What does it mean?</span>

not much. just a minor note that there is a &#39;XXX&#39; in &quot;XXX: In this implementation&quot;
comment.

	-ss
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=19951">Shaohua Li</a> - Nov. 5, 2015, 6:17 p.m.</div>
<pre class="content">
On Wed, Nov 04, 2015 at 05:05:47PM -0500, Daniel Micay wrote:
<span class="quote">&gt; &gt; With enough pages at once, though, munmap would be fine, too.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; That implies lots of page faults and zeroing though. The zeroing alone</span>
<span class="quote">&gt; is a major performance issue.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; There are separate issues with munmap since it ends up resulting in a</span>
<span class="quote">&gt; lot more virtual memory fragmentation. It would help if the kernel used</span>
<span class="quote">&gt; first-best-fit for mmap instead of the current naive algorithm (bonus:</span>
<span class="quote">&gt; O(log n) worst-case, not O(n)). Since allocators like jemalloc and</span>
<span class="quote">&gt; PartitionAlloc want 2M aligned spans, mixing them with other allocators</span>
<span class="quote">&gt; can also accelerate the VM fragmentation caused by the dumb mmap</span>
<span class="quote">&gt; algorithm (i.e. they make a 2M aligned mapping, some other mmap user</span>
<span class="quote">&gt; does 4k, now there&#39;s a nearly 2M gap when the next 2M region is made and</span>
<span class="quote">&gt; the kernel keeps going rather than reusing it). Anyway, that&#39;s a totally</span>
<span class="quote">&gt; separate issue from this. Just felt like complaining :).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; Maybe what&#39;s really needed is a MADV_FREE variant that takes an iovec.</span>
<span class="quote">&gt; &gt; On an all-cores multithreaded mm, the TLB shootdown broadcast takes</span>
<span class="quote">&gt; &gt; thousands of cycles on each core more or less regardless of how much</span>
<span class="quote">&gt; &gt; of the TLB gets zapped.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; That would work very well. The allocator ends up having a sequence of</span>
<span class="quote">&gt; dirty spans that it needs to purge in one go. As long as purging is</span>
<span class="quote">&gt; fairly spread out, the cost of a single TLB shootdown isn&#39;t that bad. It</span>
<span class="quote">&gt; is extremely bad if it needs to do it over and over to purge a bunch of</span>
<span class="quote">&gt; ranges, which can happen if the memory has ended up being very, very</span>
<span class="quote">&gt; fragmentated despite the efforts to compact it (depends on what the</span>
<span class="quote">&gt; application ends up doing).</span>

I posted a patch doing exactly iovec madvise. Doesn&#39;t support MADV_FREE yet
though, but should be easy to do it.

http://marc.info/?l=linux-mm&amp;m=144615663522661&amp;w=2
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=110221">Daniel Micay</a> - Nov. 5, 2015, 8:13 p.m.</div>
<pre class="content">
<span class="quote">&gt; I posted a patch doing exactly iovec madvise. Doesn&#39;t support MADV_FREE yet</span>
<span class="quote">&gt; though, but should be easy to do it.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; http://marc.info/?l=linux-mm&amp;m=144615663522661&amp;w=2</span>

I think that would be a great way to deal with this. It keeps the nice
property of still being able to drop pages in allocations that have been
handed out but not yet touched. The allocator just needs to be designed
to do lots of purging in one go (i.e. something like an 8:1 active:clean
ratio triggers purging and it goes all the way to 16:1).
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=110221">Daniel Micay</a> - Nov. 5, 2015, 8:14 p.m.</div>
<pre class="content">
<span class="quote">&gt; active:clean</span>

active:dirty*, sigh.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=11632">John Stultz</a> - Dec. 1, 2015, 10:30 p.m.</div>
<pre class="content">
On Wed, Nov 4, 2015 at 12:00 PM, Shaohua Li &lt;shli@kernel.org&gt; wrote:
<span class="quote">&gt; Compared to MADV_DONTNEED, MADV_FREE&#39;s lazy memory free is a huge win to reduce</span>
<span class="quote">&gt; page fault. But there is one issue remaining, the TLB flush. Both MADV_DONTNEED</span>
<span class="quote">&gt; and MADV_FREE do TLB flush. TLB flush overhead is quite big in contemporary</span>
<span class="quote">&gt; multi-thread applications. In our production workload, we observed 80% CPU</span>
<span class="quote">&gt; spending on TLB flush triggered by jemalloc madvise(MADV_DONTNEED) sometimes.</span>
<span class="quote">&gt; We haven&#39;t tested MADV_FREE yet, but the result should be similar. It&#39;s hard to</span>
<span class="quote">&gt; avoid the TLB flush issue with MADV_FREE, because it helps avoid data</span>
<span class="quote">&gt; corruption.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; The new proposal tries to fix the TLB issue. We introduce two madvise verbs:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; MARK_FREE. Userspace notifies kernel the memory range can be discarded. Kernel</span>
<span class="quote">&gt; just records the range in current stage. Should memory pressure happen, page</span>
<span class="quote">&gt; reclaim can free the memory directly regardless the pte state.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; MARK_NOFREE. Userspace notifies kernel the memory range will be reused soon.</span>
<span class="quote">&gt; Kernel deletes the record and prevents page reclaim discards the memory. If the</span>
<span class="quote">&gt; memory isn&#39;t reclaimed, userspace will access the old memory, otherwise do</span>
<span class="quote">&gt; normal page fault handling.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; The point is to let userspace notify kernel if memory can be discarded, instead</span>
<span class="quote">&gt; of depending on pte dirty bit used by MADV_FREE. With these, no TLB flush is</span>
<span class="quote">&gt; required till page reclaim actually frees the memory (page reclaim need do the</span>
<span class="quote">&gt; TLB flush for MADV_FREE too). It still preserves the lazy memory free merit of</span>
<span class="quote">&gt; MADV_FREE.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Compared to MADV_FREE, reusing memory with the new proposal isn&#39;t transparent,</span>
<span class="quote">&gt; eg must call MARK_NOFREE. But it&#39;s easy to utilize the new API in jemalloc.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; We don&#39;t have code to backup this yet, sorry. We&#39;d like to discuss it if it</span>
<span class="quote">&gt; makes sense.</span>

Sorry to be so slow to reply here!

As Minchan mentioned, this is very similar in concept to the volatile
ranges work Minchan and I tried to push for a few years.

Here&#39;s some of the coverage (in reverse chronological order)
https://lwn.net/Articles/602650/
https://lwn.net/Articles/592042/
https://lwn.net/Articles/590991/
http://permalink.gmane.org/gmane.linux.kernel.mm/98848
http://permalink.gmane.org/gmane.linux.kernel.mm/98676
https://lwn.net/Articles/522135/
https://lwn.net/Kernel/Index/#Volatile_ranges


If you are interested in reviving the patch set, I&#39;d love to hear
about it. I think its a really compelling feature for kernel
right-sizing of userspace caches.

thanks
-john
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/include/linux/rmap.h b/include/linux/rmap.h</span>
<span class="p_header">index 29446aeef36e..f4c992826242 100644</span>
<span class="p_header">--- a/include/linux/rmap.h</span>
<span class="p_header">+++ b/include/linux/rmap.h</span>
<span class="p_chunk">@@ -85,6 +85,7 @@</span> <span class="p_context"> enum ttu_flags {</span>
 	TTU_UNMAP = 1,			/* unmap mode */
 	TTU_MIGRATION = 2,		/* migration mode */
 	TTU_MUNLOCK = 4,		/* munlock mode */
<span class="p_add">+	TTU_FREE = 8,			/* free mode */</span>
 
 	TTU_IGNORE_MLOCK = (1 &lt;&lt; 8),	/* ignore mlock */
 	TTU_IGNORE_ACCESS = (1 &lt;&lt; 9),	/* don&#39;t age */
<span class="p_header">diff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h</span>
<span class="p_header">index 9246d32dc973..2b1cef88b827 100644</span>
<span class="p_header">--- a/include/linux/vm_event_item.h</span>
<span class="p_header">+++ b/include/linux/vm_event_item.h</span>
<span class="p_chunk">@@ -25,6 +25,7 @@</span> <span class="p_context"> enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,</span>
 		FOR_ALL_ZONES(PGALLOC),
 		PGFREE, PGACTIVATE, PGDEACTIVATE,
 		PGFAULT, PGMAJFAULT,
<span class="p_add">+		PGLAZYFREED,</span>
 		FOR_ALL_ZONES(PGREFILL),
 		FOR_ALL_ZONES(PGSTEAL_KSWAPD),
 		FOR_ALL_ZONES(PGSTEAL_DIRECT),
<span class="p_header">diff --git a/include/uapi/asm-generic/mman-common.h b/include/uapi/asm-generic/mman-common.h</span>
<span class="p_header">index ddc3b36f1046..7a94102b7a02 100644</span>
<span class="p_header">--- a/include/uapi/asm-generic/mman-common.h</span>
<span class="p_header">+++ b/include/uapi/asm-generic/mman-common.h</span>
<span class="p_chunk">@@ -34,6 +34,7 @@</span> <span class="p_context"></span>
 #define MADV_SEQUENTIAL	2		/* expect sequential page references */
 #define MADV_WILLNEED	3		/* will need these pages */
 #define MADV_DONTNEED	4		/* don&#39;t need these pages */
<span class="p_add">+#define MADV_FREE	5		/* free pages only if memory pressure */</span>
 
 /* common parameters: try to keep these consistent across architectures */
 #define MADV_REMOVE	9		/* remove these pages &amp; resources */
<span class="p_header">diff --git a/mm/madvise.c b/mm/madvise.c</span>
<span class="p_header">index c889fcbb530e..a8813f7b37b3 100644</span>
<span class="p_header">--- a/mm/madvise.c</span>
<span class="p_header">+++ b/mm/madvise.c</span>
<span class="p_chunk">@@ -20,6 +20,9 @@</span> <span class="p_context"></span>
 #include &lt;linux/backing-dev.h&gt;
 #include &lt;linux/swap.h&gt;
 #include &lt;linux/swapops.h&gt;
<span class="p_add">+#include &lt;linux/mmu_notifier.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/tlb.h&gt;</span>
 
 /*
  * Any behaviour which results in changes to the vma-&gt;vm_flags needs to
<span class="p_chunk">@@ -32,6 +35,7 @@</span> <span class="p_context"> static int madvise_need_mmap_write(int behavior)</span>
 	case MADV_REMOVE:
 	case MADV_WILLNEED:
 	case MADV_DONTNEED:
<span class="p_add">+	case MADV_FREE:</span>
 		return 0;
 	default:
 		/* be safe, default to 1. list exceptions explicitly */
<span class="p_chunk">@@ -256,6 +260,125 @@</span> <span class="p_context"> static long madvise_willneed(struct vm_area_struct *vma,</span>
 	return 0;
 }
 
<span class="p_add">+static int madvise_free_pte_range(pmd_t *pmd, unsigned long addr,</span>
<span class="p_add">+				unsigned long end, struct mm_walk *walk)</span>
<span class="p_add">+</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct mmu_gather *tlb = walk-&gt;private;</span>
<span class="p_add">+	struct mm_struct *mm = tlb-&gt;mm;</span>
<span class="p_add">+	struct vm_area_struct *vma = walk-&gt;vma;</span>
<span class="p_add">+	spinlock_t *ptl;</span>
<span class="p_add">+	pte_t *pte, ptent;</span>
<span class="p_add">+	struct page *page;</span>
<span class="p_add">+</span>
<span class="p_add">+	split_huge_page_pmd(vma, addr, pmd);</span>
<span class="p_add">+	if (pmd_trans_unstable(pmd))</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	pte = pte_offset_map_lock(mm, pmd, addr, &amp;ptl);</span>
<span class="p_add">+	arch_enter_lazy_mmu_mode();</span>
<span class="p_add">+	for (; addr != end; pte++, addr += PAGE_SIZE) {</span>
<span class="p_add">+		ptent = *pte;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!pte_present(ptent))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		page = vm_normal_page(vma, addr, ptent);</span>
<span class="p_add">+		if (!page)</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (PageSwapCache(page)) {</span>
<span class="p_add">+			if (!trylock_page(page))</span>
<span class="p_add">+				continue;</span>
<span class="p_add">+</span>
<span class="p_add">+			if (!try_to_free_swap(page)) {</span>
<span class="p_add">+				unlock_page(page);</span>
<span class="p_add">+				continue;</span>
<span class="p_add">+			}</span>
<span class="p_add">+</span>
<span class="p_add">+			ClearPageDirty(page);</span>
<span class="p_add">+			unlock_page(page);</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		if (pte_young(ptent) || pte_dirty(ptent)) {</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * Some of architecture(ex, PPC) don&#39;t update TLB</span>
<span class="p_add">+			 * with set_pte_at and tlb_remove_tlb_entry so for</span>
<span class="p_add">+			 * the portability, remap the pte with old|clean</span>
<span class="p_add">+			 * after pte clearing.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			ptent = ptep_get_and_clear_full(mm, addr, pte,</span>
<span class="p_add">+							tlb-&gt;fullmm);</span>
<span class="p_add">+</span>
<span class="p_add">+			ptent = pte_mkold(ptent);</span>
<span class="p_add">+			ptent = pte_mkclean(ptent);</span>
<span class="p_add">+			set_pte_at(mm, addr, pte, ptent);</span>
<span class="p_add">+			tlb_remove_tlb_entry(tlb, pte, addr);</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	arch_leave_lazy_mmu_mode();</span>
<span class="p_add">+	pte_unmap_unlock(pte - 1, ptl);</span>
<span class="p_add">+	cond_resched();</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void madvise_free_page_range(struct mmu_gather *tlb,</span>
<span class="p_add">+			     struct vm_area_struct *vma,</span>
<span class="p_add">+			     unsigned long addr, unsigned long end)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct mm_walk free_walk = {</span>
<span class="p_add">+		.pmd_entry = madvise_free_pte_range,</span>
<span class="p_add">+		.mm = vma-&gt;vm_mm,</span>
<span class="p_add">+		.private = tlb,</span>
<span class="p_add">+	};</span>
<span class="p_add">+</span>
<span class="p_add">+	tlb_start_vma(tlb, vma);</span>
<span class="p_add">+	walk_page_range(addr, end, &amp;free_walk);</span>
<span class="p_add">+	tlb_end_vma(tlb, vma);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int madvise_free_single_vma(struct vm_area_struct *vma,</span>
<span class="p_add">+			unsigned long start_addr, unsigned long end_addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long start, end;</span>
<span class="p_add">+	struct mm_struct *mm = vma-&gt;vm_mm;</span>
<span class="p_add">+	struct mmu_gather tlb;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (vma-&gt;vm_flags &amp; (VM_LOCKED|VM_HUGETLB|VM_PFNMAP))</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* MADV_FREE works for only anon vma at the moment */</span>
<span class="p_add">+	if (!vma_is_anonymous(vma))</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+</span>
<span class="p_add">+	start = max(vma-&gt;vm_start, start_addr);</span>
<span class="p_add">+	if (start &gt;= vma-&gt;vm_end)</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+	end = min(vma-&gt;vm_end, end_addr);</span>
<span class="p_add">+	if (end &lt;= vma-&gt;vm_start)</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+</span>
<span class="p_add">+	lru_add_drain();</span>
<span class="p_add">+	tlb_gather_mmu(&amp;tlb, mm, start, end);</span>
<span class="p_add">+	update_hiwater_rss(mm);</span>
<span class="p_add">+</span>
<span class="p_add">+	mmu_notifier_invalidate_range_start(mm, start, end);</span>
<span class="p_add">+	madvise_free_page_range(&amp;tlb, vma, start, end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_end(mm, start, end);</span>
<span class="p_add">+	tlb_finish_mmu(&amp;tlb, start, end);</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static long madvise_free(struct vm_area_struct *vma,</span>
<span class="p_add">+			     struct vm_area_struct **prev,</span>
<span class="p_add">+			     unsigned long start, unsigned long end)</span>
<span class="p_add">+{</span>
<span class="p_add">+	*prev = vma;</span>
<span class="p_add">+	return madvise_free_single_vma(vma, start, end);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /*
  * Application no longer needs these pages.  If the pages are dirty,
  * it&#39;s OK to just throw them away.  The app will be more careful about
<span class="p_chunk">@@ -379,6 +502,14 @@</span> <span class="p_context"> madvise_vma(struct vm_area_struct *vma, struct vm_area_struct **prev,</span>
 		return madvise_remove(vma, prev, start, end);
 	case MADV_WILLNEED:
 		return madvise_willneed(vma, prev, start, end);
<span class="p_add">+	case MADV_FREE:</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * XXX: In this implementation, MADV_FREE works like</span>
<span class="p_add">+		 * MADV_DONTNEED on swapless system or full swap.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (get_nr_swap_pages() &gt; 0)</span>
<span class="p_add">+			return madvise_free(vma, prev, start, end);</span>
<span class="p_add">+		/* passthrough */</span>
 	case MADV_DONTNEED:
 		return madvise_dontneed(vma, prev, start, end);
 	default:
<span class="p_chunk">@@ -398,6 +529,7 @@</span> <span class="p_context"> madvise_behavior_valid(int behavior)</span>
 	case MADV_REMOVE:
 	case MADV_WILLNEED:
 	case MADV_DONTNEED:
<span class="p_add">+	case MADV_FREE:</span>
 #ifdef CONFIG_KSM
 	case MADV_MERGEABLE:
 	case MADV_UNMERGEABLE:
<span class="p_header">diff --git a/mm/rmap.c b/mm/rmap.c</span>
<span class="p_header">index f5b5c1f3dcd7..9449e91839ab 100644</span>
<span class="p_header">--- a/mm/rmap.c</span>
<span class="p_header">+++ b/mm/rmap.c</span>
<span class="p_chunk">@@ -1374,6 +1374,12 @@</span> <span class="p_context"> static int try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
 		swp_entry_t entry = { .val = page_private(page) };
 		pte_t swp_pte;
 
<span class="p_add">+		if (!PageDirty(page) &amp;&amp; (flags &amp; TTU_FREE)) {</span>
<span class="p_add">+			/* It&#39;s a freeable page by MADV_FREE */</span>
<span class="p_add">+			dec_mm_counter(mm, MM_ANONPAGES);</span>
<span class="p_add">+			goto discard;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
 		if (PageSwapCache(page)) {
 			/*
 			 * Store the swap location in the pte.
<span class="p_chunk">@@ -1414,6 +1420,7 @@</span> <span class="p_context"> static int try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
 	} else
 		dec_mm_counter(mm, MM_FILEPAGES);
 
<span class="p_add">+discard:</span>
 	page_remove_rmap(page);
 	page_cache_release(page);
 
<span class="p_header">diff --git a/mm/swap_state.c b/mm/swap_state.c</span>
<span class="p_header">index d504adb7fa5f..10f63eded7b7 100644</span>
<span class="p_header">--- a/mm/swap_state.c</span>
<span class="p_header">+++ b/mm/swap_state.c</span>
<span class="p_chunk">@@ -185,13 +185,12 @@</span> <span class="p_context"> int add_to_swap(struct page *page, struct list_head *list)</span>
 	 * deadlock in the swap out path.
 	 */
 	/*
<span class="p_del">-	 * Add it to the swap cache and mark it dirty</span>
<span class="p_add">+	 * Add it to the swap cache.</span>
 	 */
 	err = add_to_swap_cache(page, entry,
 			__GFP_HIGH|__GFP_NOMEMALLOC|__GFP_NOWARN);
 
<span class="p_del">-	if (!err) {	/* Success */</span>
<span class="p_del">-		SetPageDirty(page);</span>
<span class="p_add">+	if (!err) {</span>
 		return 1;
 	} else {	/* -ENOMEM radix-tree allocation failure */
 		/*
<span class="p_header">diff --git a/mm/vmscan.c b/mm/vmscan.c</span>
<span class="p_header">index 7f63a9381f71..7a415b9fdd34 100644</span>
<span class="p_header">--- a/mm/vmscan.c</span>
<span class="p_header">+++ b/mm/vmscan.c</span>
<span class="p_chunk">@@ -906,6 +906,7 @@</span> <span class="p_context"> static unsigned long shrink_page_list(struct list_head *page_list,</span>
 		int may_enter_fs;
 		enum page_references references = PAGEREF_RECLAIM_CLEAN;
 		bool dirty, writeback;
<span class="p_add">+		bool freeable = false;</span>
 
 		cond_resched();
 
<span class="p_chunk">@@ -1049,6 +1050,7 @@</span> <span class="p_context"> static unsigned long shrink_page_list(struct list_head *page_list,</span>
 				goto keep_locked;
 			if (!add_to_swap(page, page_list))
 				goto activate_locked;
<span class="p_add">+			freeable = true;</span>
 			may_enter_fs = 1;
 
 			/* Adding to swap updated mapping */
<span class="p_chunk">@@ -1060,8 +1062,9 @@</span> <span class="p_context"> static unsigned long shrink_page_list(struct list_head *page_list,</span>
 		 * processes. Try to unmap it here.
 		 */
 		if (page_mapped(page) &amp;&amp; mapping) {
<span class="p_del">-			switch (try_to_unmap(page,</span>
<span class="p_del">-					ttu_flags|TTU_BATCH_FLUSH)) {</span>
<span class="p_add">+			switch (try_to_unmap(page, freeable ?</span>
<span class="p_add">+				(ttu_flags | TTU_BATCH_FLUSH | TTU_FREE) :</span>
<span class="p_add">+				(ttu_flags | TTU_BATCH_FLUSH))) {</span>
 			case SWAP_FAIL:
 				goto activate_locked;
 			case SWAP_AGAIN:
<span class="p_chunk">@@ -1186,6 +1189,9 @@</span> <span class="p_context"> static unsigned long shrink_page_list(struct list_head *page_list,</span>
 		 */
 		__clear_page_locked(page);
 free_it:
<span class="p_add">+		if (freeable &amp;&amp; !PageDirty(page))</span>
<span class="p_add">+			count_vm_event(PGLAZYFREED);</span>
<span class="p_add">+</span>
 		nr_reclaimed++;
 
 		/*
<span class="p_header">diff --git a/mm/vmstat.c b/mm/vmstat.c</span>
<span class="p_header">index fbf14485a049..59d45b22355f 100644</span>
<span class="p_header">--- a/mm/vmstat.c</span>
<span class="p_header">+++ b/mm/vmstat.c</span>
<span class="p_chunk">@@ -759,6 +759,7 @@</span> <span class="p_context"> const char * const vmstat_text[] = {</span>
 
 	&quot;pgfault&quot;,
 	&quot;pgmajfault&quot;,
<span class="p_add">+	&quot;pglazyfreed&quot;,</span>
 
 	TEXTS_FOR_ZONES(&quot;pgrefill&quot;)
 	TEXTS_FOR_ZONES(&quot;pgsteal_kswapd&quot;)

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



