
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[HMM,07/16] mm/migrate: new memory migration helper for use with device memory v4 - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [HMM,07/16] mm/migrate: new memory migration helper for use with device memory v4</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=2554">Jerome Glisse</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>March 16, 2017, 4:05 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1489680335-6594-8-git-send-email-jglisse@redhat.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9628639/mbox/"
   >mbox</a>
|
   <a href="/patch/9628639/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9628639/">/patch/9628639/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	7A0516048C for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 16 Mar 2017 15:16:13 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 6AD9F203C0
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 16 Mar 2017 15:16:13 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 5F52E28543; Thu, 16 Mar 2017 15:16:13 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 0A96226E4F
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 16 Mar 2017 15:16:12 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1754631AbdCPPQG (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 16 Mar 2017 11:16:06 -0400
Received: from mx1.redhat.com ([209.132.183.28]:48968 &quot;EHLO mx1.redhat.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1752836AbdCPPQC (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 16 Mar 2017 11:16:02 -0400
Received: from smtp.corp.redhat.com
	(int-mx02.intmail.prod.int.phx2.redhat.com [10.5.11.12])
	(using TLSv1.2 with cipher AECDH-AES256-SHA (256/256 bits))
	(No client certificate requested)
	by mx1.redhat.com (Postfix) with ESMTPS id 5D8F6624B1;
	Thu, 16 Mar 2017 15:03:58 +0000 (UTC)
DMARC-Filter: OpenDMARC Filter v1.3.2 mx1.redhat.com 5D8F6624B1
Authentication-Results: ext-mx10.extmail.prod.ext.phx2.redhat.com;
	dmarc=none (p=none dis=none) header.from=redhat.com
Authentication-Results: ext-mx10.extmail.prod.ext.phx2.redhat.com;
	spf=pass smtp.mailfrom=jglisse@redhat.com
DKIM-Filter: OpenDKIM Filter v2.11.0 mx1.redhat.com 5D8F6624B1
Received: from xgl-cortex.ml2.eng.bos.redhat.com
	(xgl-cortex.ml2.eng.bos.redhat.com [10.19.160.80])
	by smtp.corp.redhat.com (Postfix) with ESMTP id 55EFC183A3;
	Thu, 16 Mar 2017 15:03:57 +0000 (UTC)
From: =?UTF-8?q?J=C3=A9r=C3=B4me=20Glisse?= &lt;jglisse@redhat.com&gt;
To: akpm@linux-foundation.org, &lt;linux-kernel@vger.kernel.org&gt;,
	linux-mm@kvack.org
Cc: John Hubbard &lt;jhubbard@nvidia.com&gt;,
	Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;,
	David Nellans &lt;dnellans@nvidia.com&gt;,
	=?UTF-8?q?J=C3=A9r=C3=B4me=20Glisse?= &lt;jglisse@redhat.com&gt;,
	Evgeny Baskakov &lt;ebaskakov@nvidia.com&gt;,
	Mark Hairgrove &lt;mhairgrove@nvidia.com&gt;,
	Sherry Cheung &lt;SCheung@nvidia.com&gt;, Subhash Gutti &lt;sgutti@nvidia.com&gt;
Subject: [HMM 07/16] mm/migrate: new memory migration helper for use with
	device memory v4
Date: Thu, 16 Mar 2017 12:05:26 -0400
Message-Id: &lt;1489680335-6594-8-git-send-email-jglisse@redhat.com&gt;
In-Reply-To: &lt;1489680335-6594-1-git-send-email-jglisse@redhat.com&gt;
References: &lt;1489680335-6594-1-git-send-email-jglisse@redhat.com&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit
X-Scanned-By: MIMEDefang 2.79 on 10.5.11.12
X-Greylist: Sender IP whitelisted, not delayed by milter-greylist-4.5.16
	(mx1.redhat.com [10.5.110.39]);
	Thu, 16 Mar 2017 15:03:58 +0000 (UTC)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2554">Jerome Glisse</a> - March 16, 2017, 4:05 p.m.</div>
<pre class="content">
This patch add a new memory migration helpers, which migrate memory
backing a range of virtual address of a process to different memory
(which can be allocated through special allocator). It differs from
numa migration by working on a range of virtual address and thus by
doing migration in chunk that can be large enough to use DMA engine
or special copy offloading engine.

Expected users are any one with heterogeneous memory where different
memory have different characteristics (latency, bandwidth, ...). As
an example IBM platform with CAPI bus can make use of this feature
to migrate between regular memory and CAPI device memory. New CPU
architecture with a pool of high performance memory not manage as
cache but presented as regular memory (while being faster and with
lower latency than DDR) will also be prime user of this patch.

Migration to private device memory will be usefull for device that
have large pool of such like GPU, NVidia plans to use HMM for that.

Changes since v3:
  - Rebase

Changes since v2:
  - droped HMM prefix and HMM specific code
Changes since v1:
  - typos fix
  - split early unmap optimization for page with single mapping
<span class="signed-off-by">
Signed-off-by: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Evgeny Baskakov &lt;ebaskakov@nvidia.com&gt;</span>
<span class="signed-off-by">Signed-off-by: John Hubbard &lt;jhubbard@nvidia.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Mark Hairgrove &lt;mhairgrove@nvidia.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Sherry Cheung &lt;SCheung@nvidia.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Subhash Gutti &lt;sgutti@nvidia.com&gt;</span>
---
 include/linux/migrate.h |  73 ++++++++
 mm/migrate.c            | 460 ++++++++++++++++++++++++++++++++++++++++++++++++
 2 files changed, 533 insertions(+)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=165331">Reza Arbab</a> - March 16, 2017, 4:24 p.m.</div>
<pre class="content">
On Thu, Mar 16, 2017 at 12:05:26PM -0400, Jérôme Glisse wrote:
<span class="quote">&gt;This patch add a new memory migration helpers, which migrate memory </span>
<span class="quote">&gt;backing a range of virtual address of a process to different memory </span>
<span class="quote">&gt;(which can be allocated through special allocator). It differs from </span>
<span class="quote">&gt;numa migration by working on a range of virtual address and thus by </span>
<span class="quote">&gt;doing migration in chunk that can be large enough to use DMA engine or </span>
<span class="quote">&gt;special copy offloading engine.</span>
<span class="reviewed-by">
Reviewed-by: Reza Arbab &lt;arbab@linux.vnet.ibm.com&gt;</span>
<span class="tested-by">Tested-by: Reza Arbab &lt;arbab@linux.vnet.ibm.com&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=4745">Balbir Singh</a> - March 16, 2017, 8:58 p.m.</div>
<pre class="content">
On Fri, Mar 17, 2017 at 3:24 AM, Reza Arbab &lt;arbab@linux.vnet.ibm.com&gt; wrote:
<span class="quote">&gt; On Thu, Mar 16, 2017 at 12:05:26PM -0400, Jérôme Glisse wrote:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; This patch add a new memory migration helpers, which migrate memory</span>
<span class="quote">&gt;&gt; backing a range of virtual address of a process to different memory (which</span>
<span class="quote">&gt;&gt; can be allocated through special allocator). It differs from numa migration</span>
<span class="quote">&gt;&gt; by working on a range of virtual address and thus by doing migration in</span>
<span class="quote">&gt;&gt; chunk that can be large enough to use DMA engine or special copy offloading</span>
<span class="quote">&gt;&gt; engine.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Reviewed-by: Reza Arbab &lt;arbab@linux.vnet.ibm.com&gt;</span>
<span class="quote">&gt; Tested-by: Reza Arbab &lt;arbab@linux.vnet.ibm.com&gt;</span>
<span class="quote">&gt;</span>
<span class="acked-by">

Acked-by: Balbir Singh &lt;bsingharora@gmail.com&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=41">Andrew Morton</a> - March 16, 2017, 11:05 p.m.</div>
<pre class="content">
On Thu, 16 Mar 2017 12:05:26 -0400 Jérôme Glisse &lt;jglisse@redhat.com&gt; wrote:
<span class="quote">
&gt; +static inline struct page *migrate_pfn_to_page(unsigned long mpfn)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	if (!(mpfn &amp; MIGRATE_PFN_VALID))</span>
<span class="quote">&gt; +		return NULL;</span>
<span class="quote">&gt; +	return pfn_to_page(mpfn &amp; MIGRATE_PFN_MASK);</span>
<span class="quote">&gt; +}</span>

i386 allnoconfig:

In file included from mm/page_alloc.c:61:
./include/linux/migrate.h: In function &#39;migrate_pfn_to_page&#39;:
./include/linux/migrate.h:139: warning: left shift count &gt;= width of type
./include/linux/migrate.h:141: warning: left shift count &gt;= width of type
./include/linux/migrate.h: In function &#39;migrate_pfn_size&#39;:
./include/linux/migrate.h:146: warning: left shift count &gt;= width of type
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=101231">John Hubbard</a> - March 17, 2017, 12:22 a.m.</div>
<pre class="content">
On 03/16/2017 04:05 PM, Andrew Morton wrote:
<span class="quote">&gt; On Thu, 16 Mar 2017 12:05:26 -0400 Jérôme Glisse &lt;jglisse@redhat.com&gt; wrote:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; +static inline struct page *migrate_pfn_to_page(unsigned long mpfn)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	if (!(mpfn &amp; MIGRATE_PFN_VALID))</span>
<span class="quote">&gt;&gt; +		return NULL;</span>
<span class="quote">&gt;&gt; +	return pfn_to_page(mpfn &amp; MIGRATE_PFN_MASK);</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; i386 allnoconfig:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; In file included from mm/page_alloc.c:61:</span>
<span class="quote">&gt; ./include/linux/migrate.h: In function &#39;migrate_pfn_to_page&#39;:</span>
<span class="quote">&gt; ./include/linux/migrate.h:139: warning: left shift count &gt;= width of type</span>
<span class="quote">&gt; ./include/linux/migrate.h:141: warning: left shift count &gt;= width of type</span>
<span class="quote">&gt; ./include/linux/migrate.h: In function &#39;migrate_pfn_size&#39;:</span>
<span class="quote">&gt; ./include/linux/migrate.h:146: warning: left shift count &gt;= width of type</span>
<span class="quote">&gt;</span>

It seems clear that this was never meant to work with &lt; 64-bit pfns:

// migrate.h excerpt:
#define MIGRATE_PFN_VALID	(1UL &lt;&lt; (BITS_PER_LONG_LONG - 1))
#define MIGRATE_PFN_MIGRATE	(1UL &lt;&lt; (BITS_PER_LONG_LONG - 2))
#define MIGRATE_PFN_HUGE	(1UL &lt;&lt; (BITS_PER_LONG_LONG - 3))
#define MIGRATE_PFN_LOCKED	(1UL &lt;&lt; (BITS_PER_LONG_LONG - 4))
#define MIGRATE_PFN_WRITE	(1UL &lt;&lt; (BITS_PER_LONG_LONG - 5))
#define MIGRATE_PFN_DEVICE	(1UL &lt;&lt; (BITS_PER_LONG_LONG - 6))
#define MIGRATE_PFN_ERROR	(1UL &lt;&lt; (BITS_PER_LONG_LONG - 7))
#define MIGRATE_PFN_MASK	((1UL &lt;&lt; (BITS_PER_LONG_LONG - PAGE_SHIFT)) - 1)

...obviously, there is not enough room for these flags, in a 32-bit pfn.

So, given the current HMM design, I think we are going to have to provide a 32-bit version of these 
routines (migrate_pfn_to_page, and related) that is a no-op, right?

thanks
John Hubbard
NVIDIA
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=4745">Balbir Singh</a> - March 17, 2017, 12:45 a.m.</div>
<pre class="content">
On Fri, Mar 17, 2017 at 11:22 AM, John Hubbard &lt;jhubbard@nvidia.com&gt; wrote:
<span class="quote">&gt; On 03/16/2017 04:05 PM, Andrew Morton wrote:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; On Thu, 16 Mar 2017 12:05:26 -0400 Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="quote">&gt;&gt; wrote:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; +static inline struct page *migrate_pfn_to_page(unsigned long mpfn)</span>
<span class="quote">&gt;&gt;&gt; +{</span>
<span class="quote">&gt;&gt;&gt; +       if (!(mpfn &amp; MIGRATE_PFN_VALID))</span>
<span class="quote">&gt;&gt;&gt; +               return NULL;</span>
<span class="quote">&gt;&gt;&gt; +       return pfn_to_page(mpfn &amp; MIGRATE_PFN_MASK);</span>
<span class="quote">&gt;&gt;&gt; +}</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; i386 allnoconfig:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; In file included from mm/page_alloc.c:61:</span>
<span class="quote">&gt;&gt; ./include/linux/migrate.h: In function &#39;migrate_pfn_to_page&#39;:</span>
<span class="quote">&gt;&gt; ./include/linux/migrate.h:139: warning: left shift count &gt;= width of type</span>
<span class="quote">&gt;&gt; ./include/linux/migrate.h:141: warning: left shift count &gt;= width of type</span>
<span class="quote">&gt;&gt; ./include/linux/migrate.h: In function &#39;migrate_pfn_size&#39;:</span>
<span class="quote">&gt;&gt; ./include/linux/migrate.h:146: warning: left shift count &gt;= width of type</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; It seems clear that this was never meant to work with &lt; 64-bit pfns:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; // migrate.h excerpt:</span>
<span class="quote">&gt; #define MIGRATE_PFN_VALID       (1UL &lt;&lt; (BITS_PER_LONG_LONG - 1))</span>
<span class="quote">&gt; #define MIGRATE_PFN_MIGRATE     (1UL &lt;&lt; (BITS_PER_LONG_LONG - 2))</span>
<span class="quote">&gt; #define MIGRATE_PFN_HUGE        (1UL &lt;&lt; (BITS_PER_LONG_LONG - 3))</span>
<span class="quote">&gt; #define MIGRATE_PFN_LOCKED      (1UL &lt;&lt; (BITS_PER_LONG_LONG - 4))</span>
<span class="quote">&gt; #define MIGRATE_PFN_WRITE       (1UL &lt;&lt; (BITS_PER_LONG_LONG - 5))</span>
<span class="quote">&gt; #define MIGRATE_PFN_DEVICE      (1UL &lt;&lt; (BITS_PER_LONG_LONG - 6))</span>
<span class="quote">&gt; #define MIGRATE_PFN_ERROR       (1UL &lt;&lt; (BITS_PER_LONG_LONG - 7))</span>
<span class="quote">&gt; #define MIGRATE_PFN_MASK        ((1UL &lt;&lt; (BITS_PER_LONG_LONG - PAGE_SHIFT))</span>
<span class="quote">&gt; - 1)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; ...obviously, there is not enough room for these flags, in a 32-bit pfn.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; So, given the current HMM design, I think we are going to have to provide a</span>
<span class="quote">&gt; 32-bit version of these routines (migrate_pfn_to_page, and related) that is</span>
<span class="quote">&gt; a no-op, right?</span>

Or make the HMM Kconfig feature 64BIT only by making it depend on 64BIT?


Balbir Singh
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=101231">John Hubbard</a> - March 17, 2017, 12:57 a.m.</div>
<pre class="content">
On 03/16/2017 05:45 PM, Balbir Singh wrote:
<span class="quote">&gt; On Fri, Mar 17, 2017 at 11:22 AM, John Hubbard &lt;jhubbard@nvidia.com&gt; wrote:</span>
<span class="quote">&gt;&gt; On 03/16/2017 04:05 PM, Andrew Morton wrote:</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; On Thu, 16 Mar 2017 12:05:26 -0400 Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="quote">&gt;&gt;&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; +static inline struct page *migrate_pfn_to_page(unsigned long mpfn)</span>
<span class="quote">&gt;&gt;&gt;&gt; +{</span>
<span class="quote">&gt;&gt;&gt;&gt; +       if (!(mpfn &amp; MIGRATE_PFN_VALID))</span>
<span class="quote">&gt;&gt;&gt;&gt; +               return NULL;</span>
<span class="quote">&gt;&gt;&gt;&gt; +       return pfn_to_page(mpfn &amp; MIGRATE_PFN_MASK);</span>
<span class="quote">&gt;&gt;&gt;&gt; +}</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; i386 allnoconfig:</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; In file included from mm/page_alloc.c:61:</span>
<span class="quote">&gt;&gt;&gt; ./include/linux/migrate.h: In function &#39;migrate_pfn_to_page&#39;:</span>
<span class="quote">&gt;&gt;&gt; ./include/linux/migrate.h:139: warning: left shift count &gt;= width of type</span>
<span class="quote">&gt;&gt;&gt; ./include/linux/migrate.h:141: warning: left shift count &gt;= width of type</span>
<span class="quote">&gt;&gt;&gt; ./include/linux/migrate.h: In function &#39;migrate_pfn_size&#39;:</span>
<span class="quote">&gt;&gt;&gt; ./include/linux/migrate.h:146: warning: left shift count &gt;= width of type</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; It seems clear that this was never meant to work with &lt; 64-bit pfns:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; // migrate.h excerpt:</span>
<span class="quote">&gt;&gt; #define MIGRATE_PFN_VALID       (1UL &lt;&lt; (BITS_PER_LONG_LONG - 1))</span>
<span class="quote">&gt;&gt; #define MIGRATE_PFN_MIGRATE     (1UL &lt;&lt; (BITS_PER_LONG_LONG - 2))</span>
<span class="quote">&gt;&gt; #define MIGRATE_PFN_HUGE        (1UL &lt;&lt; (BITS_PER_LONG_LONG - 3))</span>
<span class="quote">&gt;&gt; #define MIGRATE_PFN_LOCKED      (1UL &lt;&lt; (BITS_PER_LONG_LONG - 4))</span>
<span class="quote">&gt;&gt; #define MIGRATE_PFN_WRITE       (1UL &lt;&lt; (BITS_PER_LONG_LONG - 5))</span>
<span class="quote">&gt;&gt; #define MIGRATE_PFN_DEVICE      (1UL &lt;&lt; (BITS_PER_LONG_LONG - 6))</span>
<span class="quote">&gt;&gt; #define MIGRATE_PFN_ERROR       (1UL &lt;&lt; (BITS_PER_LONG_LONG - 7))</span>
<span class="quote">&gt;&gt; #define MIGRATE_PFN_MASK        ((1UL &lt;&lt; (BITS_PER_LONG_LONG - PAGE_SHIFT))</span>
<span class="quote">&gt;&gt; - 1)</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; ...obviously, there is not enough room for these flags, in a 32-bit pfn.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; So, given the current HMM design, I think we are going to have to provide a</span>
<span class="quote">&gt;&gt; 32-bit version of these routines (migrate_pfn_to_page, and related) that is</span>
<span class="quote">&gt;&gt; a no-op, right?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Or make the HMM Kconfig feature 64BIT only by making it depend on 64BIT?</span>
<span class="quote">&gt;</span>

Yes, that was my first reaction too, but these particular routines are aspiring to be generic 
routines--in fact, you have had an influence there, because these might possibly help with NUMA 
migrations. :)

So it would look odd to see this:

#ifdef CONFIG_HMM
int migrate_vma(const struct migrate_vma_ops *ops,
		struct vm_area_struct *vma,
		unsigned long mentries,
		unsigned long start,
		unsigned long end,
		unsigned long *src,
		unsigned long *dst,
		void *private)
{
    //...implementation
#endif

...because migrate_vma() does not sound HMM-specific, and it is, after all, in migrate.h and 
migrate.c. We probably want this a more generic approach (not sure if I&#39;ve picked exactly the right 
token to #ifdef on, but it&#39;s close):

#ifdef CONFIG_64BIT
int migrate_vma(const struct migrate_vma_ops *ops,
		struct vm_area_struct *vma,
		unsigned long mentries,
		unsigned long start,
		unsigned long end,
		unsigned long *src,
		unsigned long *dst,
		void *private)
{
    /* ... full implementation */
}

#else
int migrate_vma(const struct migrate_vma_ops *ops,
		struct vm_area_struct *vma,
		unsigned long mentries,
		unsigned long start,
		unsigned long end,
		unsigned long *src,
		unsigned long *dst,
		void *private)
{
    return -EINVAL; /* or something more appropriate */
}
#endif

thanks
John Hubbard
NVIDIA
<span class="quote">
&gt;</span>
<span class="quote">&gt; Balbir Singh</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2554">Jerome Glisse</a> - March 17, 2017, 1:52 a.m.</div>
<pre class="content">
<span class="quote">&gt; On 03/16/2017 05:45 PM, Balbir Singh wrote:</span>
<span class="quote">&gt; &gt; On Fri, Mar 17, 2017 at 11:22 AM, John Hubbard &lt;jhubbard@nvidia.com&gt; wrote:</span>
<span class="quote">&gt; &gt;&gt; On 03/16/2017 04:05 PM, Andrew Morton wrote:</span>
<span class="quote">&gt; &gt;&gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt; On Thu, 16 Mar 2017 12:05:26 -0400 Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt; wrote:</span>
<span class="quote">&gt; &gt;&gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +static inline struct page *migrate_pfn_to_page(unsigned long mpfn)</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +{</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +       if (!(mpfn &amp; MIGRATE_PFN_VALID))</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +               return NULL;</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +       return pfn_to_page(mpfn &amp; MIGRATE_PFN_MASK);</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +}</span>
<span class="quote">&gt; &gt;&gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt; i386 allnoconfig:</span>
<span class="quote">&gt; &gt;&gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt; In file included from mm/page_alloc.c:61:</span>
<span class="quote">&gt; &gt;&gt;&gt; ./include/linux/migrate.h: In function &#39;migrate_pfn_to_page&#39;:</span>
<span class="quote">&gt; &gt;&gt;&gt; ./include/linux/migrate.h:139: warning: left shift count &gt;= width of type</span>
<span class="quote">&gt; &gt;&gt;&gt; ./include/linux/migrate.h:141: warning: left shift count &gt;= width of type</span>
<span class="quote">&gt; &gt;&gt;&gt; ./include/linux/migrate.h: In function &#39;migrate_pfn_size&#39;:</span>
<span class="quote">&gt; &gt;&gt;&gt; ./include/linux/migrate.h:146: warning: left shift count &gt;= width of type</span>
<span class="quote">&gt; &gt;&gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; It seems clear that this was never meant to work with &lt; 64-bit pfns:</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; // migrate.h excerpt:</span>
<span class="quote">&gt; &gt;&gt; #define MIGRATE_PFN_VALID       (1UL &lt;&lt; (BITS_PER_LONG_LONG - 1))</span>
<span class="quote">&gt; &gt;&gt; #define MIGRATE_PFN_MIGRATE     (1UL &lt;&lt; (BITS_PER_LONG_LONG - 2))</span>
<span class="quote">&gt; &gt;&gt; #define MIGRATE_PFN_HUGE        (1UL &lt;&lt; (BITS_PER_LONG_LONG - 3))</span>
<span class="quote">&gt; &gt;&gt; #define MIGRATE_PFN_LOCKED      (1UL &lt;&lt; (BITS_PER_LONG_LONG - 4))</span>
<span class="quote">&gt; &gt;&gt; #define MIGRATE_PFN_WRITE       (1UL &lt;&lt; (BITS_PER_LONG_LONG - 5))</span>
<span class="quote">&gt; &gt;&gt; #define MIGRATE_PFN_DEVICE      (1UL &lt;&lt; (BITS_PER_LONG_LONG - 6))</span>
<span class="quote">&gt; &gt;&gt; #define MIGRATE_PFN_ERROR       (1UL &lt;&lt; (BITS_PER_LONG_LONG - 7))</span>
<span class="quote">&gt; &gt;&gt; #define MIGRATE_PFN_MASK        ((1UL &lt;&lt; (BITS_PER_LONG_LONG -</span>
<span class="quote">&gt; &gt;&gt; PAGE_SHIFT))</span>
<span class="quote">&gt; &gt;&gt; - 1)</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; ...obviously, there is not enough room for these flags, in a 32-bit pfn.</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; So, given the current HMM design, I think we are going to have to provide</span>
<span class="quote">&gt; &gt;&gt; a</span>
<span class="quote">&gt; &gt;&gt; 32-bit version of these routines (migrate_pfn_to_page, and related) that</span>
<span class="quote">&gt; &gt;&gt; is</span>
<span class="quote">&gt; &gt;&gt; a no-op, right?</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Or make the HMM Kconfig feature 64BIT only by making it depend on 64BIT?</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Yes, that was my first reaction too, but these particular routines are</span>
<span class="quote">&gt; aspiring to be generic</span>
<span class="quote">&gt; routines--in fact, you have had an influence there, because these might</span>
<span class="quote">&gt; possibly help with NUMA</span>
<span class="quote">&gt; migrations. :)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So it would look odd to see this:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; #ifdef CONFIG_HMM</span>
<span class="quote">&gt; int migrate_vma(const struct migrate_vma_ops *ops,</span>
<span class="quote">&gt; 		struct vm_area_struct *vma,</span>
<span class="quote">&gt; 		unsigned long mentries,</span>
<span class="quote">&gt; 		unsigned long start,</span>
<span class="quote">&gt; 		unsigned long end,</span>
<span class="quote">&gt; 		unsigned long *src,</span>
<span class="quote">&gt; 		unsigned long *dst,</span>
<span class="quote">&gt; 		void *private)</span>
<span class="quote">&gt; {</span>
<span class="quote">&gt;     //...implementation</span>
<span class="quote">&gt; #endif</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; ...because migrate_vma() does not sound HMM-specific, and it is, after all,</span>
<span class="quote">&gt; in migrate.h and</span>
<span class="quote">&gt; migrate.c. We probably want this a more generic approach (not sure if I&#39;ve</span>
<span class="quote">&gt; picked exactly the right</span>
<span class="quote">&gt; token to #ifdef on, but it&#39;s close):</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; #ifdef CONFIG_64BIT</span>
<span class="quote">&gt; int migrate_vma(const struct migrate_vma_ops *ops,</span>
<span class="quote">&gt; 		struct vm_area_struct *vma,</span>
<span class="quote">&gt; 		unsigned long mentries,</span>
<span class="quote">&gt; 		unsigned long start,</span>
<span class="quote">&gt; 		unsigned long end,</span>
<span class="quote">&gt; 		unsigned long *src,</span>
<span class="quote">&gt; 		unsigned long *dst,</span>
<span class="quote">&gt; 		void *private)</span>
<span class="quote">&gt; {</span>
<span class="quote">&gt;     /* ... full implementation */</span>
<span class="quote">&gt; }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; #else</span>
<span class="quote">&gt; int migrate_vma(const struct migrate_vma_ops *ops,</span>
<span class="quote">&gt; 		struct vm_area_struct *vma,</span>
<span class="quote">&gt; 		unsigned long mentries,</span>
<span class="quote">&gt; 		unsigned long start,</span>
<span class="quote">&gt; 		unsigned long end,</span>
<span class="quote">&gt; 		unsigned long *src,</span>
<span class="quote">&gt; 		unsigned long *dst,</span>
<span class="quote">&gt; 		void *private)</span>
<span class="quote">&gt; {</span>
<span class="quote">&gt;     return -EINVAL; /* or something more appropriate */</span>
<span class="quote">&gt; }</span>
<span class="quote">&gt; #endif</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; thanks</span>
<span class="quote">&gt; John Hubbard</span>
<span class="quote">&gt; NVIDIA</span>

The original intention was for it to be 64bit only, 32bit is a dying
species and before splitting out hmm_ prefix from this code and moving
it to be generic it was behind a 64bit flag.

If latter one someone really care about 32bit we can only move to u64

Cheers,
Jérôme
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=41">Andrew Morton</a> - March 17, 2017, 3:32 a.m.</div>
<pre class="content">
On Thu, 16 Mar 2017 21:52:23 -0400 (EDT) Jerome Glisse &lt;jglisse@redhat.com&gt; wrote:
<span class="quote">
&gt; The original intention was for it to be 64bit only, 32bit is a dying</span>
<span class="quote">&gt; species and before splitting out hmm_ prefix from this code and moving</span>
<span class="quote">&gt; it to be generic it was behind a 64bit flag.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; If latter one someone really care about 32bit we can only move to u64</span>

I think that&#39;s the best compromise.  If someone wants this on 32-bit
then they&#39;re free to get it working.  That &quot;someone&quot; will actually be
able to test it, which you clearly won&#39;t be doing!

However, please do check that the impact of this patchset on 32-bit&#39;s
`size vmlinux&#39; is minimal.  Preferably zero.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=4745">Balbir Singh</a> - March 17, 2017, 3:42 a.m.</div>
<pre class="content">
<span class="quote">&gt;&gt; Or make the HMM Kconfig feature 64BIT only by making it depend on 64BIT?</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Yes, that was my first reaction too, but these particular routines are</span>
<span class="quote">&gt; aspiring to be generic routines--in fact, you have had an influence there,</span>
<span class="quote">&gt; because these might possibly help with NUMA migrations. :)</span>
<span class="quote">&gt;</span>

Yes, I still stick to them being generic, but I&#39;d be OK if they worked
just for 64 bit systems.
Having said that even the 64 bit works version work for upto physical
sizes of 64 - PAGE_SHIFT
which is a little limiting I think.

One option is to make pfn&#39;s unsigned long long and do 32 and 64 bit computations
separately

Option 2, could be something like you said

a. Define a __weak migrate_vma to return -EINVAL
b. In a 64BIT only file define migrate_vma

Option 3

Something totally different

If we care to support 32 bit we go with 1, else option 2 is a good
starting point. There might
be other ways of doing option 2, like you&#39;ve suggested

Balbir
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/include/linux/migrate.h b/include/linux/migrate.h</span>
<span class="p_header">index 0a66ddd..6c610ee 100644</span>
<span class="p_header">--- a/include/linux/migrate.h</span>
<span class="p_header">+++ b/include/linux/migrate.h</span>
<span class="p_chunk">@@ -124,4 +124,77 @@</span> <span class="p_context"> static inline int migrate_misplaced_transhuge_page(struct mm_struct *mm,</span>
 }
 #endif /* CONFIG_NUMA_BALANCING &amp;&amp; CONFIG_TRANSPARENT_HUGEPAGE*/
 
<span class="p_add">+</span>
<span class="p_add">+#define MIGRATE_PFN_VALID	(1UL &lt;&lt; (BITS_PER_LONG_LONG - 1))</span>
<span class="p_add">+#define MIGRATE_PFN_MIGRATE	(1UL &lt;&lt; (BITS_PER_LONG_LONG - 2))</span>
<span class="p_add">+#define MIGRATE_PFN_HUGE	(1UL &lt;&lt; (BITS_PER_LONG_LONG - 3))</span>
<span class="p_add">+#define MIGRATE_PFN_LOCKED	(1UL &lt;&lt; (BITS_PER_LONG_LONG - 4))</span>
<span class="p_add">+#define MIGRATE_PFN_WRITE	(1UL &lt;&lt; (BITS_PER_LONG_LONG - 5))</span>
<span class="p_add">+#define MIGRATE_PFN_MASK	((1UL &lt;&lt; (BITS_PER_LONG_LONG - PAGE_SHIFT)) - 1)</span>
<span class="p_add">+</span>
<span class="p_add">+static inline struct page *migrate_pfn_to_page(unsigned long mpfn)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!(mpfn &amp; MIGRATE_PFN_VALID))</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	return pfn_to_page(mpfn &amp; MIGRATE_PFN_MASK);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline unsigned long migrate_pfn_size(unsigned long mpfn)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return mpfn &amp; MIGRATE_PFN_HUGE ? PMD_SIZE : PAGE_SIZE;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * struct migrate_vma_ops - migrate operation callback</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * @alloc_and_copy: alloc destination memoiry and copy source to it</span>
<span class="p_add">+ * @finalize_and_map: allow caller to inspect successfull migrated page</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * migrate_vma() allow memory migration to use DMA  engine to perform copy from</span>
<span class="p_add">+ * source to destination memory it also allow caller to use its own memory</span>
<span class="p_add">+ * allocator for destination memory.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Note that in alloc_and_copy device driver can decide not to migrate some of</span>
<span class="p_add">+ * the entry by simply setting corresponding dst entry 0.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Destination page must locked and MIGRATE_PFN_LOCKED set in the corresponding</span>
<span class="p_add">+ * entry of dstarray. It is expected that page allocated will have an elevated</span>
<span class="p_add">+ * refcount and that a put_page() will free the page.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Device driver might want to allocate with an extra-refcount if they want to</span>
<span class="p_add">+ * control deallocation of failed migration inside finalize_and_map() callback.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * The finalize_and_map() callback must use the MIGRATE_PFN_MIGRATE flag to</span>
<span class="p_add">+ * determine which page have been successfully migrated (it is set in the src</span>
<span class="p_add">+ * array for each entry that have been successfully migrated).</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * For migration from device memory to system memory device driver must set any</span>
<span class="p_add">+ * dst entry to MIGRATE_PFN_ERROR for any entry it can not migrate back due to</span>
<span class="p_add">+ * hardware fatal failure that can not be recovered. Such failure will trigger</span>
<span class="p_add">+ * a SIGBUS for the process trying to access such memory.</span>
<span class="p_add">+ */</span>
<span class="p_add">+struct migrate_vma_ops {</span>
<span class="p_add">+	void (*alloc_and_copy)(struct vm_area_struct *vma,</span>
<span class="p_add">+			       const unsigned long *src,</span>
<span class="p_add">+			       unsigned long *dst,</span>
<span class="p_add">+			       unsigned long start,</span>
<span class="p_add">+			       unsigned long end,</span>
<span class="p_add">+			       void *private);</span>
<span class="p_add">+	void (*finalize_and_map)(struct vm_area_struct *vma,</span>
<span class="p_add">+				 const unsigned long *src,</span>
<span class="p_add">+				 const unsigned long *dst,</span>
<span class="p_add">+				 unsigned long start,</span>
<span class="p_add">+				 unsigned long end,</span>
<span class="p_add">+				 void *private);</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+int migrate_vma(const struct migrate_vma_ops *ops,</span>
<span class="p_add">+		struct vm_area_struct *vma,</span>
<span class="p_add">+		unsigned long mentries,</span>
<span class="p_add">+		unsigned long start,</span>
<span class="p_add">+		unsigned long end,</span>
<span class="p_add">+		unsigned long *src,</span>
<span class="p_add">+		unsigned long *dst,</span>
<span class="p_add">+		void *private);</span>
<span class="p_add">+</span>
 #endif /* _LINUX_MIGRATE_H */
<span class="p_header">diff --git a/mm/migrate.c b/mm/migrate.c</span>
<span class="p_header">index cb911ce..e37d796 100644</span>
<span class="p_header">--- a/mm/migrate.c</span>
<span class="p_header">+++ b/mm/migrate.c</span>
<span class="p_chunk">@@ -393,6 +393,14 @@</span> <span class="p_context"> int migrate_page_move_mapping(struct address_space *mapping,</span>
 	int expected_count = 1 + extra_count;
 	void **pslot;
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * ZONE_DEVICE pages have 1 refcount always held by their device</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * Note that DAX memory will never reach that point as it does not have</span>
<span class="p_add">+	 * the MEMORY_DEVICE_ALLOW_MIGRATE flag set (see memory_hotplug.h).</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	expected_count += is_zone_device_page(page);</span>
<span class="p_add">+</span>
 	if (!mapping) {
 		/* Anonymous page without mapping */
 		if (page_count(page) != expected_count)
<span class="p_chunk">@@ -2061,3 +2069,455 @@</span> <span class="p_context"> int migrate_misplaced_transhuge_page(struct mm_struct *mm,</span>
 #endif /* CONFIG_NUMA_BALANCING */
 
 #endif /* CONFIG_NUMA */
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+struct migrate_vma {</span>
<span class="p_add">+	struct vm_area_struct	*vma;</span>
<span class="p_add">+	unsigned long		*dst;</span>
<span class="p_add">+	unsigned long		*src;</span>
<span class="p_add">+	unsigned long		cpages;</span>
<span class="p_add">+	unsigned long		npages;</span>
<span class="p_add">+	unsigned long		mpages;</span>
<span class="p_add">+	unsigned long		start;</span>
<span class="p_add">+	unsigned long		end;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int migrate_vma_array_full(struct migrate_vma *migrate)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return migrate-&gt;npages &gt;= migrate-&gt;mpages ? -ENOSPC : 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int migrate_vma_collect_hole(unsigned long start,</span>
<span class="p_add">+				    unsigned long end,</span>
<span class="p_add">+				    struct mm_walk *walk)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct migrate_vma *migrate = walk-&gt;private;</span>
<span class="p_add">+	unsigned long addr, next;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (addr = start &amp; PAGE_MASK; addr &lt; end; addr = next) {</span>
<span class="p_add">+		unsigned long npages, i;</span>
<span class="p_add">+		int ret;</span>
<span class="p_add">+</span>
<span class="p_add">+		next = pmd_addr_end(addr, end);</span>
<span class="p_add">+		npages = (next - addr) &gt;&gt; PAGE_SHIFT;</span>
<span class="p_add">+		if (npages == (PMD_SIZE &gt;&gt; PAGE_SHIFT)) {</span>
<span class="p_add">+			migrate-&gt;src[migrate-&gt;npages++] = MIGRATE_PFN_HUGE;</span>
<span class="p_add">+			ret = migrate_vma_array_full(migrate);</span>
<span class="p_add">+			if (ret)</span>
<span class="p_add">+				return ret;</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			for (i = 0; i &lt; npages; ++i) {</span>
<span class="p_add">+				migrate-&gt;src[migrate-&gt;npages++] = 0;</span>
<span class="p_add">+				ret = migrate_vma_array_full(migrate);</span>
<span class="p_add">+				if (ret)</span>
<span class="p_add">+					return ret;</span>
<span class="p_add">+			}</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int migrate_vma_collect_pmd(pmd_t *pmdp,</span>
<span class="p_add">+				   unsigned long start,</span>
<span class="p_add">+				   unsigned long end,</span>
<span class="p_add">+				   struct mm_walk *walk)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct migrate_vma *migrate = walk-&gt;private;</span>
<span class="p_add">+	struct mm_struct *mm = walk-&gt;vma-&gt;vm_mm;</span>
<span class="p_add">+	unsigned long addr = start;</span>
<span class="p_add">+	spinlock_t *ptl;</span>
<span class="p_add">+	pte_t *ptep;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (pmd_none(*pmdp) || pmd_trans_unstable(pmdp)) {</span>
<span class="p_add">+		/* FIXME support THP */</span>
<span class="p_add">+		return migrate_vma_collect_hole(start, end, walk);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	ptep = pte_offset_map_lock(mm, pmdp, addr, &amp;ptl);</span>
<span class="p_add">+	for (; addr &lt; end; addr += PAGE_SIZE, ptep++) {</span>
<span class="p_add">+		unsigned long flags, pfn;</span>
<span class="p_add">+		struct page *page;</span>
<span class="p_add">+		pte_t pte;</span>
<span class="p_add">+		int ret;</span>
<span class="p_add">+</span>
<span class="p_add">+		pte = *ptep;</span>
<span class="p_add">+		pfn = pte_pfn(pte);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!pte_present(pte)) {</span>
<span class="p_add">+			flags = pfn = 0;</span>
<span class="p_add">+			goto next;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		/* FIXME support THP */</span>
<span class="p_add">+		page = vm_normal_page(migrate-&gt;vma, addr, pte);</span>
<span class="p_add">+		if (!page || !page-&gt;mapping || PageTransCompound(page)) {</span>
<span class="p_add">+			flags = pfn = 0;</span>
<span class="p_add">+			goto next;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * By getting a reference on the page we pin it and that blocks</span>
<span class="p_add">+		 * any kind of migration. Side effect is that it &quot;freezes&quot; the</span>
<span class="p_add">+		 * pte.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * We drop this reference after isolating the page from the lru</span>
<span class="p_add">+		 * for non device page (device page are not on the lru and thus</span>
<span class="p_add">+		 * can&#39;t be dropped from it).</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		get_page(page);</span>
<span class="p_add">+		migrate-&gt;cpages++;</span>
<span class="p_add">+		flags = MIGRATE_PFN_VALID | MIGRATE_PFN_MIGRATE;</span>
<span class="p_add">+		flags |= pte_write(pte) ? MIGRATE_PFN_WRITE : 0;</span>
<span class="p_add">+</span>
<span class="p_add">+next:</span>
<span class="p_add">+		migrate-&gt;src[migrate-&gt;npages++] = pfn | flags;</span>
<span class="p_add">+		ret = migrate_vma_array_full(migrate);</span>
<span class="p_add">+		if (ret) {</span>
<span class="p_add">+			pte_unmap_unlock(ptep, ptl);</span>
<span class="p_add">+			return ret;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+	pte_unmap_unlock(ptep - 1, ptl);</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * migrate_vma_collect() - collect page over range of virtual addresses</span>
<span class="p_add">+ * @migrate: migrate struct containing all migration information</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This will walk the CPU page table. For each virtual address backed by a</span>
<span class="p_add">+ * valid page, it updates the src array and takes a reference on the page, in</span>
<span class="p_add">+ * order to pin the page until we lock it and unmap it.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static void migrate_vma_collect(struct migrate_vma *migrate)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct mm_walk mm_walk;</span>
<span class="p_add">+</span>
<span class="p_add">+	mm_walk.pmd_entry = migrate_vma_collect_pmd;</span>
<span class="p_add">+	mm_walk.pte_entry = NULL;</span>
<span class="p_add">+	mm_walk.pte_hole = migrate_vma_collect_hole;</span>
<span class="p_add">+	mm_walk.hugetlb_entry = NULL;</span>
<span class="p_add">+	mm_walk.test_walk = NULL;</span>
<span class="p_add">+	mm_walk.vma = migrate-&gt;vma;</span>
<span class="p_add">+	mm_walk.mm = migrate-&gt;vma-&gt;vm_mm;</span>
<span class="p_add">+	mm_walk.private = migrate;</span>
<span class="p_add">+</span>
<span class="p_add">+	walk_page_range(migrate-&gt;start, migrate-&gt;end, &amp;mm_walk);</span>
<span class="p_add">+</span>
<span class="p_add">+	migrate-&gt;end = migrate-&gt;start + (migrate-&gt;npages &lt;&lt; PAGE_SHIFT);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * migrate_vma_check_page() - check if page is pinned or not</span>
<span class="p_add">+ * @page: struct page to check</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Pinned pages cannot be migrated. This is the same test as in</span>
<span class="p_add">+ * migrate_page_move_mapping(), except that here we allow migration of a</span>
<span class="p_add">+ * ZONE_DEVICE page.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static bool migrate_vma_check_page(struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * One extra ref because caller holds an extra reference, either from</span>
<span class="p_add">+	 * isolate_lru_page() for a regular page, or migrate_vma_collect() for</span>
<span class="p_add">+	 * a device page.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	int extra = 1;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * FIXME support THP (transparent huge page), it is bit more complex to</span>
<span class="p_add">+	 * check them than regular pages, because they can be mapped with a pmd</span>
<span class="p_add">+	 * or with a pte (split pte mapping).</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (PageCompound(page))</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+</span>
<span class="p_add">+	if ((page_count(page) - extra) &gt; page_mapcount(page))</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+</span>
<span class="p_add">+	return true;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * migrate_vma_prepare() - lock pages and isolate them from the lru</span>
<span class="p_add">+ * @migrate: migrate struct containing all migration informations</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This locks pages that have been collected by migrate_vma_collect(). Once each</span>
<span class="p_add">+ * page is locked it is isolated from the lru (for non-device pages). Finally,</span>
<span class="p_add">+ * the ref taken by migrate_vma_collect() is dropped, as locked pages cannot be</span>
<span class="p_add">+ * migrated by concurrent kernel threads.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static void migrate_vma_prepare(struct migrate_vma *migrate)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long addr = migrate-&gt;start, i, size;</span>
<span class="p_add">+	const unsigned long npages = migrate-&gt;npages;</span>
<span class="p_add">+	bool allow_drain = true;</span>
<span class="p_add">+</span>
<span class="p_add">+	lru_add_drain();</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i &lt; npages &amp;&amp; migrate-&gt;cpages; i++, addr += size) {</span>
<span class="p_add">+		struct page *page = migrate_pfn_to_page(migrate-&gt;src[i]);</span>
<span class="p_add">+		size = migrate_pfn_size(migrate-&gt;src[i]);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!page)</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		lock_page(page);</span>
<span class="p_add">+		migrate-&gt;src[i] |= MIGRATE_PFN_LOCKED;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!PageLRU(page) &amp;&amp; allow_drain) {</span>
<span class="p_add">+			/* Drain CPU&#39;s pagevec */</span>
<span class="p_add">+			lru_add_drain_all();</span>
<span class="p_add">+			allow_drain = false;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		if (isolate_lru_page(page)) {</span>
<span class="p_add">+			migrate-&gt;src[i] = 0;</span>
<span class="p_add">+			unlock_page(page);</span>
<span class="p_add">+			migrate-&gt;cpages--;</span>
<span class="p_add">+			put_page(page);</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		/* Drop the reference we took in collect */</span>
<span class="p_add">+		put_page(page);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!migrate_vma_check_page(page)) {</span>
<span class="p_add">+			migrate-&gt;src[i] = 0;</span>
<span class="p_add">+			unlock_page(page);</span>
<span class="p_add">+			migrate-&gt;cpages--;</span>
<span class="p_add">+</span>
<span class="p_add">+			putback_lru_page(page);</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * migrate_vma_unmap() - replace page mapping with special migration pte entry</span>
<span class="p_add">+ * @migrate: migrate struct containing all migration information</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Replace page mapping (CPU page table pte) with special migration pte entry</span>
<span class="p_add">+ * and check again if it has been pinned. Pinned pages are restored because we</span>
<span class="p_add">+ * cannot migrate them.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This is the last step before we call the device driver callback to allocate</span>
<span class="p_add">+ * destination memory and copy contents of original page over to new page.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static void migrate_vma_unmap(struct migrate_vma *migrate)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int flags = TTU_MIGRATION | TTU_IGNORE_MLOCK | TTU_IGNORE_ACCESS;</span>
<span class="p_add">+	unsigned long addr = migrate-&gt;start, i, restore = 0, size;</span>
<span class="p_add">+	const unsigned long npages = migrate-&gt;npages;</span>
<span class="p_add">+	const unsigned long start = migrate-&gt;start;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i &lt; npages &amp;&amp; migrate-&gt;cpages; addr += size, i++) {</span>
<span class="p_add">+		struct page *page = migrate_pfn_to_page(migrate-&gt;src[i]);</span>
<span class="p_add">+		size = migrate_pfn_size(migrate-&gt;src[i]);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!page || !(migrate-&gt;src[i] &amp; MIGRATE_PFN_MIGRATE))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		try_to_unmap(page, flags);</span>
<span class="p_add">+		if (page_mapped(page) || !migrate_vma_check_page(page)) {</span>
<span class="p_add">+			migrate-&gt;src[i] &amp;= ~MIGRATE_PFN_MIGRATE;</span>
<span class="p_add">+			migrate-&gt;cpages--;</span>
<span class="p_add">+			restore++;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	for (addr = start, i = 0; i &lt; npages &amp;&amp; restore; addr += size, i++) {</span>
<span class="p_add">+		struct page *page = migrate_pfn_to_page(migrate-&gt;src[i]);</span>
<span class="p_add">+		size = migrate_pfn_size(migrate-&gt;src[i]);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!page || (migrate-&gt;src[i] &amp; MIGRATE_PFN_MIGRATE))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		remove_migration_ptes(page, page, false);</span>
<span class="p_add">+</span>
<span class="p_add">+		migrate-&gt;src[i] = 0;</span>
<span class="p_add">+		unlock_page(page);</span>
<span class="p_add">+		restore--;</span>
<span class="p_add">+</span>
<span class="p_add">+		putback_lru_page(page);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * migrate_vma_pages() - migrate meta-data from src page to dst page</span>
<span class="p_add">+ * @migrate: migrate struct containing all migration information</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This migrate struct page meta-data from source struct page to destination</span>
<span class="p_add">+ * struct page. This effectively finishes the migration from source page to the</span>
<span class="p_add">+ * destination page.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static void migrate_vma_pages(struct migrate_vma *migrate)</span>
<span class="p_add">+{</span>
<span class="p_add">+	const unsigned long npages = migrate-&gt;npages;</span>
<span class="p_add">+	unsigned long addr, i, size;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0, addr = migrate-&gt;start; i &lt; npages; addr += size, i++) {</span>
<span class="p_add">+		struct page *newpage = migrate_pfn_to_page(migrate-&gt;dst[i]);</span>
<span class="p_add">+		struct page *page = migrate_pfn_to_page(migrate-&gt;src[i]);</span>
<span class="p_add">+		struct address_space *mapping;</span>
<span class="p_add">+		int r;</span>
<span class="p_add">+</span>
<span class="p_add">+		size = migrate_pfn_size(migrate-&gt;src[i]);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!page || !newpage)</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		if (!(migrate-&gt;src[i] &amp; MIGRATE_PFN_MIGRATE))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		mapping = page_mapping(page);</span>
<span class="p_add">+</span>
<span class="p_add">+		r = migrate_page(mapping, newpage, page, MIGRATE_SYNC, false);</span>
<span class="p_add">+		if (r != MIGRATEPAGE_SUCCESS)</span>
<span class="p_add">+			migrate-&gt;src[i] &amp;= ~MIGRATE_PFN_MIGRATE;</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * migrate_vma_finalize() - restore CPU page table entry</span>
<span class="p_add">+ * @migrate: migrate struct containing all migration information</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This replaces the special migration pte entry with either a mapping to the</span>
<span class="p_add">+ * new page if migration was successful for that page, or to the original page</span>
<span class="p_add">+ * otherwise.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This also unlocks the pages and puts them back on the lru, or drops the extra</span>
<span class="p_add">+ * refcount, for device pages.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static void migrate_vma_finalize(struct migrate_vma *migrate)</span>
<span class="p_add">+{</span>
<span class="p_add">+	const unsigned long npages = migrate-&gt;npages;</span>
<span class="p_add">+	unsigned long addr, i, size;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0, addr = migrate-&gt;start; i &lt; npages; addr += size, i++) {</span>
<span class="p_add">+		struct page *newpage = migrate_pfn_to_page(migrate-&gt;dst[i]);</span>
<span class="p_add">+		struct page *page = migrate_pfn_to_page(migrate-&gt;src[i]);</span>
<span class="p_add">+		size = migrate_pfn_size(migrate-&gt;src[i]);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!page)</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		if (!(migrate-&gt;src[i] &amp; MIGRATE_PFN_MIGRATE) || !newpage) {</span>
<span class="p_add">+			if (newpage) {</span>
<span class="p_add">+				unlock_page(newpage);</span>
<span class="p_add">+				put_page(newpage);</span>
<span class="p_add">+			}</span>
<span class="p_add">+			newpage = page;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		remove_migration_ptes(page, newpage, false);</span>
<span class="p_add">+		unlock_page(page);</span>
<span class="p_add">+		migrate-&gt;cpages--;</span>
<span class="p_add">+</span>
<span class="p_add">+		putback_lru_page(page);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (newpage != page) {</span>
<span class="p_add">+			unlock_page(newpage);</span>
<span class="p_add">+			putback_lru_page(newpage);</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * migrate_vma() - migrate a range of memory inside vma using accelerated copy</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * @ops: migration callback for allocating destination memory and copying</span>
<span class="p_add">+ * @vma: virtual memory area containing the range to be migrated</span>
<span class="p_add">+ * @mentries: maximum number of entry in src or dst pfns array</span>
<span class="p_add">+ * @start: start address of the range to migrate (inclusive)</span>
<span class="p_add">+ * @end: end address of the range to migrate (exclusive)</span>
<span class="p_add">+ * @src: array of hmm_pfn_t containing source pfns</span>
<span class="p_add">+ * @dst: array of hmm_pfn_t containing destination pfns</span>
<span class="p_add">+ * @private: pointer passed back to each of the callback</span>
<span class="p_add">+ * Returns: 0 on success, error code otherwise</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This will try to migrate a range of memory using callback to allocate and</span>
<span class="p_add">+ * copy memory from source to destination. This function will first collect,</span>
<span class="p_add">+ * lock and unmap pages in the range and then call alloc_and_copy() callback</span>
<span class="p_add">+ * for device driver to allocate destination memory and copy from source.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Then it will proceed and try to effectively migrate the page (struct page</span>
<span class="p_add">+ * metadata), a step that can fail for various reasons. Before updating CPU page</span>
<span class="p_add">+ * table it will call finalize_and_map() callback so that the device driver can</span>
<span class="p_add">+ * inspect what has been successfully migrated and update its own page tables</span>
<span class="p_add">+ * (this latter aspect is not mandatory and only make senses for some users of</span>
<span class="p_add">+ * this API).</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Finally the function update CPU page table and unlock the pages before</span>
<span class="p_add">+ * returning 0.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * It will return an error code only if one of the arguments is invalid.</span>
<span class="p_add">+ */</span>
<span class="p_add">+int migrate_vma(const struct migrate_vma_ops *ops,</span>
<span class="p_add">+		struct vm_area_struct *vma,</span>
<span class="p_add">+		unsigned long mentries,</span>
<span class="p_add">+		unsigned long start,</span>
<span class="p_add">+		unsigned long end,</span>
<span class="p_add">+		unsigned long *src,</span>
<span class="p_add">+		unsigned long *dst,</span>
<span class="p_add">+		void *private)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct migrate_vma migrate;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Sanity check the arguments */</span>
<span class="p_add">+	start &amp;= PAGE_MASK;</span>
<span class="p_add">+	end &amp;= PAGE_MASK;</span>
<span class="p_add">+	if (is_vm_hugetlb_page(vma) || (vma-&gt;vm_flags &amp; VM_SPECIAL))</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+	if (!vma || !ops || !src || !dst || start &gt;= end)</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+	if (start &lt; vma-&gt;vm_start || start &gt;= vma-&gt;vm_end)</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+	if (end &lt;= vma-&gt;vm_start || end &gt; vma-&gt;vm_end)</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+</span>
<span class="p_add">+	memset(src, 0, sizeof(*src) * ((end - start) &gt;&gt; PAGE_SHIFT));</span>
<span class="p_add">+	migrate.src = src;</span>
<span class="p_add">+	migrate.dst = dst;</span>
<span class="p_add">+	migrate.start = start;</span>
<span class="p_add">+	migrate.npages = 0;</span>
<span class="p_add">+	migrate.cpages = 0;</span>
<span class="p_add">+	migrate.mpages = mentries;</span>
<span class="p_add">+	migrate.end = end;</span>
<span class="p_add">+	migrate.vma = vma;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Collect, and try to unmap source pages */</span>
<span class="p_add">+	migrate_vma_collect(&amp;migrate);</span>
<span class="p_add">+	if (!migrate.cpages)</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Lock and isolate page */</span>
<span class="p_add">+	migrate_vma_prepare(&amp;migrate);</span>
<span class="p_add">+	if (!migrate.cpages)</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Unmap pages */</span>
<span class="p_add">+	migrate_vma_unmap(&amp;migrate);</span>
<span class="p_add">+	if (!migrate.cpages)</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * At this point pages are locked and unmapped, and thus they have</span>
<span class="p_add">+	 * stable content and can safely be copied to destination memory that</span>
<span class="p_add">+	 * is allocated by the callback.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * Note that migration can fail in migrate_vma_struct_page() for each</span>
<span class="p_add">+	 * individual page.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	ops-&gt;alloc_and_copy(vma, src, dst, start, end, private);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* This does the real migration of struct page */</span>
<span class="p_add">+	migrate_vma_pages(&amp;migrate);</span>
<span class="p_add">+</span>
<span class="p_add">+	ops-&gt;finalize_and_map(vma, src, dst, start, end, private);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Unlock and remap pages */</span>
<span class="p_add">+	migrate_vma_finalize(&amp;migrate);</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(migrate_vma);</span>

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



