
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[01/27] mm, vmstat: Add infrastructure for per-node vmstats - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [01/27] mm, vmstat: Add infrastructure for per-node vmstats</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=138281">Mel Gorman</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>June 9, 2016, 6:04 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1465495483-11855-2-git-send-email-mgorman@techsingularity.net&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9167639/mbox/"
   >mbox</a>
|
   <a href="/patch/9167639/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9167639/">/patch/9167639/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	12FD5607DA for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  9 Jun 2016 18:05:42 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 0939F28347
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  9 Jun 2016 18:05:42 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id F174A2835A; Thu,  9 Jun 2016 18:05:41 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 8B97028347
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  9 Jun 2016 18:05:39 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S932984AbcFISF3 (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 9 Jun 2016 14:05:29 -0400
Received: from outbound-smtp09.blacknight.com ([46.22.139.14]:48026 &quot;EHLO
	outbound-smtp09.blacknight.com&quot; rhost-flags-OK-OK-OK-OK)
	by vger.kernel.org with ESMTP id S932948AbcFISFX (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 9 Jun 2016 14:05:23 -0400
Received: from mail.blacknight.com (pemlinmail05.blacknight.ie
	[81.17.254.26])
	by outbound-smtp09.blacknight.com (Postfix) with ESMTPS id
	5AA851C2008 for &lt;linux-kernel@vger.kernel.org&gt;;
	Thu,  9 Jun 2016 19:05:04 +0100 (IST)
Received: (qmail 1778 invoked from network); 9 Jun 2016 18:05:04 -0000
Received: from unknown (HELO stampy.163woodhaven.lan)
	(mgorman@techsingularity.net@[37.228.231.136])
	by 81.17.254.9 with ESMTPA; 9 Jun 2016 18:05:04 -0000
From: Mel Gorman &lt;mgorman@techsingularity.net&gt;
To: Andrew Morton &lt;akpm@linux-foundation.org&gt;, Linux-MM &lt;linux-mm@kvack.org&gt;
Cc: Rik van Riel &lt;riel@surriel.com&gt;, Vlastimil Babka &lt;vbabka@suse.cz&gt;,
	Johannes Weiner &lt;hannes@cmpxchg.org&gt;,
	LKML &lt;linux-kernel@vger.kernel.org&gt;,
	Mel Gorman &lt;mgorman@techsingularity.net&gt;
Subject: [PATCH 01/27] mm, vmstat: Add infrastructure for per-node vmstats
Date: Thu,  9 Jun 2016 19:04:17 +0100
Message-Id: &lt;1465495483-11855-2-git-send-email-mgorman@techsingularity.net&gt;
X-Mailer: git-send-email 2.6.4
In-Reply-To: &lt;1465495483-11855-1-git-send-email-mgorman@techsingularity.net&gt;
References: &lt;1465495483-11855-1-git-send-email-mgorman@techsingularity.net&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=138281">Mel Gorman</a> - June 9, 2016, 6:04 p.m.</div>
<pre class="content">
References: bnc#969297 PM performance -- intel_pstate
Patch-mainline: No, expected 4.7 and queued in linux-mm
Patch-name: patches.suse/mm-vmstat-Add-infrastructure-for-per-node-vmstats.patch

VM statistic counters for reclaim decisions are zone-based. If the kernel
is to reclaim on a per-node basis then we need to track per-node statistics
but there is no infrastructure for that. The most notable change is that
the old node_page_state is renamed to sum_zone_node_page_state.  The new
node_page_state takes a pglist_data and uses per-node stats but none exist
yet. There is some renaming such as vm_stat to vm_zone_stat and the addition
of vm_node_stat and the renaming of mod_state to mod_zone_state. Otherwise,
this is mostly a mechanical patch with no functional change. There is a
lot of similarity between the node and zone helpers which is unfortunate
but there was no obvious way of reusing the code and maintaining type safety.
<span class="signed-off-by">
Signed-off-by: Mel Gorman &lt;mgorman@techsingularity.net&gt;</span>
<span class="acked-by">Acked-by: Johannes Weiner &lt;hannes@cmpxchg.org&gt;</span>
<span class="signed-off-by">Signed-off-by: Mel Gorman &lt;mgorman@suse.com&gt;</span>
---
 drivers/base/node.c    |  72 +++++++------
 include/linux/mmzone.h |  13 +++
 include/linux/vmstat.h |  92 +++++++++++++---
 mm/page_alloc.c        |  10 +-
 mm/vmstat.c            | 282 +++++++++++++++++++++++++++++++++++++++++++++----
 mm/workingset.c        |   9 +-
 6 files changed, 404 insertions(+), 74 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=72672">Vlastimil Babka</a> - June 10, 2016, 1:31 p.m.</div>
<pre class="content">
On 06/09/2016 08:04 PM, Mel Gorman wrote:
<span class="quote">&gt; References: bnc#969297 PM performance -- intel_pstate</span>
<span class="quote">&gt; Patch-mainline: No, expected 4.7 and queued in linux-mm</span>
<span class="quote">&gt; Patch-name: patches.suse/mm-vmstat-Add-infrastructure-for-per-node-vmstats.patch</span>

Remove?
<span class="quote">
&gt; VM statistic counters for reclaim decisions are zone-based. If the kernel</span>
<span class="quote">&gt; is to reclaim on a per-node basis then we need to track per-node statistics</span>
<span class="quote">&gt; but there is no infrastructure for that. The most notable change is that</span>
<span class="quote">&gt; the old node_page_state is renamed to sum_zone_node_page_state.  The new</span>
<span class="quote">&gt; node_page_state takes a pglist_data and uses per-node stats but none exist</span>
<span class="quote">&gt; yet. There is some renaming such as vm_stat to vm_zone_stat and the addition</span>
<span class="quote">&gt; of vm_node_stat and the renaming of mod_state to mod_zone_state. Otherwise,</span>
<span class="quote">&gt; this is mostly a mechanical patch with no functional change. There is a</span>
<span class="quote">&gt; lot of similarity between the node and zone helpers which is unfortunate</span>
<span class="quote">&gt; but there was no obvious way of reusing the code and maintaining type safety.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Mel Gorman &lt;mgorman@techsingularity.net&gt;</span>
<span class="quote">&gt; Acked-by: Johannes Weiner &lt;hannes@cmpxchg.org&gt;</span>
<span class="quote">&gt; Signed-off-by: Mel Gorman &lt;mgorman@suse.com&gt;</span>
<span class="acked-by">
Acked-by: Vlastimil Babka &lt;vbabka@suse.cz&gt;</span>

Some nitpicks below.
<span class="quote">
&gt; @@ -237,12 +286,26 @@ static inline void __inc_zone_page_state(struct page *page,</span>
<span class="quote">&gt;  	__inc_zone_state(page_zone(page), item);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +static inline void __inc_node_page_state(struct page *page,</span>
<span class="quote">&gt; +			enum node_stat_item item)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	__inc_node_state(page_zone(page)-&gt;zone_pgdat, item);</span>

This page -&gt; node translation looks needlessly ineffective. How about
using NODE_DATA(page_to_nid(page)).
<span class="quote">
&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  static inline void __dec_zone_page_state(struct page *page,</span>
<span class="quote">&gt;  			enum zone_stat_item item)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	__dec_zone_state(page_zone(page), item);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +static inline void __dec_node_page_state(struct page *page,</span>
<span class="quote">&gt; +			enum node_stat_item item)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	__dec_node_state(page_zone(page)-&gt;zone_pgdat, item);</span>
<span class="quote">&gt; +}</span>

Ditto.
<span class="quote">
&gt; @@ -188,9 +190,13 @@ void refresh_zone_stat_thresholds(void)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		threshold = calculate_normal_threshold(zone);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -		for_each_online_cpu(cpu)</span>
<span class="quote">&gt; +		for_each_online_cpu(cpu) {</span>
<span class="quote">&gt; +			struct pglist_data *pgdat = zone-&gt;zone_pgdat;</span>

Move the variable outside?
<span class="quote">
&gt;  			per_cpu_ptr(zone-&gt;pageset, cpu)-&gt;stat_threshold</span>
<span class="quote">&gt;  							= threshold;</span>
<span class="quote">&gt; +			per_cpu_ptr(pgdat-&gt;per_cpu_nodestats, cpu)-&gt;stat_threshold</span>
<span class="quote">&gt; +							= threshold;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		/*</span>
<span class="quote">&gt;  		 * Only set percpu_drift_mark if there is a danger that</span>
<span class="quote">&gt;  void __inc_zone_page_state(struct page *page, enum zone_stat_item item)</span>

[...]
<span class="quote">
&gt;  {</span>
<span class="quote">&gt;  	__inc_zone_state(page_zone(page), item);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  EXPORT_SYMBOL(__inc_zone_page_state);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +void __inc_node_page_state(struct page *page, enum node_stat_item item)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	__inc_node_state(page_zone(page)-&gt;zone_pgdat, item);</span>

Same page -&gt; node thing here.
<span class="quote">

&gt; +void __dec_node_page_state(struct page *page, enum node_stat_item item)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	__dec_node_state(page_zone(page)-&gt;zone_pgdat, item);</span>

And here.
<span class="quote">
&gt;  </span>
<span class="quote">&gt;  void dec_zone_page_state(struct page *page, enum zone_stat_item item)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	mod_state(page_zone(page), item, -1, -1);</span>
<span class="quote">&gt; +	mod_zone_state(page_zone(page), item, -1, -1);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  EXPORT_SYMBOL(dec_zone_page_state);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline void mod_node_state(struct pglist_data *pgdat,</span>
<span class="quote">&gt; +       enum node_stat_item item, int delta, int overstep_mode)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct per_cpu_nodestat __percpu *pcp = pgdat-&gt;per_cpu_nodestats;</span>
<span class="quote">&gt; +	s8 __percpu *p = pcp-&gt;vm_node_stat_diff + item;</span>
<span class="quote">&gt; +	long o, n, t, z;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	do {</span>
<span class="quote">&gt; +		z = 0;  /* overflow to zone counters */</span>

s/zone/node/?
<span class="quote">
&gt; +</span>
<span class="quote">&gt; +		/*</span>
<span class="quote">&gt; +		 * The fetching of the stat_threshold is racy. We may apply</span>
<span class="quote">&gt; +		 * a counter threshold to the wrong the cpu if we get</span>
<span class="quote">&gt; +		 * rescheduled while executing here. However, the next</span>
<span class="quote">&gt; +		 * counter update will apply the threshold again and</span>
<span class="quote">&gt; +		 * therefore bring the counter under the threshold again.</span>
<span class="quote">&gt; +		 *</span>
<span class="quote">&gt; +		 * Most of the time the thresholds are the same anyways</span>
<span class="quote">&gt; +		 * for all cpus in a zone.</span>

same here.
<span class="quote">
&gt; +		 */</span>
<span class="quote">&gt; +		t = this_cpu_read(pcp-&gt;stat_threshold);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		o = this_cpu_read(*p);</span>
<span class="quote">&gt; +		n = delta + o;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		if (n &gt; t || n &lt; -t) {</span>
<span class="quote">&gt; +			int os = overstep_mode * (t &gt;&gt; 1) ;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			/* Overflow must be added to zone counters */</span>

and here.
<span class="quote">
&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +void inc_node_page_state(struct page *page, enum node_stat_item item)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	mod_node_state(page_zone(page)-&gt;zone_pgdat, item, 1, 1);</span>

Ditto about page -&gt; nid.
<span class="quote">
&gt; +}</span>
<span class="quote">&gt; +EXPORT_SYMBOL(inc_node_page_state);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +void dec_node_page_state(struct page *page, enum node_stat_item item)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	mod_node_state(page_zone(page)-&gt;zone_pgdat, item, -1, -1);</span>
<span class="quote">&gt; +}</span>

Ditto.
<span class="quote">
&gt; +EXPORT_SYMBOL(dec_node_page_state);</span>
<span class="quote">&gt;  #else</span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt;   * Use interrupt disable to serialize counter updates</span>
<span class="quote">&gt; @@ -436,21 +568,69 @@ void dec_zone_page_state(struct page *page, enum zone_stat_item item)</span>
<span class="quote">&gt;  	local_irq_restore(flags);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  EXPORT_SYMBOL(dec_zone_page_state);</span>
<span class="quote">&gt; -#endif</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +void inc_node_state(struct pglist_data *pgdat, enum node_stat_item item)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	unsigned long flags;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	local_irq_save(flags);</span>
<span class="quote">&gt; +	__inc_node_state(pgdat, item);</span>
<span class="quote">&gt; +	local_irq_restore(flags);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +EXPORT_SYMBOL(inc_node_state);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +void mod_node_page_state(struct pglist_data *pgdat, enum node_stat_item item,</span>
<span class="quote">&gt; +					long delta)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	unsigned long flags;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	local_irq_save(flags);</span>
<span class="quote">&gt; +	__mod_node_page_state(pgdat, item, delta);</span>
<span class="quote">&gt; +	local_irq_restore(flags);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +EXPORT_SYMBOL(mod_node_page_state);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +void inc_node_page_state(struct page *page, enum node_stat_item item)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	unsigned long flags;</span>
<span class="quote">&gt; +	struct pglist_data *pgdat;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	pgdat = page_zone(page)-&gt;zone_pgdat;</span>

And here.

,9 +736,11 @@ static int refresh_cpu_vm_stats(bool do_pagesets)
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  void cpu_vm_stats_fold(int cpu)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; +	struct pglist_data *pgdat;</span>
<span class="quote">&gt;  	struct zone *zone;</span>
<span class="quote">&gt;  	int i;</span>
<span class="quote">&gt; -	int global_diff[NR_VM_ZONE_STAT_ITEMS] = { 0, };</span>
<span class="quote">&gt; +	int global_zone_diff[NR_VM_ZONE_STAT_ITEMS] = { 0, };</span>
<span class="quote">&gt; +	int global_node_diff[NR_VM_NODE_STAT_ITEMS] = { 0, };</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	for_each_populated_zone(zone) {</span>
<span class="quote">&gt;  		struct per_cpu_pageset *p;</span>
<span class="quote">&gt; @@ -555,11 +754,27 @@ void cpu_vm_stats_fold(int cpu)</span>
<span class="quote">&gt;  				v = p-&gt;vm_stat_diff[i];</span>
<span class="quote">&gt;  				p-&gt;vm_stat_diff[i] = 0;</span>
<span class="quote">&gt;  				atomic_long_add(v, &amp;zone-&gt;vm_stat[i]);</span>
<span class="quote">&gt; -				global_diff[i] += v;</span>
<span class="quote">&gt; +				global_zone_diff[i] += v;</span>
<span class="quote">&gt;  			}</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=138281">Mel Gorman</a> - June 10, 2016, 1:47 p.m.</div>
<pre class="content">
On Fri, Jun 10, 2016 at 03:31:41PM +0200, Vlastimil Babka wrote:
<span class="quote">&gt; On 06/09/2016 08:04 PM, Mel Gorman wrote:</span>
<span class="quote">&gt; &gt; References: bnc#969297 PM performance -- intel_pstate</span>
<span class="quote">&gt; &gt; Patch-mainline: No, expected 4.7 and queued in linux-mm</span>
<span class="quote">&gt; &gt; Patch-name: patches.suse/mm-vmstat-Add-infrastructure-for-per-node-vmstats.patch</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Remove?</span>
<span class="quote">&gt; </span>

Yes. Clearly I fat-fingers a cherry pick and used the wrong command that
added distro-specific metadata. Sorry.
<span class="quote">
&gt; &gt; VM statistic counters for reclaim decisions are zone-based. If the kernel</span>
<span class="quote">&gt; &gt; is to reclaim on a per-node basis then we need to track per-node statistics</span>
<span class="quote">&gt; &gt; but there is no infrastructure for that. The most notable change is that</span>
<span class="quote">&gt; &gt; the old node_page_state is renamed to sum_zone_node_page_state.  The new</span>
<span class="quote">&gt; &gt; node_page_state takes a pglist_data and uses per-node stats but none exist</span>
<span class="quote">&gt; &gt; yet. There is some renaming such as vm_stat to vm_zone_stat and the addition</span>
<span class="quote">&gt; &gt; of vm_node_stat and the renaming of mod_state to mod_zone_state. Otherwise,</span>
<span class="quote">&gt; &gt; this is mostly a mechanical patch with no functional change. There is a</span>
<span class="quote">&gt; &gt; lot of similarity between the node and zone helpers which is unfortunate</span>
<span class="quote">&gt; &gt; but there was no obvious way of reusing the code and maintaining type safety.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Signed-off-by: Mel Gorman &lt;mgorman@techsingularity.net&gt;</span>
<span class="quote">&gt; &gt; Acked-by: Johannes Weiner &lt;hannes@cmpxchg.org&gt;</span>
<span class="quote">&gt; &gt; Signed-off-by: Mel Gorman &lt;mgorman@suse.com&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Acked-by: Vlastimil Babka &lt;vbabka@suse.cz&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Some nitpicks below.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; @@ -237,12 +286,26 @@ static inline void __inc_zone_page_state(struct page *page,</span>
<span class="quote">&gt; &gt;  	__inc_zone_state(page_zone(page), item);</span>
<span class="quote">&gt; &gt;  }</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; +static inline void __inc_node_page_state(struct page *page,</span>
<span class="quote">&gt; &gt; +			enum node_stat_item item)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	__inc_node_state(page_zone(page)-&gt;zone_pgdat, item);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This page -&gt; node translation looks needlessly ineffective. How about</span>
<span class="quote">&gt; using NODE_DATA(page_to_nid(page)).</span>
<span class="quote">&gt; </span>

Yes, I will. I won&#39;t answer the individual feedbacks. They all seem
reasonable.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=1731">Christoph Lameter</a> - June 13, 2016, 5:26 p.m.</div>
<pre class="content">
On Thu, 9 Jun 2016, Mel Gorman wrote:
<span class="quote">
&gt; VM statistic counters for reclaim decisions are zone-based. If the kernel</span>
<span class="quote">&gt; is to reclaim on a per-node basis then we need to track per-node statistics</span>
<span class="quote">&gt; but there is no infrastructure for that. The most notable change is that</span>

There is node_page_state() so the value of any counter per node is already
available. Note that some of the counters (NUMA_xx) for example do not
make much sense as per zone counters and are effectively used as per node
counters.

So the main effect you are looking for is to have the counters stored in
the per node structure as opposed to the per zone struct in order to
avoid the summing? Doing so duplicates a large amount of code it seems.

If you do this then also move over certain counters that have more of a
per node use from per zone to per node. Like the NUMA_xxx counters.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=138281">Mel Gorman</a> - June 14, 2016, 2:25 p.m.</div>
<pre class="content">
On Mon, Jun 13, 2016 at 12:26:13PM -0500, Christoph Lameter wrote:
<span class="quote">&gt; On Thu, 9 Jun 2016, Mel Gorman wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; VM statistic counters for reclaim decisions are zone-based. If the kernel</span>
<span class="quote">&gt; &gt; is to reclaim on a per-node basis then we need to track per-node statistics</span>
<span class="quote">&gt; &gt; but there is no infrastructure for that. The most notable change is that</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; There is node_page_state() so the value of any counter per node is already</span>
<span class="quote">&gt; available. Note that some of the counters (NUMA_xx) for example do not</span>
<span class="quote">&gt; make much sense as per zone counters and are effectively used as per node</span>
<span class="quote">&gt; counters.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So the main effect you are looking for is to have the counters stored in</span>
<span class="quote">&gt; the per node structure as opposed to the per zone struct in order to</span>
<span class="quote">&gt; avoid the summing?</span>

Yes.
<span class="quote">
&gt; Doing so duplicates a large amount of code it seems.</span>
<span class="quote">&gt; </span>

Also yes. I considered macro magic to cover it but it turned into a
major mess. They could always be summed so it would be a minor
performance dent and a heavier cache footprint.
<span class="quote">
&gt; If you do this then also move over certain counters that have more of a</span>
<span class="quote">&gt; per node use from per zone to per node. Like the NUMA_xxx counters.</span>
<span class="quote">&gt; </span>

As the NUMA counters are consumed by userspace, I worried that it would
break some tools. If the rest of the series gets solidified then I will
do it as a single patch on top so it can be reverted if necessary
relatively easily.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/drivers/base/node.c b/drivers/base/node.c</span>
<span class="p_header">index 560751bad294..efb81da250a8 100644</span>
<span class="p_header">--- a/drivers/base/node.c</span>
<span class="p_header">+++ b/drivers/base/node.c</span>
<span class="p_chunk">@@ -74,16 +74,16 @@</span> <span class="p_context"> static ssize_t node_read_meminfo(struct device *dev,</span>
 		       nid, K(i.totalram),
 		       nid, K(i.freeram),
 		       nid, K(i.totalram - i.freeram),
<span class="p_del">-		       nid, K(node_page_state(nid, NR_ACTIVE_ANON) +</span>
<span class="p_del">-				node_page_state(nid, NR_ACTIVE_FILE)),</span>
<span class="p_del">-		       nid, K(node_page_state(nid, NR_INACTIVE_ANON) +</span>
<span class="p_del">-				node_page_state(nid, NR_INACTIVE_FILE)),</span>
<span class="p_del">-		       nid, K(node_page_state(nid, NR_ACTIVE_ANON)),</span>
<span class="p_del">-		       nid, K(node_page_state(nid, NR_INACTIVE_ANON)),</span>
<span class="p_del">-		       nid, K(node_page_state(nid, NR_ACTIVE_FILE)),</span>
<span class="p_del">-		       nid, K(node_page_state(nid, NR_INACTIVE_FILE)),</span>
<span class="p_del">-		       nid, K(node_page_state(nid, NR_UNEVICTABLE)),</span>
<span class="p_del">-		       nid, K(node_page_state(nid, NR_MLOCK)));</span>
<span class="p_add">+		       nid, K(sum_zone_node_page_state(nid, NR_ACTIVE_ANON) +</span>
<span class="p_add">+				sum_zone_node_page_state(nid, NR_ACTIVE_FILE)),</span>
<span class="p_add">+		       nid, K(sum_zone_node_page_state(nid, NR_INACTIVE_ANON) +</span>
<span class="p_add">+				sum_zone_node_page_state(nid, NR_INACTIVE_FILE)),</span>
<span class="p_add">+		       nid, K(sum_zone_node_page_state(nid, NR_ACTIVE_ANON)),</span>
<span class="p_add">+		       nid, K(sum_zone_node_page_state(nid, NR_INACTIVE_ANON)),</span>
<span class="p_add">+		       nid, K(sum_zone_node_page_state(nid, NR_ACTIVE_FILE)),</span>
<span class="p_add">+		       nid, K(sum_zone_node_page_state(nid, NR_INACTIVE_FILE)),</span>
<span class="p_add">+		       nid, K(sum_zone_node_page_state(nid, NR_UNEVICTABLE)),</span>
<span class="p_add">+		       nid, K(sum_zone_node_page_state(nid, NR_MLOCK)));</span>
 
 #ifdef CONFIG_HIGHMEM
 	n += sprintf(buf + n,
<span class="p_chunk">@@ -115,28 +115,28 @@</span> <span class="p_context"> static ssize_t node_read_meminfo(struct device *dev,</span>
 		       &quot;Node %d AnonHugePages:  %8lu kB\n&quot;
 #endif
 			,
<span class="p_del">-		       nid, K(node_page_state(nid, NR_FILE_DIRTY)),</span>
<span class="p_del">-		       nid, K(node_page_state(nid, NR_WRITEBACK)),</span>
<span class="p_del">-		       nid, K(node_page_state(nid, NR_FILE_PAGES)),</span>
<span class="p_del">-		       nid, K(node_page_state(nid, NR_FILE_MAPPED)),</span>
<span class="p_del">-		       nid, K(node_page_state(nid, NR_ANON_PAGES)),</span>
<span class="p_add">+		       nid, K(sum_zone_node_page_state(nid, NR_FILE_DIRTY)),</span>
<span class="p_add">+		       nid, K(sum_zone_node_page_state(nid, NR_WRITEBACK)),</span>
<span class="p_add">+		       nid, K(sum_zone_node_page_state(nid, NR_FILE_PAGES)),</span>
<span class="p_add">+		       nid, K(sum_zone_node_page_state(nid, NR_FILE_MAPPED)),</span>
<span class="p_add">+		       nid, K(sum_zone_node_page_state(nid, NR_ANON_PAGES)),</span>
 		       nid, K(i.sharedram),
<span class="p_del">-		       nid, node_page_state(nid, NR_KERNEL_STACK) *</span>
<span class="p_add">+		       nid, sum_zone_node_page_state(nid, NR_KERNEL_STACK) *</span>
 				THREAD_SIZE / 1024,
<span class="p_del">-		       nid, K(node_page_state(nid, NR_PAGETABLE)),</span>
<span class="p_del">-		       nid, K(node_page_state(nid, NR_UNSTABLE_NFS)),</span>
<span class="p_del">-		       nid, K(node_page_state(nid, NR_BOUNCE)),</span>
<span class="p_del">-		       nid, K(node_page_state(nid, NR_WRITEBACK_TEMP)),</span>
<span class="p_del">-		       nid, K(node_page_state(nid, NR_SLAB_RECLAIMABLE) +</span>
<span class="p_del">-				node_page_state(nid, NR_SLAB_UNRECLAIMABLE)),</span>
<span class="p_del">-		       nid, K(node_page_state(nid, NR_SLAB_RECLAIMABLE)),</span>
<span class="p_add">+		       nid, K(sum_zone_node_page_state(nid, NR_PAGETABLE)),</span>
<span class="p_add">+		       nid, K(sum_zone_node_page_state(nid, NR_UNSTABLE_NFS)),</span>
<span class="p_add">+		       nid, K(sum_zone_node_page_state(nid, NR_BOUNCE)),</span>
<span class="p_add">+		       nid, K(sum_zone_node_page_state(nid, NR_WRITEBACK_TEMP)),</span>
<span class="p_add">+		       nid, K(sum_zone_node_page_state(nid, NR_SLAB_RECLAIMABLE) +</span>
<span class="p_add">+				sum_zone_node_page_state(nid, NR_SLAB_UNRECLAIMABLE)),</span>
<span class="p_add">+		       nid, K(sum_zone_node_page_state(nid, NR_SLAB_RECLAIMABLE)),</span>
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE
<span class="p_del">-		       nid, K(node_page_state(nid, NR_SLAB_UNRECLAIMABLE))</span>
<span class="p_add">+		       nid, K(sum_zone_node_page_state(nid, NR_SLAB_UNRECLAIMABLE))</span>
 			, nid,
<span class="p_del">-			K(node_page_state(nid, NR_ANON_TRANSPARENT_HUGEPAGES) *</span>
<span class="p_add">+			K(sum_zone_node_page_state(nid, NR_ANON_TRANSPARENT_HUGEPAGES) *</span>
 			HPAGE_PMD_NR));
 #else
<span class="p_del">-		       nid, K(node_page_state(nid, NR_SLAB_UNRECLAIMABLE)));</span>
<span class="p_add">+		       nid, K(sum_zone_node_page_state(nid, NR_SLAB_UNRECLAIMABLE)));</span>
 #endif
 	n += hugetlb_report_node_meminfo(nid, buf + n);
 	return n;
<span class="p_chunk">@@ -155,12 +155,12 @@</span> <span class="p_context"> static ssize_t node_read_numastat(struct device *dev,</span>
 		       &quot;interleave_hit %lu\n&quot;
 		       &quot;local_node %lu\n&quot;
 		       &quot;other_node %lu\n&quot;,
<span class="p_del">-		       node_page_state(dev-&gt;id, NUMA_HIT),</span>
<span class="p_del">-		       node_page_state(dev-&gt;id, NUMA_MISS),</span>
<span class="p_del">-		       node_page_state(dev-&gt;id, NUMA_FOREIGN),</span>
<span class="p_del">-		       node_page_state(dev-&gt;id, NUMA_INTERLEAVE_HIT),</span>
<span class="p_del">-		       node_page_state(dev-&gt;id, NUMA_LOCAL),</span>
<span class="p_del">-		       node_page_state(dev-&gt;id, NUMA_OTHER));</span>
<span class="p_add">+		       sum_zone_node_page_state(dev-&gt;id, NUMA_HIT),</span>
<span class="p_add">+		       sum_zone_node_page_state(dev-&gt;id, NUMA_MISS),</span>
<span class="p_add">+		       sum_zone_node_page_state(dev-&gt;id, NUMA_FOREIGN),</span>
<span class="p_add">+		       sum_zone_node_page_state(dev-&gt;id, NUMA_INTERLEAVE_HIT),</span>
<span class="p_add">+		       sum_zone_node_page_state(dev-&gt;id, NUMA_LOCAL),</span>
<span class="p_add">+		       sum_zone_node_page_state(dev-&gt;id, NUMA_OTHER));</span>
 }
 static DEVICE_ATTR(numastat, S_IRUGO, node_read_numastat, NULL);
 
<span class="p_chunk">@@ -168,12 +168,18 @@</span> <span class="p_context"> static ssize_t node_read_vmstat(struct device *dev,</span>
 				struct device_attribute *attr, char *buf)
 {
 	int nid = dev-&gt;id;
<span class="p_add">+	struct pglist_data *pgdat = NODE_DATA(nid);</span>
 	int i;
 	int n = 0;
 
 	for (i = 0; i &lt; NR_VM_ZONE_STAT_ITEMS; i++)
 		n += sprintf(buf+n, &quot;%s %lu\n&quot;, vmstat_text[i],
<span class="p_del">-			     node_page_state(nid, i));</span>
<span class="p_add">+			     sum_zone_node_page_state(nid, i));</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i &lt; NR_VM_NODE_STAT_ITEMS; i++)</span>
<span class="p_add">+		n += sprintf(buf+n, &quot;%s %lu\n&quot;,</span>
<span class="p_add">+			     vmstat_text[i + NR_VM_ZONE_STAT_ITEMS],</span>
<span class="p_add">+			     node_page_state(pgdat, i));</span>
 
 	return n;
 }
<span class="p_header">diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h</span>
<span class="p_header">index 54df45c03ba2..b299c3af798e 100644</span>
<span class="p_header">--- a/include/linux/mmzone.h</span>
<span class="p_header">+++ b/include/linux/mmzone.h</span>
<span class="p_chunk">@@ -158,6 +158,10 @@</span> <span class="p_context"> enum zone_stat_item {</span>
 	NR_FREE_CMA_PAGES,
 	NR_VM_ZONE_STAT_ITEMS };
 
<span class="p_add">+enum node_stat_item {</span>
<span class="p_add">+	NR_VM_NODE_STAT_ITEMS</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
 /*
  * We do arithmetic on the LRU lists in various places in the code,
  * so it is important to keep the active lists LRU_ACTIVE higher in
<span class="p_chunk">@@ -265,6 +269,11 @@</span> <span class="p_context"> struct per_cpu_pageset {</span>
 #endif
 };
 
<span class="p_add">+struct per_cpu_nodestat {</span>
<span class="p_add">+	s8 stat_threshold;</span>
<span class="p_add">+	s8 vm_node_stat_diff[NR_VM_NODE_STAT_ITEMS];</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
 #endif /* !__GENERATING_BOUNDS.H */
 
 enum zone_type {
<span class="p_chunk">@@ -693,6 +702,10 @@</span> <span class="p_context"> typedef struct pglist_data {</span>
 	struct list_head split_queue;
 	unsigned long split_queue_len;
 #endif
<span class="p_add">+</span>
<span class="p_add">+	/* Per-node vmstats */</span>
<span class="p_add">+	struct per_cpu_nodestat __percpu *per_cpu_nodestats;</span>
<span class="p_add">+	atomic_long_t		vm_stat[NR_VM_NODE_STAT_ITEMS];</span>
 } pg_data_t;
 
 #define node_present_pages(nid)	(NODE_DATA(nid)-&gt;node_present_pages)
<span class="p_header">diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h</span>
<span class="p_header">index 0aa613df463e..40629576f8de 100644</span>
<span class="p_header">--- a/include/linux/vmstat.h</span>
<span class="p_header">+++ b/include/linux/vmstat.h</span>
<span class="p_chunk">@@ -112,20 +112,38 @@</span> <span class="p_context"> static inline void vm_events_fold_cpu(int cpu)</span>
 		zone_idx(zone), delta)
 
 /*
<span class="p_del">- * Zone based page accounting with per cpu differentials.</span>
<span class="p_add">+ * Zone and node-based page accounting with per cpu differentials.</span>
  */
<span class="p_del">-extern atomic_long_t vm_stat[NR_VM_ZONE_STAT_ITEMS];</span>
<span class="p_add">+extern atomic_long_t vm_zone_stat[NR_VM_ZONE_STAT_ITEMS];</span>
<span class="p_add">+extern atomic_long_t vm_node_stat[NR_VM_NODE_STAT_ITEMS];</span>
 
 static inline void zone_page_state_add(long x, struct zone *zone,
 				 enum zone_stat_item item)
 {
 	atomic_long_add(x, &amp;zone-&gt;vm_stat[item]);
<span class="p_del">-	atomic_long_add(x, &amp;vm_stat[item]);</span>
<span class="p_add">+	atomic_long_add(x, &amp;vm_zone_stat[item]);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void node_page_state_add(long x, struct pglist_data *pgdat,</span>
<span class="p_add">+				 enum node_stat_item item)</span>
<span class="p_add">+{</span>
<span class="p_add">+	atomic_long_add(x, &amp;pgdat-&gt;vm_stat[item]);</span>
<span class="p_add">+	atomic_long_add(x, &amp;vm_node_stat[item]);</span>
 }
 
 static inline unsigned long global_page_state(enum zone_stat_item item)
 {
<span class="p_del">-	long x = atomic_long_read(&amp;vm_stat[item]);</span>
<span class="p_add">+	long x = atomic_long_read(&amp;vm_zone_stat[item]);</span>
<span class="p_add">+#ifdef CONFIG_SMP</span>
<span class="p_add">+	if (x &lt; 0)</span>
<span class="p_add">+		x = 0;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	return x;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline unsigned long global_node_page_state(enum node_stat_item item)</span>
<span class="p_add">+{</span>
<span class="p_add">+	long x = atomic_long_read(&amp;vm_node_stat[item]);</span>
 #ifdef CONFIG_SMP
 	if (x &lt; 0)
 		x = 0;
<span class="p_chunk">@@ -167,31 +185,44 @@</span> <span class="p_context"> static inline unsigned long zone_page_state_snapshot(struct zone *zone,</span>
 }
 
 #ifdef CONFIG_NUMA
<span class="p_del">-</span>
<span class="p_del">-extern unsigned long node_page_state(int node, enum zone_stat_item item);</span>
<span class="p_del">-</span>
<span class="p_add">+extern unsigned long sum_zone_node_page_state(int node,</span>
<span class="p_add">+						enum zone_stat_item item);</span>
<span class="p_add">+extern unsigned long node_page_state(struct pglist_data *pgdat,</span>
<span class="p_add">+						enum node_stat_item item);</span>
 #else
<span class="p_del">-</span>
<span class="p_del">-#define node_page_state(node, item) global_page_state(item)</span>
<span class="p_del">-</span>
<span class="p_add">+#define sum_zone_node_page_state(node, item) global_node_page_state(item)</span>
<span class="p_add">+#define node_page_state(node, item) global_node_page_state(item)</span>
 #endif /* CONFIG_NUMA */
 
 #define add_zone_page_state(__z, __i, __d) mod_zone_page_state(__z, __i, __d)
 #define sub_zone_page_state(__z, __i, __d) mod_zone_page_state(__z, __i, -(__d))
<span class="p_add">+#define add_node_page_state(__p, __i, __d) mod_node_page_state(__p, __i, __d)</span>
<span class="p_add">+#define sub_node_page_state(__p, __i, __d) mod_node_page_state(__p, __i, -(__d))</span>
 
 #ifdef CONFIG_SMP
 void __mod_zone_page_state(struct zone *, enum zone_stat_item item, long);
 void __inc_zone_page_state(struct page *, enum zone_stat_item);
 void __dec_zone_page_state(struct page *, enum zone_stat_item);
 
<span class="p_add">+void __mod_node_page_state(struct pglist_data *, enum node_stat_item item, long);</span>
<span class="p_add">+void __inc_node_page_state(struct page *, enum node_stat_item);</span>
<span class="p_add">+void __dec_node_page_state(struct page *, enum node_stat_item);</span>
<span class="p_add">+</span>
 void mod_zone_page_state(struct zone *, enum zone_stat_item, long);
 void inc_zone_page_state(struct page *, enum zone_stat_item);
 void dec_zone_page_state(struct page *, enum zone_stat_item);
 
<span class="p_add">+void mod_node_page_state(struct pglist_data *, enum node_stat_item, long);</span>
<span class="p_add">+void inc_node_page_state(struct page *, enum node_stat_item);</span>
<span class="p_add">+void dec_node_page_state(struct page *, enum node_stat_item);</span>
<span class="p_add">+</span>
 extern void inc_zone_state(struct zone *, enum zone_stat_item);
<span class="p_add">+extern void inc_node_state(struct pglist_data *, enum node_stat_item);</span>
 extern void __inc_zone_state(struct zone *, enum zone_stat_item);
<span class="p_add">+extern void __inc_node_state(struct pglist_data *, enum node_stat_item);</span>
 extern void dec_zone_state(struct zone *, enum zone_stat_item);
 extern void __dec_zone_state(struct zone *, enum zone_stat_item);
<span class="p_add">+extern void __dec_node_state(struct pglist_data *, enum node_stat_item);</span>
 
 void quiet_vmstat(void);
 void cpu_vm_stats_fold(int cpu);
<span class="p_chunk">@@ -219,16 +250,34 @@</span> <span class="p_context"> static inline void __mod_zone_page_state(struct zone *zone,</span>
 	zone_page_state_add(delta, zone, item);
 }
 
<span class="p_add">+static inline void __mod_node_page_state(struct pglist_data *pgdat,</span>
<span class="p_add">+			enum node_stat_item item, int delta)</span>
<span class="p_add">+{</span>
<span class="p_add">+	node_page_state_add(delta, pgdat, item);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static inline void __inc_zone_state(struct zone *zone, enum zone_stat_item item)
 {
 	atomic_long_inc(&amp;zone-&gt;vm_stat[item]);
<span class="p_del">-	atomic_long_inc(&amp;vm_stat[item]);</span>
<span class="p_add">+	atomic_long_inc(&amp;vm_zone_stat[item]);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void __inc_node_state(struct pglist_data *pgdat, enum node_stat_item item)</span>
<span class="p_add">+{</span>
<span class="p_add">+	atomic_long_inc(&amp;pgdat-&gt;vm_stat[item]);</span>
<span class="p_add">+	atomic_long_inc(&amp;vm_node_stat[item]);</span>
 }
 
 static inline void __dec_zone_state(struct zone *zone, enum zone_stat_item item)
 {
 	atomic_long_dec(&amp;zone-&gt;vm_stat[item]);
<span class="p_del">-	atomic_long_dec(&amp;vm_stat[item]);</span>
<span class="p_add">+	atomic_long_dec(&amp;vm_zone_stat[item]);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void __dec_node_state(struct pglist_data *pgdat, enum node_stat_item item)</span>
<span class="p_add">+{</span>
<span class="p_add">+	atomic_long_dec(&amp;pgdat-&gt;vm_stat[item]);</span>
<span class="p_add">+	atomic_long_dec(&amp;vm_node_stat[item]);</span>
 }
 
 static inline void __inc_zone_page_state(struct page *page,
<span class="p_chunk">@@ -237,12 +286,26 @@</span> <span class="p_context"> static inline void __inc_zone_page_state(struct page *page,</span>
 	__inc_zone_state(page_zone(page), item);
 }
 
<span class="p_add">+static inline void __inc_node_page_state(struct page *page,</span>
<span class="p_add">+			enum node_stat_item item)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__inc_node_state(page_zone(page)-&gt;zone_pgdat, item);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
 static inline void __dec_zone_page_state(struct page *page,
 			enum zone_stat_item item)
 {
 	__dec_zone_state(page_zone(page), item);
 }
 
<span class="p_add">+static inline void __dec_node_page_state(struct page *page,</span>
<span class="p_add">+			enum node_stat_item item)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__dec_node_state(page_zone(page)-&gt;zone_pgdat, item);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
 /*
  * We only use atomic operations to update counters. So there is no need to
  * disable interrupts.
<span class="p_chunk">@@ -251,7 +314,12 @@</span> <span class="p_context"> static inline void __dec_zone_page_state(struct page *page,</span>
 #define dec_zone_page_state __dec_zone_page_state
 #define mod_zone_page_state __mod_zone_page_state
 
<span class="p_add">+#define inc_node_page_state __inc_node_page_state</span>
<span class="p_add">+#define dec_node_page_state __dec_node_page_state</span>
<span class="p_add">+#define mod_node_page_state __mod_node_page_state</span>
<span class="p_add">+</span>
 #define inc_zone_state __inc_zone_state
<span class="p_add">+#define inc_node_state __inc_node_state</span>
 #define dec_zone_state __dec_zone_state
 
 #define set_pgdat_percpu_threshold(pgdat, callback) { }
<span class="p_header">diff --git a/mm/page_alloc.c b/mm/page_alloc.c</span>
<span class="p_header">index a46547389e53..9d71af25acf9 100644</span>
<span class="p_header">--- a/mm/page_alloc.c</span>
<span class="p_header">+++ b/mm/page_alloc.c</span>
<span class="p_chunk">@@ -4209,8 +4209,8 @@</span> <span class="p_context"> void si_meminfo_node(struct sysinfo *val, int nid)</span>
 	for (zone_type = 0; zone_type &lt; MAX_NR_ZONES; zone_type++)
 		managed_pages += pgdat-&gt;node_zones[zone_type].managed_pages;
 	val-&gt;totalram = managed_pages;
<span class="p_del">-	val-&gt;sharedram = node_page_state(nid, NR_SHMEM);</span>
<span class="p_del">-	val-&gt;freeram = node_page_state(nid, NR_FREE_PAGES);</span>
<span class="p_add">+	val-&gt;sharedram = sum_zone_node_page_state(nid, NR_SHMEM);</span>
<span class="p_add">+	val-&gt;freeram = sum_zone_node_page_state(nid, NR_FREE_PAGES);</span>
 #ifdef CONFIG_HIGHMEM
 	for (zone_type = 0; zone_type &lt; MAX_NR_ZONES; zone_type++) {
 		struct zone *zone = &amp;pgdat-&gt;node_zones[zone_type];
<span class="p_chunk">@@ -5316,6 +5316,11 @@</span> <span class="p_context"> static void __meminit setup_zone_pageset(struct zone *zone)</span>
 	zone-&gt;pageset = alloc_percpu(struct per_cpu_pageset);
 	for_each_possible_cpu(cpu)
 		zone_pageset_init(zone, cpu);
<span class="p_add">+</span>
<span class="p_add">+	if (!zone-&gt;zone_pgdat-&gt;per_cpu_nodestats) {</span>
<span class="p_add">+		zone-&gt;zone_pgdat-&gt;per_cpu_nodestats =</span>
<span class="p_add">+			alloc_percpu(struct per_cpu_nodestat);</span>
<span class="p_add">+	}</span>
 }
 
 /*
<span class="p_chunk">@@ -6021,6 +6026,7 @@</span> <span class="p_context"> void __paginginit free_area_init_node(int nid, unsigned long *zones_size,</span>
 	reset_deferred_meminit(pgdat);
 	pgdat-&gt;node_id = nid;
 	pgdat-&gt;node_start_pfn = node_start_pfn;
<span class="p_add">+	pgdat-&gt;per_cpu_nodestats = NULL;</span>
 #ifdef CONFIG_HAVE_MEMBLOCK_NODE_MAP
 	get_pfn_range_for_nid(nid, &amp;start_pfn, &amp;end_pfn);
 	pr_info(&quot;Initmem setup node %d [mem %#018Lx-%#018Lx]\n&quot;, nid,
<span class="p_header">diff --git a/mm/vmstat.c b/mm/vmstat.c</span>
<span class="p_header">index 076c39e3ba09..e1d0deeec98e 100644</span>
<span class="p_header">--- a/mm/vmstat.c</span>
<span class="p_header">+++ b/mm/vmstat.c</span>
<span class="p_chunk">@@ -98,8 +98,10 @@</span> <span class="p_context"> void vm_events_fold_cpu(int cpu)</span>
  *
  * vm_stat contains the global counters
  */
<span class="p_del">-atomic_long_t vm_stat[NR_VM_ZONE_STAT_ITEMS] __cacheline_aligned_in_smp;</span>
<span class="p_del">-EXPORT_SYMBOL(vm_stat);</span>
<span class="p_add">+atomic_long_t vm_zone_stat[NR_VM_ZONE_STAT_ITEMS] __cacheline_aligned_in_smp;</span>
<span class="p_add">+atomic_long_t vm_node_stat[NR_VM_NODE_STAT_ITEMS] __cacheline_aligned_in_smp;</span>
<span class="p_add">+EXPORT_SYMBOL(vm_zone_stat);</span>
<span class="p_add">+EXPORT_SYMBOL(vm_node_stat);</span>
 
 #ifdef CONFIG_SMP
 
<span class="p_chunk">@@ -188,9 +190,13 @@</span> <span class="p_context"> void refresh_zone_stat_thresholds(void)</span>
 
 		threshold = calculate_normal_threshold(zone);
 
<span class="p_del">-		for_each_online_cpu(cpu)</span>
<span class="p_add">+		for_each_online_cpu(cpu) {</span>
<span class="p_add">+			struct pglist_data *pgdat = zone-&gt;zone_pgdat;</span>
 			per_cpu_ptr(zone-&gt;pageset, cpu)-&gt;stat_threshold
 							= threshold;
<span class="p_add">+			per_cpu_ptr(pgdat-&gt;per_cpu_nodestats, cpu)-&gt;stat_threshold</span>
<span class="p_add">+							= threshold;</span>
<span class="p_add">+		}</span>
 
 		/*
 		 * Only set percpu_drift_mark if there is a danger that
<span class="p_chunk">@@ -250,6 +256,26 @@</span> <span class="p_context"> void __mod_zone_page_state(struct zone *zone, enum zone_stat_item item,</span>
 }
 EXPORT_SYMBOL(__mod_zone_page_state);
 
<span class="p_add">+void __mod_node_page_state(struct pglist_data *pgdat, enum node_stat_item item,</span>
<span class="p_add">+				long delta)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct per_cpu_nodestat __percpu *pcp = pgdat-&gt;per_cpu_nodestats;</span>
<span class="p_add">+	s8 __percpu *p = pcp-&gt;vm_node_stat_diff + item;</span>
<span class="p_add">+	long x;</span>
<span class="p_add">+	long t;</span>
<span class="p_add">+</span>
<span class="p_add">+	x = delta + __this_cpu_read(*p);</span>
<span class="p_add">+</span>
<span class="p_add">+	t = __this_cpu_read(pcp-&gt;stat_threshold);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (unlikely(x &gt; t || x &lt; -t)) {</span>
<span class="p_add">+		node_page_state_add(x, pgdat, item);</span>
<span class="p_add">+		x = 0;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	__this_cpu_write(*p, x);</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(__mod_node_page_state);</span>
<span class="p_add">+</span>
 /*
  * Optimized increment and decrement functions.
  *
<span class="p_chunk">@@ -289,12 +315,34 @@</span> <span class="p_context"> void __inc_zone_state(struct zone *zone, enum zone_stat_item item)</span>
 	}
 }
 
<span class="p_add">+void __inc_node_state(struct pglist_data *pgdat, enum node_stat_item item)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct per_cpu_nodestat __percpu *pcp = pgdat-&gt;per_cpu_nodestats;</span>
<span class="p_add">+	s8 __percpu *p = pcp-&gt;vm_node_stat_diff + item;</span>
<span class="p_add">+	s8 v, t;</span>
<span class="p_add">+</span>
<span class="p_add">+	v = __this_cpu_inc_return(*p);</span>
<span class="p_add">+	t = __this_cpu_read(pcp-&gt;stat_threshold);</span>
<span class="p_add">+	if (unlikely(v &gt; t)) {</span>
<span class="p_add">+		s8 overstep = t &gt;&gt; 1;</span>
<span class="p_add">+</span>
<span class="p_add">+		node_page_state_add(v + overstep, pgdat, item);</span>
<span class="p_add">+		__this_cpu_write(*p, -overstep);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 void __inc_zone_page_state(struct page *page, enum zone_stat_item item)
 {
 	__inc_zone_state(page_zone(page), item);
 }
 EXPORT_SYMBOL(__inc_zone_page_state);
 
<span class="p_add">+void __inc_node_page_state(struct page *page, enum node_stat_item item)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__inc_node_state(page_zone(page)-&gt;zone_pgdat, item);</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(__inc_node_page_state);</span>
<span class="p_add">+</span>
 void __dec_zone_state(struct zone *zone, enum zone_stat_item item)
 {
 	struct per_cpu_pageset __percpu *pcp = zone-&gt;pageset;
<span class="p_chunk">@@ -311,12 +359,34 @@</span> <span class="p_context"> void __dec_zone_state(struct zone *zone, enum zone_stat_item item)</span>
 	}
 }
 
<span class="p_add">+void __dec_node_state(struct pglist_data *pgdat, enum node_stat_item item)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct per_cpu_nodestat __percpu *pcp = pgdat-&gt;per_cpu_nodestats;</span>
<span class="p_add">+	s8 __percpu *p = pcp-&gt;vm_node_stat_diff + item;</span>
<span class="p_add">+	s8 v, t;</span>
<span class="p_add">+</span>
<span class="p_add">+	v = __this_cpu_dec_return(*p);</span>
<span class="p_add">+	t = __this_cpu_read(pcp-&gt;stat_threshold);</span>
<span class="p_add">+	if (unlikely(v &lt; - t)) {</span>
<span class="p_add">+		s8 overstep = t &gt;&gt; 1;</span>
<span class="p_add">+</span>
<span class="p_add">+		node_page_state_add(v - overstep, pgdat, item);</span>
<span class="p_add">+		__this_cpu_write(*p, overstep);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 void __dec_zone_page_state(struct page *page, enum zone_stat_item item)
 {
 	__dec_zone_state(page_zone(page), item);
 }
 EXPORT_SYMBOL(__dec_zone_page_state);
 
<span class="p_add">+void __dec_node_page_state(struct page *page, enum node_stat_item item)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__dec_node_state(page_zone(page)-&gt;zone_pgdat, item);</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(__dec_node_page_state);</span>
<span class="p_add">+</span>
 #ifdef CONFIG_HAVE_CMPXCHG_LOCAL
 /*
  * If we have cmpxchg_local support then we do not need to incur the overhead
<span class="p_chunk">@@ -330,8 +400,8 @@</span> <span class="p_context"> EXPORT_SYMBOL(__dec_zone_page_state);</span>
  *     1       Overstepping half of threshold
  *     -1      Overstepping minus half of threshold
 */
<span class="p_del">-static inline void mod_state(struct zone *zone, enum zone_stat_item item,</span>
<span class="p_del">-			     long delta, int overstep_mode)</span>
<span class="p_add">+static inline void mod_zone_state(struct zone *zone,</span>
<span class="p_add">+       enum zone_stat_item item, long delta, int overstep_mode)</span>
 {
 	struct per_cpu_pageset __percpu *pcp = zone-&gt;pageset;
 	s8 __percpu *p = pcp-&gt;vm_stat_diff + item;
<span class="p_chunk">@@ -371,26 +441,88 @@</span> <span class="p_context"> static inline void mod_state(struct zone *zone, enum zone_stat_item item,</span>
 void mod_zone_page_state(struct zone *zone, enum zone_stat_item item,
 			 long delta)
 {
<span class="p_del">-	mod_state(zone, item, delta, 0);</span>
<span class="p_add">+	mod_zone_state(zone, item, delta, 0);</span>
 }
 EXPORT_SYMBOL(mod_zone_page_state);
 
 void inc_zone_state(struct zone *zone, enum zone_stat_item item)
 {
<span class="p_del">-	mod_state(zone, item, 1, 1);</span>
<span class="p_add">+	mod_zone_state(zone, item, 1, 1);</span>
 }
 
 void inc_zone_page_state(struct page *page, enum zone_stat_item item)
 {
<span class="p_del">-	mod_state(page_zone(page), item, 1, 1);</span>
<span class="p_add">+	mod_zone_state(page_zone(page), item, 1, 1);</span>
 }
 EXPORT_SYMBOL(inc_zone_page_state);
 
 void dec_zone_page_state(struct page *page, enum zone_stat_item item)
 {
<span class="p_del">-	mod_state(page_zone(page), item, -1, -1);</span>
<span class="p_add">+	mod_zone_state(page_zone(page), item, -1, -1);</span>
 }
 EXPORT_SYMBOL(dec_zone_page_state);
<span class="p_add">+</span>
<span class="p_add">+static inline void mod_node_state(struct pglist_data *pgdat,</span>
<span class="p_add">+       enum node_stat_item item, int delta, int overstep_mode)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct per_cpu_nodestat __percpu *pcp = pgdat-&gt;per_cpu_nodestats;</span>
<span class="p_add">+	s8 __percpu *p = pcp-&gt;vm_node_stat_diff + item;</span>
<span class="p_add">+	long o, n, t, z;</span>
<span class="p_add">+</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		z = 0;  /* overflow to zone counters */</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * The fetching of the stat_threshold is racy. We may apply</span>
<span class="p_add">+		 * a counter threshold to the wrong the cpu if we get</span>
<span class="p_add">+		 * rescheduled while executing here. However, the next</span>
<span class="p_add">+		 * counter update will apply the threshold again and</span>
<span class="p_add">+		 * therefore bring the counter under the threshold again.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * Most of the time the thresholds are the same anyways</span>
<span class="p_add">+		 * for all cpus in a zone.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		t = this_cpu_read(pcp-&gt;stat_threshold);</span>
<span class="p_add">+</span>
<span class="p_add">+		o = this_cpu_read(*p);</span>
<span class="p_add">+		n = delta + o;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (n &gt; t || n &lt; -t) {</span>
<span class="p_add">+			int os = overstep_mode * (t &gt;&gt; 1) ;</span>
<span class="p_add">+</span>
<span class="p_add">+			/* Overflow must be added to zone counters */</span>
<span class="p_add">+			z = n + os;</span>
<span class="p_add">+			n = -os;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	} while (this_cpu_cmpxchg(*p, o, n) != o);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (z)</span>
<span class="p_add">+		node_page_state_add(z, pgdat, item);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void mod_node_page_state(struct pglist_data *pgdat, enum node_stat_item item,</span>
<span class="p_add">+					long delta)</span>
<span class="p_add">+{</span>
<span class="p_add">+	mod_node_state(pgdat, item, delta, 0);</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(mod_node_page_state);</span>
<span class="p_add">+</span>
<span class="p_add">+void inc_node_state(struct pglist_data *pgdat, enum node_stat_item item)</span>
<span class="p_add">+{</span>
<span class="p_add">+	mod_node_state(pgdat, item, 1, 1);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void inc_node_page_state(struct page *page, enum node_stat_item item)</span>
<span class="p_add">+{</span>
<span class="p_add">+	mod_node_state(page_zone(page)-&gt;zone_pgdat, item, 1, 1);</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(inc_node_page_state);</span>
<span class="p_add">+</span>
<span class="p_add">+void dec_node_page_state(struct page *page, enum node_stat_item item)</span>
<span class="p_add">+{</span>
<span class="p_add">+	mod_node_state(page_zone(page)-&gt;zone_pgdat, item, -1, -1);</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(dec_node_page_state);</span>
 #else
 /*
  * Use interrupt disable to serialize counter updates
<span class="p_chunk">@@ -436,21 +568,69 @@</span> <span class="p_context"> void dec_zone_page_state(struct page *page, enum zone_stat_item item)</span>
 	local_irq_restore(flags);
 }
 EXPORT_SYMBOL(dec_zone_page_state);
<span class="p_del">-#endif</span>
 
<span class="p_add">+void inc_node_state(struct pglist_data *pgdat, enum node_stat_item item)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+</span>
<span class="p_add">+	local_irq_save(flags);</span>
<span class="p_add">+	__inc_node_state(pgdat, item);</span>
<span class="p_add">+	local_irq_restore(flags);</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(inc_node_state);</span>
<span class="p_add">+</span>
<span class="p_add">+void mod_node_page_state(struct pglist_data *pgdat, enum node_stat_item item,</span>
<span class="p_add">+					long delta)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+</span>
<span class="p_add">+	local_irq_save(flags);</span>
<span class="p_add">+	__mod_node_page_state(pgdat, item, delta);</span>
<span class="p_add">+	local_irq_restore(flags);</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(mod_node_page_state);</span>
<span class="p_add">+</span>
<span class="p_add">+void inc_node_page_state(struct page *page, enum node_stat_item item)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+	struct pglist_data *pgdat;</span>
<span class="p_add">+</span>
<span class="p_add">+	pgdat = page_zone(page)-&gt;zone_pgdat;</span>
<span class="p_add">+	local_irq_save(flags);</span>
<span class="p_add">+	__inc_node_state(pgdat, item);</span>
<span class="p_add">+	local_irq_restore(flags);</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(inc_node_page_state);</span>
<span class="p_add">+</span>
<span class="p_add">+void dec_node_page_state(struct page *page, enum node_stat_item item)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+</span>
<span class="p_add">+	local_irq_save(flags);</span>
<span class="p_add">+	__dec_node_page_state(page, item);</span>
<span class="p_add">+	local_irq_restore(flags);</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(dec_node_page_state);</span>
<span class="p_add">+#endif</span>
 
 /*
  * Fold a differential into the global counters.
  * Returns the number of counters updated.
  */
<span class="p_del">-static int fold_diff(int *diff)</span>
<span class="p_add">+static int fold_diff(int *zone_diff, int *node_diff)</span>
 {
 	int i;
 	int changes = 0;
 
 	for (i = 0; i &lt; NR_VM_ZONE_STAT_ITEMS; i++)
<span class="p_del">-		if (diff[i]) {</span>
<span class="p_del">-			atomic_long_add(diff[i], &amp;vm_stat[i]);</span>
<span class="p_add">+		if (zone_diff[i]) {</span>
<span class="p_add">+			atomic_long_add(zone_diff[i], &amp;vm_zone_stat[i]);</span>
<span class="p_add">+			changes++;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i &lt; NR_VM_NODE_STAT_ITEMS; i++)</span>
<span class="p_add">+		if (node_diff[i]) {</span>
<span class="p_add">+			atomic_long_add(node_diff[i], &amp;vm_node_stat[i]);</span>
 			changes++;
 	}
 	return changes;
<span class="p_chunk">@@ -474,9 +654,11 @@</span> <span class="p_context"> static int fold_diff(int *diff)</span>
  */
 static int refresh_cpu_vm_stats(bool do_pagesets)
 {
<span class="p_add">+	struct pglist_data *pgdat;</span>
 	struct zone *zone;
 	int i;
<span class="p_del">-	int global_diff[NR_VM_ZONE_STAT_ITEMS] = { 0, };</span>
<span class="p_add">+	int global_zone_diff[NR_VM_ZONE_STAT_ITEMS] = { 0, };</span>
<span class="p_add">+	int global_node_diff[NR_VM_NODE_STAT_ITEMS] = { 0, };</span>
 	int changes = 0;
 
 	for_each_populated_zone(zone) {
<span class="p_chunk">@@ -489,7 +671,7 @@</span> <span class="p_context"> static int refresh_cpu_vm_stats(bool do_pagesets)</span>
 			if (v) {
 
 				atomic_long_add(v, &amp;zone-&gt;vm_stat[i]);
<span class="p_del">-				global_diff[i] += v;</span>
<span class="p_add">+				global_zone_diff[i] += v;</span>
 #ifdef CONFIG_NUMA
 				/* 3 seconds idle till flush */
 				__this_cpu_write(p-&gt;expire, 3);
<span class="p_chunk">@@ -528,7 +710,22 @@</span> <span class="p_context"> static int refresh_cpu_vm_stats(bool do_pagesets)</span>
 		}
 #endif
 	}
<span class="p_del">-	changes += fold_diff(global_diff);</span>
<span class="p_add">+</span>
<span class="p_add">+	for_each_online_pgdat(pgdat) {</span>
<span class="p_add">+		struct per_cpu_nodestat __percpu *p = pgdat-&gt;per_cpu_nodestats;</span>
<span class="p_add">+</span>
<span class="p_add">+		for (i = 0; i &lt; NR_VM_NODE_STAT_ITEMS; i++) {</span>
<span class="p_add">+			int v;</span>
<span class="p_add">+</span>
<span class="p_add">+			v = this_cpu_xchg(p-&gt;vm_node_stat_diff[i], 0);</span>
<span class="p_add">+			if (v) {</span>
<span class="p_add">+				atomic_long_add(v, &amp;pgdat-&gt;vm_stat[i]);</span>
<span class="p_add">+				global_node_diff[i] += v;</span>
<span class="p_add">+			}</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	changes += fold_diff(global_zone_diff, global_node_diff);</span>
 	return changes;
 }
 
<span class="p_chunk">@@ -539,9 +736,11 @@</span> <span class="p_context"> static int refresh_cpu_vm_stats(bool do_pagesets)</span>
  */
 void cpu_vm_stats_fold(int cpu)
 {
<span class="p_add">+	struct pglist_data *pgdat;</span>
 	struct zone *zone;
 	int i;
<span class="p_del">-	int global_diff[NR_VM_ZONE_STAT_ITEMS] = { 0, };</span>
<span class="p_add">+	int global_zone_diff[NR_VM_ZONE_STAT_ITEMS] = { 0, };</span>
<span class="p_add">+	int global_node_diff[NR_VM_NODE_STAT_ITEMS] = { 0, };</span>
 
 	for_each_populated_zone(zone) {
 		struct per_cpu_pageset *p;
<span class="p_chunk">@@ -555,11 +754,27 @@</span> <span class="p_context"> void cpu_vm_stats_fold(int cpu)</span>
 				v = p-&gt;vm_stat_diff[i];
 				p-&gt;vm_stat_diff[i] = 0;
 				atomic_long_add(v, &amp;zone-&gt;vm_stat[i]);
<span class="p_del">-				global_diff[i] += v;</span>
<span class="p_add">+				global_zone_diff[i] += v;</span>
 			}
 	}
 
<span class="p_del">-	fold_diff(global_diff);</span>
<span class="p_add">+	for_each_online_pgdat(pgdat) {</span>
<span class="p_add">+		struct per_cpu_nodestat *p;</span>
<span class="p_add">+</span>
<span class="p_add">+		p = per_cpu_ptr(pgdat-&gt;per_cpu_nodestats, cpu);</span>
<span class="p_add">+</span>
<span class="p_add">+		for (i = 0; i &lt; NR_VM_NODE_STAT_ITEMS; i++)</span>
<span class="p_add">+			if (p-&gt;vm_node_stat_diff[i]) {</span>
<span class="p_add">+				int v;</span>
<span class="p_add">+</span>
<span class="p_add">+				v = p-&gt;vm_node_stat_diff[i];</span>
<span class="p_add">+				p-&gt;vm_node_stat_diff[i] = 0;</span>
<span class="p_add">+				atomic_long_add(v, &amp;pgdat-&gt;vm_stat[i]);</span>
<span class="p_add">+				global_node_diff[i] += v;</span>
<span class="p_add">+			}</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	fold_diff(global_zone_diff, global_node_diff);</span>
 }
 
 /*
<span class="p_chunk">@@ -575,16 +790,19 @@</span> <span class="p_context"> void drain_zonestat(struct zone *zone, struct per_cpu_pageset *pset)</span>
 			int v = pset-&gt;vm_stat_diff[i];
 			pset-&gt;vm_stat_diff[i] = 0;
 			atomic_long_add(v, &amp;zone-&gt;vm_stat[i]);
<span class="p_del">-			atomic_long_add(v, &amp;vm_stat[i]);</span>
<span class="p_add">+			atomic_long_add(v, &amp;vm_zone_stat[i]);</span>
 		}
 }
 #endif
 
 #ifdef CONFIG_NUMA
 /*
<span class="p_del">- * Determine the per node value of a stat item.</span>
<span class="p_add">+ * Determine the per node value of a stat item. This function</span>
<span class="p_add">+ * is called frequently in a NUMA machine, so try to be as</span>
<span class="p_add">+ * frugal as possible.</span>
  */
<span class="p_del">-unsigned long node_page_state(int node, enum zone_stat_item item)</span>
<span class="p_add">+unsigned long sum_zone_node_page_state(int node,</span>
<span class="p_add">+				 enum zone_stat_item item)</span>
 {
 	struct zone *zones = NODE_DATA(node)-&gt;node_zones;
 	int i;
<span class="p_chunk">@@ -596,6 +814,19 @@</span> <span class="p_context"> unsigned long node_page_state(int node, enum zone_stat_item item)</span>
 	return count;
 }
 
<span class="p_add">+/*</span>
<span class="p_add">+ * Determine the per node value of a stat item.</span>
<span class="p_add">+ */</span>
<span class="p_add">+unsigned long node_page_state(struct pglist_data *pgdat,</span>
<span class="p_add">+				enum node_stat_item item)</span>
<span class="p_add">+{</span>
<span class="p_add">+	long x = atomic_long_read(&amp;pgdat-&gt;vm_stat[item]);</span>
<span class="p_add">+#ifdef CONFIG_SMP</span>
<span class="p_add">+	if (x &lt; 0)</span>
<span class="p_add">+		x = 0;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	return x;</span>
<span class="p_add">+}</span>
 #endif
 
 #ifdef CONFIG_COMPACTION
<span class="p_chunk">@@ -1295,6 +1526,7 @@</span> <span class="p_context"> static void *vmstat_start(struct seq_file *m, loff_t *pos)</span>
 	if (*pos &gt;= ARRAY_SIZE(vmstat_text))
 		return NULL;
 	stat_items_size = NR_VM_ZONE_STAT_ITEMS * sizeof(unsigned long) +
<span class="p_add">+			  NR_VM_NODE_STAT_ITEMS * sizeof(unsigned long) +</span>
 			  NR_VM_WRITEBACK_STAT_ITEMS * sizeof(unsigned long);
 
 #ifdef CONFIG_VM_EVENT_COUNTERS
<span class="p_chunk">@@ -1309,6 +1541,10 @@</span> <span class="p_context"> static void *vmstat_start(struct seq_file *m, loff_t *pos)</span>
 		v[i] = global_page_state(i);
 	v += NR_VM_ZONE_STAT_ITEMS;
 
<span class="p_add">+	for (i = 0; i &lt; NR_VM_NODE_STAT_ITEMS; i++)</span>
<span class="p_add">+		v[i] = global_node_page_state(i);</span>
<span class="p_add">+	v += NR_VM_NODE_STAT_ITEMS;</span>
<span class="p_add">+</span>
 	global_dirty_limits(v + NR_DIRTY_BG_THRESHOLD,
 			    v + NR_DIRTY_THRESHOLD);
 	v += NR_VM_WRITEBACK_STAT_ITEMS;
<span class="p_chunk">@@ -1398,7 +1634,7 @@</span> <span class="p_context"> int vmstat_refresh(struct ctl_table *table, int write,</span>
 	if (err)
 		return err;
 	for (i = 0; i &lt; NR_VM_ZONE_STAT_ITEMS; i++) {
<span class="p_del">-		val = atomic_long_read(&amp;vm_stat[i]);</span>
<span class="p_add">+		val = atomic_long_read(&amp;vm_zone_stat[i]);</span>
 		if (val &lt; 0) {
 			switch (i) {
 			case NR_ALLOC_BATCH:
<span class="p_header">diff --git a/mm/workingset.c b/mm/workingset.c</span>
<span class="p_header">index 8a75f8d2916a..ac36efa8c754 100644</span>
<span class="p_header">--- a/mm/workingset.c</span>
<span class="p_header">+++ b/mm/workingset.c</span>
<span class="p_chunk">@@ -349,12 +349,13 @@</span> <span class="p_context"> static unsigned long count_shadow_nodes(struct shrinker *shrinker,</span>
 	shadow_nodes = list_lru_shrink_count(&amp;workingset_shadow_nodes, sc);
 	local_irq_enable();
 
<span class="p_del">-	if (memcg_kmem_enabled())</span>
<span class="p_add">+	if (memcg_kmem_enabled()) {</span>
 		pages = mem_cgroup_node_nr_lru_pages(sc-&gt;memcg, sc-&gt;nid,
 						     LRU_ALL_FILE);
<span class="p_del">-	else</span>
<span class="p_del">-		pages = node_page_state(sc-&gt;nid, NR_ACTIVE_FILE) +</span>
<span class="p_del">-			node_page_state(sc-&gt;nid, NR_INACTIVE_FILE);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		pages = sum_zone_node_page_state(sc-&gt;nid, NR_ACTIVE_FILE) +</span>
<span class="p_add">+			sum_zone_node_page_state(sc-&gt;nid, NR_INACTIVE_FILE);</span>
<span class="p_add">+	}</span>
 
 	/*
 	 * Active cache pages are limited to 50% of memory, and shadow

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



