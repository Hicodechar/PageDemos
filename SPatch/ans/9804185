
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[4/5] x86/mm: Prepare to expose larger address space to userspace - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [4/5] x86/mm: Prepare to expose larger address space to userspace</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=40781">Kirill A. Shutemov</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>June 22, 2017, 12:26 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20170622122608.80435-5-kirill.shutemov@linux.intel.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9804185/mbox/"
   >mbox</a>
|
   <a href="/patch/9804185/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9804185/">/patch/9804185/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	39B4A60386 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 22 Jun 2017 12:26:28 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 2B108223B2
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 22 Jun 2017 12:26:28 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 20010285E5; Thu, 22 Jun 2017 12:26:28 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 3F723223B2
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 22 Jun 2017 12:26:27 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1753261AbdFVM0T (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 22 Jun 2017 08:26:19 -0400
Received: from mga03.intel.com ([134.134.136.65]:8550 &quot;EHLO mga03.intel.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1752059AbdFVM0R (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 22 Jun 2017 08:26:17 -0400
Received: from orsmga001.jf.intel.com ([10.7.209.18])
	by orsmga103.jf.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384;
	22 Jun 2017 05:26:16 -0700
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i=&quot;5.39,373,1493708400&quot;; d=&quot;scan&#39;208&quot;;a=&quot;1143780680&quot;
Received: from black.fi.intel.com ([10.237.72.28])
	by orsmga001.jf.intel.com with ESMTP; 22 Jun 2017 05:26:12 -0700
Received: by black.fi.intel.com (Postfix, from userid 1000)
	id C1FA21B2; Thu, 22 Jun 2017 15:26:11 +0300 (EEST)
From: &quot;Kirill A. Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt;
To: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;, x86@kernel.org,
	Thomas Gleixner &lt;tglx@linutronix.de&gt;, Ingo Molnar &lt;mingo@redhat.com&gt;,
	&quot;H. Peter Anvin&quot; &lt;hpa@zytor.com&gt;
Cc: Andi Kleen &lt;ak@linux.intel.com&gt;, Dave Hansen &lt;dave.hansen@intel.com&gt;,
	Andy Lutomirski &lt;luto@amacapital.net&gt;,
	linux-arch@vger.kernel.org, linux-mm@kvack.org,
	linux-kernel@vger.kernel.org,
	&quot;Kirill A. Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt;
Subject: [PATCH 4/5] x86/mm: Prepare to expose larger address space to
	userspace
Date: Thu, 22 Jun 2017 15:26:07 +0300
Message-Id: &lt;20170622122608.80435-5-kirill.shutemov@linux.intel.com&gt;
X-Mailer: git-send-email 2.11.0
In-Reply-To: &lt;20170622122608.80435-1-kirill.shutemov@linux.intel.com&gt;
References: &lt;20170622122608.80435-1-kirill.shutemov@linux.intel.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=40781">Kirill A. Shutemov</a> - June 22, 2017, 12:26 p.m.</div>
<pre class="content">
On x86, 5-level paging enables 56-bit userspace virtual address space.
Not all user space is ready to handle wide addresses. It&#39;s known that
at least some JIT compilers use higher bits in pointers to encode their
information. It collides with valid pointers with 5-level paging and
leads to crashes.

To mitigate this, we are not going to allocate virtual address space
above 47-bit by default.

But userspace can ask for allocation from full address space by
specifying hint address (with or without MAP_FIXED) above 47-bits.

If hint address set above 47-bit, but MAP_FIXED is not specified, we try
to look for unmapped area by specified address. If it&#39;s already
occupied, we look for unmapped area in *full* address space, rather than
from 47-bit window.

A high hint address would only affect the allocation in question, but not
any future mmap()s.

Specifying high hint address on older kernel or on machine without 5-level
paging support is safe. The hint will be ignored and kernel will fall back
to allocation from 47-bit address space.

This approach helps to easily make application&#39;s memory allocator aware
about large address space without manually tracking allocated virtual
address space.

The patch puts all machinery in place, but not yet allows userspace to have
mappings above 47-bit -- TASK_SIZE_MAX has to be raised to get the effect.
<span class="signed-off-by">
Signed-off-by: Kirill A. Shutemov &lt;kirill.shutemov@linux.intel.com&gt;</span>
---
 arch/x86/include/asm/elf.h       |  4 ++--
 arch/x86/include/asm/processor.h |  7 +++++--
 arch/x86/kernel/sys_x86_64.c     | 21 +++++++++++++++++----
 arch/x86/mm/hugetlbpage.c        | 21 +++++++++++++++++----
 arch/x86/mm/mmap.c               |  6 +++---
 5 files changed, 44 insertions(+), 15 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/include/asm/elf.h b/arch/x86/include/asm/elf.h</span>
<span class="p_header">index e68e9ff588c8..7bcd15827a5b 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/elf.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/elf.h</span>
<span class="p_chunk">@@ -250,7 +250,7 @@</span> <span class="p_context"> extern int force_personality32;</span>
    the loader.  We need to make sure that it is out of the way of the program
    that it will &quot;exec&quot;, and that there is sufficient room for the brk.  */
 
<span class="p_del">-#define ELF_ET_DYN_BASE		(TASK_SIZE / 3 * 2)</span>
<span class="p_add">+#define ELF_ET_DYN_BASE		(TASK_SIZE_LOW / 3 * 2)</span>
 
 /* This yields a mask that user programs can use to figure out what
    instruction set this CPU supports.  This could be done in user space,
<span class="p_chunk">@@ -304,7 +304,7 @@</span> <span class="p_context"> static inline int mmap_is_ia32(void)</span>
 }
 
 extern unsigned long task_size_32bit(void);
<span class="p_del">-extern unsigned long task_size_64bit(void);</span>
<span class="p_add">+extern unsigned long task_size_64bit(int full_addr_space);</span>
 extern unsigned long get_mmap_base(int is_legacy);
 
 #ifdef CONFIG_X86_32
<span class="p_header">diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h</span>
<span class="p_header">index 97e9cada4945..42b87689ecd7 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/processor.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/processor.h</span>
<span class="p_chunk">@@ -802,6 +802,7 @@</span> <span class="p_context"> static inline void spin_lock_prefetch(const void *x)</span>
  */
 #define IA32_PAGE_OFFSET	PAGE_OFFSET
 #define TASK_SIZE		PAGE_OFFSET
<span class="p_add">+#define TASK_SIZE_LOW		TASK_SIZE</span>
 #define TASK_SIZE_MAX		TASK_SIZE
 #define DEFAULT_MAP_WINDOW	TASK_SIZE
 #define STACK_TOP		TASK_SIZE
<span class="p_chunk">@@ -853,12 +854,14 @@</span> <span class="p_context"> static inline void spin_lock_prefetch(const void *x)</span>
 #define IA32_PAGE_OFFSET	((current-&gt;personality &amp; ADDR_LIMIT_3GB) ? \
 					0xc0000000 : 0xFFFFe000)
 
<span class="p_add">+#define TASK_SIZE_LOW		(test_thread_flag(TIF_ADDR32) ? \</span>
<span class="p_add">+					IA32_PAGE_OFFSET : DEFAULT_MAP_WINDOW)</span>
 #define TASK_SIZE		(test_thread_flag(TIF_ADDR32) ? \
 					IA32_PAGE_OFFSET : TASK_SIZE_MAX)
 #define TASK_SIZE_OF(child)	((test_tsk_thread_flag(child, TIF_ADDR32)) ? \
 					IA32_PAGE_OFFSET : TASK_SIZE_MAX)
 
<span class="p_del">-#define STACK_TOP		TASK_SIZE</span>
<span class="p_add">+#define STACK_TOP		TASK_SIZE_LOW</span>
 #define STACK_TOP_MAX		TASK_SIZE_MAX
 
 #define INIT_THREAD  {						\
<span class="p_chunk">@@ -881,7 +884,7 @@</span> <span class="p_context"> extern void start_thread(struct pt_regs *regs, unsigned long new_ip,</span>
  * space during mmap&#39;s.
  */
 #define __TASK_UNMAPPED_BASE(task_size)	(PAGE_ALIGN(task_size / 3))
<span class="p_del">-#define TASK_UNMAPPED_BASE		__TASK_UNMAPPED_BASE(TASK_SIZE)</span>
<span class="p_add">+#define TASK_UNMAPPED_BASE		__TASK_UNMAPPED_BASE(TASK_SIZE_LOW)</span>
 
 #define KSTK_EIP(task)		(task_pt_regs(task)-&gt;ip)
 
<span class="p_header">diff --git a/arch/x86/kernel/sys_x86_64.c b/arch/x86/kernel/sys_x86_64.c</span>
<span class="p_header">index f840e895d871..73e4d28112f8 100644</span>
<span class="p_header">--- a/arch/x86/kernel/sys_x86_64.c</span>
<span class="p_header">+++ b/arch/x86/kernel/sys_x86_64.c</span>
<span class="p_chunk">@@ -101,8 +101,8 @@</span> <span class="p_context"> SYSCALL_DEFINE6(mmap, unsigned long, addr, unsigned long, len,</span>
 	return error;
 }
 
<span class="p_del">-static void find_start_end(unsigned long flags, unsigned long *begin,</span>
<span class="p_del">-			   unsigned long *end)</span>
<span class="p_add">+static void find_start_end(unsigned long addr, unsigned long flags,</span>
<span class="p_add">+		unsigned long *begin, unsigned long *end)</span>
 {
 	if (!in_compat_syscall() &amp;&amp; (flags &amp; MAP_32BIT)) {
 		/* This is usually used needed to map code in small
<span class="p_chunk">@@ -121,7 +121,10 @@</span> <span class="p_context"> static void find_start_end(unsigned long flags, unsigned long *begin,</span>
 	}
 
 	*begin	= get_mmap_base(1);
<span class="p_del">-	*end	= in_compat_syscall() ? task_size_32bit() : task_size_64bit();</span>
<span class="p_add">+	if (in_compat_syscall())</span>
<span class="p_add">+		*end = task_size_32bit();</span>
<span class="p_add">+	else</span>
<span class="p_add">+		*end = task_size_64bit(addr &gt; DEFAULT_MAP_WINDOW);</span>
 }
 
 unsigned long
<span class="p_chunk">@@ -140,7 +143,7 @@</span> <span class="p_context"> arch_get_unmapped_area(struct file *filp, unsigned long addr,</span>
 	if (flags &amp; MAP_FIXED)
 		return addr;
 
<span class="p_del">-	find_start_end(flags, &amp;begin, &amp;end);</span>
<span class="p_add">+	find_start_end(addr, flags, &amp;begin, &amp;end);</span>
 
 	if (len &gt; end)
 		return -ENOMEM;
<span class="p_chunk">@@ -204,6 +207,16 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
 	info.length = len;
 	info.low_limit = PAGE_SIZE;
 	info.high_limit = get_mmap_base(0);
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If hint address is above DEFAULT_MAP_WINDOW, look for unmapped area</span>
<span class="p_add">+	 * in the full address space.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * !in_compat_syscall() check to avoid high addresses for x32.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (addr &gt; DEFAULT_MAP_WINDOW &amp;&amp; !in_compat_syscall())</span>
<span class="p_add">+		info.high_limit += TASK_SIZE_MAX - DEFAULT_MAP_WINDOW;</span>
<span class="p_add">+</span>
 	info.align_mask = 0;
 	info.align_offset = pgoff &lt;&lt; PAGE_SHIFT;
 	if (filp) {
<span class="p_header">diff --git a/arch/x86/mm/hugetlbpage.c b/arch/x86/mm/hugetlbpage.c</span>
<span class="p_header">index afd5f2152300..6aa0a679e3f3 100644</span>
<span class="p_header">--- a/arch/x86/mm/hugetlbpage.c</span>
<span class="p_header">+++ b/arch/x86/mm/hugetlbpage.c</span>
<span class="p_chunk">@@ -86,25 +86,38 @@</span> <span class="p_context"> static unsigned long hugetlb_get_unmapped_area_bottomup(struct file *file,</span>
 	info.flags = 0;
 	info.length = len;
 	info.low_limit = get_mmap_base(1);
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If hint address is above DEFAULT_MAP_WINDOW, look for unmapped area</span>
<span class="p_add">+	 * in the full address space.</span>
<span class="p_add">+	 */</span>
 	info.high_limit = in_compat_syscall() ?
<span class="p_del">-		task_size_32bit() : task_size_64bit();</span>
<span class="p_add">+		task_size_32bit() : task_size_64bit(addr &gt; DEFAULT_MAP_WINDOW);</span>
<span class="p_add">+</span>
 	info.align_mask = PAGE_MASK &amp; ~huge_page_mask(h);
 	info.align_offset = 0;
 	return vm_unmapped_area(&amp;info);
 }
 
 static unsigned long hugetlb_get_unmapped_area_topdown(struct file *file,
<span class="p_del">-		unsigned long addr0, unsigned long len,</span>
<span class="p_add">+		unsigned long addr, unsigned long len,</span>
 		unsigned long pgoff, unsigned long flags)
 {
 	struct hstate *h = hstate_file(file);
 	struct vm_unmapped_area_info info;
<span class="p_del">-	unsigned long addr;</span>
 
 	info.flags = VM_UNMAPPED_AREA_TOPDOWN;
 	info.length = len;
 	info.low_limit = PAGE_SIZE;
 	info.high_limit = get_mmap_base(0);
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If hint address is above DEFAULT_MAP_WINDOW, look for unmapped area</span>
<span class="p_add">+	 * in the full address space.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (addr &gt; DEFAULT_MAP_WINDOW &amp;&amp; !in_compat_syscall())</span>
<span class="p_add">+		info.high_limit += TASK_SIZE_MAX - DEFAULT_MAP_WINDOW;</span>
<span class="p_add">+</span>
 	info.align_mask = PAGE_MASK &amp; ~huge_page_mask(h);
 	info.align_offset = 0;
 	addr = vm_unmapped_area(&amp;info);
<span class="p_chunk">@@ -119,7 +132,7 @@</span> <span class="p_context"> static unsigned long hugetlb_get_unmapped_area_topdown(struct file *file,</span>
 		VM_BUG_ON(addr != -ENOMEM);
 		info.flags = 0;
 		info.low_limit = TASK_UNMAPPED_BASE;
<span class="p_del">-		info.high_limit = TASK_SIZE;</span>
<span class="p_add">+		info.high_limit = TASK_SIZE_LOW;</span>
 		addr = vm_unmapped_area(&amp;info);
 	}
 
<span class="p_header">diff --git a/arch/x86/mm/mmap.c b/arch/x86/mm/mmap.c</span>
<span class="p_header">index b99c1a29dcca..4e5a0449da1c 100644</span>
<span class="p_header">--- a/arch/x86/mm/mmap.c</span>
<span class="p_header">+++ b/arch/x86/mm/mmap.c</span>
<span class="p_chunk">@@ -42,9 +42,9 @@</span> <span class="p_context"> unsigned long task_size_32bit(void)</span>
 	return IA32_PAGE_OFFSET;
 }
 
<span class="p_del">-unsigned long task_size_64bit(void)</span>
<span class="p_add">+unsigned long task_size_64bit(int full_addr_space)</span>
 {
<span class="p_del">-	return TASK_SIZE_MAX;</span>
<span class="p_add">+	return full_addr_space ? TASK_SIZE_MAX : DEFAULT_MAP_WINDOW;</span>
 }
 
 static unsigned long stack_maxrandom_size(unsigned long task_size)
<span class="p_chunk">@@ -140,7 +140,7 @@</span> <span class="p_context"> void arch_pick_mmap_layout(struct mm_struct *mm)</span>
 		mm-&gt;get_unmapped_area = arch_get_unmapped_area_topdown;
 
 	arch_pick_mmap_base(&amp;mm-&gt;mmap_base, &amp;mm-&gt;mmap_legacy_base,
<span class="p_del">-			arch_rnd(mmap64_rnd_bits), task_size_64bit());</span>
<span class="p_add">+			arch_rnd(mmap64_rnd_bits), task_size_64bit(0));</span>
 
 #ifdef CONFIG_HAVE_ARCH_COMPAT_MMAP_BASES
 	/*

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



