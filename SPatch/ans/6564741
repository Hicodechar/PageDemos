
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[3/3] mm: Defer flush of writable TLB entries - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [3/3] mm: Defer flush of writable TLB entries</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=24451">Mel Gorman</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>June 8, 2015, 12:50 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1433767854-24408-4-git-send-email-mgorman@suse.de&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/6564741/mbox/"
   >mbox</a>
|
   <a href="/patch/6564741/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/6564741/">/patch/6564741/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork2.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork2.web.kernel.org (Postfix) with ESMTP id D376DC0020
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  8 Jun 2015 12:51:33 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id D325020531
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  8 Jun 2015 12:51:32 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id 146A120525
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  8 Jun 2015 12:51:31 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1753055AbbFHMvX (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 8 Jun 2015 08:51:23 -0400
Received: from cantor2.suse.de ([195.135.220.15]:46134 &quot;EHLO mx2.suse.de&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1752331AbbFHMvB (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 8 Jun 2015 08:51:01 -0400
X-Virus-Scanned: by amavisd-new at test-mx.suse.de
Received: from relay2.suse.de (charybdis-ext.suse.de [195.135.220.254])
	by mx2.suse.de (Postfix) with ESMTP id 34126AC8C;
	Mon,  8 Jun 2015 12:51:00 +0000 (UTC)
From: Mel Gorman &lt;mgorman@suse.de&gt;
To: Andrew Morton &lt;akpm@linux-foundation.org&gt;
Cc: Rik van Riel &lt;riel@redhat.com&gt;, Hugh Dickins &lt;hughd@google.com&gt;,
	Minchan Kim &lt;minchan@kernel.org&gt;, Dave Hansen &lt;dave.hansen@intel.com&gt;,
	Andi Kleen &lt;andi@firstfloor.org&gt;, H Peter Anvin &lt;hpa@zytor.com&gt;,
	Ingo Molnar &lt;mingo@kernel.org&gt;, Linux-MM &lt;linux-mm@kvack.org&gt;,
	LKML &lt;linux-kernel@vger.kernel.org&gt;, Mel Gorman &lt;mgorman@suse.de&gt;
Subject: [PATCH 3/3] mm: Defer flush of writable TLB entries
Date: Mon,  8 Jun 2015 13:50:54 +0100
Message-Id: &lt;1433767854-24408-4-git-send-email-mgorman@suse.de&gt;
X-Mailer: git-send-email 2.3.5
In-Reply-To: &lt;1433767854-24408-1-git-send-email-mgorman@suse.de&gt;
References: &lt;1433767854-24408-1-git-send-email-mgorman@suse.de&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-6.9 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	T_RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=24451">Mel Gorman</a> - June 8, 2015, 12:50 p.m.</div>
<pre class="content">
If a PTE is unmapped and it&#39;s dirty then it was writable recently. Due
to deferred TLB flushing, it&#39;s best to assume a writable TLB cache entry
exists. With that assumption, the TLB must be flushed before any IO can
start or the page is freed to avoid lost writes or data corruption. This
patch defers flushing of potentially writable TLBs as long as possible.
<span class="signed-off-by">
Signed-off-by: Mel Gorman &lt;mgorman@suse.de&gt;</span>
---
 include/linux/sched.h |  1 +
 mm/internal.h         |  4 ++++
 mm/rmap.c             | 28 +++++++++++++++++++++-------
 mm/vmscan.c           |  7 ++++++-
 4 files changed, 32 insertions(+), 8 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/include/linux/sched.h b/include/linux/sched.h</span>
<span class="p_header">index 57ff61b16565..827d9b123bd5 100644</span>
<span class="p_header">--- a/include/linux/sched.h</span>
<span class="p_header">+++ b/include/linux/sched.h</span>
<span class="p_chunk">@@ -1296,6 +1296,7 @@</span> <span class="p_context"> enum perf_event_task_context {</span>
 struct tlbflush_unmap_batch {
 	struct cpumask cpumask;
 	unsigned long nr_pages;
<span class="p_add">+	bool writable;</span>
 	unsigned long pfns[BATCH_TLBFLUSH_SIZE];
 };
 
<span class="p_header">diff --git a/mm/internal.h b/mm/internal.h</span>
<span class="p_header">index 8cbb68ccc731..ecf47a01420d 100644</span>
<span class="p_header">--- a/mm/internal.h</span>
<span class="p_header">+++ b/mm/internal.h</span>
<span class="p_chunk">@@ -438,10 +438,14 @@</span> <span class="p_context"> struct tlbflush_unmap_batch;</span>
 
 #ifdef CONFIG_ARCH_SUPPORTS_LOCAL_TLB_PFN_FLUSH
 void try_to_unmap_flush(void);
<span class="p_add">+void try_to_unmap_flush_dirty(void);</span>
 #else
 static inline void try_to_unmap_flush(void)
 {
 }
<span class="p_add">+static inline void try_to_unmap_flush_dirty(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
 
 #endif /* CONFIG_ARCH_SUPPORTS_LOCAL_TLB_PFN_FLUSH */
 #endif	/* __MM_INTERNAL_H */
<span class="p_header">diff --git a/mm/rmap.c b/mm/rmap.c</span>
<span class="p_header">index a8dbba62398a..3c8ebacfe6ef 100644</span>
<span class="p_header">--- a/mm/rmap.c</span>
<span class="p_header">+++ b/mm/rmap.c</span>
<span class="p_chunk">@@ -621,11 +621,21 @@</span> <span class="p_context"> void try_to_unmap_flush(void)</span>
 	}
 	cpumask_clear(&amp;tlb_ubc-&gt;cpumask);
 	tlb_ubc-&gt;nr_pages = 0;
<span class="p_add">+	tlb_ubc-&gt;writable = false;</span>
 	preempt_enable();
 }
 
<span class="p_add">+/* Flush iff there are potentially writable TLB entries that can race with IO */</span>
<span class="p_add">+void try_to_unmap_flush_dirty(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct tlbflush_unmap_batch *tlb_ubc = current-&gt;tlb_ubc;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (tlb_ubc &amp;&amp; tlb_ubc-&gt;writable)</span>
<span class="p_add">+		try_to_unmap_flush();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static void set_tlb_ubc_flush_pending(struct mm_struct *mm,
<span class="p_del">-		struct page *page)</span>
<span class="p_add">+		struct page *page, bool writable)</span>
 {
 	struct tlbflush_unmap_batch *tlb_ubc = current-&gt;tlb_ubc;
 
<span class="p_chunk">@@ -633,6 +643,14 @@</span> <span class="p_context"> static void set_tlb_ubc_flush_pending(struct mm_struct *mm,</span>
 	tlb_ubc-&gt;pfns[tlb_ubc-&gt;nr_pages] = page_to_pfn(page);
 	tlb_ubc-&gt;nr_pages++;
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If the PTE was dirty then it&#39;s best to assume it&#39;s writable. The</span>
<span class="p_add">+	 * caller must use try_to_unmap_flush_dirty() or try_to_unmap_flush()</span>
<span class="p_add">+	 * before the page any IO is initiated.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (writable)</span>
<span class="p_add">+		tlb_ubc-&gt;writable = true;</span>
<span class="p_add">+</span>
 	if (tlb_ubc-&gt;nr_pages == BATCH_TLBFLUSH_SIZE)
 		try_to_unmap_flush();
 }
<span class="p_chunk">@@ -657,7 +675,7 @@</span> <span class="p_context"> static bool should_defer_flush(struct mm_struct *mm, enum ttu_flags flags)</span>
 }
 #else
 static void set_tlb_ubc_flush_pending(struct mm_struct *mm,
<span class="p_del">-		struct page *page)</span>
<span class="p_add">+		struct page *page, bool writable)</span>
 {
 }
 
<span class="p_chunk">@@ -1309,11 +1327,7 @@</span> <span class="p_context"> static int try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
 		 */
 		pteval = ptep_get_and_clear(mm, address, pte);
 
<span class="p_del">-		/* Potentially writable TLBs must be flushed before IO */</span>
<span class="p_del">-		if (pte_dirty(pteval))</span>
<span class="p_del">-			flush_tlb_page(vma, address);</span>
<span class="p_del">-		else</span>
<span class="p_del">-			set_tlb_ubc_flush_pending(mm, page);</span>
<span class="p_add">+		set_tlb_ubc_flush_pending(mm, page, pte_dirty(pteval));</span>
 	} else {
 		pteval = ptep_clear_flush(vma, address, pte);
 	}
<span class="p_header">diff --git a/mm/vmscan.c b/mm/vmscan.c</span>
<span class="p_header">index 5121742ccb87..0055224c52d4 100644</span>
<span class="p_header">--- a/mm/vmscan.c</span>
<span class="p_header">+++ b/mm/vmscan.c</span>
<span class="p_chunk">@@ -1065,7 +1065,12 @@</span> <span class="p_context"> static unsigned long shrink_page_list(struct list_head *page_list,</span>
 			if (!sc-&gt;may_writepage)
 				goto keep_locked;
 
<span class="p_del">-			/* Page is dirty, try to write it out here */</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * Page is dirty. Flush the TLB if a writable entry</span>
<span class="p_add">+			 * potentially exists to avoid CPU writes after IO</span>
<span class="p_add">+			 * starts and then write it out here</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			try_to_unmap_flush_dirty();</span>
 			switch (pageout(page, mapping, sc)) {
 			case PAGE_KEEP:
 				goto keep_locked;

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



