
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v3,3/6] iommu: add ARM short descriptor page table allocator. - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v3,3/6] iommu: add ARM short descriptor page table allocator.</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=123111">Yong Wu</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>July 16, 2015, 9:04 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1437037475-9065-4-git-send-email-yong.wu@mediatek.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/6805341/mbox/"
   >mbox</a>
|
   <a href="/patch/6805341/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/6805341/">/patch/6805341/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork2.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork2.web.kernel.org (Postfix) with ESMTP id ECB6EC05AC
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 16 Jul 2015 09:05:59 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 09DCD2077A
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 16 Jul 2015 09:05:58 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id C22FD20726
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 16 Jul 2015 09:05:54 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S932098AbbGPJFl (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 16 Jul 2015 05:05:41 -0400
Received: from mailgw01.mediatek.com ([210.61.82.183]:59940 &quot;EHLO
	mailgw01.mediatek.com&quot; rhost-flags-OK-FAIL-OK-FAIL) by
	vger.kernel.org with ESMTP id S1754548AbbGPJFc (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 16 Jul 2015 05:05:32 -0400
X-Listener-Flag: 11101
Received: from mtkhts07.mediatek.inc [(172.21.101.69)] by
	mailgw01.mediatek.com (envelope-from &lt;yong.wu@mediatek.com&gt;)
	(mhqrelay.mediatek.com ESMTP with TLS)
	with ESMTP id 1382822416; Thu, 16 Jul 2015 17:05:30 +0800
Received: from mhfsdcap03.localdomain (10.17.3.153) by mtkhts07.mediatek.inc
	(172.21.101.73) with Microsoft SMTP Server id 14.3.181.6;
	Thu, 16 Jul 2015 17:05:28 +0800
From: Yong Wu &lt;yong.wu@mediatek.com&gt;
To: Joerg Roedel &lt;joro@8bytes.org&gt;, Thierry Reding &lt;treding@nvidia.com&gt;,
	Mark Rutland &lt;mark.rutland@arm.com&gt;,
	Matthias Brugger &lt;matthias.bgg@gmail.com&gt;
CC: Robin Murphy &lt;robin.murphy@arm.com&gt;, Will Deacon &lt;will.deacon@arm.com&gt;,
	Daniel Kurtz &lt;djkurtz@google.com&gt;, Tomasz Figa &lt;tfiga@google.com&gt;,
	Lucas Stach &lt;l.stach@pengutronix.de&gt;, Rob Herring &lt;robh+dt@kernel.org&gt;,
	Catalin Marinas &lt;catalin.marinas@arm.com&gt;,
	&lt;linux-mediatek@lists.infradead.org&gt;,
	Sasha Hauer &lt;kernel@pengutronix.de&gt;,
	&lt;srv_heupstream@mediatek.com&gt;, &lt;devicetree@vger.kernel.org&gt;,
	&lt;linux-kernel@vger.kernel.org&gt;, &lt;linux-arm-kernel@lists.infradead.org&gt;,
	&lt;iommu@lists.linux-foundation.org&gt;, &lt;pebolle@tiscali.nl&gt;,
	&lt;arnd@arndb.de&gt;, &lt;mitchelh@codeaurora.org&gt;,
	&lt;cloud.chou@mediatek.com&gt;, &lt;frederic.chen@mediatek.com&gt;,
	&lt;yong.wu@mediatek.com&gt;
Subject: [PATCH v3 3/6] iommu: add ARM short descriptor page table allocator.
Date: Thu, 16 Jul 2015 17:04:32 +0800
Message-ID: &lt;1437037475-9065-4-git-send-email-yong.wu@mediatek.com&gt;
X-Mailer: git-send-email 1.7.9.5
In-Reply-To: &lt;1437037475-9065-1-git-send-email-yong.wu@mediatek.com&gt;
References: &lt;1437037475-9065-1-git-send-email-yong.wu@mediatek.com&gt;
MIME-Version: 1.0
Content-Type: text/plain
X-MTK: N
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-8.2 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=123111">Yong Wu</a> - July 16, 2015, 9:04 a.m.</div>
<pre class="content">
This patch is for ARM Short Descriptor Format.
<span class="signed-off-by">
Signed-off-by: Yong Wu &lt;yong.wu@mediatek.com&gt;</span>
---
 drivers/iommu/Kconfig                |   18 +
 drivers/iommu/Makefile               |    1 +
 drivers/iommu/io-pgtable-arm-short.c |  742 ++++++++++++++++++++++++++++++++++
 drivers/iommu/io-pgtable-arm.c       |    3 -
 drivers/iommu/io-pgtable.c           |    4 +
 drivers/iommu/io-pgtable.h           |   13 +
 6 files changed, 778 insertions(+), 3 deletions(-)
 create mode 100644 drivers/iommu/io-pgtable-arm-short.c
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=7096">Will Deacon</a> - July 21, 2015, 5:11 p.m.</div>
<pre class="content">
Hello,

This is looking better, but I still have some concerns.

On Thu, Jul 16, 2015 at 10:04:32AM +0100, Yong Wu wrote:
<span class="quote">&gt; This patch is for ARM Short Descriptor Format.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Yong Wu &lt;yong.wu@mediatek.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  drivers/iommu/Kconfig                |   18 +</span>
<span class="quote">&gt;  drivers/iommu/Makefile               |    1 +</span>
<span class="quote">&gt;  drivers/iommu/io-pgtable-arm-short.c |  742 ++++++++++++++++++++++++++++++++++</span>
<span class="quote">&gt;  drivers/iommu/io-pgtable-arm.c       |    3 -</span>
<span class="quote">&gt;  drivers/iommu/io-pgtable.c           |    4 +</span>
<span class="quote">&gt;  drivers/iommu/io-pgtable.h           |   13 +</span>
<span class="quote">&gt;  6 files changed, 778 insertions(+), 3 deletions(-)</span>
<span class="quote">&gt;  create mode 100644 drivers/iommu/io-pgtable-arm-short.c</span>

[...]
<span class="quote">
&gt; +#define ARM_SHORT_PGDIR_SHIFT                  20</span>
<span class="quote">&gt; +#define ARM_SHORT_PAGE_SHIFT                   12</span>
<span class="quote">&gt; +#define ARM_SHORT_PTRS_PER_PTE                 \</span>
<span class="quote">&gt; +       (1 &lt;&lt; (ARM_SHORT_PGDIR_SHIFT - ARM_SHORT_PAGE_SHIFT))</span>
<span class="quote">&gt; +#define ARM_SHORT_BYTES_PER_PTE                        \</span>
<span class="quote">&gt; +       (ARM_SHORT_PTRS_PER_PTE * sizeof(arm_short_iopte))</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/* level 1 pagetable */</span>
<span class="quote">&gt; +#define ARM_SHORT_PGD_TYPE_PGTABLE             BIT(0)</span>
<span class="quote">&gt; +#define ARM_SHORT_PGD_SECTION_XN               (BIT(0) | BIT(4))</span>
<span class="quote">&gt; +#define ARM_SHORT_PGD_TYPE_SECTION             BIT(1)</span>
<span class="quote">&gt; +#define ARM_SHORT_PGD_PGTABLE_XN               BIT(2)</span>

This should be PXN, but I&#39;m not even sure why we care for a table at
level 1.
<span class="quote">
&gt; +#define ARM_SHORT_PGD_B                                BIT(2)</span>
<span class="quote">&gt; +#define ARM_SHORT_PGD_C                                BIT(3)</span>
<span class="quote">&gt; +#define ARM_SHORT_PGD_PGTABLE_NS               BIT(3)</span>
<span class="quote">&gt; +#define ARM_SHORT_PGD_IMPLE                    BIT(9)</span>
<span class="quote">&gt; +#define ARM_SHORT_PGD_TEX0                     BIT(12)</span>
<span class="quote">&gt; +#define ARM_SHORT_PGD_S                                BIT(16)</span>
<span class="quote">&gt; +#define ARM_SHORT_PGD_nG                       BIT(17)</span>
<span class="quote">&gt; +#define ARM_SHORT_PGD_SUPERSECTION             BIT(18)</span>
<span class="quote">&gt; +#define ARM_SHORT_PGD_SECTION_NS               BIT(19)</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define ARM_SHORT_PGD_TYPE_SUPERSECTION                \</span>
<span class="quote">&gt; +       (ARM_SHORT_PGD_TYPE_SECTION | ARM_SHORT_PGD_SUPERSECTION)</span>
<span class="quote">&gt; +#define ARM_SHORT_PGD_SECTION_TYPE_MSK         \</span>
<span class="quote">&gt; +       (ARM_SHORT_PGD_TYPE_SECTION | ARM_SHORT_PGD_SUPERSECTION)</span>
<span class="quote">&gt; +#define ARM_SHORT_PGD_PGTABLE_TYPE_MSK         \</span>
<span class="quote">&gt; +       (ARM_SHORT_PGD_TYPE_SECTION | ARM_SHORT_PGD_TYPE_PGTABLE)</span>
<span class="quote">&gt; +#define ARM_SHORT_PGD_TYPE_IS_PGTABLE(pgd)     \</span>
<span class="quote">&gt; +       (((pgd) &amp; ARM_SHORT_PGD_PGTABLE_TYPE_MSK) == ARM_SHORT_PGD_TYPE_PGTABLE)</span>
<span class="quote">&gt; +#define ARM_SHORT_PGD_TYPE_IS_SECTION(pgd)     \</span>
<span class="quote">&gt; +       (((pgd) &amp; ARM_SHORT_PGD_SECTION_TYPE_MSK) == ARM_SHORT_PGD_TYPE_SECTION)</span>
<span class="quote">&gt; +#define ARM_SHORT_PGD_TYPE_IS_SUPERSECTION(pgd)        \</span>
<span class="quote">&gt; +       (((pgd) &amp; ARM_SHORT_PGD_SECTION_TYPE_MSK) == \</span>
<span class="quote">&gt; +       ARM_SHORT_PGD_TYPE_SUPERSECTION)</span>
<span class="quote">&gt; +#define ARM_SHORT_PGD_PGTABLE_MSK              0xfffffc00</span>
<span class="quote">&gt; +#define ARM_SHORT_PGD_SECTION_MSK              (~(SZ_1M - 1))</span>
<span class="quote">&gt; +#define ARM_SHORT_PGD_SUPERSECTION_MSK         (~(SZ_16M - 1))</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/* level 2 pagetable */</span>
<span class="quote">&gt; +#define ARM_SHORT_PTE_TYPE_LARGE               BIT(0)</span>
<span class="quote">&gt; +#define ARM_SHORT_PTE_SMALL_XN                 BIT(0)</span>
<span class="quote">&gt; +#define ARM_SHORT_PTE_TYPE_SMALL               BIT(1)</span>
<span class="quote">&gt; +#define ARM_SHORT_PTE_B                                BIT(2)</span>
<span class="quote">&gt; +#define ARM_SHORT_PTE_C                                BIT(3)</span>
<span class="quote">&gt; +#define ARM_SHORT_PTE_SMALL_TEX0               BIT(6)</span>
<span class="quote">&gt; +#define ARM_SHORT_PTE_IMPLE                    BIT(9)</span>

This is AP[2] for small pages.
<span class="quote">
&gt; +#define ARM_SHORT_PTE_S                                BIT(10)</span>
<span class="quote">&gt; +#define ARM_SHORT_PTE_nG                       BIT(11)</span>
<span class="quote">&gt; +#define ARM_SHORT_PTE_LARGE_TEX0               BIT(12)</span>
<span class="quote">&gt; +#define ARM_SHORT_PTE_LARGE_XN                 BIT(15)</span>
<span class="quote">&gt; +#define ARM_SHORT_PTE_LARGE_MSK                        (~(SZ_64K - 1))</span>
<span class="quote">&gt; +#define ARM_SHORT_PTE_SMALL_MSK                        (~(SZ_4K - 1))</span>
<span class="quote">&gt; +#define ARM_SHORT_PTE_TYPE_MSK                 \</span>
<span class="quote">&gt; +       (ARM_SHORT_PTE_TYPE_LARGE | ARM_SHORT_PTE_TYPE_SMALL)</span>
<span class="quote">&gt; +#define ARM_SHORT_PTE_TYPE_IS_SMALLPAGE(pte)   \</span>
<span class="quote">&gt; +       (((((pte) &amp; ARM_SHORT_PTE_TYPE_MSK) &gt;&gt; 1) &lt;&lt; 1)\</span>
<span class="quote">&gt; +       == ARM_SHORT_PTE_TYPE_SMALL)</span>

I see what you&#39;re trying to do here, but the shifting is confusing. I
think it&#39;s better doing something like:

	((pte) &amp; ARM_SHORT_PTE_TYPE_SMALL) == ARM_SHORT_PTE_TYPE_SMALL

or even just:

	(pte) &amp; ARM_SHORT_PTE_TYPE_SMALL
<span class="quote">
&gt; +#define ARM_SHORT_PTE_TYPE_IS_LARGEPAGE(pte)   \</span>
<span class="quote">&gt; +       (((pte) &amp; ARM_SHORT_PTE_TYPE_MSK) == ARM_SHORT_PTE_TYPE_LARGE)</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define ARM_SHORT_PGD_IDX(a)                   ((a) &gt;&gt; ARM_SHORT_PGDIR_SHIFT)</span>
<span class="quote">&gt; +#define ARM_SHORT_PTE_IDX(a)                   \</span>
<span class="quote">&gt; +       (((a) &gt;&gt; ARM_SHORT_PAGE_SHIFT) &amp; (ARM_SHORT_PTRS_PER_PTE - 1))</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define ARM_SHORT_GET_PTE_VA(pgd)              \</span>
<span class="quote">&gt; +       (phys_to_virt((unsigned long)pgd &amp; ARM_SHORT_PGD_PGTABLE_MSK))</span>

Maybe better named as ARM_SHORT_GET_PGTABLE_VA ?
<span class="quote">
&gt; +</span>
<span class="quote">&gt; +#define ARM_SHORT_PTE_LARGE_GET_PROT(pte)      \</span>
<span class="quote">&gt; +       (((pte) &amp; (~ARM_SHORT_PTE_LARGE_MSK)) &amp; ~ARM_SHORT_PTE_TYPE_MSK)</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define ARM_SHORT_PGD_GET_PROT(pgd)            \</span>
<span class="quote">&gt; +       (((pgd) &amp; (~ARM_SHORT_PGD_SECTION_MSK)) &amp; ~ARM_SHORT_PGD_SUPERSECTION)</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static bool selftest_running;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static arm_short_iopte *</span>
<span class="quote">&gt; +arm_short_get_pte_in_pgd(arm_short_iopte pgd, unsigned int iova)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       arm_short_iopte *pte;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       pte = ARM_SHORT_GET_PTE_VA(pgd);</span>
<span class="quote">&gt; +       pte += ARM_SHORT_PTE_IDX(iova);</span>
<span class="quote">&gt; +       return pte;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static void _arm_short_free_pgtable(struct arm_short_io_pgtable *data,</span>
<span class="quote">&gt; +                                   arm_short_iopte *pgd)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       const struct iommu_gather_ops *tlb = data-&gt;iop.cfg.tlb;</span>
<span class="quote">&gt; +       arm_short_iopte *pte;</span>
<span class="quote">&gt; +       int i;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       pte = ARM_SHORT_GET_PTE_VA(*pgd);</span>
<span class="quote">&gt; +       for (i = 0; i &lt; ARM_SHORT_PTRS_PER_PTE; i++) {</span>
<span class="quote">&gt; +               if (pte[i] != 0)</span>
<span class="quote">&gt; +                       return;</span>
<span class="quote">&gt; +       }</span>

Do you actually need this loop if you&#39;re not warning or returning an error?
<span class="quote">
&gt; +</span>
<span class="quote">&gt; +       /* Free whole pte and set pgd to zero while all pte is unmap */</span>
<span class="quote">&gt; +       kmem_cache_free(data-&gt;ptekmem, pte);</span>
<span class="quote">&gt; +       *pgd = 0;</span>

I still don&#39;t think this is safe. What stops the page table walker from
speculatively walking freed memory?
<span class="quote">
&gt; +       tlb-&gt;flush_pgtable(pgd, sizeof(*pgd), data-&gt;iop.cookie);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static arm_short_iopte</span>
<span class="quote">&gt; +__arm_short_pte_prot(struct arm_short_io_pgtable *data, int prot, bool large)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       arm_short_iopte pteprot;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       pteprot = ARM_SHORT_PTE_S | ARM_SHORT_PTE_nG;</span>
<span class="quote">&gt; +       pteprot |= large ? ARM_SHORT_PTE_TYPE_LARGE :</span>
<span class="quote">&gt; +                               ARM_SHORT_PTE_TYPE_SMALL;</span>
<span class="quote">&gt; +       if (prot &amp; IOMMU_CACHE)</span>
<span class="quote">&gt; +               pteprot |=  ARM_SHORT_PTE_B | ARM_SHORT_PTE_C;</span>
<span class="quote">&gt; +       if (prot &amp; IOMMU_WRITE)</span>
<span class="quote">&gt; +               pteprot |= large ? ARM_SHORT_PTE_LARGE_TEX0 :</span>
<span class="quote">&gt; +                               ARM_SHORT_PTE_SMALL_TEX0;</span>

This doesn&#39;t make any sense. TEX[2:0] is all about memory attributes, not
permissions, so you&#39;re making the mapping write-back, write-allocate but
that&#39;s not what the IOMMU_* values are about.
<span class="quote">
&gt; +       if (prot &amp; IOMMU_NOEXEC)</span>
<span class="quote">&gt; +               pteprot |= large ? ARM_SHORT_PTE_LARGE_XN :</span>
<span class="quote">&gt; +                       ARM_SHORT_PTE_SMALL_XN;</span>
<span class="quote">&gt; +       return pteprot;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static arm_short_iopte</span>
<span class="quote">&gt; +__arm_short_pgd_prot(struct arm_short_io_pgtable *data, int prot, bool super)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       arm_short_iopte pgdprot;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       pgdprot = ARM_SHORT_PGD_S | ARM_SHORT_PGD_nG;</span>
<span class="quote">&gt; +       pgdprot |= super ? ARM_SHORT_PGD_TYPE_SUPERSECTION :</span>
<span class="quote">&gt; +                               ARM_SHORT_PGD_TYPE_SECTION;</span>
<span class="quote">&gt; +       if (prot &amp; IOMMU_CACHE)</span>
<span class="quote">&gt; +               pgdprot |= ARM_SHORT_PGD_C | ARM_SHORT_PGD_B;</span>
<span class="quote">&gt; +       if (prot &amp; IOMMU_WRITE)</span>
<span class="quote">&gt; +               pgdprot |= ARM_SHORT_PGD_TEX0;</span>

Likewise.
<span class="quote">
&gt; +       if (prot &amp; IOMMU_NOEXEC)</span>
<span class="quote">&gt; +               pgdprot |= ARM_SHORT_PGD_SECTION_XN;</span>
<span class="quote">&gt; +       if (data-&gt;iop.cfg.quirks &amp; IO_PGTABLE_QUIRK_ARM_NS)</span>
<span class="quote">&gt; +               pgdprot |= ARM_SHORT_PGD_SECTION_NS;</span>
<span class="quote">&gt; +       return pgdprot;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static arm_short_iopte</span>
<span class="quote">&gt; +__arm_short_pte_prot_split(struct arm_short_io_pgtable *data,</span>
<span class="quote">&gt; +                          arm_short_iopte pgdprot,</span>
<span class="quote">&gt; +                          arm_short_iopte pteprot_large,</span>
<span class="quote">&gt; +                          bool large)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       arm_short_iopte pteprot = 0;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       pteprot = ARM_SHORT_PTE_S | ARM_SHORT_PTE_nG;</span>
<span class="quote">&gt; +       pteprot |= large ? ARM_SHORT_PTE_TYPE_LARGE :</span>
<span class="quote">&gt; +                               ARM_SHORT_PTE_TYPE_SMALL;</span>
<span class="quote">&gt; +       /* section to pte prot */</span>
<span class="quote">&gt; +       if (pgdprot &amp; ARM_SHORT_PGD_C)</span>
<span class="quote">&gt; +               pteprot |= ARM_SHORT_PTE_C;</span>
<span class="quote">&gt; +       if (pgdprot &amp; ARM_SHORT_PGD_B)</span>
<span class="quote">&gt; +               pteprot |= ARM_SHORT_PTE_B;</span>
<span class="quote">&gt; +       if (pgdprot &amp; ARM_SHORT_PGD_TEX0)</span>
<span class="quote">&gt; +               pteprot |= large ? ARM_SHORT_PTE_LARGE_TEX0 :</span>
<span class="quote">&gt; +                               ARM_SHORT_PTE_SMALL_TEX0;</span>
<span class="quote">&gt; +       if (pgdprot &amp; ARM_SHORT_PGD_nG)</span>
<span class="quote">&gt; +               pteprot |= ARM_SHORT_PTE_nG;</span>
<span class="quote">&gt; +       if (pgdprot &amp; ARM_SHORT_PGD_SECTION_XN)</span>
<span class="quote">&gt; +               pteprot |= large ? ARM_SHORT_PTE_LARGE_XN :</span>
<span class="quote">&gt; +                               ARM_SHORT_PTE_SMALL_XN;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       /* large page to small page pte prot. Only large page may split */</span>
<span class="quote">&gt; +       if (pteprot_large &amp;&amp; !large) {</span>
<span class="quote">&gt; +               if (pteprot_large &amp; ARM_SHORT_PTE_LARGE_TEX0)</span>
<span class="quote">&gt; +                       pteprot |= ARM_SHORT_PTE_SMALL_TEX0;</span>
<span class="quote">&gt; +               if (pteprot_large &amp; ARM_SHORT_PTE_LARGE_XN)</span>
<span class="quote">&gt; +                       pteprot |= ARM_SHORT_PTE_SMALL_XN;</span>
<span class="quote">&gt; +       }</span>
<span class="quote">&gt; +       return pteprot;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static arm_short_iopte</span>
<span class="quote">&gt; +__arm_short_pgtable_prot(struct arm_short_io_pgtable *data, bool noexec)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       arm_short_iopte pgdprot = 0;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       pgdprot = ARM_SHORT_PGD_TYPE_PGTABLE;</span>
<span class="quote">&gt; +       if (data-&gt;iop.cfg.quirks &amp; IO_PGTABLE_QUIRK_ARM_NS)</span>
<span class="quote">&gt; +               pgdprot |= ARM_SHORT_PGD_PGTABLE_NS;</span>
<span class="quote">&gt; +       if (noexec)</span>
<span class="quote">&gt; +               pgdprot |= ARM_SHORT_PGD_PGTABLE_XN;</span>

I don&#39;t think you need to worry about XN bits for PGTABLEs.
<span class="quote">
&gt; +       return pgdprot;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static int</span>
<span class="quote">&gt; +_arm_short_map(struct arm_short_io_pgtable *data,</span>
<span class="quote">&gt; +              unsigned int iova, phys_addr_t paddr,</span>
<span class="quote">&gt; +              arm_short_iopte pgdprot, arm_short_iopte pteprot,</span>
<span class="quote">&gt; +              bool large)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       const struct iommu_gather_ops *tlb = data-&gt;iop.cfg.tlb;</span>
<span class="quote">&gt; +       arm_short_iopte *pgd = data-&gt;pgd, *pte;</span>
<span class="quote">&gt; +       void *cookie = data-&gt;iop.cookie, *pte_va;</span>
<span class="quote">&gt; +       unsigned int ptenr = large ? 16 : 1;</span>
<span class="quote">&gt; +       int i, quirk = data-&gt;iop.cfg.quirks;</span>
<span class="quote">&gt; +       bool ptenew = false;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       pgd += ARM_SHORT_PGD_IDX(iova);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       if (!pteprot) { /* section or supersection */</span>
<span class="quote">&gt; +               if (quirk &amp; IO_PGTABLE_QUIRK_SHORT_MTK)</span>
<span class="quote">&gt; +                       pgdprot &amp;= ~ARM_SHORT_PGD_SECTION_XN;</span>
<span class="quote">&gt; +               pte = pgd;</span>
<span class="quote">&gt; +               pteprot = pgdprot;</span>
<span class="quote">&gt; +       } else {        /* page or largepage */</span>
<span class="quote">&gt; +               if (quirk &amp; IO_PGTABLE_QUIRK_SHORT_MTK) {</span>
<span class="quote">&gt; +                       if (large) { /* special Bit */</span>

This definitely needs a better comment! What exactly are you doing here
and what is that quirk all about?
<span class="quote">
&gt; +                               if (pteprot &amp; ARM_SHORT_PTE_LARGE_TEX0) {</span>
<span class="quote">&gt; +                                       pteprot &amp;= ~ARM_SHORT_PTE_LARGE_TEX0;</span>
<span class="quote">&gt; +                                       pteprot |= ARM_SHORT_PTE_SMALL_TEX0;</span>
<span class="quote">&gt; +                               }</span>
<span class="quote">&gt; +                               pteprot &amp;= ~ARM_SHORT_PTE_LARGE_XN;</span>
<span class="quote">&gt; +                       } else {</span>
<span class="quote">&gt; +                               pteprot &amp;= ~ARM_SHORT_PTE_SMALL_XN;</span>
<span class="quote">&gt; +                       }</span>
<span class="quote">&gt; +               }</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +               if (!(*pgd)) {</span>
<span class="quote">&gt; +                       pte_va = kmem_cache_zalloc(data-&gt;ptekmem, GFP_ATOMIC);</span>
<span class="quote">&gt; +                       if (unlikely(!pte_va))</span>
<span class="quote">&gt; +                               return -ENOMEM;</span>
<span class="quote">&gt; +                       ptenew = true;</span>
<span class="quote">&gt; +                       *pgd = virt_to_phys(pte_va) | pgdprot;</span>
<span class="quote">&gt; +                       kmemleak_ignore(pte_va);</span>
<span class="quote">&gt; +                       tlb-&gt;flush_pgtable(pgd, sizeof(*pgd), cookie);</span>

I think you need to flush this before it becomes visible to the walker.
<span class="quote">
&gt; +               }</span>
<span class="quote">&gt; +               pte = arm_short_get_pte_in_pgd(*pgd, iova);</span>
<span class="quote">&gt; +       }</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       pteprot |= (arm_short_iopte)paddr;</span>
<span class="quote">&gt; +       for (i = 0; i &lt; ptenr; i++) {</span>
<span class="quote">&gt; +               if (pte[i]) {/* Someone else may have allocated for this pte */</span>
<span class="quote">&gt; +                       WARN_ON(!selftest_running);</span>
<span class="quote">&gt; +                       goto err_exist_pte;</span>
<span class="quote">&gt; +               }</span>
<span class="quote">&gt; +               pte[i] = pteprot;</span>
<span class="quote">&gt; +       }</span>
<span class="quote">&gt; +       tlb-&gt;flush_pgtable(pte, ptenr * sizeof(*pte), cookie);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       return 0;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +err_exist_pte:</span>
<span class="quote">&gt; +       while (i--)</span>
<span class="quote">&gt; +               pte[i] = 0;</span>
<span class="quote">&gt; +       if (ptenew)</span>
<span class="quote">&gt; +               kmem_cache_free(data-&gt;ptekmem, pte_va);</span>
<span class="quote">&gt; +       return -EEXIST;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static int arm_short_map(struct io_pgtable_ops *ops, unsigned long iova,</span>
<span class="quote">&gt; +                        phys_addr_t paddr, size_t size, int prot)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       struct arm_short_io_pgtable *data = io_pgtable_ops_to_data(ops);</span>
<span class="quote">&gt; +       const struct iommu_gather_ops *tlb = data-&gt;iop.cfg.tlb;</span>
<span class="quote">&gt; +       int ret;</span>
<span class="quote">&gt; +       arm_short_iopte pgdprot = 0, pteprot = 0;</span>
<span class="quote">&gt; +       bool large;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       /* If no access, then nothing to do */</span>
<span class="quote">&gt; +       if (!(prot &amp; (IOMMU_READ | IOMMU_WRITE)))</span>
<span class="quote">&gt; +               return 0;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       switch (size) {</span>
<span class="quote">&gt; +       case SZ_4K:</span>
<span class="quote">&gt; +       case SZ_64K:</span>
<span class="quote">&gt; +               large = (size == SZ_64K) ? true : false;</span>
<span class="quote">&gt; +               pteprot = __arm_short_pte_prot(data, prot, large);</span>
<span class="quote">&gt; +               pgdprot = __arm_short_pgtable_prot(data, prot &amp; IOMMU_NOEXEC);</span>
<span class="quote">&gt; +               break;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       case SZ_1M:</span>
<span class="quote">&gt; +       case SZ_16M:</span>
<span class="quote">&gt; +               large = (size == SZ_16M) ? true : false;</span>
<span class="quote">&gt; +               pgdprot = __arm_short_pgd_prot(data, prot, large);</span>
<span class="quote">&gt; +               break;</span>
<span class="quote">&gt; +       default:</span>
<span class="quote">&gt; +               return -EINVAL;</span>
<span class="quote">&gt; +       }</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       if (WARN_ON((iova | paddr) &amp; (size - 1)))</span>
<span class="quote">&gt; +               return -EINVAL;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       ret = _arm_short_map(data, iova, paddr, pgdprot, pteprot, large);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       tlb-&gt;tlb_add_flush(iova, size, true, data-&gt;iop.cookie);</span>
<span class="quote">&gt; +       tlb-&gt;tlb_sync(data-&gt;iop.cookie);</span>

In _arm_short_map, it looks like you can only go from invalid -&gt; valid,
so why do you need to flush the TLB here?
<span class="quote">
&gt; +       return ret;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static phys_addr_t arm_short_iova_to_phys(struct io_pgtable_ops *ops,</span>
<span class="quote">&gt; +                                         unsigned long iova)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       struct arm_short_io_pgtable *data = io_pgtable_ops_to_data(ops);</span>
<span class="quote">&gt; +       arm_short_iopte *pte, *pgd = data-&gt;pgd;</span>
<span class="quote">&gt; +       phys_addr_t pa = 0;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       pgd += ARM_SHORT_PGD_IDX(iova);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       if (ARM_SHORT_PGD_TYPE_IS_PGTABLE(*pgd)) {</span>
<span class="quote">&gt; +               pte = arm_short_get_pte_in_pgd(*pgd, iova);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +               if (ARM_SHORT_PTE_TYPE_IS_LARGEPAGE(*pte)) {</span>
<span class="quote">&gt; +                       pa = (*pte) &amp; ARM_SHORT_PTE_LARGE_MSK;</span>
<span class="quote">&gt; +                       pa |= iova &amp; ~ARM_SHORT_PTE_LARGE_MSK;</span>
<span class="quote">&gt; +               } else if (ARM_SHORT_PTE_TYPE_IS_SMALLPAGE(*pte)) {</span>
<span class="quote">&gt; +                       pa = (*pte) &amp; ARM_SHORT_PTE_SMALL_MSK;</span>
<span class="quote">&gt; +                       pa |= iova &amp; ~ARM_SHORT_PTE_SMALL_MSK;</span>
<span class="quote">&gt; +               }</span>
<span class="quote">&gt; +       } else if (ARM_SHORT_PGD_TYPE_IS_SECTION(*pgd)) {</span>
<span class="quote">&gt; +               pa = (*pgd) &amp; ARM_SHORT_PGD_SECTION_MSK;</span>
<span class="quote">&gt; +               pa |= iova &amp; ~ARM_SHORT_PGD_SECTION_MSK;</span>
<span class="quote">&gt; +       } else if (ARM_SHORT_PGD_TYPE_IS_SUPERSECTION(*pgd)) {</span>
<span class="quote">&gt; +               pa = (*pgd) &amp; ARM_SHORT_PGD_SUPERSECTION_MSK;</span>
<span class="quote">&gt; +               pa |= iova &amp; ~ARM_SHORT_PGD_SUPERSECTION_MSK;</span>
<span class="quote">&gt; +       }</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       return pa;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static int</span>
<span class="quote">&gt; +arm_short_split_blk_unmap(struct io_pgtable_ops *ops, unsigned int iova,</span>
<span class="quote">&gt; +                         phys_addr_t paddr, size_t size,</span>
<span class="quote">&gt; +                         arm_short_iopte pgdprotup, arm_short_iopte pteprotup,</span>
<span class="quote">&gt; +                         size_t blk_size)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       struct arm_short_io_pgtable *data = io_pgtable_ops_to_data(ops);</span>
<span class="quote">&gt; +       const struct iommu_gather_ops *tlb = data-&gt;iop.cfg.tlb;</span>
<span class="quote">&gt; +       struct io_pgtable_cfg *cfg = &amp;data-&gt;iop.cfg;</span>
<span class="quote">&gt; +       unsigned long *pgbitmap = &amp;cfg-&gt;pgsize_bitmap;</span>
<span class="quote">&gt; +       unsigned int blk_base, blk_start, blk_end;</span>
<span class="quote">&gt; +       arm_short_iopte pgdprot, pteprot;</span>
<span class="quote">&gt; +       size_t mapsize = 0, nextmapsize;</span>
<span class="quote">&gt; +       phys_addr_t blk_paddr;</span>
<span class="quote">&gt; +       int ret;</span>
<span class="quote">&gt; +       unsigned int i;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       /* find the nearest mapsize */</span>
<span class="quote">&gt; +       for (i = find_first_bit(pgbitmap, BITS_PER_LONG);</span>
<span class="quote">&gt; +            i &lt; BITS_PER_LONG &amp;&amp; ((1 &lt;&lt; i) &lt; blk_size) &amp;&amp;</span>
<span class="quote">&gt; +            IS_ALIGNED(size, 1 &lt;&lt; i);</span>
<span class="quote">&gt; +            i = find_next_bit(pgbitmap, BITS_PER_LONG, i + 1))</span>
<span class="quote">&gt; +               mapsize = 1 &lt;&lt; i;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       if (WARN_ON(!mapsize))</span>
<span class="quote">&gt; +               return 0; /* Bytes unmapped */</span>
<span class="quote">&gt; +       nextmapsize = 1 &lt;&lt; i;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       blk_base = iova &amp; ~(blk_size - 1);</span>
<span class="quote">&gt; +       blk_start = blk_base;</span>
<span class="quote">&gt; +       blk_end = blk_start + blk_size;</span>
<span class="quote">&gt; +       blk_paddr = paddr;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       for (; blk_start &lt; blk_end;</span>
<span class="quote">&gt; +            blk_start += mapsize, blk_paddr += mapsize) {</span>
<span class="quote">&gt; +               /* Unmap! */</span>
<span class="quote">&gt; +               if (blk_start == iova)</span>
<span class="quote">&gt; +                       continue;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +               /* Try to upper map */</span>
<span class="quote">&gt; +               if (blk_base != blk_start &amp;&amp;</span>
<span class="quote">&gt; +                   IS_ALIGNED(blk_start | blk_paddr, nextmapsize) &amp;&amp;</span>
<span class="quote">&gt; +                   mapsize != nextmapsize) {</span>
<span class="quote">&gt; +                       mapsize = nextmapsize;</span>
<span class="quote">&gt; +                       i = find_next_bit(pgbitmap, BITS_PER_LONG, i + 1);</span>
<span class="quote">&gt; +                       if (i &lt; BITS_PER_LONG)</span>
<span class="quote">&gt; +                               nextmapsize = 1 &lt;&lt; i;</span>
<span class="quote">&gt; +               }</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +               if (mapsize == SZ_1M) {</span>
<span class="quote">&gt; +                       pgdprot = pgdprotup;</span>
<span class="quote">&gt; +                       pgdprot |= __arm_short_pgd_prot(data, 0, false);</span>
<span class="quote">&gt; +                       pteprot = 0;</span>
<span class="quote">&gt; +               } else { /* small or large page */</span>
<span class="quote">&gt; +                       bool noexec = (blk_size == SZ_64K) ?</span>
<span class="quote">&gt; +                               (pteprotup &amp; ARM_SHORT_PTE_LARGE_XN) :</span>
<span class="quote">&gt; +                               (pgdprotup &amp; ARM_SHORT_PGD_SECTION_XN);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +                       pteprot = __arm_short_pte_prot_split(</span>
<span class="quote">&gt; +                                               data, pgdprotup, pteprotup,</span>
<span class="quote">&gt; +                                               mapsize == SZ_64K);</span>
<span class="quote">&gt; +                       pgdprot = __arm_short_pgtable_prot(data, noexec);</span>
<span class="quote">&gt; +               }</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +               ret = _arm_short_map(data, blk_start, blk_paddr, pgdprot,</span>
<span class="quote">&gt; +                                    pteprot, mapsize == SZ_64K);</span>
<span class="quote">&gt; +               if (ret &lt; 0) {</span>
<span class="quote">&gt; +                       /* Free the table we allocated */</span>
<span class="quote">&gt; +                       arm_short_iopte *pgd = data-&gt;pgd, *pte;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +                       pgd += ARM_SHORT_PGD_IDX(blk_base);</span>
<span class="quote">&gt; +                       if (*pgd) {</span>
<span class="quote">&gt; +                               pte = ARM_SHORT_GET_PTE_VA(*pgd);</span>
<span class="quote">&gt; +                               kmem_cache_free(data-&gt;ptekmem, pte);</span>
<span class="quote">&gt; +                               *pgd = 0;</span>
<span class="quote">&gt; +                               tlb-&gt;flush_pgtable(pgd, sizeof(*pgd),</span>
<span class="quote">&gt; +                                                  data-&gt;iop.cookie);</span>

Again, the ordering looks totally wrong here. You need to:

  (1) Zero the pgd pointer
  (2) Flush the pointer, so the walker can no longer follow the page table
  (3) Invalidate the TLB, so we don&#39;t have any cached intermediate walks
  (4) Sync the TLB
  (5) Free the memory

That said, I don&#39;t fully follow the logic here.
<span class="quote">
&gt; +                       }</span>
<span class="quote">&gt; +                       return 0;/* Bytes unmapped */</span>
<span class="quote">&gt; +               }</span>
<span class="quote">&gt; +               tlb-&gt;tlb_add_flush(blk_start, mapsize, true, data-&gt;iop.cookie);</span>
<span class="quote">&gt; +               tlb-&gt;tlb_sync(data-&gt;iop.cookie);</span>

Why are you doing this here for every iteration?

Will
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=123111">Yong Wu</a> - July 24, 2015, 5:24 a.m.</div>
<pre class="content">
Hi Will,
    Thanks for your review so detail.
    When you are free, please help me check whether it&#39;s ok if it&#39;s
changed like below.
    Thanks very much.

On Tue, 2015-07-21 at 18:11 +0100, Will Deacon wrote:
<span class="quote">&gt; Hello,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This is looking better, but I still have some concerns.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; On Thu, Jul 16, 2015 at 10:04:32AM +0100, Yong Wu wrote:</span>
<span class="quote">&gt; &gt; This patch is for ARM Short Descriptor Format.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Signed-off-by: Yong Wu &lt;yong.wu@mediatek.com&gt;</span>
<span class="quote">&gt; &gt; ---</span>
<span class="quote">&gt; &gt;  drivers/iommu/Kconfig                |   18 +</span>
<span class="quote">&gt; &gt;  drivers/iommu/Makefile               |    1 +</span>
<span class="quote">&gt; &gt;  drivers/iommu/io-pgtable-arm-short.c |  742 ++++++++++++++++++++++++++++++++++</span>
<span class="quote">&gt; &gt;  drivers/iommu/io-pgtable-arm.c       |    3 -</span>
<span class="quote">&gt; &gt;  drivers/iommu/io-pgtable.c           |    4 +</span>
<span class="quote">&gt; &gt;  drivers/iommu/io-pgtable.h           |   13 +</span>
<span class="quote">&gt; &gt;  6 files changed, 778 insertions(+), 3 deletions(-)</span>
<span class="quote">&gt; &gt;  create mode 100644 drivers/iommu/io-pgtable-arm-short.c</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; [...]</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; +#define ARM_SHORT_PGDIR_SHIFT                  20</span>
<span class="quote">&gt; &gt; +#define ARM_SHORT_PAGE_SHIFT                   12</span>
<span class="quote">&gt; &gt; +#define ARM_SHORT_PTRS_PER_PTE                 \</span>
<span class="quote">&gt; &gt; +       (1 &lt;&lt; (ARM_SHORT_PGDIR_SHIFT - ARM_SHORT_PAGE_SHIFT))</span>
<span class="quote">&gt; &gt; +#define ARM_SHORT_BYTES_PER_PTE                        \</span>
<span class="quote">&gt; &gt; +       (ARM_SHORT_PTRS_PER_PTE * sizeof(arm_short_iopte))</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +/* level 1 pagetable */</span>
<span class="quote">&gt; &gt; +#define ARM_SHORT_PGD_TYPE_PGTABLE             BIT(0)</span>
<span class="quote">&gt; &gt; +#define ARM_SHORT_PGD_SECTION_XN               (BIT(0) | BIT(4))</span>
<span class="quote">&gt; &gt; +#define ARM_SHORT_PGD_TYPE_SECTION             BIT(1)</span>
<span class="quote">&gt; &gt; +#define ARM_SHORT_PGD_PGTABLE_XN               BIT(2)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This should be PXN, but I&#39;m not even sure why we care for a table at</span>
<span class="quote">&gt; level 1.</span>

I will delete it.
<span class="quote">
&gt; </span>
<span class="quote">&gt; &gt; +#define ARM_SHORT_PGD_B                                BIT(2)</span>
<span class="quote">&gt; &gt; +#define ARM_SHORT_PGD_C                                BIT(3)</span>
<span class="quote">&gt; &gt; +#define ARM_SHORT_PGD_PGTABLE_NS               BIT(3)</span>
<span class="quote">&gt; &gt; +#define ARM_SHORT_PGD_IMPLE                    BIT(9)</span>
<span class="quote">&gt; &gt; +#define ARM_SHORT_PGD_TEX0                     BIT(12)</span>
<span class="quote">&gt; &gt; +#define ARM_SHORT_PGD_S                                BIT(16)</span>
<span class="quote">&gt; &gt; +#define ARM_SHORT_PGD_nG                       BIT(17)</span>
<span class="quote">&gt; &gt; +#define ARM_SHORT_PGD_SUPERSECTION             BIT(18)</span>
<span class="quote">&gt; &gt; +#define ARM_SHORT_PGD_SECTION_NS               BIT(19)</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +#define ARM_SHORT_PGD_TYPE_SUPERSECTION                \</span>
<span class="quote">&gt; &gt; +       (ARM_SHORT_PGD_TYPE_SECTION | ARM_SHORT_PGD_SUPERSECTION)</span>
<span class="quote">&gt; &gt; +#define ARM_SHORT_PGD_SECTION_TYPE_MSK         \</span>
<span class="quote">&gt; &gt; +       (ARM_SHORT_PGD_TYPE_SECTION | ARM_SHORT_PGD_SUPERSECTION)</span>
<span class="quote">&gt; &gt; +#define ARM_SHORT_PGD_PGTABLE_TYPE_MSK         \</span>
<span class="quote">&gt; &gt; +       (ARM_SHORT_PGD_TYPE_SECTION | ARM_SHORT_PGD_TYPE_PGTABLE)</span>
<span class="quote">&gt; &gt; +#define ARM_SHORT_PGD_TYPE_IS_PGTABLE(pgd)     \</span>
<span class="quote">&gt; &gt; +       (((pgd) &amp; ARM_SHORT_PGD_PGTABLE_TYPE_MSK) == ARM_SHORT_PGD_TYPE_PGTABLE)</span>
<span class="quote">&gt; &gt; +#define ARM_SHORT_PGD_TYPE_IS_SECTION(pgd)     \</span>
<span class="quote">&gt; &gt; +       (((pgd) &amp; ARM_SHORT_PGD_SECTION_TYPE_MSK) == ARM_SHORT_PGD_TYPE_SECTION)</span>
<span class="quote">&gt; &gt; +#define ARM_SHORT_PGD_TYPE_IS_SUPERSECTION(pgd)        \</span>
<span class="quote">&gt; &gt; +       (((pgd) &amp; ARM_SHORT_PGD_SECTION_TYPE_MSK) == \</span>
<span class="quote">&gt; &gt; +       ARM_SHORT_PGD_TYPE_SUPERSECTION)</span>
<span class="quote">&gt; &gt; +#define ARM_SHORT_PGD_PGTABLE_MSK              0xfffffc00</span>
<span class="quote">&gt; &gt; +#define ARM_SHORT_PGD_SECTION_MSK              (~(SZ_1M - 1))</span>
<span class="quote">&gt; &gt; +#define ARM_SHORT_PGD_SUPERSECTION_MSK         (~(SZ_16M - 1))</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +/* level 2 pagetable */</span>
<span class="quote">&gt; &gt; +#define ARM_SHORT_PTE_TYPE_LARGE               BIT(0)</span>
<span class="quote">&gt; &gt; +#define ARM_SHORT_PTE_SMALL_XN                 BIT(0)</span>
<span class="quote">&gt; &gt; +#define ARM_SHORT_PTE_TYPE_SMALL               BIT(1)</span>
<span class="quote">&gt; &gt; +#define ARM_SHORT_PTE_B                                BIT(2)</span>
<span class="quote">&gt; &gt; +#define ARM_SHORT_PTE_C                                BIT(3)</span>
<span class="quote">&gt; &gt; +#define ARM_SHORT_PTE_SMALL_TEX0               BIT(6)</span>
<span class="quote">&gt; &gt; +#define ARM_SHORT_PTE_IMPLE                    BIT(9)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This is AP[2] for small pages.</span>

Sorry, In our pagetable bit9 in PGD and PTE is PA[32] that is for  the
dram size over 4G. I didn&#39;t care it is different in PTE of the standard
spec.
And I don&#39;t use the AP[2] currently, so I only delete this line in next
time.
<span class="quote">
&gt; </span>
<span class="quote">&gt; &gt; +#define ARM_SHORT_PTE_S                                BIT(10)</span>
<span class="quote">&gt; &gt; +#define ARM_SHORT_PTE_nG                       BIT(11)</span>
<span class="quote">&gt; &gt; +#define ARM_SHORT_PTE_LARGE_TEX0               BIT(12)</span>
<span class="quote">&gt; &gt; +#define ARM_SHORT_PTE_LARGE_XN                 BIT(15)</span>
<span class="quote">&gt; &gt; +#define ARM_SHORT_PTE_LARGE_MSK                        (~(SZ_64K - 1))</span>
<span class="quote">&gt; &gt; +#define ARM_SHORT_PTE_SMALL_MSK                        (~(SZ_4K - 1))</span>
<span class="quote">&gt; &gt; +#define ARM_SHORT_PTE_TYPE_MSK                 \</span>
<span class="quote">&gt; &gt; +       (ARM_SHORT_PTE_TYPE_LARGE | ARM_SHORT_PTE_TYPE_SMALL)</span>
<span class="quote">&gt; &gt; +#define ARM_SHORT_PTE_TYPE_IS_SMALLPAGE(pte)   \</span>
<span class="quote">&gt; &gt; +       (((((pte) &amp; ARM_SHORT_PTE_TYPE_MSK) &gt;&gt; 1) &lt;&lt; 1)\</span>
<span class="quote">&gt; &gt; +       == ARM_SHORT_PTE_TYPE_SMALL)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I see what you&#39;re trying to do here, but the shifting is confusing. I</span>
<span class="quote">&gt; think it&#39;s better doing something like:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	((pte) &amp; ARM_SHORT_PTE_TYPE_SMALL) == ARM_SHORT_PTE_TYPE_SMALL</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; or even just:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	(pte) &amp; ARM_SHORT_PTE_TYPE_SMALL</span>

Thanks. I will use this:
((pte) &amp; ARM_SHORT_PTE_TYPE_SMALL) == ARM_SHORT_PTE_TYPE_SMALL

This line may be close to the ARM_SHORT_PTE_TYPE_IS_LARGEPAGE below.
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; +#define ARM_SHORT_PTE_TYPE_IS_LARGEPAGE(pte)   \</span>
<span class="quote">&gt; &gt; +       (((pte) &amp; ARM_SHORT_PTE_TYPE_MSK) == ARM_SHORT_PTE_TYPE_LARGE)</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +#define ARM_SHORT_PGD_IDX(a)                   ((a) &gt;&gt; ARM_SHORT_PGDIR_SHIFT)</span>
<span class="quote">&gt; &gt; +#define ARM_SHORT_PTE_IDX(a)                   \</span>
<span class="quote">&gt; &gt; +       (((a) &gt;&gt; ARM_SHORT_PAGE_SHIFT) &amp; (ARM_SHORT_PTRS_PER_PTE - 1))</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +#define ARM_SHORT_GET_PTE_VA(pgd)              \</span>
<span class="quote">&gt; &gt; +       (phys_to_virt((unsigned long)pgd &amp; ARM_SHORT_PGD_PGTABLE_MSK))</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Maybe better named as ARM_SHORT_GET_PGTABLE_VA ?</span>

Thanks.
<span class="quote">
&gt; </span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +#define ARM_SHORT_PTE_LARGE_GET_PROT(pte)      \</span>
<span class="quote">&gt; &gt; +       (((pte) &amp; (~ARM_SHORT_PTE_LARGE_MSK)) &amp; ~ARM_SHORT_PTE_TYPE_MSK)</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +#define ARM_SHORT_PGD_GET_PROT(pgd)            \</span>
<span class="quote">&gt; &gt; +       (((pgd) &amp; (~ARM_SHORT_PGD_SECTION_MSK)) &amp; ~ARM_SHORT_PGD_SUPERSECTION)</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static bool selftest_running;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static arm_short_iopte *</span>
<span class="quote">&gt; &gt; +arm_short_get_pte_in_pgd(arm_short_iopte pgd, unsigned int iova)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +       arm_short_iopte *pte;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +       pte = ARM_SHORT_GET_PTE_VA(pgd);</span>
<span class="quote">&gt; &gt; +       pte += ARM_SHORT_PTE_IDX(iova);</span>
<span class="quote">&gt; &gt; +       return pte;</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static void _arm_short_free_pgtable(struct arm_short_io_pgtable *data,</span>
<span class="quote">&gt; &gt; +                                   arm_short_iopte *pgd)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +       const struct iommu_gather_ops *tlb = data-&gt;iop.cfg.tlb;</span>
<span class="quote">&gt; &gt; +       arm_short_iopte *pte;</span>
<span class="quote">&gt; &gt; +       int i;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +       pte = ARM_SHORT_GET_PTE_VA(*pgd);</span>
<span class="quote">&gt; &gt; +       for (i = 0; i &lt; ARM_SHORT_PTRS_PER_PTE; i++) {</span>
<span class="quote">&gt; &gt; +               if (pte[i] != 0)</span>
<span class="quote">&gt; &gt; +                       return;</span>
<span class="quote">&gt; &gt; +       }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Do you actually need this loop if you&#39;re not warning or returning an error?</span>

I can&#39;t return an error here.

Here I only walk all the pte items in the pgd, 
If there is some pte item exist, we do nothing, It is a ok case too.
If all the pte items are unmapped, We have to free the memory of
pgtable(kmem).
<span class="quote">
&gt; </span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +       /* Free whole pte and set pgd to zero while all pte is unmap */</span>
<span class="quote">&gt; &gt; +       kmem_cache_free(data-&gt;ptekmem, pte);</span>
<span class="quote">&gt; &gt; +       *pgd = 0;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I still don&#39;t think this is safe. What stops the page table walker from</span>
<span class="quote">&gt; speculatively walking freed memory?</span>

      Sorry. I didn&#39;t care the free flow this time. 

      I will change like below:    
static bool _arm_short_free_pgtable(struct arm_short_io_pgtable *data,
arm_short_iopte *pgd)
{
 /* if whole the pte items of this pgd are unmapped, return true.
  * if others return fail.
  */
  for(*****)
}
   In arm_short_unmap, I will compare the return value and following the
free 5 steps as you suggestion below.
<span class="quote">
&gt; </span>
<span class="quote">&gt; &gt; +       tlb-&gt;flush_pgtable(pgd, sizeof(*pgd), data-&gt;iop.cookie);</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static arm_short_iopte</span>
<span class="quote">&gt; &gt; +__arm_short_pte_prot(struct arm_short_io_pgtable *data, int prot, bool large)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +       arm_short_iopte pteprot;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +       pteprot = ARM_SHORT_PTE_S | ARM_SHORT_PTE_nG;</span>
<span class="quote">&gt; &gt; +       pteprot |= large ? ARM_SHORT_PTE_TYPE_LARGE :</span>
<span class="quote">&gt; &gt; +                               ARM_SHORT_PTE_TYPE_SMALL;</span>
<span class="quote">&gt; &gt; +       if (prot &amp; IOMMU_CACHE)</span>
<span class="quote">&gt; &gt; +               pteprot |=  ARM_SHORT_PTE_B | ARM_SHORT_PTE_C;</span>
<span class="quote">&gt; &gt; +       if (prot &amp; IOMMU_WRITE)</span>
<span class="quote">&gt; &gt; +               pteprot |= large ? ARM_SHORT_PTE_LARGE_TEX0 :</span>
<span class="quote">&gt; &gt; +                               ARM_SHORT_PTE_SMALL_TEX0;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This doesn&#39;t make any sense. TEX[2:0] is all about memory attributes, not</span>
<span class="quote">&gt; permissions, so you&#39;re making the mapping write-back, write-allocate but</span>
<span class="quote">&gt; that&#39;s not what the IOMMU_* values are about.</span>

     I will delete it.
<span class="quote">
&gt; </span>
<span class="quote">&gt; &gt; +       if (prot &amp; IOMMU_NOEXEC)</span>
<span class="quote">&gt; &gt; +               pteprot |= large ? ARM_SHORT_PTE_LARGE_XN :</span>
<span class="quote">&gt; &gt; +                       ARM_SHORT_PTE_SMALL_XN;</span>
<span class="quote">&gt; &gt; +       return pteprot;</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static arm_short_iopte</span>
<span class="quote">&gt; &gt; +__arm_short_pgd_prot(struct arm_short_io_pgtable *data, int prot, bool super)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +       arm_short_iopte pgdprot;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +       pgdprot = ARM_SHORT_PGD_S | ARM_SHORT_PGD_nG;</span>
<span class="quote">&gt; &gt; +       pgdprot |= super ? ARM_SHORT_PGD_TYPE_SUPERSECTION :</span>
<span class="quote">&gt; &gt; +                               ARM_SHORT_PGD_TYPE_SECTION;</span>
<span class="quote">&gt; &gt; +       if (prot &amp; IOMMU_CACHE)</span>
<span class="quote">&gt; &gt; +               pgdprot |= ARM_SHORT_PGD_C | ARM_SHORT_PGD_B;</span>
<span class="quote">&gt; &gt; +       if (prot &amp; IOMMU_WRITE)</span>
<span class="quote">&gt; &gt; +               pgdprot |= ARM_SHORT_PGD_TEX0;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Likewise.</span>

I will delete it.
<span class="quote">
&gt; </span>
<span class="quote">&gt; &gt; +       if (prot &amp; IOMMU_NOEXEC)</span>
<span class="quote">&gt; &gt; +               pgdprot |= ARM_SHORT_PGD_SECTION_XN;</span>
<span class="quote">&gt; &gt; +       if (data-&gt;iop.cfg.quirks &amp; IO_PGTABLE_QUIRK_ARM_NS)</span>
<span class="quote">&gt; &gt; +               pgdprot |= ARM_SHORT_PGD_SECTION_NS;</span>
<span class="quote">&gt; &gt; +       return pgdprot;</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static arm_short_iopte</span>
<span class="quote">&gt; &gt; +__arm_short_pte_prot_split(struct arm_short_io_pgtable *data,</span>
<span class="quote">&gt; &gt; +                          arm_short_iopte pgdprot,</span>
<span class="quote">&gt; &gt; +                          arm_short_iopte pteprot_large,</span>
<span class="quote">&gt; &gt; +                          bool large)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +       arm_short_iopte pteprot = 0;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +       pteprot = ARM_SHORT_PTE_S | ARM_SHORT_PTE_nG;</span>
<span class="quote">&gt; &gt; +       pteprot |= large ? ARM_SHORT_PTE_TYPE_LARGE :</span>
<span class="quote">&gt; &gt; +                               ARM_SHORT_PTE_TYPE_SMALL;</span>
<span class="quote">&gt; &gt; +       /* section to pte prot */</span>
<span class="quote">&gt; &gt; +       if (pgdprot &amp; ARM_SHORT_PGD_C)</span>
<span class="quote">&gt; &gt; +               pteprot |= ARM_SHORT_PTE_C;</span>
<span class="quote">&gt; &gt; +       if (pgdprot &amp; ARM_SHORT_PGD_B)</span>
<span class="quote">&gt; &gt; +               pteprot |= ARM_SHORT_PTE_B;</span>
<span class="quote">&gt; &gt; +       if (pgdprot &amp; ARM_SHORT_PGD_TEX0)</span>
<span class="quote">&gt; &gt; +               pteprot |= large ? ARM_SHORT_PTE_LARGE_TEX0 :</span>
<span class="quote">&gt; &gt; +                               ARM_SHORT_PTE_SMALL_TEX0;</span>

Here I also delete it.
<span class="quote">
&gt; &gt; +       if (pgdprot &amp; ARM_SHORT_PGD_nG)</span>
<span class="quote">&gt; &gt; +               pteprot |= ARM_SHORT_PTE_nG;</span>
<span class="quote">&gt; &gt; +       if (pgdprot &amp; ARM_SHORT_PGD_SECTION_XN)</span>
<span class="quote">&gt; &gt; +               pteprot |= large ? ARM_SHORT_PTE_LARGE_XN :</span>
<span class="quote">&gt; &gt; +                               ARM_SHORT_PTE_SMALL_XN;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +       /* large page to small page pte prot. Only large page may split */</span>
<span class="quote">&gt; &gt; +       if (pteprot_large &amp;&amp; !large) {</span>
<span class="quote">&gt; &gt; +               if (pteprot_large &amp; ARM_SHORT_PTE_LARGE_TEX0)</span>
<span class="quote">&gt; &gt; +                       pteprot |= ARM_SHORT_PTE_SMALL_TEX0;</span>

Delete this too.
<span class="quote">
&gt; &gt; +               if (pteprot_large &amp; ARM_SHORT_PTE_LARGE_XN)</span>
<span class="quote">&gt; &gt; +                       pteprot |= ARM_SHORT_PTE_SMALL_XN;</span>
<span class="quote">&gt; &gt; +       }</span>
<span class="quote">&gt; &gt; +       return pteprot;</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static arm_short_iopte</span>
<span class="quote">&gt; &gt; +__arm_short_pgtable_prot(struct arm_short_io_pgtable *data, bool noexec)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +       arm_short_iopte pgdprot = 0;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +       pgdprot = ARM_SHORT_PGD_TYPE_PGTABLE;</span>
<span class="quote">&gt; &gt; +       if (data-&gt;iop.cfg.quirks &amp; IO_PGTABLE_QUIRK_ARM_NS)</span>
<span class="quote">&gt; &gt; +               pgdprot |= ARM_SHORT_PGD_PGTABLE_NS;</span>
<span class="quote">&gt; &gt; +       if (noexec)</span>
<span class="quote">&gt; &gt; +               pgdprot |= ARM_SHORT_PGD_PGTABLE_XN;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I don&#39;t think you need to worry about XN bits for PGTABLEs.</span>

I will delete it. 
MTK don&#39;t have XN bit in PGTABLEs,
I prepared to add all the bits according to the standard spec, and add
MTK quirk to disable this bit for our special bit.
<span class="quote">
&gt; </span>
<span class="quote">&gt; &gt; +       return pgdprot;</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static int</span>
<span class="quote">&gt; &gt; +_arm_short_map(struct arm_short_io_pgtable *data,</span>
<span class="quote">&gt; &gt; +              unsigned int iova, phys_addr_t paddr,</span>
<span class="quote">&gt; &gt; +              arm_short_iopte pgdprot, arm_short_iopte pteprot,</span>
<span class="quote">&gt; &gt; +              bool large)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +       const struct iommu_gather_ops *tlb = data-&gt;iop.cfg.tlb;</span>
<span class="quote">&gt; &gt; +       arm_short_iopte *pgd = data-&gt;pgd, *pte;</span>
<span class="quote">&gt; &gt; +       void *cookie = data-&gt;iop.cookie, *pte_va;</span>
<span class="quote">&gt; &gt; +       unsigned int ptenr = large ? 16 : 1;</span>
<span class="quote">&gt; &gt; +       int i, quirk = data-&gt;iop.cfg.quirks;</span>
<span class="quote">&gt; &gt; +       bool ptenew = false;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +       pgd += ARM_SHORT_PGD_IDX(iova);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +       if (!pteprot) { /* section or supersection */</span>
<span class="quote">&gt; &gt; +               if (quirk &amp; IO_PGTABLE_QUIRK_SHORT_MTK)</span>
<span class="quote">&gt; &gt; +                       pgdprot &amp;= ~ARM_SHORT_PGD_SECTION_XN;</span>
<span class="quote">&gt; &gt; +               pte = pgd;</span>
<span class="quote">&gt; &gt; +               pteprot = pgdprot;</span>
<span class="quote">&gt; &gt; +       } else {        /* page or largepage */</span>
<span class="quote">&gt; &gt; +               if (quirk &amp; IO_PGTABLE_QUIRK_SHORT_MTK) {</span>
<span class="quote">&gt; &gt; +                       if (large) { /* special Bit */</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This definitely needs a better comment! What exactly are you doing here</span>
<span class="quote">&gt; and what is that quirk all about?</span>

I use this quirk is for MTK Special Bit as we don&#39;t have the XN bit in
pagetable.

And the TEX0 will be deleted. Then it will like this:

//==========
    if (quirk &amp; IO_PGTABLE_QUIRK_SHORT_MTK) {
            if (large)  
                  pteprot &amp;= ~ARM_SHORT_PTE_LARGE_XN;
            else
                  pteprot &amp;= ~ARM_SHORT_PTE_SMALL_XN;
//=========
And move the comment close to the definition of the quirk.

like this:
#define IO_PGTABLE_QUIRK_SHORT_MTK   BIT(2)/* MTK Speical Bit
*/         			
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; +                               if (pteprot &amp; ARM_SHORT_PTE_LARGE_TEX0) {</span>
<span class="quote">&gt; &gt; +                                       pteprot &amp;= ~ARM_SHORT_PTE_LARGE_TEX0;</span>
<span class="quote">&gt; &gt; +                                       pteprot |= ARM_SHORT_PTE_SMALL_TEX0;</span>
<span class="quote">&gt; &gt; +                               }</span>
<span class="quote">&gt; &gt; +                               pteprot &amp;= ~ARM_SHORT_PTE_LARGE_XN;</span>
<span class="quote">&gt; &gt; +                       } else {</span>
<span class="quote">&gt; &gt; +                               pteprot &amp;= ~ARM_SHORT_PTE_SMALL_XN;</span>
<span class="quote">&gt; &gt; +                       }</span>
<span class="quote">&gt; &gt; +               }</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +               if (!(*pgd)) {</span>
<span class="quote">&gt; &gt; +                       pte_va = kmem_cache_zalloc(data-&gt;ptekmem, GFP_ATOMIC);</span>
<span class="quote">&gt; &gt; +                       if (unlikely(!pte_va))</span>
<span class="quote">&gt; &gt; +                               return -ENOMEM;</span>
<span class="quote">&gt; &gt; +                       ptenew = true;</span>
<span class="quote">&gt; &gt; +                       *pgd = virt_to_phys(pte_va) | pgdprot;</span>
<span class="quote">&gt; &gt; +                       kmemleak_ignore(pte_va);</span>
<span class="quote">&gt; &gt; +                       tlb-&gt;flush_pgtable(pgd, sizeof(*pgd), cookie);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I think you need to flush this before it becomes visible to the walker.</span>

I have flushed pgtable here, Do you meaning flush tlb here?

From the comment below, you don&#39;t think tlb-flush is necessary while the
new pte item is from invalid -&gt; valid,
<span class="quote">
&gt; </span>
<span class="quote">&gt; &gt; +               }</span>
<span class="quote">&gt; &gt; +               pte = arm_short_get_pte_in_pgd(*pgd, iova);</span>
<span class="quote">&gt; &gt; +       }</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +       pteprot |= (arm_short_iopte)paddr;</span>
<span class="quote">&gt; &gt; +       for (i = 0; i &lt; ptenr; i++) {</span>
<span class="quote">&gt; &gt; +               if (pte[i]) {/* Someone else may have allocated for this pte */</span>
<span class="quote">&gt; &gt; +                       WARN_ON(!selftest_running);</span>
<span class="quote">&gt; &gt; +                       goto err_exist_pte;</span>
<span class="quote">&gt; &gt; +               }</span>
<span class="quote">&gt; &gt; +               pte[i] = pteprot;</span>
<span class="quote">&gt; &gt; +       }</span>
<span class="quote">&gt; &gt; +       tlb-&gt;flush_pgtable(pte, ptenr * sizeof(*pte), cookie);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +       return 0;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +err_exist_pte:</span>
<span class="quote">&gt; &gt; +       while (i--)</span>
<span class="quote">&gt; &gt; +               pte[i] = 0;</span>
<span class="quote">&gt; &gt; +       if (ptenew)</span>
<span class="quote">&gt; &gt; +               kmem_cache_free(data-&gt;ptekmem, pte_va);</span>
<span class="quote">&gt; &gt; +       return -EEXIST;</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static int arm_short_map(struct io_pgtable_ops *ops, unsigned long iova,</span>
<span class="quote">&gt; &gt; +                        phys_addr_t paddr, size_t size, int prot)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +       struct arm_short_io_pgtable *data = io_pgtable_ops_to_data(ops);</span>
<span class="quote">&gt; &gt; +       const struct iommu_gather_ops *tlb = data-&gt;iop.cfg.tlb;</span>
<span class="quote">&gt; &gt; +       int ret;</span>
<span class="quote">&gt; &gt; +       arm_short_iopte pgdprot = 0, pteprot = 0;</span>
<span class="quote">&gt; &gt; +       bool large;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +       /* If no access, then nothing to do */</span>
<span class="quote">&gt; &gt; +       if (!(prot &amp; (IOMMU_READ | IOMMU_WRITE)))</span>
<span class="quote">&gt; &gt; +               return 0;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +       switch (size) {</span>
<span class="quote">&gt; &gt; +       case SZ_4K:</span>
<span class="quote">&gt; &gt; +       case SZ_64K:</span>
<span class="quote">&gt; &gt; +               large = (size == SZ_64K) ? true : false;</span>
<span class="quote">&gt; &gt; +               pteprot = __arm_short_pte_prot(data, prot, large);</span>
<span class="quote">&gt; &gt; +               pgdprot = __arm_short_pgtable_prot(data, prot &amp; IOMMU_NOEXEC);</span>
<span class="quote">&gt; &gt; +               break;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +       case SZ_1M:</span>
<span class="quote">&gt; &gt; +       case SZ_16M:</span>
<span class="quote">&gt; &gt; +               large = (size == SZ_16M) ? true : false;</span>
<span class="quote">&gt; &gt; +               pgdprot = __arm_short_pgd_prot(data, prot, large);</span>
<span class="quote">&gt; &gt; +               break;</span>
<span class="quote">&gt; &gt; +       default:</span>
<span class="quote">&gt; &gt; +               return -EINVAL;</span>
<span class="quote">&gt; &gt; +       }</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +       if (WARN_ON((iova | paddr) &amp; (size - 1)))</span>
<span class="quote">&gt; &gt; +               return -EINVAL;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +       ret = _arm_short_map(data, iova, paddr, pgdprot, pteprot, large);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +       tlb-&gt;tlb_add_flush(iova, size, true, data-&gt;iop.cookie);</span>
<span class="quote">&gt; &gt; +       tlb-&gt;tlb_sync(data-&gt;iop.cookie);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; In _arm_short_map, it looks like you can only go from invalid -&gt; valid,</span>
<span class="quote">&gt; so why do you need to flush the TLB here?</span>

I will delete tlb flush here. 
Then the tlb flush is only called after unmap and split.
<span class="quote">
&gt; </span>
<span class="quote">&gt; &gt; +       return ret;</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static phys_addr_t arm_short_iova_to_phys(struct io_pgtable_ops *ops,</span>
<span class="quote">&gt; &gt; +                                         unsigned long iova)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +       struct arm_short_io_pgtable *data = io_pgtable_ops_to_data(ops);</span>
<span class="quote">&gt; &gt; +       arm_short_iopte *pte, *pgd = data-&gt;pgd;</span>
<span class="quote">&gt; &gt; +       phys_addr_t pa = 0;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +       pgd += ARM_SHORT_PGD_IDX(iova);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +       if (ARM_SHORT_PGD_TYPE_IS_PGTABLE(*pgd)) {</span>
<span class="quote">&gt; &gt; +               pte = arm_short_get_pte_in_pgd(*pgd, iova);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +               if (ARM_SHORT_PTE_TYPE_IS_LARGEPAGE(*pte)) {</span>
<span class="quote">&gt; &gt; +                       pa = (*pte) &amp; ARM_SHORT_PTE_LARGE_MSK;</span>
<span class="quote">&gt; &gt; +                       pa |= iova &amp; ~ARM_SHORT_PTE_LARGE_MSK;</span>
<span class="quote">&gt; &gt; +               } else if (ARM_SHORT_PTE_TYPE_IS_SMALLPAGE(*pte)) {</span>
<span class="quote">&gt; &gt; +                       pa = (*pte) &amp; ARM_SHORT_PTE_SMALL_MSK;</span>
<span class="quote">&gt; &gt; +                       pa |= iova &amp; ~ARM_SHORT_PTE_SMALL_MSK;</span>
<span class="quote">&gt; &gt; +               }</span>
<span class="quote">&gt; &gt; +       } else if (ARM_SHORT_PGD_TYPE_IS_SECTION(*pgd)) {</span>
<span class="quote">&gt; &gt; +               pa = (*pgd) &amp; ARM_SHORT_PGD_SECTION_MSK;</span>
<span class="quote">&gt; &gt; +               pa |= iova &amp; ~ARM_SHORT_PGD_SECTION_MSK;</span>
<span class="quote">&gt; &gt; +       } else if (ARM_SHORT_PGD_TYPE_IS_SUPERSECTION(*pgd)) {</span>
<span class="quote">&gt; &gt; +               pa = (*pgd) &amp; ARM_SHORT_PGD_SUPERSECTION_MSK;</span>
<span class="quote">&gt; &gt; +               pa |= iova &amp; ~ARM_SHORT_PGD_SUPERSECTION_MSK;</span>
<span class="quote">&gt; &gt; +       }</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +       return pa;</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static int</span>
<span class="quote">&gt; &gt; +arm_short_split_blk_unmap(struct io_pgtable_ops *ops, unsigned int iova,</span>
<span class="quote">&gt; &gt; +                         phys_addr_t paddr, size_t size,</span>
<span class="quote">&gt; &gt; +                         arm_short_iopte pgdprotup, arm_short_iopte pteprotup,</span>
<span class="quote">&gt; &gt; +                         size_t blk_size)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +       struct arm_short_io_pgtable *data = io_pgtable_ops_to_data(ops);</span>
<span class="quote">&gt; &gt; +       const struct iommu_gather_ops *tlb = data-&gt;iop.cfg.tlb;</span>
<span class="quote">&gt; &gt; +       struct io_pgtable_cfg *cfg = &amp;data-&gt;iop.cfg;</span>
<span class="quote">&gt; &gt; +       unsigned long *pgbitmap = &amp;cfg-&gt;pgsize_bitmap;</span>
<span class="quote">&gt; &gt; +       unsigned int blk_base, blk_start, blk_end;</span>
<span class="quote">&gt; &gt; +       arm_short_iopte pgdprot, pteprot;</span>
<span class="quote">&gt; &gt; +       size_t mapsize = 0, nextmapsize;</span>
<span class="quote">&gt; &gt; +       phys_addr_t blk_paddr;</span>
<span class="quote">&gt; &gt; +       int ret;</span>
<span class="quote">&gt; &gt; +       unsigned int i;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +       /* find the nearest mapsize */</span>
<span class="quote">&gt; &gt; +       for (i = find_first_bit(pgbitmap, BITS_PER_LONG);</span>
<span class="quote">&gt; &gt; +            i &lt; BITS_PER_LONG &amp;&amp; ((1 &lt;&lt; i) &lt; blk_size) &amp;&amp;</span>
<span class="quote">&gt; &gt; +            IS_ALIGNED(size, 1 &lt;&lt; i);</span>
<span class="quote">&gt; &gt; +            i = find_next_bit(pgbitmap, BITS_PER_LONG, i + 1))</span>
<span class="quote">&gt; &gt; +               mapsize = 1 &lt;&lt; i;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +       if (WARN_ON(!mapsize))</span>
<span class="quote">&gt; &gt; +               return 0; /* Bytes unmapped */</span>
<span class="quote">&gt; &gt; +       nextmapsize = 1 &lt;&lt; i;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +       blk_base = iova &amp; ~(blk_size - 1);</span>
<span class="quote">&gt; &gt; +       blk_start = blk_base;</span>
<span class="quote">&gt; &gt; +       blk_end = blk_start + blk_size;</span>
<span class="quote">&gt; &gt; +       blk_paddr = paddr;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +       for (; blk_start &lt; blk_end;</span>
<span class="quote">&gt; &gt; +            blk_start += mapsize, blk_paddr += mapsize) {</span>
<span class="quote">&gt; &gt; +               /* Unmap! */</span>
<span class="quote">&gt; &gt; +               if (blk_start == iova)</span>
<span class="quote">&gt; &gt; +                       continue;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +               /* Try to upper map */</span>
<span class="quote">&gt; &gt; +               if (blk_base != blk_start &amp;&amp;</span>
<span class="quote">&gt; &gt; +                   IS_ALIGNED(blk_start | blk_paddr, nextmapsize) &amp;&amp;</span>
<span class="quote">&gt; &gt; +                   mapsize != nextmapsize) {</span>
<span class="quote">&gt; &gt; +                       mapsize = nextmapsize;</span>
<span class="quote">&gt; &gt; +                       i = find_next_bit(pgbitmap, BITS_PER_LONG, i + 1);</span>
<span class="quote">&gt; &gt; +                       if (i &lt; BITS_PER_LONG)</span>
<span class="quote">&gt; &gt; +                               nextmapsize = 1 &lt;&lt; i;</span>
<span class="quote">&gt; &gt; +               }</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +               if (mapsize == SZ_1M) {</span>
<span class="quote">&gt; &gt; +                       pgdprot = pgdprotup;</span>
<span class="quote">&gt; &gt; +                       pgdprot |= __arm_short_pgd_prot(data, 0, false);</span>
<span class="quote">&gt; &gt; +                       pteprot = 0;</span>
<span class="quote">&gt; &gt; +               } else { /* small or large page */</span>
<span class="quote">&gt; &gt; +                       bool noexec = (blk_size == SZ_64K) ?</span>
<span class="quote">&gt; &gt; +                               (pteprotup &amp; ARM_SHORT_PTE_LARGE_XN) :</span>
<span class="quote">&gt; &gt; +                               (pgdprotup &amp; ARM_SHORT_PGD_SECTION_XN);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +                       pteprot = __arm_short_pte_prot_split(</span>
<span class="quote">&gt; &gt; +                                               data, pgdprotup, pteprotup,</span>
<span class="quote">&gt; &gt; +                                               mapsize == SZ_64K);</span>
<span class="quote">&gt; &gt; +                       pgdprot = __arm_short_pgtable_prot(data, noexec);</span>
<span class="quote">&gt; &gt; +               }</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +               ret = _arm_short_map(data, blk_start, blk_paddr, pgdprot,</span>
<span class="quote">&gt; &gt; +                                    pteprot, mapsize == SZ_64K);</span>
<span class="quote">&gt; &gt; +               if (ret &lt; 0) {</span>
<span class="quote">&gt; &gt; +                       /* Free the table we allocated */</span>
<span class="quote">&gt; &gt; +                       arm_short_iopte *pgd = data-&gt;pgd, *pte;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +                       pgd += ARM_SHORT_PGD_IDX(blk_base);</span>
<span class="quote">&gt; &gt; +                       if (*pgd) {</span>
<span class="quote">&gt; &gt; +                               pte = ARM_SHORT_GET_PTE_VA(*pgd);</span>
<span class="quote">&gt; &gt; +                               kmem_cache_free(data-&gt;ptekmem, pte);</span>
<span class="quote">&gt; &gt; +                               *pgd = 0;</span>
<span class="quote">&gt; &gt; +                               tlb-&gt;flush_pgtable(pgd, sizeof(*pgd),</span>
<span class="quote">&gt; &gt; +                                                  data-&gt;iop.cookie);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Again, the ordering looks totally wrong here. You need to:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;   (1) Zero the pgd pointer</span>
<span class="quote">&gt;   (2) Flush the pointer, so the walker can no longer follow the page table</span>
<span class="quote">&gt;   (3) Invalidate the TLB, so we don&#39;t have any cached intermediate walks</span>
<span class="quote">&gt;   (4) Sync the TLB</span>
<span class="quote">&gt;   (5) Free the memory</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; That said, I don&#39;t fully follow the logic here.</span>

Sorry. I didn&#39;t care the free flow specially. I will change it like
below:
//=======
       *pgd = 0;
       tlb-&gt;flush_pgtable(pgd, sizeof(*pgd), data-&gt;iop.cookie);
       tlb-&gt;tlb_add_flush(blk_base, blk_size, true, data-&gt;iop.cookie);
       tlb-&gt;tlb_sync(data-&gt;iop.cookie);
       kmem_cache_free(data-&gt;ptekmem, pte);
//============
<span class="quote">
&gt; </span>
<span class="quote">&gt; &gt; +                       }</span>
<span class="quote">&gt; &gt; +                       return 0;/* Bytes unmapped */</span>
<span class="quote">&gt; &gt; +               }</span>
<span class="quote">&gt; &gt; +               tlb-&gt;tlb_add_flush(blk_start, mapsize, true, data-&gt;iop.cookie);</span>
<span class="quote">&gt; &gt; +               tlb-&gt;tlb_sync(data-&gt;iop.cookie);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Why are you doing this here for every iteration?</span>

I will move it out from the loop.
<span class="quote">
&gt; Will</span>


--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=7096">Will Deacon</a> - July 24, 2015, 4:53 p.m.</div>
<pre class="content">
On Fri, Jul 24, 2015 at 06:24:26AM +0100, Yong Wu wrote:
<span class="quote">&gt; On Tue, 2015-07-21 at 18:11 +0100, Will Deacon wrote:</span>
<span class="quote">&gt; &gt; On Thu, Jul 16, 2015 at 10:04:32AM +0100, Yong Wu wrote:</span>
<span class="quote">&gt; &gt; &gt; +/* level 2 pagetable */</span>
<span class="quote">&gt; &gt; &gt; +#define ARM_SHORT_PTE_TYPE_LARGE               BIT(0)</span>
<span class="quote">&gt; &gt; &gt; +#define ARM_SHORT_PTE_SMALL_XN                 BIT(0)</span>
<span class="quote">&gt; &gt; &gt; +#define ARM_SHORT_PTE_TYPE_SMALL               BIT(1)</span>
<span class="quote">&gt; &gt; &gt; +#define ARM_SHORT_PTE_B                                BIT(2)</span>
<span class="quote">&gt; &gt; &gt; +#define ARM_SHORT_PTE_C                                BIT(3)</span>
<span class="quote">&gt; &gt; &gt; +#define ARM_SHORT_PTE_SMALL_TEX0               BIT(6)</span>
<span class="quote">&gt; &gt; &gt; +#define ARM_SHORT_PTE_IMPLE                    BIT(9)</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; This is AP[2] for small pages.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Sorry, In our pagetable bit9 in PGD and PTE is PA[32] that is for  the</span>
<span class="quote">&gt; dram size over 4G. I didn&#39;t care it is different in PTE of the standard</span>
<span class="quote">&gt; spec.</span>
<span class="quote">&gt; And I don&#39;t use the AP[2] currently, so I only delete this line in next</span>
<span class="quote">&gt; time.</span>

Is this related to the &quot;special bit&quot;. What would be good is a comment
next to the #define for the quirk describing *exactly* that differs in
your implementation. Without that, it&#39;s very difficult to know what is
intentional and what is actually broken.
<span class="quote">
&gt; &gt; &gt; +static arm_short_iopte</span>
<span class="quote">&gt; &gt; &gt; +__arm_short_pte_prot(struct arm_short_io_pgtable *data, int prot, bool large)</span>
<span class="quote">&gt; &gt; &gt; +{</span>
<span class="quote">&gt; &gt; &gt; +       arm_short_iopte pteprot;</span>
<span class="quote">&gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; &gt; +       pteprot = ARM_SHORT_PTE_S | ARM_SHORT_PTE_nG;</span>
<span class="quote">&gt; &gt; &gt; +       pteprot |= large ? ARM_SHORT_PTE_TYPE_LARGE :</span>
<span class="quote">&gt; &gt; &gt; +                               ARM_SHORT_PTE_TYPE_SMALL;</span>
<span class="quote">&gt; &gt; &gt; +       if (prot &amp; IOMMU_CACHE)</span>
<span class="quote">&gt; &gt; &gt; +               pteprot |=  ARM_SHORT_PTE_B | ARM_SHORT_PTE_C;</span>
<span class="quote">&gt; &gt; &gt; +       if (prot &amp; IOMMU_WRITE)</span>
<span class="quote">&gt; &gt; &gt; +               pteprot |= large ? ARM_SHORT_PTE_LARGE_TEX0 :</span>
<span class="quote">&gt; &gt; &gt; +                               ARM_SHORT_PTE_SMALL_TEX0;</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; This doesn&#39;t make any sense. TEX[2:0] is all about memory attributes, not</span>
<span class="quote">&gt; &gt; permissions, so you&#39;re making the mapping write-back, write-allocate but</span>
<span class="quote">&gt; &gt; that&#39;s not what the IOMMU_* values are about.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;      I will delete it.</span>

Well, can you not control mapping permissions with the AP bits? The idea
of the IOMMU flags are:

  IOMMU_CACHE : Install a normal, cacheable mapping (you&#39;ve got this right)
  IOMMU_READ : Allow read access for the device
  IOMMU_WRITE : Allow write access for the device
  IOMMU_NOEXEC : Disallow execute access for the device

so the caller to iommu_map passes in a bitmap of these, which you need to
encode in the page-table entry.
<span class="quote">
&gt; &gt; &gt; +static int</span>
<span class="quote">&gt; &gt; &gt; +_arm_short_map(struct arm_short_io_pgtable *data,</span>
<span class="quote">&gt; &gt; &gt; +              unsigned int iova, phys_addr_t paddr,</span>
<span class="quote">&gt; &gt; &gt; +              arm_short_iopte pgdprot, arm_short_iopte pteprot,</span>
<span class="quote">&gt; &gt; &gt; +              bool large)</span>
<span class="quote">&gt; &gt; &gt; +{</span>
<span class="quote">&gt; &gt; &gt; +       const struct iommu_gather_ops *tlb = data-&gt;iop.cfg.tlb;</span>
<span class="quote">&gt; &gt; &gt; +       arm_short_iopte *pgd = data-&gt;pgd, *pte;</span>
<span class="quote">&gt; &gt; &gt; +       void *cookie = data-&gt;iop.cookie, *pte_va;</span>
<span class="quote">&gt; &gt; &gt; +       unsigned int ptenr = large ? 16 : 1;</span>
<span class="quote">&gt; &gt; &gt; +       int i, quirk = data-&gt;iop.cfg.quirks;</span>
<span class="quote">&gt; &gt; &gt; +       bool ptenew = false;</span>
<span class="quote">&gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; &gt; +       pgd += ARM_SHORT_PGD_IDX(iova);</span>
<span class="quote">&gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; &gt; +       if (!pteprot) { /* section or supersection */</span>
<span class="quote">&gt; &gt; &gt; +               if (quirk &amp; IO_PGTABLE_QUIRK_SHORT_MTK)</span>
<span class="quote">&gt; &gt; &gt; +                       pgdprot &amp;= ~ARM_SHORT_PGD_SECTION_XN;</span>
<span class="quote">&gt; &gt; &gt; +               pte = pgd;</span>
<span class="quote">&gt; &gt; &gt; +               pteprot = pgdprot;</span>
<span class="quote">&gt; &gt; &gt; +       } else {        /* page or largepage */</span>
<span class="quote">&gt; &gt; &gt; +               if (quirk &amp; IO_PGTABLE_QUIRK_SHORT_MTK) {</span>
<span class="quote">&gt; &gt; &gt; +                       if (large) { /* special Bit */</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; This definitely needs a better comment! What exactly are you doing here</span>
<span class="quote">&gt; &gt; and what is that quirk all about?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I use this quirk is for MTK Special Bit as we don&#39;t have the XN bit in</span>
<span class="quote">&gt; pagetable.</span>

I&#39;m still not really clear about what this is.
<span class="quote">
&gt; &gt; &gt; +               if (!(*pgd)) {</span>
<span class="quote">&gt; &gt; &gt; +                       pte_va = kmem_cache_zalloc(data-&gt;ptekmem, GFP_ATOMIC);</span>
<span class="quote">&gt; &gt; &gt; +                       if (unlikely(!pte_va))</span>
<span class="quote">&gt; &gt; &gt; +                               return -ENOMEM;</span>
<span class="quote">&gt; &gt; &gt; +                       ptenew = true;</span>
<span class="quote">&gt; &gt; &gt; +                       *pgd = virt_to_phys(pte_va) | pgdprot;</span>
<span class="quote">&gt; &gt; &gt; +                       kmemleak_ignore(pte_va);</span>
<span class="quote">&gt; &gt; &gt; +                       tlb-&gt;flush_pgtable(pgd, sizeof(*pgd), cookie);</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; I think you need to flush this before it becomes visible to the walker.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I have flushed pgtable here, Do you meaning flush tlb here?</span>

No. afaict, you allocate the pte table using kmem_cache_zalloc but you never
flush it. However, you update the pgd to point at this table, so the walker
can potentially see garbage instead of the zeroed entries.

Will
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=123111">Yong Wu</a> - July 27, 2015, 4:21 a.m.</div>
<pre class="content">
On Fri, 2015-07-24 at 17:53 +0100, Will Deacon wrote:
<span class="quote">&gt; On Fri, Jul 24, 2015 at 06:24:26AM +0100, Yong Wu wrote:</span>
<span class="quote">&gt; &gt; On Tue, 2015-07-21 at 18:11 +0100, Will Deacon wrote:</span>
<span class="quote">&gt; &gt; &gt; On Thu, Jul 16, 2015 at 10:04:32AM +0100, Yong Wu wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; +/* level 2 pagetable */</span>
<span class="quote">&gt; &gt; &gt; &gt; +#define ARM_SHORT_PTE_TYPE_LARGE               BIT(0)</span>
<span class="quote">&gt; &gt; &gt; &gt; +#define ARM_SHORT_PTE_SMALL_XN                 BIT(0)</span>
<span class="quote">&gt; &gt; &gt; &gt; +#define ARM_SHORT_PTE_TYPE_SMALL               BIT(1)</span>
<span class="quote">&gt; &gt; &gt; &gt; +#define ARM_SHORT_PTE_B                                BIT(2)</span>
<span class="quote">&gt; &gt; &gt; &gt; +#define ARM_SHORT_PTE_C                                BIT(3)</span>
<span class="quote">&gt; &gt; &gt; &gt; +#define ARM_SHORT_PTE_SMALL_TEX0               BIT(6)</span>
<span class="quote">&gt; &gt; &gt; &gt; +#define ARM_SHORT_PTE_IMPLE                    BIT(9)</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; This is AP[2] for small pages.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Sorry, In our pagetable bit9 in PGD and PTE is PA[32] that is for  the</span>
<span class="quote">&gt; &gt; dram size over 4G. I didn&#39;t care it is different in PTE of the standard</span>
<span class="quote">&gt; &gt; spec.</span>
<span class="quote">&gt; &gt; And I don&#39;t use the AP[2] currently, so I only delete this line in next</span>
<span class="quote">&gt; &gt; time.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Is this related to the &quot;special bit&quot;. What would be good is a comment</span>
<span class="quote">&gt; next to the #define for the quirk describing *exactly* that differs in</span>
<span class="quote">&gt; your implementation. Without that, it&#39;s very difficult to know what is</span>
<span class="quote">&gt; intentional and what is actually broken.</span>

I will add the comment alongside the #define.
<span class="quote">
&gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; +static arm_short_iopte</span>
<span class="quote">&gt; &gt; &gt; &gt; +__arm_short_pte_prot(struct arm_short_io_pgtable *data, int prot, bool large)</span>
<span class="quote">&gt; &gt; &gt; &gt; +{</span>
<span class="quote">&gt; &gt; &gt; &gt; +       arm_short_iopte pteprot;</span>
<span class="quote">&gt; &gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; &gt; &gt; +       pteprot = ARM_SHORT_PTE_S | ARM_SHORT_PTE_nG;</span>
<span class="quote">&gt; &gt; &gt; &gt; +       pteprot |= large ? ARM_SHORT_PTE_TYPE_LARGE :</span>
<span class="quote">&gt; &gt; &gt; &gt; +                               ARM_SHORT_PTE_TYPE_SMALL;</span>
<span class="quote">&gt; &gt; &gt; &gt; +       if (prot &amp; IOMMU_CACHE)</span>
<span class="quote">&gt; &gt; &gt; &gt; +               pteprot |=  ARM_SHORT_PTE_B | ARM_SHORT_PTE_C;</span>
<span class="quote">&gt; &gt; &gt; &gt; +       if (prot &amp; IOMMU_WRITE)</span>
<span class="quote">&gt; &gt; &gt; &gt; +               pteprot |= large ? ARM_SHORT_PTE_LARGE_TEX0 :</span>
<span class="quote">&gt; &gt; &gt; &gt; +                               ARM_SHORT_PTE_SMALL_TEX0;</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; This doesn&#39;t make any sense. TEX[2:0] is all about memory attributes, not</span>
<span class="quote">&gt; &gt; &gt; permissions, so you&#39;re making the mapping write-back, write-allocate but</span>
<span class="quote">&gt; &gt; &gt; that&#39;s not what the IOMMU_* values are about.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt;      I will delete it.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Well, can you not control mapping permissions with the AP bits? The idea</span>
<span class="quote">&gt; of the IOMMU flags are:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;   IOMMU_CACHE : Install a normal, cacheable mapping (you&#39;ve got this right)</span>
<span class="quote">&gt;   IOMMU_READ : Allow read access for the device</span>
<span class="quote">&gt;   IOMMU_WRITE : Allow write access for the device</span>
<span class="quote">&gt;   IOMMU_NOEXEC : Disallow execute access for the device</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; so the caller to iommu_map passes in a bitmap of these, which you need to</span>
<span class="quote">&gt; encode in the page-table entry.</span>

From the spec, AP[2] differentiate the read/write and readonly.
How about this?: 
//===============
  #define ARM_SHORT_PGD_FULL_ACCESS  (3 &lt;&lt; 10) 
  #define ARM_SHORT_PGD_RDONLY       BIT(15)

  pgdprot |= ARM_SHORT_PGD_FULL_ACCESS;/* or other names? */
  if(!(prot &amp; IOMMU_WRITE) &amp;&amp; (prot &amp; IOMMU_READ))
     pgdprot |= ARM_SHORT_PGD_RDONLY;
//===============
pte is the same. 

Sorry, Our HW don&#39;t meet the standard spec fully. it don&#39;t implement the
AP bits.
<span class="quote">
&gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; +static int</span>
<span class="quote">&gt; &gt; &gt; &gt; +_arm_short_map(struct arm_short_io_pgtable *data,</span>
<span class="quote">&gt; &gt; &gt; &gt; +              unsigned int iova, phys_addr_t paddr,</span>
<span class="quote">&gt; &gt; &gt; &gt; +              arm_short_iopte pgdprot, arm_short_iopte pteprot,</span>
<span class="quote">&gt; &gt; &gt; &gt; +              bool large)</span>
<span class="quote">&gt; &gt; &gt; &gt; +{</span>
<span class="quote">&gt; &gt; &gt; &gt; +       const struct iommu_gather_ops *tlb = data-&gt;iop.cfg.tlb;</span>
<span class="quote">&gt; &gt; &gt; &gt; +       arm_short_iopte *pgd = data-&gt;pgd, *pte;</span>
<span class="quote">&gt; &gt; &gt; &gt; +       void *cookie = data-&gt;iop.cookie, *pte_va;</span>
<span class="quote">&gt; &gt; &gt; &gt; +       unsigned int ptenr = large ? 16 : 1;</span>
<span class="quote">&gt; &gt; &gt; &gt; +       int i, quirk = data-&gt;iop.cfg.quirks;</span>
<span class="quote">&gt; &gt; &gt; &gt; +       bool ptenew = false;</span>
<span class="quote">&gt; &gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; &gt; &gt; +       pgd += ARM_SHORT_PGD_IDX(iova);</span>
<span class="quote">&gt; &gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; &gt; &gt; +       if (!pteprot) { /* section or supersection */</span>
<span class="quote">&gt; &gt; &gt; &gt; +               if (quirk &amp; IO_PGTABLE_QUIRK_SHORT_MTK)</span>
<span class="quote">&gt; &gt; &gt; &gt; +                       pgdprot &amp;= ~ARM_SHORT_PGD_SECTION_XN;</span>
<span class="quote">&gt; &gt; &gt; &gt; +               pte = pgd;</span>
<span class="quote">&gt; &gt; &gt; &gt; +               pteprot = pgdprot;</span>
<span class="quote">&gt; &gt; &gt; &gt; +       } else {        /* page or largepage */</span>
<span class="quote">&gt; &gt; &gt; &gt; +               if (quirk &amp; IO_PGTABLE_QUIRK_SHORT_MTK) {</span>
<span class="quote">&gt; &gt; &gt; &gt; +                       if (large) { /* special Bit */</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; This definitely needs a better comment! What exactly are you doing here</span>
<span class="quote">&gt; &gt; &gt; and what is that quirk all about?</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I use this quirk is for MTK Special Bit as we don&#39;t have the XN bit in</span>
<span class="quote">&gt; &gt; pagetable.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I&#39;m still not really clear about what this is.</span>

There is some difference between the standard spec and MTK HW,
Our hw don&#39;t implement some bits, like XN and AP.
So I add a quirk for MTK special.
<span class="quote">
&gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; +               if (!(*pgd)) {</span>
<span class="quote">&gt; &gt; &gt; &gt; +                       pte_va = kmem_cache_zalloc(data-&gt;ptekmem, GFP_ATOMIC);</span>
<span class="quote">&gt; &gt; &gt; &gt; +                       if (unlikely(!pte_va))</span>
<span class="quote">&gt; &gt; &gt; &gt; +                               return -ENOMEM;</span>
<span class="quote">&gt; &gt; &gt; &gt; +                       ptenew = true;</span>
<span class="quote">&gt; &gt; &gt; &gt; +                       *pgd = virt_to_phys(pte_va) | pgdprot;</span>
<span class="quote">&gt; &gt; &gt; &gt; +                       kmemleak_ignore(pte_va);</span>
<span class="quote">&gt; &gt; &gt; &gt; +                       tlb-&gt;flush_pgtable(pgd, sizeof(*pgd), cookie);</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; I think you need to flush this before it becomes visible to the walker.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I have flushed pgtable here, Do you meaning flush tlb here?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; No. afaict, you allocate the pte table using kmem_cache_zalloc but you never</span>
<span class="quote">&gt; flush it. However, you update the pgd to point at this table, so the walker</span>
<span class="quote">&gt; can potentially see garbage instead of the zeroed entries.</span>

Thanks. I will add :
tlb-&gt;flush_pgtable(pte_va, ARM_SHORT_BYTES_PER_PTE, cookie);
<span class="quote">
&gt; </span>
<span class="quote">&gt; Will</span>


--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=77581">Robin Murphy</a> - July 27, 2015, 2:05 p.m.</div>
<pre class="content">
On 27/07/15 05:21, Yong Wu wrote:
[...]
<span class="quote">&gt;&gt;&gt;&gt;&gt; +static arm_short_iopte</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; +__arm_short_pte_prot(struct arm_short_io_pgtable *data, int prot, bool large)</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; +{</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; +       arm_short_iopte pteprot;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; +       pteprot = ARM_SHORT_PTE_S | ARM_SHORT_PTE_nG;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; +       pteprot |= large ? ARM_SHORT_PTE_TYPE_LARGE :</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; +                               ARM_SHORT_PTE_TYPE_SMALL;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; +       if (prot &amp; IOMMU_CACHE)</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; +               pteprot |=  ARM_SHORT_PTE_B | ARM_SHORT_PTE_C;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; +       if (prot &amp; IOMMU_WRITE)</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; +               pteprot |= large ? ARM_SHORT_PTE_LARGE_TEX0 :</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; +                               ARM_SHORT_PTE_SMALL_TEX0;</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; This doesn&#39;t make any sense. TEX[2:0] is all about memory attributes, not</span>
<span class="quote">&gt;&gt;&gt;&gt; permissions, so you&#39;re making the mapping write-back, write-allocate but</span>
<span class="quote">&gt;&gt;&gt;&gt; that&#39;s not what the IOMMU_* values are about.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;       I will delete it.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Well, can you not control mapping permissions with the AP bits? The idea</span>
<span class="quote">&gt;&gt; of the IOMMU flags are:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;    IOMMU_CACHE : Install a normal, cacheable mapping (you&#39;ve got this right)</span>
<span class="quote">&gt;&gt;    IOMMU_READ : Allow read access for the device</span>
<span class="quote">&gt;&gt;    IOMMU_WRITE : Allow write access for the device</span>
<span class="quote">&gt;&gt;    IOMMU_NOEXEC : Disallow execute access for the device</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; so the caller to iommu_map passes in a bitmap of these, which you need to</span>
<span class="quote">&gt;&gt; encode in the page-table entry.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  From the spec, AP[2] differentiate the read/write and readonly.</span>
<span class="quote">&gt; How about this?:</span>
<span class="quote">&gt; //===============</span>
<span class="quote">&gt;    #define ARM_SHORT_PGD_FULL_ACCESS  (3 &lt;&lt; 10)</span>
<span class="quote">&gt;    #define ARM_SHORT_PGD_RDONLY       BIT(15)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;    pgdprot |= ARM_SHORT_PGD_FULL_ACCESS;/* or other names? */</span>
<span class="quote">&gt;    if(!(prot &amp; IOMMU_WRITE) &amp;&amp; (prot &amp; IOMMU_READ))</span>
<span class="quote">&gt;       pgdprot |= ARM_SHORT_PGD_RDONLY;</span>
<span class="quote">&gt; //===============</span>
<span class="quote">&gt; pte is the same.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Sorry, Our HW don&#39;t meet the standard spec fully. it don&#39;t implement the</span>
<span class="quote">&gt; AP bits.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; +static int</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; +_arm_short_map(struct arm_short_io_pgtable *data,</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; +              unsigned int iova, phys_addr_t paddr,</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; +              arm_short_iopte pgdprot, arm_short_iopte pteprot,</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; +              bool large)</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; +{</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; +       const struct iommu_gather_ops *tlb = data-&gt;iop.cfg.tlb;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; +       arm_short_iopte *pgd = data-&gt;pgd, *pte;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; +       void *cookie = data-&gt;iop.cookie, *pte_va;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; +       unsigned int ptenr = large ? 16 : 1;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; +       int i, quirk = data-&gt;iop.cfg.quirks;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; +       bool ptenew = false;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; +       pgd += ARM_SHORT_PGD_IDX(iova);</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; +       if (!pteprot) { /* section or supersection */</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; +               if (quirk &amp; IO_PGTABLE_QUIRK_SHORT_MTK)</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; +                       pgdprot &amp;= ~ARM_SHORT_PGD_SECTION_XN;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; +               pte = pgd;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; +               pteprot = pgdprot;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; +       } else {        /* page or largepage */</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; +               if (quirk &amp; IO_PGTABLE_QUIRK_SHORT_MTK) {</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; +                       if (large) { /* special Bit */</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; This definitely needs a better comment! What exactly are you doing here</span>
<span class="quote">&gt;&gt;&gt;&gt; and what is that quirk all about?</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; I use this quirk is for MTK Special Bit as we don&#39;t have the XN bit in</span>
<span class="quote">&gt;&gt;&gt; pagetable.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; I&#39;m still not really clear about what this is.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; There is some difference between the standard spec and MTK HW,</span>
<span class="quote">&gt; Our hw don&#39;t implement some bits, like XN and AP.</span>
<span class="quote">&gt; So I add a quirk for MTK special.</span>

When you say it doesn&#39;t implement these bits, do you mean that having 
them set will lead to Bad Things happening in the hardware, or that it 
will simply ignore them and not enforce any of the protections they 
imply? The former case would definitely want clearly documenting 
somewhere, whereas for the latter case I&#39;m not sure it&#39;s even worth the 
complication of having a quirk - if the value doesn&#39;t matter there seems 
little point in doing a special dance just for the sake of semantic 
correctness of the in-memory PTEs, in my opinion.

Robin.

--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=7096">Will Deacon</a> - July 27, 2015, 2:11 p.m.</div>
<pre class="content">
On Mon, Jul 27, 2015 at 03:05:38PM +0100, Robin Murphy wrote:
<span class="quote">&gt; On 27/07/15 05:21, Yong Wu wrote:</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt; +       } else {        /* page or largepage */</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt; +               if (quirk &amp; IO_PGTABLE_QUIRK_SHORT_MTK) {</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt; +                       if (large) { /* special Bit */</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; This definitely needs a better comment! What exactly are you doing here</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; and what is that quirk all about?</span>
<span class="quote">&gt; &gt;&gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt; I use this quirk is for MTK Special Bit as we don&#39;t have the XN bit in</span>
<span class="quote">&gt; &gt;&gt;&gt; pagetable.</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; I&#39;m still not really clear about what this is.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; There is some difference between the standard spec and MTK HW,</span>
<span class="quote">&gt; &gt; Our hw don&#39;t implement some bits, like XN and AP.</span>
<span class="quote">&gt; &gt; So I add a quirk for MTK special.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; When you say it doesn&#39;t implement these bits, do you mean that having </span>
<span class="quote">&gt; them set will lead to Bad Things happening in the hardware, or that it </span>
<span class="quote">&gt; will simply ignore them and not enforce any of the protections they </span>
<span class="quote">&gt; imply? The former case would definitely want clearly documenting </span>
<span class="quote">&gt; somewhere, whereas for the latter case I&#39;m not sure it&#39;s even worth the </span>
<span class="quote">&gt; complication of having a quirk - if the value doesn&#39;t matter there seems </span>
<span class="quote">&gt; little point in doing a special dance just for the sake of semantic </span>
<span class="quote">&gt; correctness of the in-memory PTEs, in my opinion.</span>

Agreed. We should only use quirks if the current (architecturally
compliant) code causes real issues with the hardware. Then the quirk can
be used to either avoid the problematic routines or to take extra steps
to make things work as the architecture intended.

I&#39;ve asked how this IOMMU differs from the architecture on a number of
occasions, but I&#39;m still yet to receive a response other than &quot;it&#39;s special&quot;.

Will
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=123111">Yong Wu</a> - July 28, 2015, 5:08 a.m.</div>
<pre class="content">
On Mon, 2015-07-27 at 15:11 +0100, Will Deacon wrote:
<span class="quote">&gt; On Mon, Jul 27, 2015 at 03:05:38PM +0100, Robin Murphy wrote:</span>
<span class="quote">&gt; &gt; On 27/07/15 05:21, Yong Wu wrote:</span>
<span class="quote">&gt; &gt; &gt;&gt;&gt;&gt;&gt; +       } else {        /* page or largepage */</span>
<span class="quote">&gt; &gt; &gt;&gt;&gt;&gt;&gt; +               if (quirk &amp; IO_PGTABLE_QUIRK_SHORT_MTK) {</span>
<span class="quote">&gt; &gt; &gt;&gt;&gt;&gt;&gt; +                       if (large) { /* special Bit */</span>
<span class="quote">&gt; &gt; &gt;&gt;&gt;&gt;</span>
<span class="quote">&gt; &gt; &gt;&gt;&gt;&gt; This definitely needs a better comment! What exactly are you doing here</span>
<span class="quote">&gt; &gt; &gt;&gt;&gt;&gt; and what is that quirk all about?</span>
<span class="quote">&gt; &gt; &gt;&gt;&gt;</span>
<span class="quote">&gt; &gt; &gt;&gt;&gt; I use this quirk is for MTK Special Bit as we don&#39;t have the XN bit in</span>
<span class="quote">&gt; &gt; &gt;&gt;&gt; pagetable.</span>
<span class="quote">&gt; &gt; &gt;&gt;</span>
<span class="quote">&gt; &gt; &gt;&gt; I&#39;m still not really clear about what this is.</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; There is some difference between the standard spec and MTK HW,</span>
<span class="quote">&gt; &gt; &gt; Our hw don&#39;t implement some bits, like XN and AP.</span>
<span class="quote">&gt; &gt; &gt; So I add a quirk for MTK special.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; When you say it doesn&#39;t implement these bits, do you mean that having </span>
<span class="quote">&gt; &gt; them set will lead to Bad Things happening in the hardware, or that it </span>
<span class="quote">&gt; &gt; will simply ignore them and not enforce any of the protections they </span>
<span class="quote">&gt; &gt; imply? The former case would definitely want clearly documenting </span>
<span class="quote">&gt; &gt; somewhere, whereas for the latter case I&#39;m not sure it&#39;s even worth the </span>
<span class="quote">&gt; &gt; complication of having a quirk - if the value doesn&#39;t matter there seems </span>
<span class="quote">&gt; &gt; little point in doing a special dance just for the sake of semantic </span>
<span class="quote">&gt; &gt; correctness of the in-memory PTEs, in my opinion.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Agreed. We should only use quirks if the current (architecturally</span>
<span class="quote">&gt; compliant) code causes real issues with the hardware. Then the quirk can</span>
<span class="quote">&gt; be used to either avoid the problematic routines or to take extra steps</span>
<span class="quote">&gt; to make things work as the architecture intended.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I&#39;ve asked how this IOMMU differs from the architecture on a number of</span>
<span class="quote">&gt; occasions, but I&#39;m still yet to receive a response other than &quot;it&#39;s special&quot;.</span>
<span class="quote">&gt; </span>

After check further with DE, Our pagetable is refer to ARM-v7&#39;s
short-descriptor which is a little different from ARM-v8. like bit0(PXN)
in section and supersection, I didn&#39;t read ARM-v7 spec before, so I add
a MTK quirk to disable PXN bit in section and supersection.(if the PXN
bit is wrote in ARM-v7 spec, HW will page fault.)

Then I write this code according to ARM-v8 spec defaultly, and add a
ARM-v7 quirk?

And there is a little different between ARM-v7 spec and MTK pagetable.
It&#39;s the XN(bit0) in small page. MTK don&#39;t implement XN bit. 
The bit[1:0] in MTK&#39;s small page should be 2&#39;b10, if it&#39;s 2&#39;b11, HW will
page fault.
(MTK don&#39;t implement AP bits too, but HW don&#39;t use them, it is ok even
though AP bits is wrote)

In the end, I will add two quirk like this, is it OK?

//===========
#define ARM_PGTABLE_QUIRK_SHORT_ARM_V7   BIT(2)  /* for ARM-v7 while
default is the ARM-v8 spec */
#define ARM_PGTABLE_QUIRK_SHORT_MTK  BIT(3)      /* MTK special */
//===========

In the ARM_V7 quirk, I only disable PXN bit in section and supersection,
In the MTK quirk, I only disbable XN in small page.

The other bits seems the same. I&#39;m not sure I write clearly and It seems
we could not copy a image of mtk pagetable here..If any question, please
help tell me.
Thanks very much.
<span class="quote">
&gt; Will</span>


--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=7096">Will Deacon</a> - July 28, 2015, 11 a.m.</div>
<pre class="content">
On Tue, Jul 28, 2015 at 06:08:14AM +0100, Yong Wu wrote:
<span class="quote">&gt; On Mon, 2015-07-27 at 15:11 +0100, Will Deacon wrote:</span>
<span class="quote">&gt; &gt; On Mon, Jul 27, 2015 at 03:05:38PM +0100, Robin Murphy wrote:</span>
<span class="quote">&gt; &gt; &gt; On 27/07/15 05:21, Yong Wu wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt; +       } else {        /* page or largepage */</span>
<span class="quote">&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt; +               if (quirk &amp; IO_PGTABLE_QUIRK_SHORT_MTK) {</span>
<span class="quote">&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt; +                       if (large) { /* special Bit */</span>
<span class="quote">&gt; &gt; &gt; &gt;&gt;&gt;&gt;</span>
<span class="quote">&gt; &gt; &gt; &gt;&gt;&gt;&gt; This definitely needs a better comment! What exactly are you doing here</span>
<span class="quote">&gt; &gt; &gt; &gt;&gt;&gt;&gt; and what is that quirk all about?</span>
<span class="quote">&gt; &gt; &gt; &gt;&gt;&gt;</span>
<span class="quote">&gt; &gt; &gt; &gt;&gt;&gt; I use this quirk is for MTK Special Bit as we don&#39;t have the XN bit in</span>
<span class="quote">&gt; &gt; &gt; &gt;&gt;&gt; pagetable.</span>
<span class="quote">&gt; &gt; &gt; &gt;&gt;</span>
<span class="quote">&gt; &gt; &gt; &gt;&gt; I&#39;m still not really clear about what this is.</span>
<span class="quote">&gt; &gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; &gt; There is some difference between the standard spec and MTK HW,</span>
<span class="quote">&gt; &gt; &gt; &gt; Our hw don&#39;t implement some bits, like XN and AP.</span>
<span class="quote">&gt; &gt; &gt; &gt; So I add a quirk for MTK special.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; When you say it doesn&#39;t implement these bits, do you mean that having </span>
<span class="quote">&gt; &gt; &gt; them set will lead to Bad Things happening in the hardware, or that it </span>
<span class="quote">&gt; &gt; &gt; will simply ignore them and not enforce any of the protections they </span>
<span class="quote">&gt; &gt; &gt; imply? The former case would definitely want clearly documenting </span>
<span class="quote">&gt; &gt; &gt; somewhere, whereas for the latter case I&#39;m not sure it&#39;s even worth the </span>
<span class="quote">&gt; &gt; &gt; complication of having a quirk - if the value doesn&#39;t matter there seems </span>
<span class="quote">&gt; &gt; &gt; little point in doing a special dance just for the sake of semantic </span>
<span class="quote">&gt; &gt; &gt; correctness of the in-memory PTEs, in my opinion.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Agreed. We should only use quirks if the current (architecturally</span>
<span class="quote">&gt; &gt; compliant) code causes real issues with the hardware. Then the quirk can</span>
<span class="quote">&gt; &gt; be used to either avoid the problematic routines or to take extra steps</span>
<span class="quote">&gt; &gt; to make things work as the architecture intended.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I&#39;ve asked how this IOMMU differs from the architecture on a number of</span>
<span class="quote">&gt; &gt; occasions, but I&#39;m still yet to receive a response other than &quot;it&#39;s special&quot;.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; After check further with DE, Our pagetable is refer to ARM-v7&#39;s</span>
<span class="quote">&gt; short-descriptor which is a little different from ARM-v8. like bit0(PXN)</span>
<span class="quote">&gt; in section and supersection, I didn&#39;t read ARM-v7 spec before, so I add</span>
<span class="quote">&gt; a MTK quirk to disable PXN bit in section and supersection.(if the PXN</span>
<span class="quote">&gt; bit is wrote in ARM-v7 spec, HW will page fault.)</span>

I&#39;ve been reviewing this using the ARMv7 ARM (Rev.C of DDI0406C) the whole
time. PXN is there as an optional field in non-LPAE implementations. That&#39;s
fine and doesn&#39;t require any quirks.
<span class="quote">
&gt; Then I write this code according to ARM-v8 spec defaultly, and add a</span>
<span class="quote">&gt; ARM-v7 quirk?</span>

No, I don&#39;t think you need this, as the v8 and v7 short-descriptor formats
look compatible to me. You should only need a quirk if architecturally
compliant code cannot work on your hardware.
<span class="quote">
&gt; And there is a little different between ARM-v7 spec and MTK pagetable.</span>
<span class="quote">&gt; It&#39;s the XN(bit0) in small page. MTK don&#39;t implement XN bit. </span>
<span class="quote">&gt; The bit[1:0] in MTK&#39;s small page should be 2&#39;b10, if it&#39;s 2&#39;b11, HW will</span>
<span class="quote">&gt; page fault.</span>

Aha, thanks! *That* is worthy of a quirk. Something like:

  IO_PGTABLE_QUIRK_ARM_NO_XN
<span class="quote">
&gt; (MTK don&#39;t implement AP bits too, but HW don&#39;t use them, it is ok even</span>
<span class="quote">&gt; though AP bits is wrote)</span>

Yeah, I think that&#39;s fine. The pgtable code will honour the request but
the h/w will ignore it.
<span class="quote">
&gt; In the end, I will add two quirk like this, is it OK?</span>

I think you only need the one I mentioned above. I don&#39;t see the need
for PXN at all (as I said in the last review).

Will
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=123111">Yong Wu</a> - July 28, 2015, 1:37 p.m.</div>
<pre class="content">
On Tue, 2015-07-28 at 12:00 +0100, Will Deacon wrote:
<span class="quote">&gt; On Tue, Jul 28, 2015 at 06:08:14AM +0100, Yong Wu wrote:</span>
<span class="quote">&gt; &gt; On Mon, 2015-07-27 at 15:11 +0100, Will Deacon wrote:</span>
<span class="quote">&gt; &gt; &gt; On Mon, Jul 27, 2015 at 03:05:38PM +0100, Robin Murphy wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; On 27/07/15 05:21, Yong Wu wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt; +       } else {        /* page or largepage */</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt; +               if (quirk &amp; IO_PGTABLE_QUIRK_SHORT_MTK) {</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt; +                       if (large) { /* special Bit */</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt; This definitely needs a better comment! What exactly are you doing here</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt; and what is that quirk all about?</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;&gt;&gt;</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;&gt;&gt; I use this quirk is for MTK Special Bit as we don&#39;t have the XN bit in</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;&gt;&gt; pagetable.</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;&gt;</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;&gt; I&#39;m still not really clear about what this is.</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; There is some difference between the standard spec and MTK HW,</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; Our hw don&#39;t implement some bits, like XN and AP.</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; So I add a quirk for MTK special.</span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; When you say it doesn&#39;t implement these bits, do you mean that having </span>
<span class="quote">&gt; &gt; &gt; &gt; them set will lead to Bad Things happening in the hardware, or that it </span>
<span class="quote">&gt; &gt; &gt; &gt; will simply ignore them and not enforce any of the protections they </span>
<span class="quote">&gt; &gt; &gt; &gt; imply? The former case would definitely want clearly documenting </span>
<span class="quote">&gt; &gt; &gt; &gt; somewhere, whereas for the latter case I&#39;m not sure it&#39;s even worth the </span>
<span class="quote">&gt; &gt; &gt; &gt; complication of having a quirk - if the value doesn&#39;t matter there seems </span>
<span class="quote">&gt; &gt; &gt; &gt; little point in doing a special dance just for the sake of semantic </span>
<span class="quote">&gt; &gt; &gt; &gt; correctness of the in-memory PTEs, in my opinion.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Agreed. We should only use quirks if the current (architecturally</span>
<span class="quote">&gt; &gt; &gt; compliant) code causes real issues with the hardware. Then the quirk can</span>
<span class="quote">&gt; &gt; &gt; be used to either avoid the problematic routines or to take extra steps</span>
<span class="quote">&gt; &gt; &gt; to make things work as the architecture intended.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; I&#39;ve asked how this IOMMU differs from the architecture on a number of</span>
<span class="quote">&gt; &gt; &gt; occasions, but I&#39;m still yet to receive a response other than &quot;it&#39;s special&quot;.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; After check further with DE, Our pagetable is refer to ARM-v7&#39;s</span>
<span class="quote">&gt; &gt; short-descriptor which is a little different from ARM-v8. like bit0(PXN)</span>
<span class="quote">&gt; &gt; in section and supersection, I didn&#39;t read ARM-v7 spec before, so I add</span>
<span class="quote">&gt; &gt; a MTK quirk to disable PXN bit in section and supersection.(if the PXN</span>
<span class="quote">&gt; &gt; bit is wrote in ARM-v7 spec, HW will page fault.)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I&#39;ve been reviewing this using the ARMv7 ARM (Rev.C of DDI0406C) the whole</span>
<span class="quote">&gt; time. PXN is there as an optional field in non-LPAE implementations. That&#39;s</span>
<span class="quote">&gt; fine and doesn&#39;t require any quirks.</span>

Thanks for your confirm.
Then I delete all the PXN bit in here?

Take a example, 
#define ARM_SHORT_PGD_SECTION_XN		(BIT(0) | BIT(4))
I will change it to &quot;BIT(4)&quot;.
<span class="quote">
&gt; </span>
<span class="quote">&gt; &gt; Then I write this code according to ARM-v8 spec defaultly, and add a</span>
<span class="quote">&gt; &gt; ARM-v7 quirk?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; No, I don&#39;t think you need this, as the v8 and v7 short-descriptor formats</span>
<span class="quote">&gt; look compatible to me. You should only need a quirk if architecturally</span>
<span class="quote">&gt; compliant code cannot work on your hardware.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; And there is a little different between ARM-v7 spec and MTK pagetable.</span>
<span class="quote">&gt; &gt; It&#39;s the XN(bit0) in small page. MTK don&#39;t implement XN bit. </span>
<span class="quote">&gt; &gt; The bit[1:0] in MTK&#39;s small page should be 2&#39;b10, if it&#39;s 2&#39;b11, HW will</span>
<span class="quote">&gt; &gt; page fault.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Aha, thanks! *That* is worthy of a quirk. Something like:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;   IO_PGTABLE_QUIRK_ARM_NO_XN</span>

Thanks, I will add it.
<span class="quote">
&gt; </span>
<span class="quote">&gt; &gt; (MTK don&#39;t implement AP bits too, but HW don&#39;t use them, it is ok even</span>
<span class="quote">&gt; &gt; though AP bits is wrote)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Yeah, I think that&#39;s fine. The pgtable code will honour the request but</span>
<span class="quote">&gt; the h/w will ignore it.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; In the end, I will add two quirk like this, is it OK?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I think you only need the one I mentioned above. I don&#39;t see the need</span>
<span class="quote">&gt; for PXN at all (as I said in the last review).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Will</span>


--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=7096">Will Deacon</a> - July 28, 2015, 1:47 p.m.</div>
<pre class="content">
On Tue, Jul 28, 2015 at 02:37:43PM +0100, Yong Wu wrote:
<span class="quote">&gt; On Tue, 2015-07-28 at 12:00 +0100, Will Deacon wrote:</span>
<span class="quote">&gt; &gt; On Tue, Jul 28, 2015 at 06:08:14AM +0100, Yong Wu wrote:</span>
<span class="quote">&gt; &gt; &gt; On Mon, 2015-07-27 at 15:11 +0100, Will Deacon wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; On Mon, Jul 27, 2015 at 03:05:38PM +0100, Robin Murphy wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; On 27/07/15 05:21, Yong Wu wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt; +       } else {        /* page or largepage */</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt; +               if (quirk &amp; IO_PGTABLE_QUIRK_SHORT_MTK) {</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt; +                       if (large) { /* special Bit */</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt; This definitely needs a better comment! What exactly are you doing here</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt; and what is that quirk all about?</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt; I use this quirk is for MTK Special Bit as we don&#39;t have the XN bit in</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt; pagetable.</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;&gt;</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;&gt; I&#39;m still not really clear about what this is.</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; There is some difference between the standard spec and MTK HW,</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; Our hw don&#39;t implement some bits, like XN and AP.</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; So I add a quirk for MTK special.</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; When you say it doesn&#39;t implement these bits, do you mean that having </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; them set will lead to Bad Things happening in the hardware, or that it </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; will simply ignore them and not enforce any of the protections they </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; imply? The former case would definitely want clearly documenting </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; somewhere, whereas for the latter case I&#39;m not sure it&#39;s even worth the </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; complication of having a quirk - if the value doesn&#39;t matter there seems </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; little point in doing a special dance just for the sake of semantic </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; correctness of the in-memory PTEs, in my opinion.</span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; Agreed. We should only use quirks if the current (architecturally</span>
<span class="quote">&gt; &gt; &gt; &gt; compliant) code causes real issues with the hardware. Then the quirk can</span>
<span class="quote">&gt; &gt; &gt; &gt; be used to either avoid the problematic routines or to take extra steps</span>
<span class="quote">&gt; &gt; &gt; &gt; to make things work as the architecture intended.</span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; I&#39;ve asked how this IOMMU differs from the architecture on a number of</span>
<span class="quote">&gt; &gt; &gt; &gt; occasions, but I&#39;m still yet to receive a response other than &quot;it&#39;s special&quot;.</span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; After check further with DE, Our pagetable is refer to ARM-v7&#39;s</span>
<span class="quote">&gt; &gt; &gt; short-descriptor which is a little different from ARM-v8. like bit0(PXN)</span>
<span class="quote">&gt; &gt; &gt; in section and supersection, I didn&#39;t read ARM-v7 spec before, so I add</span>
<span class="quote">&gt; &gt; &gt; a MTK quirk to disable PXN bit in section and supersection.(if the PXN</span>
<span class="quote">&gt; &gt; &gt; bit is wrote in ARM-v7 spec, HW will page fault.)</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I&#39;ve been reviewing this using the ARMv7 ARM (Rev.C of DDI0406C) the whole</span>
<span class="quote">&gt; &gt; time. PXN is there as an optional field in non-LPAE implementations. That&#39;s</span>
<span class="quote">&gt; &gt; fine and doesn&#39;t require any quirks.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Thanks for your confirm.</span>
<span class="quote">&gt; Then I delete all the PXN bit in here?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Take a example, </span>
<span class="quote">&gt; #define ARM_SHORT_PGD_SECTION_XN		(BIT(0) | BIT(4))</span>
<span class="quote">&gt; I will change it to &quot;BIT(4)&quot;.</span>

Yes. Then the PXN bit can be added later as a quirk when we have an
implementation that supports it.

Will
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=7096">Will Deacon</a> - July 31, 2015, 11:32 a.m.</div>
<pre class="content">
On Fri, Jul 31, 2015 at 08:55:37AM +0100, Yong Wu wrote:
<span class="quote">&gt;     About the AP bits, I may have to add a new quirk for it...</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;   Current I add AP in pte like this:</span>
<span class="quote">&gt; #define ARM_SHORT_PTE_RD_WR        (3 &lt;&lt; 4)</span>
<span class="quote">&gt; #define ARM_SHORT_PTE_RDONLY       BIT(9)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; pteprot |=  ARM_SHORT_PTE_RD_WR;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  If(!(prot &amp; IOMMU_WRITE) &amp;&amp; (prot &amp; IOMMU_READ))</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt;       pteprot |= ARM_SHORT_PTE_RDONLY;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The problem is that the BIT(9) in the level1 and level2 pagetable of our</span>
<span class="quote">&gt; HW has been used for PA[32] that is for the dram size over 4G.</span>

Aha, now *thats* a case of page-table abuse!
<span class="quote">
&gt; so I had to add a quirk to disable bit9 while RDONLY case.</span>
<span class="quote">&gt; (If BIT9 isn&#39;t disabled, the HW treat it as the PA[32] case then it will</span>
<span class="quote">&gt; translation fault..)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; like: IO_PGTABLE_QUIRK_SHORT_MTK ?</span>

Given that you don&#39;t have XN either, maybe IO_PGTABLE_QUIRK_NO_PERMS?
When set, IOMMU_READ/WRITE/EXEC are ignored and the mapping will never
generate a permission fault.

Will
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=123111">Yong Wu</a> - Sept. 14, 2015, 12:25 p.m.</div>
<pre class="content">
On Tue, 2015-07-21 at 18:11 +0100, Will Deacon wrote:
[...]
<span class="quote">&gt; &gt; +static int arm_short_map(struct io_pgtable_ops *ops, unsigned long iova,</span>
<span class="quote">&gt; &gt; +                        phys_addr_t paddr, size_t size, int prot)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +       struct arm_short_io_pgtable *data = io_pgtable_ops_to_data(ops);</span>
<span class="quote">&gt; &gt; +       const struct iommu_gather_ops *tlb = data-&gt;iop.cfg.tlb;</span>
<span class="quote">&gt; &gt; +       int ret;</span>
<span class="quote">&gt; &gt; +       arm_short_iopte pgdprot = 0, pteprot = 0;</span>
<span class="quote">&gt; &gt; +       bool large;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +       /* If no access, then nothing to do */</span>
<span class="quote">&gt; &gt; +       if (!(prot &amp; (IOMMU_READ | IOMMU_WRITE)))</span>
<span class="quote">&gt; &gt; +               return 0;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +       switch (size) {</span>
<span class="quote">&gt; &gt; +       case SZ_4K:</span>
<span class="quote">&gt; &gt; +       case SZ_64K:</span>
<span class="quote">&gt; &gt; +               large = (size == SZ_64K) ? true : false;</span>
<span class="quote">&gt; &gt; +               pteprot = __arm_short_pte_prot(data, prot, large);</span>
<span class="quote">&gt; &gt; +               pgdprot = __arm_short_pgtable_prot(data, prot &amp; IOMMU_NOEXEC);</span>
<span class="quote">&gt; &gt; +               break;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +       case SZ_1M:</span>
<span class="quote">&gt; &gt; +       case SZ_16M:</span>
<span class="quote">&gt; &gt; +               large = (size == SZ_16M) ? true : false;</span>
<span class="quote">&gt; &gt; +               pgdprot = __arm_short_pgd_prot(data, prot, large);</span>
<span class="quote">&gt; &gt; +               break;</span>
<span class="quote">&gt; &gt; +       default:</span>
<span class="quote">&gt; &gt; +               return -EINVAL;</span>
<span class="quote">&gt; &gt; +       }</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +       if (WARN_ON((iova | paddr) &amp; (size - 1)))</span>
<span class="quote">&gt; &gt; +               return -EINVAL;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +       ret = _arm_short_map(data, iova, paddr, pgdprot, pteprot, large);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +       tlb-&gt;tlb_add_flush(iova, size, true, data-&gt;iop.cookie);</span>
<span class="quote">&gt; &gt; +       tlb-&gt;tlb_sync(data-&gt;iop.cookie);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; In _arm_short_map, it looks like you can only go from invalid -&gt; valid,</span>
<span class="quote">&gt; so why do you need to flush the TLB here?</span>

Hi Will,
   Here is about flush-tlb after map iova, I have deleted it in v4
following this suggestion. But We meet a problem about it.

Take a example with JPEG. the test steps is:
a).JPEG HW decode a picture with the source iova,like 0xfd780000.
b).JPEG HW decode done, It will unmap the iova(write 0 in pagetable and
flush tlb).
c).JPEG HW decode the second picture, whose source iova is also
0xfd780000.
   Then our HW maybe fail due to it will auto prefetch, It may prefecth
between the step b) and c). then the HW may fetch the pagetable content
which has been unmapped in step b). then the HW will get the iova&#39;s
physical address is 0, It will translation fault!

    So I think our HW need flush-tlb after map iova. Could we add a
QUIRK like &quot;IO_PGTABLE_QUIRK_AUTO_PREFETCH_ENABLE&quot; for it?
If it&#39;s not allowed, we will have to add this in our internal function
mtk_iommu_map of mtk_iommu.c.
Thanks.
<span class="quote">
&gt; </span>
<span class="quote">&gt; &gt; +       return ret;</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
[...]


--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=7096">Will Deacon</a> - Sept. 16, 2015, 12:55 p.m.</div>
<pre class="content">
Hello Yong,

On Mon, Sep 14, 2015 at 01:25:00PM +0100, Yong Wu wrote:
<span class="quote">&gt; On Tue, 2015-07-21 at 18:11 +0100, Will Deacon wrote:</span>
<span class="quote">&gt; &gt; &gt; +       ret = _arm_short_map(data, iova, paddr, pgdprot, pteprot, large);</span>
<span class="quote">&gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; &gt; +       tlb-&gt;tlb_add_flush(iova, size, true, data-&gt;iop.cookie);</span>
<span class="quote">&gt; &gt; &gt; +       tlb-&gt;tlb_sync(data-&gt;iop.cookie);</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; In _arm_short_map, it looks like you can only go from invalid -&gt; valid,</span>
<span class="quote">&gt; &gt; so why do you need to flush the TLB here?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Hi Will,</span>
<span class="quote">&gt;    Here is about flush-tlb after map iova, I have deleted it in v4</span>
<span class="quote">&gt; following this suggestion. But We meet a problem about it.</span>

Ok.
<span class="quote">
&gt; Take a example with JPEG. the test steps is:</span>
<span class="quote">&gt; a).JPEG HW decode a picture with the source iova,like 0xfd780000.</span>
<span class="quote">&gt; b).JPEG HW decode done, It will unmap the iova(write 0 in pagetable and</span>
<span class="quote">&gt; flush tlb).</span>
<span class="quote">&gt; c).JPEG HW decode the second picture, whose source iova is also</span>
<span class="quote">&gt; 0xfd780000.</span>
<span class="quote">&gt;    Then our HW maybe fail due to it will auto prefetch, It may prefecth</span>
<span class="quote">&gt; between the step b) and c). then the HW may fetch the pagetable content</span>
<span class="quote">&gt; which has been unmapped in step b). then the HW will get the iova&#39;s</span>
<span class="quote">&gt; physical address is 0, It will translation fault!</span>

Oh no! So-called &quot;negative caching&quot; is certainly prohibited by the ARM
architecture, but if you&#39;ve built it then we can probably work around it
as an additional quirk. I assume the prefetcher stops prefetching when
it sees an invalid descriptor?
<span class="quote">
&gt;     So I think our HW need flush-tlb after map iova. Could we add a</span>
<span class="quote">&gt; QUIRK like &quot;IO_PGTABLE_QUIRK_AUTO_PREFETCH_ENABLE&quot; for it?</span>
<span class="quote">&gt; If it&#39;s not allowed, we will have to add this in our internal function</span>
<span class="quote">&gt; mtk_iommu_map of mtk_iommu.c.</span>

Actually, this type of quirk is ringing bells with me (I think another
IOMMU needed something similar in the past), so maybe just add
IO_PGTABLE_QUIRK_TLBI_ON_MAP?

Will
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=123111">Yong Wu</a> - Sept. 17, 2015, 2:38 a.m.</div>
<pre class="content">
On Wed, 2015-09-16 at 13:55 +0100, Will Deacon wrote:
<span class="quote">&gt; Hello Yong,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; On Mon, Sep 14, 2015 at 01:25:00PM +0100, Yong Wu wrote:</span>
<span class="quote">&gt; &gt; On Tue, 2015-07-21 at 18:11 +0100, Will Deacon wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; +       ret = _arm_short_map(data, iova, paddr, pgdprot, pteprot, large);</span>
<span class="quote">&gt; &gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; &gt; &gt; +       tlb-&gt;tlb_add_flush(iova, size, true, data-&gt;iop.cookie);</span>
<span class="quote">&gt; &gt; &gt; &gt; +       tlb-&gt;tlb_sync(data-&gt;iop.cookie);</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; In _arm_short_map, it looks like you can only go from invalid -&gt; valid,</span>
<span class="quote">&gt; &gt; &gt; so why do you need to flush the TLB here?</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Hi Will,</span>
<span class="quote">&gt; &gt;    Here is about flush-tlb after map iova, I have deleted it in v4</span>
<span class="quote">&gt; &gt; following this suggestion. But We meet a problem about it.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Ok.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; Take a example with JPEG. the test steps is:</span>
<span class="quote">&gt; &gt; a).JPEG HW decode a picture with the source iova,like 0xfd780000.</span>
<span class="quote">&gt; &gt; b).JPEG HW decode done, It will unmap the iova(write 0 in pagetable and</span>
<span class="quote">&gt; &gt; flush tlb).</span>
<span class="quote">&gt; &gt; c).JPEG HW decode the second picture, whose source iova is also</span>
<span class="quote">&gt; &gt; 0xfd780000.</span>
<span class="quote">&gt; &gt;    Then our HW maybe fail due to it will auto prefetch, It may prefecth</span>
<span class="quote">&gt; &gt; between the step b) and c). then the HW may fetch the pagetable content</span>
<span class="quote">&gt; &gt; which has been unmapped in step b). then the HW will get the iova&#39;s</span>
<span class="quote">&gt; &gt; physical address is 0, It will translation fault!</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Oh no! So-called &quot;negative caching&quot; is certainly prohibited by the ARM</span>
<span class="quote">&gt; architecture, but if you&#39;ve built it then we can probably work around it</span>
<span class="quote">&gt; as an additional quirk. I assume the prefetcher stops prefetching when</span>
<span class="quote">&gt; it sees an invalid descriptor?</span>

Yes, If it&#39;s a invalid descriptor, the HW will stop prefetch.
<span class="quote">
&gt; </span>
<span class="quote">&gt; &gt;     So I think our HW need flush-tlb after map iova. Could we add a</span>
<span class="quote">&gt; &gt; QUIRK like &quot;IO_PGTABLE_QUIRK_AUTO_PREFETCH_ENABLE&quot; for it?</span>
<span class="quote">&gt; &gt; If it&#39;s not allowed, we will have to add this in our internal function</span>
<span class="quote">&gt; &gt; mtk_iommu_map of mtk_iommu.c.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Actually, this type of quirk is ringing bells with me (I think another</span>
<span class="quote">&gt; IOMMU needed something similar in the past), so maybe just add</span>
<span class="quote">&gt; IO_PGTABLE_QUIRK_TLBI_ON_MAP?</span>

Thanks. I will add it like:
//=====================
ret = _arm_short_map(data, iova, paddr, pgdprot, pteprot, large);

if (data-&gt;iop.cfg.quirk &amp; IO_PGTABLE_QUIRK_TLBI_ON_MAP) {
	tlb-&gt;tlb_add_flush(iova, size, true, data-&gt;iop.cookie);
	tlb-&gt;tlb_sync(data-&gt;iop.cookie);
}
//======================
It will flush-tlb every time after map-iova. then the HW will fetch the
new PA from the dram.
<span class="quote">
&gt; </span>
<span class="quote">&gt; Will</span>


--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/drivers/iommu/Kconfig b/drivers/iommu/Kconfig</span>
<span class="p_header">index f1fb1d3..f50dbf3 100644</span>
<span class="p_header">--- a/drivers/iommu/Kconfig</span>
<span class="p_header">+++ b/drivers/iommu/Kconfig</span>
<span class="p_chunk">@@ -39,6 +39,24 @@</span> <span class="p_context"> config IOMMU_IO_PGTABLE_LPAE_SELFTEST</span>
 
 	  If unsure, say N here.
 
<span class="p_add">+config IOMMU_IO_PGTABLE_SHORT</span>
<span class="p_add">+	bool &quot;ARMv7/v8 Short Descriptor Format&quot;</span>
<span class="p_add">+	select IOMMU_IO_PGTABLE</span>
<span class="p_add">+	depends on ARM || ARM64 || COMPILE_TEST</span>
<span class="p_add">+	help</span>
<span class="p_add">+	  Enable support for the ARM Short descriptor pagetable format.</span>
<span class="p_add">+	  This allocator supports 2 levels translation tables which supports</span>
<span class="p_add">+	  a memory map based on memory sections or pages.</span>
<span class="p_add">+</span>
<span class="p_add">+config IOMMU_IO_PGTABLE_SHORT_SELFTEST</span>
<span class="p_add">+	bool &quot;Short Descriptor selftests&quot;</span>
<span class="p_add">+	depends on IOMMU_IO_PGTABLE_SHORT</span>
<span class="p_add">+	help</span>
<span class="p_add">+	  Enable self-tests for Short Descriptor page table allocator.</span>
<span class="p_add">+	  This performs a series of page-table consistency checks during boot.</span>
<span class="p_add">+</span>
<span class="p_add">+	  If unsure, say N here.</span>
<span class="p_add">+</span>
 endmenu
 
 config IOMMU_IOVA
<span class="p_header">diff --git a/drivers/iommu/Makefile b/drivers/iommu/Makefile</span>
<span class="p_header">index c6dcc51..06df3e6 100644</span>
<span class="p_header">--- a/drivers/iommu/Makefile</span>
<span class="p_header">+++ b/drivers/iommu/Makefile</span>
<span class="p_chunk">@@ -3,6 +3,7 @@</span> <span class="p_context"> obj-$(CONFIG_IOMMU_API) += iommu-traces.o</span>
 obj-$(CONFIG_IOMMU_API) += iommu-sysfs.o
 obj-$(CONFIG_IOMMU_IO_PGTABLE) += io-pgtable.o
 obj-$(CONFIG_IOMMU_IO_PGTABLE_LPAE) += io-pgtable-arm.o
<span class="p_add">+obj-$(CONFIG_IOMMU_IO_PGTABLE_SHORT) += io-pgtable-arm-short.o</span>
 obj-$(CONFIG_IOMMU_IOVA) += iova.o
 obj-$(CONFIG_OF_IOMMU)	+= of_iommu.o
 obj-$(CONFIG_MSM_IOMMU) += msm_iommu.o msm_iommu_dev.o
<span class="p_header">diff --git a/drivers/iommu/io-pgtable-arm-short.c b/drivers/iommu/io-pgtable-arm-short.c</span>
new file mode 100644
<span class="p_header">index 0000000..340d590</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/drivers/iommu/io-pgtable-arm-short.c</span>
<span class="p_chunk">@@ -0,0 +1,742 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (c) 2014-2015 MediaTek Inc.</span>
<span class="p_add">+ * Author: Yong Wu &lt;yong.wu@mediatek.com&gt;</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of the GNU General Public License version 2 as</span>
<span class="p_add">+ * published by the Free Software Foundation.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ * GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define pr_fmt(fmt)	&quot;arm-short-desc io-pgtable: &quot;fmt</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/err.h&gt;</span>
<span class="p_add">+#include &lt;linux/slab.h&gt;</span>
<span class="p_add">+#include &lt;linux/iommu.h&gt;</span>
<span class="p_add">+#include &lt;linux/errno.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#include &quot;io-pgtable.h&quot;</span>
<span class="p_add">+</span>
<span class="p_add">+typedef u32 arm_short_iopte;</span>
<span class="p_add">+</span>
<span class="p_add">+struct arm_short_io_pgtable {</span>
<span class="p_add">+	struct io_pgtable	iop;</span>
<span class="p_add">+	struct kmem_cache	*ptekmem;</span>
<span class="p_add">+	size_t			pgd_size;</span>
<span class="p_add">+	void			*pgd;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+#define io_pgtable_to_data(x)			\</span>
<span class="p_add">+	container_of((x), struct arm_short_io_pgtable, iop)</span>
<span class="p_add">+</span>
<span class="p_add">+#define io_pgtable_ops_to_data(x)		\</span>
<span class="p_add">+	io_pgtable_to_data(io_pgtable_ops_to_pgtable(x))</span>
<span class="p_add">+</span>
<span class="p_add">+#define ARM_SHORT_PGDIR_SHIFT			20</span>
<span class="p_add">+#define ARM_SHORT_PAGE_SHIFT			12</span>
<span class="p_add">+#define ARM_SHORT_PTRS_PER_PTE			\</span>
<span class="p_add">+	(1 &lt;&lt; (ARM_SHORT_PGDIR_SHIFT - ARM_SHORT_PAGE_SHIFT))</span>
<span class="p_add">+#define ARM_SHORT_BYTES_PER_PTE			\</span>
<span class="p_add">+	(ARM_SHORT_PTRS_PER_PTE * sizeof(arm_short_iopte))</span>
<span class="p_add">+</span>
<span class="p_add">+/* level 1 pagetable */</span>
<span class="p_add">+#define ARM_SHORT_PGD_TYPE_PGTABLE		BIT(0)</span>
<span class="p_add">+#define ARM_SHORT_PGD_SECTION_XN		(BIT(0) | BIT(4))</span>
<span class="p_add">+#define ARM_SHORT_PGD_TYPE_SECTION		BIT(1)</span>
<span class="p_add">+#define ARM_SHORT_PGD_PGTABLE_XN		BIT(2)</span>
<span class="p_add">+#define ARM_SHORT_PGD_B				BIT(2)</span>
<span class="p_add">+#define ARM_SHORT_PGD_C				BIT(3)</span>
<span class="p_add">+#define ARM_SHORT_PGD_PGTABLE_NS		BIT(3)</span>
<span class="p_add">+#define ARM_SHORT_PGD_IMPLE			BIT(9)</span>
<span class="p_add">+#define ARM_SHORT_PGD_TEX0			BIT(12)</span>
<span class="p_add">+#define ARM_SHORT_PGD_S				BIT(16)</span>
<span class="p_add">+#define ARM_SHORT_PGD_nG			BIT(17)</span>
<span class="p_add">+#define ARM_SHORT_PGD_SUPERSECTION		BIT(18)</span>
<span class="p_add">+#define ARM_SHORT_PGD_SECTION_NS		BIT(19)</span>
<span class="p_add">+</span>
<span class="p_add">+#define ARM_SHORT_PGD_TYPE_SUPERSECTION		\</span>
<span class="p_add">+	(ARM_SHORT_PGD_TYPE_SECTION | ARM_SHORT_PGD_SUPERSECTION)</span>
<span class="p_add">+#define ARM_SHORT_PGD_SECTION_TYPE_MSK		\</span>
<span class="p_add">+	(ARM_SHORT_PGD_TYPE_SECTION | ARM_SHORT_PGD_SUPERSECTION)</span>
<span class="p_add">+#define ARM_SHORT_PGD_PGTABLE_TYPE_MSK		\</span>
<span class="p_add">+	(ARM_SHORT_PGD_TYPE_SECTION | ARM_SHORT_PGD_TYPE_PGTABLE)</span>
<span class="p_add">+#define ARM_SHORT_PGD_TYPE_IS_PGTABLE(pgd)	\</span>
<span class="p_add">+	(((pgd) &amp; ARM_SHORT_PGD_PGTABLE_TYPE_MSK) == ARM_SHORT_PGD_TYPE_PGTABLE)</span>
<span class="p_add">+#define ARM_SHORT_PGD_TYPE_IS_SECTION(pgd)	\</span>
<span class="p_add">+	(((pgd) &amp; ARM_SHORT_PGD_SECTION_TYPE_MSK) == ARM_SHORT_PGD_TYPE_SECTION)</span>
<span class="p_add">+#define ARM_SHORT_PGD_TYPE_IS_SUPERSECTION(pgd)	\</span>
<span class="p_add">+	(((pgd) &amp; ARM_SHORT_PGD_SECTION_TYPE_MSK) == \</span>
<span class="p_add">+	ARM_SHORT_PGD_TYPE_SUPERSECTION)</span>
<span class="p_add">+#define ARM_SHORT_PGD_PGTABLE_MSK		0xfffffc00</span>
<span class="p_add">+#define ARM_SHORT_PGD_SECTION_MSK		(~(SZ_1M - 1))</span>
<span class="p_add">+#define ARM_SHORT_PGD_SUPERSECTION_MSK		(~(SZ_16M - 1))</span>
<span class="p_add">+</span>
<span class="p_add">+/* level 2 pagetable */</span>
<span class="p_add">+#define ARM_SHORT_PTE_TYPE_LARGE		BIT(0)</span>
<span class="p_add">+#define ARM_SHORT_PTE_SMALL_XN			BIT(0)</span>
<span class="p_add">+#define ARM_SHORT_PTE_TYPE_SMALL		BIT(1)</span>
<span class="p_add">+#define ARM_SHORT_PTE_B				BIT(2)</span>
<span class="p_add">+#define ARM_SHORT_PTE_C				BIT(3)</span>
<span class="p_add">+#define ARM_SHORT_PTE_SMALL_TEX0		BIT(6)</span>
<span class="p_add">+#define ARM_SHORT_PTE_IMPLE			BIT(9)</span>
<span class="p_add">+#define ARM_SHORT_PTE_S				BIT(10)</span>
<span class="p_add">+#define ARM_SHORT_PTE_nG			BIT(11)</span>
<span class="p_add">+#define ARM_SHORT_PTE_LARGE_TEX0		BIT(12)</span>
<span class="p_add">+#define ARM_SHORT_PTE_LARGE_XN			BIT(15)</span>
<span class="p_add">+#define ARM_SHORT_PTE_LARGE_MSK			(~(SZ_64K - 1))</span>
<span class="p_add">+#define ARM_SHORT_PTE_SMALL_MSK			(~(SZ_4K - 1))</span>
<span class="p_add">+#define ARM_SHORT_PTE_TYPE_MSK			\</span>
<span class="p_add">+	(ARM_SHORT_PTE_TYPE_LARGE | ARM_SHORT_PTE_TYPE_SMALL)</span>
<span class="p_add">+#define ARM_SHORT_PTE_TYPE_IS_SMALLPAGE(pte)	\</span>
<span class="p_add">+	(((((pte) &amp; ARM_SHORT_PTE_TYPE_MSK) &gt;&gt; 1) &lt;&lt; 1)\</span>
<span class="p_add">+	== ARM_SHORT_PTE_TYPE_SMALL)</span>
<span class="p_add">+#define ARM_SHORT_PTE_TYPE_IS_LARGEPAGE(pte)	\</span>
<span class="p_add">+	(((pte) &amp; ARM_SHORT_PTE_TYPE_MSK) == ARM_SHORT_PTE_TYPE_LARGE)</span>
<span class="p_add">+</span>
<span class="p_add">+#define ARM_SHORT_PGD_IDX(a)			((a) &gt;&gt; ARM_SHORT_PGDIR_SHIFT)</span>
<span class="p_add">+#define ARM_SHORT_PTE_IDX(a)			\</span>
<span class="p_add">+	(((a) &gt;&gt; ARM_SHORT_PAGE_SHIFT) &amp; (ARM_SHORT_PTRS_PER_PTE - 1))</span>
<span class="p_add">+</span>
<span class="p_add">+#define ARM_SHORT_GET_PTE_VA(pgd)		\</span>
<span class="p_add">+	(phys_to_virt((unsigned long)pgd &amp; ARM_SHORT_PGD_PGTABLE_MSK))</span>
<span class="p_add">+</span>
<span class="p_add">+#define ARM_SHORT_PTE_LARGE_GET_PROT(pte)	\</span>
<span class="p_add">+	(((pte) &amp; (~ARM_SHORT_PTE_LARGE_MSK)) &amp; ~ARM_SHORT_PTE_TYPE_MSK)</span>
<span class="p_add">+</span>
<span class="p_add">+#define ARM_SHORT_PGD_GET_PROT(pgd)		\</span>
<span class="p_add">+	(((pgd) &amp; (~ARM_SHORT_PGD_SECTION_MSK)) &amp; ~ARM_SHORT_PGD_SUPERSECTION)</span>
<span class="p_add">+</span>
<span class="p_add">+static bool selftest_running;</span>
<span class="p_add">+</span>
<span class="p_add">+static arm_short_iopte *</span>
<span class="p_add">+arm_short_get_pte_in_pgd(arm_short_iopte pgd, unsigned int iova)</span>
<span class="p_add">+{</span>
<span class="p_add">+	arm_short_iopte *pte;</span>
<span class="p_add">+</span>
<span class="p_add">+	pte = ARM_SHORT_GET_PTE_VA(pgd);</span>
<span class="p_add">+	pte += ARM_SHORT_PTE_IDX(iova);</span>
<span class="p_add">+	return pte;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void _arm_short_free_pgtable(struct arm_short_io_pgtable *data,</span>
<span class="p_add">+				    arm_short_iopte *pgd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	const struct iommu_gather_ops *tlb = data-&gt;iop.cfg.tlb;</span>
<span class="p_add">+	arm_short_iopte *pte;</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+</span>
<span class="p_add">+	pte = ARM_SHORT_GET_PTE_VA(*pgd);</span>
<span class="p_add">+	for (i = 0; i &lt; ARM_SHORT_PTRS_PER_PTE; i++) {</span>
<span class="p_add">+		if (pte[i] != 0)</span>
<span class="p_add">+			return;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Free whole pte and set pgd to zero while all pte is unmap */</span>
<span class="p_add">+	kmem_cache_free(data-&gt;ptekmem, pte);</span>
<span class="p_add">+	*pgd = 0;</span>
<span class="p_add">+	tlb-&gt;flush_pgtable(pgd, sizeof(*pgd), data-&gt;iop.cookie);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static arm_short_iopte</span>
<span class="p_add">+__arm_short_pte_prot(struct arm_short_io_pgtable *data, int prot, bool large)</span>
<span class="p_add">+{</span>
<span class="p_add">+	arm_short_iopte pteprot;</span>
<span class="p_add">+</span>
<span class="p_add">+	pteprot = ARM_SHORT_PTE_S | ARM_SHORT_PTE_nG;</span>
<span class="p_add">+	pteprot |= large ? ARM_SHORT_PTE_TYPE_LARGE :</span>
<span class="p_add">+				ARM_SHORT_PTE_TYPE_SMALL;</span>
<span class="p_add">+	if (prot &amp; IOMMU_CACHE)</span>
<span class="p_add">+		pteprot |=  ARM_SHORT_PTE_B | ARM_SHORT_PTE_C;</span>
<span class="p_add">+	if (prot &amp; IOMMU_WRITE)</span>
<span class="p_add">+		pteprot |= large ? ARM_SHORT_PTE_LARGE_TEX0 :</span>
<span class="p_add">+				ARM_SHORT_PTE_SMALL_TEX0;</span>
<span class="p_add">+	if (prot &amp; IOMMU_NOEXEC)</span>
<span class="p_add">+		pteprot |= large ? ARM_SHORT_PTE_LARGE_XN :</span>
<span class="p_add">+			ARM_SHORT_PTE_SMALL_XN;</span>
<span class="p_add">+	return pteprot;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static arm_short_iopte</span>
<span class="p_add">+__arm_short_pgd_prot(struct arm_short_io_pgtable *data, int prot, bool super)</span>
<span class="p_add">+{</span>
<span class="p_add">+	arm_short_iopte pgdprot;</span>
<span class="p_add">+</span>
<span class="p_add">+	pgdprot = ARM_SHORT_PGD_S | ARM_SHORT_PGD_nG;</span>
<span class="p_add">+	pgdprot |= super ? ARM_SHORT_PGD_TYPE_SUPERSECTION :</span>
<span class="p_add">+				ARM_SHORT_PGD_TYPE_SECTION;</span>
<span class="p_add">+	if (prot &amp; IOMMU_CACHE)</span>
<span class="p_add">+		pgdprot |= ARM_SHORT_PGD_C | ARM_SHORT_PGD_B;</span>
<span class="p_add">+	if (prot &amp; IOMMU_WRITE)</span>
<span class="p_add">+		pgdprot |= ARM_SHORT_PGD_TEX0;</span>
<span class="p_add">+	if (prot &amp; IOMMU_NOEXEC)</span>
<span class="p_add">+		pgdprot |= ARM_SHORT_PGD_SECTION_XN;</span>
<span class="p_add">+	if (data-&gt;iop.cfg.quirks &amp; IO_PGTABLE_QUIRK_ARM_NS)</span>
<span class="p_add">+		pgdprot |= ARM_SHORT_PGD_SECTION_NS;</span>
<span class="p_add">+	return pgdprot;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static arm_short_iopte</span>
<span class="p_add">+__arm_short_pte_prot_split(struct arm_short_io_pgtable *data,</span>
<span class="p_add">+			   arm_short_iopte pgdprot,</span>
<span class="p_add">+			   arm_short_iopte pteprot_large,</span>
<span class="p_add">+			   bool large)</span>
<span class="p_add">+{</span>
<span class="p_add">+	arm_short_iopte pteprot = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	pteprot = ARM_SHORT_PTE_S | ARM_SHORT_PTE_nG;</span>
<span class="p_add">+	pteprot |= large ? ARM_SHORT_PTE_TYPE_LARGE :</span>
<span class="p_add">+				ARM_SHORT_PTE_TYPE_SMALL;</span>
<span class="p_add">+	/* section to pte prot */</span>
<span class="p_add">+	if (pgdprot &amp; ARM_SHORT_PGD_C)</span>
<span class="p_add">+		pteprot |= ARM_SHORT_PTE_C;</span>
<span class="p_add">+	if (pgdprot &amp; ARM_SHORT_PGD_B)</span>
<span class="p_add">+		pteprot |= ARM_SHORT_PTE_B;</span>
<span class="p_add">+	if (pgdprot &amp; ARM_SHORT_PGD_TEX0)</span>
<span class="p_add">+		pteprot |= large ? ARM_SHORT_PTE_LARGE_TEX0 :</span>
<span class="p_add">+				ARM_SHORT_PTE_SMALL_TEX0;</span>
<span class="p_add">+	if (pgdprot &amp; ARM_SHORT_PGD_nG)</span>
<span class="p_add">+		pteprot |= ARM_SHORT_PTE_nG;</span>
<span class="p_add">+	if (pgdprot &amp; ARM_SHORT_PGD_SECTION_XN)</span>
<span class="p_add">+		pteprot |= large ? ARM_SHORT_PTE_LARGE_XN :</span>
<span class="p_add">+				ARM_SHORT_PTE_SMALL_XN;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* large page to small page pte prot. Only large page may split */</span>
<span class="p_add">+	if (pteprot_large &amp;&amp; !large) {</span>
<span class="p_add">+		if (pteprot_large &amp; ARM_SHORT_PTE_LARGE_TEX0)</span>
<span class="p_add">+			pteprot |= ARM_SHORT_PTE_SMALL_TEX0;</span>
<span class="p_add">+		if (pteprot_large &amp; ARM_SHORT_PTE_LARGE_XN)</span>
<span class="p_add">+			pteprot |= ARM_SHORT_PTE_SMALL_XN;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return pteprot;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static arm_short_iopte</span>
<span class="p_add">+__arm_short_pgtable_prot(struct arm_short_io_pgtable *data, bool noexec)</span>
<span class="p_add">+{</span>
<span class="p_add">+	arm_short_iopte pgdprot = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	pgdprot = ARM_SHORT_PGD_TYPE_PGTABLE;</span>
<span class="p_add">+	if (data-&gt;iop.cfg.quirks &amp; IO_PGTABLE_QUIRK_ARM_NS)</span>
<span class="p_add">+		pgdprot |= ARM_SHORT_PGD_PGTABLE_NS;</span>
<span class="p_add">+	if (noexec)</span>
<span class="p_add">+		pgdprot |= ARM_SHORT_PGD_PGTABLE_XN;</span>
<span class="p_add">+	return pgdprot;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int</span>
<span class="p_add">+_arm_short_map(struct arm_short_io_pgtable *data,</span>
<span class="p_add">+	       unsigned int iova, phys_addr_t paddr,</span>
<span class="p_add">+	       arm_short_iopte pgdprot, arm_short_iopte pteprot,</span>
<span class="p_add">+	       bool large)</span>
<span class="p_add">+{</span>
<span class="p_add">+	const struct iommu_gather_ops *tlb = data-&gt;iop.cfg.tlb;</span>
<span class="p_add">+	arm_short_iopte *pgd = data-&gt;pgd, *pte;</span>
<span class="p_add">+	void *cookie = data-&gt;iop.cookie, *pte_va;</span>
<span class="p_add">+	unsigned int ptenr = large ? 16 : 1;</span>
<span class="p_add">+	int i, quirk = data-&gt;iop.cfg.quirks;</span>
<span class="p_add">+	bool ptenew = false;</span>
<span class="p_add">+</span>
<span class="p_add">+	pgd += ARM_SHORT_PGD_IDX(iova);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!pteprot) { /* section or supersection */</span>
<span class="p_add">+		if (quirk &amp; IO_PGTABLE_QUIRK_SHORT_MTK)</span>
<span class="p_add">+			pgdprot &amp;= ~ARM_SHORT_PGD_SECTION_XN;</span>
<span class="p_add">+		pte = pgd;</span>
<span class="p_add">+		pteprot = pgdprot;</span>
<span class="p_add">+	} else {        /* page or largepage */</span>
<span class="p_add">+		if (quirk &amp; IO_PGTABLE_QUIRK_SHORT_MTK) {</span>
<span class="p_add">+			if (large) { /* special Bit */</span>
<span class="p_add">+				if (pteprot &amp; ARM_SHORT_PTE_LARGE_TEX0) {</span>
<span class="p_add">+					pteprot &amp;= ~ARM_SHORT_PTE_LARGE_TEX0;</span>
<span class="p_add">+					pteprot |= ARM_SHORT_PTE_SMALL_TEX0;</span>
<span class="p_add">+				}</span>
<span class="p_add">+				pteprot &amp;= ~ARM_SHORT_PTE_LARGE_XN;</span>
<span class="p_add">+			} else {</span>
<span class="p_add">+				pteprot &amp;= ~ARM_SHORT_PTE_SMALL_XN;</span>
<span class="p_add">+			}</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!(*pgd)) {</span>
<span class="p_add">+			pte_va = kmem_cache_zalloc(data-&gt;ptekmem, GFP_ATOMIC);</span>
<span class="p_add">+			if (unlikely(!pte_va))</span>
<span class="p_add">+				return -ENOMEM;</span>
<span class="p_add">+			ptenew = true;</span>
<span class="p_add">+			*pgd = virt_to_phys(pte_va) | pgdprot;</span>
<span class="p_add">+			kmemleak_ignore(pte_va);</span>
<span class="p_add">+			tlb-&gt;flush_pgtable(pgd, sizeof(*pgd), cookie);</span>
<span class="p_add">+		}</span>
<span class="p_add">+		pte = arm_short_get_pte_in_pgd(*pgd, iova);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	pteprot |= (arm_short_iopte)paddr;</span>
<span class="p_add">+	for (i = 0; i &lt; ptenr; i++) {</span>
<span class="p_add">+		if (pte[i]) {/* Someone else may have allocated for this pte */</span>
<span class="p_add">+			WARN_ON(!selftest_running);</span>
<span class="p_add">+			goto err_exist_pte;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		pte[i] = pteprot;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	tlb-&gt;flush_pgtable(pte, ptenr * sizeof(*pte), cookie);</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+err_exist_pte:</span>
<span class="p_add">+	while (i--)</span>
<span class="p_add">+		pte[i] = 0;</span>
<span class="p_add">+	if (ptenew)</span>
<span class="p_add">+		kmem_cache_free(data-&gt;ptekmem, pte_va);</span>
<span class="p_add">+	return -EEXIST;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int arm_short_map(struct io_pgtable_ops *ops, unsigned long iova,</span>
<span class="p_add">+			 phys_addr_t paddr, size_t size, int prot)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct arm_short_io_pgtable *data = io_pgtable_ops_to_data(ops);</span>
<span class="p_add">+	const struct iommu_gather_ops *tlb = data-&gt;iop.cfg.tlb;</span>
<span class="p_add">+	int ret;</span>
<span class="p_add">+	arm_short_iopte pgdprot = 0, pteprot = 0;</span>
<span class="p_add">+	bool large;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* If no access, then nothing to do */</span>
<span class="p_add">+	if (!(prot &amp; (IOMMU_READ | IOMMU_WRITE)))</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	switch (size) {</span>
<span class="p_add">+	case SZ_4K:</span>
<span class="p_add">+	case SZ_64K:</span>
<span class="p_add">+		large = (size == SZ_64K) ? true : false;</span>
<span class="p_add">+		pteprot = __arm_short_pte_prot(data, prot, large);</span>
<span class="p_add">+		pgdprot = __arm_short_pgtable_prot(data, prot &amp; IOMMU_NOEXEC);</span>
<span class="p_add">+		break;</span>
<span class="p_add">+</span>
<span class="p_add">+	case SZ_1M:</span>
<span class="p_add">+	case SZ_16M:</span>
<span class="p_add">+		large = (size == SZ_16M) ? true : false;</span>
<span class="p_add">+		pgdprot = __arm_short_pgd_prot(data, prot, large);</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	default:</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (WARN_ON((iova | paddr) &amp; (size - 1)))</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+</span>
<span class="p_add">+	ret = _arm_short_map(data, iova, paddr, pgdprot, pteprot, large);</span>
<span class="p_add">+</span>
<span class="p_add">+	tlb-&gt;tlb_add_flush(iova, size, true, data-&gt;iop.cookie);</span>
<span class="p_add">+	tlb-&gt;tlb_sync(data-&gt;iop.cookie);</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static phys_addr_t arm_short_iova_to_phys(struct io_pgtable_ops *ops,</span>
<span class="p_add">+					  unsigned long iova)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct arm_short_io_pgtable *data = io_pgtable_ops_to_data(ops);</span>
<span class="p_add">+	arm_short_iopte *pte, *pgd = data-&gt;pgd;</span>
<span class="p_add">+	phys_addr_t pa = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	pgd += ARM_SHORT_PGD_IDX(iova);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (ARM_SHORT_PGD_TYPE_IS_PGTABLE(*pgd)) {</span>
<span class="p_add">+		pte = arm_short_get_pte_in_pgd(*pgd, iova);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (ARM_SHORT_PTE_TYPE_IS_LARGEPAGE(*pte)) {</span>
<span class="p_add">+			pa = (*pte) &amp; ARM_SHORT_PTE_LARGE_MSK;</span>
<span class="p_add">+			pa |= iova &amp; ~ARM_SHORT_PTE_LARGE_MSK;</span>
<span class="p_add">+		} else if (ARM_SHORT_PTE_TYPE_IS_SMALLPAGE(*pte)) {</span>
<span class="p_add">+			pa = (*pte) &amp; ARM_SHORT_PTE_SMALL_MSK;</span>
<span class="p_add">+			pa |= iova &amp; ~ARM_SHORT_PTE_SMALL_MSK;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	} else if (ARM_SHORT_PGD_TYPE_IS_SECTION(*pgd)) {</span>
<span class="p_add">+		pa = (*pgd) &amp; ARM_SHORT_PGD_SECTION_MSK;</span>
<span class="p_add">+		pa |= iova &amp; ~ARM_SHORT_PGD_SECTION_MSK;</span>
<span class="p_add">+	} else if (ARM_SHORT_PGD_TYPE_IS_SUPERSECTION(*pgd)) {</span>
<span class="p_add">+		pa = (*pgd) &amp; ARM_SHORT_PGD_SUPERSECTION_MSK;</span>
<span class="p_add">+		pa |= iova &amp; ~ARM_SHORT_PGD_SUPERSECTION_MSK;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return pa;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int</span>
<span class="p_add">+arm_short_split_blk_unmap(struct io_pgtable_ops *ops, unsigned int iova,</span>
<span class="p_add">+			  phys_addr_t paddr, size_t size,</span>
<span class="p_add">+			  arm_short_iopte pgdprotup, arm_short_iopte pteprotup,</span>
<span class="p_add">+			  size_t blk_size)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct arm_short_io_pgtable *data = io_pgtable_ops_to_data(ops);</span>
<span class="p_add">+	const struct iommu_gather_ops *tlb = data-&gt;iop.cfg.tlb;</span>
<span class="p_add">+	struct io_pgtable_cfg *cfg = &amp;data-&gt;iop.cfg;</span>
<span class="p_add">+	unsigned long *pgbitmap = &amp;cfg-&gt;pgsize_bitmap;</span>
<span class="p_add">+	unsigned int blk_base, blk_start, blk_end;</span>
<span class="p_add">+	arm_short_iopte pgdprot, pteprot;</span>
<span class="p_add">+	size_t mapsize = 0, nextmapsize;</span>
<span class="p_add">+	phys_addr_t blk_paddr;</span>
<span class="p_add">+	int ret;</span>
<span class="p_add">+	unsigned int i;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* find the nearest mapsize */</span>
<span class="p_add">+	for (i = find_first_bit(pgbitmap, BITS_PER_LONG);</span>
<span class="p_add">+	     i &lt; BITS_PER_LONG &amp;&amp; ((1 &lt;&lt; i) &lt; blk_size) &amp;&amp;</span>
<span class="p_add">+	     IS_ALIGNED(size, 1 &lt;&lt; i);</span>
<span class="p_add">+	     i = find_next_bit(pgbitmap, BITS_PER_LONG, i + 1))</span>
<span class="p_add">+		mapsize = 1 &lt;&lt; i;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (WARN_ON(!mapsize))</span>
<span class="p_add">+		return 0; /* Bytes unmapped */</span>
<span class="p_add">+	nextmapsize = 1 &lt;&lt; i;</span>
<span class="p_add">+</span>
<span class="p_add">+	blk_base = iova &amp; ~(blk_size - 1);</span>
<span class="p_add">+	blk_start = blk_base;</span>
<span class="p_add">+	blk_end = blk_start + blk_size;</span>
<span class="p_add">+	blk_paddr = paddr;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (; blk_start &lt; blk_end;</span>
<span class="p_add">+	     blk_start += mapsize, blk_paddr += mapsize) {</span>
<span class="p_add">+		/* Unmap! */</span>
<span class="p_add">+		if (blk_start == iova)</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		/* Try to upper map */</span>
<span class="p_add">+		if (blk_base != blk_start &amp;&amp;</span>
<span class="p_add">+		    IS_ALIGNED(blk_start | blk_paddr, nextmapsize) &amp;&amp;</span>
<span class="p_add">+		    mapsize != nextmapsize) {</span>
<span class="p_add">+			mapsize = nextmapsize;</span>
<span class="p_add">+			i = find_next_bit(pgbitmap, BITS_PER_LONG, i + 1);</span>
<span class="p_add">+			if (i &lt; BITS_PER_LONG)</span>
<span class="p_add">+				nextmapsize = 1 &lt;&lt; i;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		if (mapsize == SZ_1M) {</span>
<span class="p_add">+			pgdprot = pgdprotup;</span>
<span class="p_add">+			pgdprot |= __arm_short_pgd_prot(data, 0, false);</span>
<span class="p_add">+			pteprot = 0;</span>
<span class="p_add">+		} else { /* small or large page */</span>
<span class="p_add">+			bool noexec = (blk_size == SZ_64K) ?</span>
<span class="p_add">+				(pteprotup &amp; ARM_SHORT_PTE_LARGE_XN) :</span>
<span class="p_add">+				(pgdprotup &amp; ARM_SHORT_PGD_SECTION_XN);</span>
<span class="p_add">+</span>
<span class="p_add">+			pteprot = __arm_short_pte_prot_split(</span>
<span class="p_add">+						data, pgdprotup, pteprotup,</span>
<span class="p_add">+						mapsize == SZ_64K);</span>
<span class="p_add">+			pgdprot = __arm_short_pgtable_prot(data, noexec);</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		ret = _arm_short_map(data, blk_start, blk_paddr, pgdprot,</span>
<span class="p_add">+				     pteprot, mapsize == SZ_64K);</span>
<span class="p_add">+		if (ret &lt; 0) {</span>
<span class="p_add">+			/* Free the table we allocated */</span>
<span class="p_add">+			arm_short_iopte *pgd = data-&gt;pgd, *pte;</span>
<span class="p_add">+</span>
<span class="p_add">+			pgd += ARM_SHORT_PGD_IDX(blk_base);</span>
<span class="p_add">+			if (*pgd) {</span>
<span class="p_add">+				pte = ARM_SHORT_GET_PTE_VA(*pgd);</span>
<span class="p_add">+				kmem_cache_free(data-&gt;ptekmem, pte);</span>
<span class="p_add">+				*pgd = 0;</span>
<span class="p_add">+				tlb-&gt;flush_pgtable(pgd, sizeof(*pgd),</span>
<span class="p_add">+						   data-&gt;iop.cookie);</span>
<span class="p_add">+			}</span>
<span class="p_add">+			return 0;/* Bytes unmapped */</span>
<span class="p_add">+		}</span>
<span class="p_add">+		tlb-&gt;tlb_add_flush(blk_start, mapsize, true, data-&gt;iop.cookie);</span>
<span class="p_add">+		tlb-&gt;tlb_sync(data-&gt;iop.cookie);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return size;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int arm_short_unmap(struct io_pgtable_ops *ops,</span>
<span class="p_add">+			   unsigned long iova,</span>
<span class="p_add">+			   size_t size)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct arm_short_io_pgtable *data = io_pgtable_ops_to_data(ops);</span>
<span class="p_add">+	const struct iommu_gather_ops *tlb = data-&gt;iop.cfg.tlb;</span>
<span class="p_add">+	void *cookie = data-&gt;iop.cookie;</span>
<span class="p_add">+	arm_short_iopte *pgd, *pte = NULL;</span>
<span class="p_add">+	arm_short_iopte pgdprot, pteprot = 0;</span>
<span class="p_add">+	phys_addr_t paddr;</span>
<span class="p_add">+	unsigned int nrtoclean, iova_base, blk_size = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	pgd = (arm_short_iopte *)data-&gt;pgd + ARM_SHORT_PGD_IDX(iova);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* get block size */</span>
<span class="p_add">+	if (ARM_SHORT_PGD_TYPE_IS_PGTABLE(*pgd)) {</span>
<span class="p_add">+		pte = arm_short_get_pte_in_pgd(*pgd, iova);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (ARM_SHORT_PTE_TYPE_IS_SMALLPAGE(*pte))</span>
<span class="p_add">+			blk_size = SZ_4K;</span>
<span class="p_add">+		else if (ARM_SHORT_PTE_TYPE_IS_LARGEPAGE(*pte))</span>
<span class="p_add">+			blk_size = SZ_64K;</span>
<span class="p_add">+		else</span>
<span class="p_add">+			WARN_ON(1);</span>
<span class="p_add">+	} else if (ARM_SHORT_PGD_TYPE_IS_SECTION(*pgd)) {</span>
<span class="p_add">+		blk_size = SZ_1M;</span>
<span class="p_add">+	} else if (ARM_SHORT_PGD_TYPE_IS_SUPERSECTION(*pgd)) {</span>
<span class="p_add">+		blk_size = SZ_16M;</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		WARN_ON(1);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	iova_base = iova &amp; ~(blk_size - 1);</span>
<span class="p_add">+	pgd = (arm_short_iopte *)data-&gt;pgd + ARM_SHORT_PGD_IDX(iova_base);</span>
<span class="p_add">+	paddr = arm_short_iova_to_phys(ops, iova_base);</span>
<span class="p_add">+	pgdprot = *pgd;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (blk_size == SZ_4K || blk_size == SZ_64K) {</span>
<span class="p_add">+		pte = arm_short_get_pte_in_pgd(*pgd, iova_base);</span>
<span class="p_add">+		pteprot = *pte;</span>
<span class="p_add">+		nrtoclean = blk_size / SZ_4K;</span>
<span class="p_add">+		memset(pte, 0, nrtoclean * sizeof(*pte));</span>
<span class="p_add">+		tlb-&gt;flush_pgtable(pte, nrtoclean * sizeof(*pte), cookie);</span>
<span class="p_add">+</span>
<span class="p_add">+		_arm_short_free_pgtable(data, pgd);</span>
<span class="p_add">+	} else if (blk_size == SZ_1M || blk_size == SZ_16M) {</span>
<span class="p_add">+		nrtoclean = blk_size / SZ_1M;</span>
<span class="p_add">+		memset(pgd, 0, nrtoclean * sizeof(*pgd));</span>
<span class="p_add">+		tlb-&gt;flush_pgtable(pgd, nrtoclean * sizeof(*pgd), cookie);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	tlb-&gt;tlb_add_flush(iova, blk_size, true, cookie);</span>
<span class="p_add">+	tlb-&gt;tlb_sync(cookie);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (blk_size &gt; size) { /* Split the block */</span>
<span class="p_add">+		return arm_short_split_blk_unmap(</span>
<span class="p_add">+				ops, iova, paddr, size,</span>
<span class="p_add">+				ARM_SHORT_PGD_GET_PROT(pgdprot),</span>
<span class="p_add">+				ARM_SHORT_PTE_LARGE_GET_PROT(pteprot),</span>
<span class="p_add">+				blk_size);</span>
<span class="p_add">+	} else if (blk_size &lt; size) {</span>
<span class="p_add">+		/* Unmap the block while remap partial again after split */</span>
<span class="p_add">+		return blk_size +</span>
<span class="p_add">+			arm_short_unmap(ops, iova + blk_size, size - blk_size);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return size;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static struct io_pgtable *</span>
<span class="p_add">+arm_short_alloc_pgtable(struct io_pgtable_cfg *cfg, void *cookie)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct arm_short_io_pgtable *data;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (cfg-&gt;ias &gt; 32)</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (cfg-&gt;oas &gt; 32)</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+	cfg-&gt;pgsize_bitmap &amp;=</span>
<span class="p_add">+		(cfg-&gt;quirks &amp; IO_PGTABLE_QUIRK_SHORT_SUPERSECTION) ?</span>
<span class="p_add">+		(SZ_4K | SZ_64K | SZ_1M | SZ_16M) : (SZ_4K | SZ_64K | SZ_1M);</span>
<span class="p_add">+</span>
<span class="p_add">+	data = kzalloc(sizeof(*data), GFP_KERNEL);</span>
<span class="p_add">+	if (!data)</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+	data-&gt;pgd_size = SZ_16K;</span>
<span class="p_add">+	data-&gt;pgd = alloc_pages_exact(data-&gt;pgd_size,</span>
<span class="p_add">+				      GFP_KERNEL | __GFP_ZERO | __GFP_DMA);</span>
<span class="p_add">+	if (!data-&gt;pgd)</span>
<span class="p_add">+		goto out_free_data;</span>
<span class="p_add">+</span>
<span class="p_add">+	cfg-&gt;tlb-&gt;flush_pgtable(data-&gt;pgd, data-&gt;pgd_size, cookie);</span>
<span class="p_add">+</span>
<span class="p_add">+	data-&gt;ptekmem = kmem_cache_create(&quot;io-pgtable-arm-short&quot;,</span>
<span class="p_add">+					  ARM_SHORT_BYTES_PER_PTE,</span>
<span class="p_add">+					  ARM_SHORT_BYTES_PER_PTE,</span>
<span class="p_add">+					  0, NULL);</span>
<span class="p_add">+	if (!data-&gt;ptekmem)</span>
<span class="p_add">+		goto out_free_pte;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* TTBRs */</span>
<span class="p_add">+	cfg-&gt;arm_short_cfg.ttbr[0] = virt_to_phys(data-&gt;pgd);</span>
<span class="p_add">+	cfg-&gt;arm_short_cfg.ttbr[1] = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	cfg-&gt;arm_short_cfg.tcr = 0;</span>
<span class="p_add">+	cfg-&gt;arm_short_cfg.nmrr = 0;</span>
<span class="p_add">+	cfg-&gt;arm_short_cfg.prrr = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	data-&gt;iop.ops = (struct io_pgtable_ops) {</span>
<span class="p_add">+		.map		= arm_short_map,</span>
<span class="p_add">+		.unmap		= arm_short_unmap,</span>
<span class="p_add">+		.iova_to_phys	= arm_short_iova_to_phys,</span>
<span class="p_add">+	};</span>
<span class="p_add">+</span>
<span class="p_add">+	return &amp;data-&gt;iop;</span>
<span class="p_add">+</span>
<span class="p_add">+out_free_pte:</span>
<span class="p_add">+	free_pages_exact(data-&gt;pgd, data-&gt;pgd_size);</span>
<span class="p_add">+out_free_data:</span>
<span class="p_add">+	kfree(data);</span>
<span class="p_add">+	return NULL;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void arm_short_free_pgtable(struct io_pgtable *iop)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct arm_short_io_pgtable *data = io_pgtable_to_data(iop);</span>
<span class="p_add">+</span>
<span class="p_add">+	kmem_cache_destroy(data-&gt;ptekmem);</span>
<span class="p_add">+	free_pages_exact(data-&gt;pgd, data-&gt;pgd_size);</span>
<span class="p_add">+	kfree(data);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+struct io_pgtable_init_fns io_pgtable_arm_short_init_fns = {</span>
<span class="p_add">+	.alloc	= arm_short_alloc_pgtable,</span>
<span class="p_add">+	.free	= arm_short_free_pgtable,</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_IOMMU_IO_PGTABLE_SHORT_SELFTEST</span>
<span class="p_add">+</span>
<span class="p_add">+static struct io_pgtable_cfg *cfg_cookie;</span>
<span class="p_add">+</span>
<span class="p_add">+static void dummy_tlb_flush_all(void *cookie)</span>
<span class="p_add">+{</span>
<span class="p_add">+	WARN_ON(cookie != cfg_cookie);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void dummy_tlb_add_flush(unsigned long iova, size_t size, bool leaf,</span>
<span class="p_add">+				void *cookie)</span>
<span class="p_add">+{</span>
<span class="p_add">+	WARN_ON(cookie != cfg_cookie);</span>
<span class="p_add">+	WARN_ON(!(size &amp; cfg_cookie-&gt;pgsize_bitmap));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void dummy_tlb_sync(void *cookie)</span>
<span class="p_add">+{</span>
<span class="p_add">+	WARN_ON(cookie != cfg_cookie);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void dummy_flush_pgtable(void *ptr, size_t size, void *cookie)</span>
<span class="p_add">+{</span>
<span class="p_add">+	WARN_ON(cookie != cfg_cookie);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static struct iommu_gather_ops dummy_tlb_ops = {</span>
<span class="p_add">+	.tlb_flush_all	= dummy_tlb_flush_all,</span>
<span class="p_add">+	.tlb_add_flush	= dummy_tlb_add_flush,</span>
<span class="p_add">+	.tlb_sync	= dummy_tlb_sync,</span>
<span class="p_add">+	.flush_pgtable	= dummy_flush_pgtable,</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+#define __FAIL(ops)	({						\</span>
<span class="p_add">+		WARN(1, &quot;selftest: test failed\n&quot;);	\</span>
<span class="p_add">+		selftest_running = false;				\</span>
<span class="p_add">+		-EFAULT;						\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+static int __init arm_short_do_selftests(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct io_pgtable_ops *ops;</span>
<span class="p_add">+	struct io_pgtable_cfg cfg = {</span>
<span class="p_add">+		.tlb = &amp;dummy_tlb_ops,</span>
<span class="p_add">+		.oas = 32,</span>
<span class="p_add">+		.ias = 32,</span>
<span class="p_add">+		.quirks = IO_PGTABLE_QUIRK_ARM_NS |</span>
<span class="p_add">+			IO_PGTABLE_QUIRK_SHORT_SUPERSECTION,</span>
<span class="p_add">+		.pgsize_bitmap = SZ_4K | SZ_64K | SZ_1M | SZ_16M,</span>
<span class="p_add">+	};</span>
<span class="p_add">+	unsigned int iova, size, iova_start;</span>
<span class="p_add">+	unsigned int i, loopnr = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	selftest_running = true;</span>
<span class="p_add">+</span>
<span class="p_add">+	cfg_cookie = &amp;cfg;</span>
<span class="p_add">+</span>
<span class="p_add">+	ops = alloc_io_pgtable_ops(ARM_SHORT_DESC, &amp;cfg, &amp;cfg);</span>
<span class="p_add">+	if (!ops) {</span>
<span class="p_add">+		pr_err(&quot;Failed to alloc short desc io pgtable\n&quot;);</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Initial sanity checks.</span>
<span class="p_add">+	 * Empty page tables shouldn&#39;t provide any translations.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (ops-&gt;iova_to_phys(ops, 42))</span>
<span class="p_add">+		return __FAIL(ops);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (ops-&gt;iova_to_phys(ops, SZ_1G + 42))</span>
<span class="p_add">+		return __FAIL(ops);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (ops-&gt;iova_to_phys(ops, SZ_2G + 42))</span>
<span class="p_add">+		return __FAIL(ops);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Distinct mappings of different granule sizes.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	iova = 0;</span>
<span class="p_add">+	i = find_first_bit(&amp;cfg.pgsize_bitmap, BITS_PER_LONG);</span>
<span class="p_add">+	while (i != BITS_PER_LONG) {</span>
<span class="p_add">+		size = 1UL &lt;&lt; i;</span>
<span class="p_add">+		if (ops-&gt;map(ops, iova, iova, size, IOMMU_READ |</span>
<span class="p_add">+						    IOMMU_WRITE |</span>
<span class="p_add">+						    IOMMU_NOEXEC |</span>
<span class="p_add">+						    IOMMU_CACHE))</span>
<span class="p_add">+			return __FAIL(ops);</span>
<span class="p_add">+</span>
<span class="p_add">+		/* Overlapping mappings */</span>
<span class="p_add">+		if (!ops-&gt;map(ops, iova, iova + size, size,</span>
<span class="p_add">+			      IOMMU_READ | IOMMU_NOEXEC))</span>
<span class="p_add">+			return __FAIL(ops);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (ops-&gt;iova_to_phys(ops, iova + 42) != (iova + 42))</span>
<span class="p_add">+			return __FAIL(ops);</span>
<span class="p_add">+</span>
<span class="p_add">+		iova += SZ_16M;</span>
<span class="p_add">+		i++;</span>
<span class="p_add">+		i = find_next_bit(&amp;cfg.pgsize_bitmap, BITS_PER_LONG, i);</span>
<span class="p_add">+		loopnr++;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Partial unmap */</span>
<span class="p_add">+	i = 1;</span>
<span class="p_add">+	size = 1UL &lt;&lt; __ffs(cfg.pgsize_bitmap);</span>
<span class="p_add">+	while (i &lt; loopnr) {</span>
<span class="p_add">+		iova_start = i * SZ_16M;</span>
<span class="p_add">+		if (ops-&gt;unmap(ops, iova_start + size, size) != size)</span>
<span class="p_add">+			return __FAIL(ops);</span>
<span class="p_add">+</span>
<span class="p_add">+		/* Remap of partial unmap */</span>
<span class="p_add">+		if (ops-&gt;map(ops, iova_start + size, size, size, IOMMU_READ))</span>
<span class="p_add">+			return __FAIL(ops);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (ops-&gt;iova_to_phys(ops, iova_start + size + 42)</span>
<span class="p_add">+		    != (size + 42))</span>
<span class="p_add">+			return __FAIL(ops);</span>
<span class="p_add">+		i++;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Full unmap */</span>
<span class="p_add">+	iova = 0;</span>
<span class="p_add">+	i = find_first_bit(&amp;cfg.pgsize_bitmap, BITS_PER_LONG);</span>
<span class="p_add">+	while (i != BITS_PER_LONG) {</span>
<span class="p_add">+		size = 1UL &lt;&lt; i;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (ops-&gt;unmap(ops, iova, size) != size)</span>
<span class="p_add">+			return __FAIL(ops);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (ops-&gt;iova_to_phys(ops, iova + 42))</span>
<span class="p_add">+			return __FAIL(ops);</span>
<span class="p_add">+</span>
<span class="p_add">+		/* Remap full block */</span>
<span class="p_add">+		if (ops-&gt;map(ops, iova, iova, size, IOMMU_WRITE))</span>
<span class="p_add">+			return __FAIL(ops);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (ops-&gt;iova_to_phys(ops, iova + 42) != (iova + 42))</span>
<span class="p_add">+			return __FAIL(ops);</span>
<span class="p_add">+</span>
<span class="p_add">+		iova += SZ_16M;</span>
<span class="p_add">+		i++;</span>
<span class="p_add">+		i = find_next_bit(&amp;cfg.pgsize_bitmap, BITS_PER_LONG, i);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	free_io_pgtable_ops(ops);</span>
<span class="p_add">+</span>
<span class="p_add">+	selftest_running = false;</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+subsys_initcall(arm_short_do_selftests);</span>
<span class="p_add">+#endif</span>
<span class="p_header">diff --git a/drivers/iommu/io-pgtable-arm.c b/drivers/iommu/io-pgtable-arm.c</span>
<span class="p_header">index 4e46021..13aad17 100644</span>
<span class="p_header">--- a/drivers/iommu/io-pgtable-arm.c</span>
<span class="p_header">+++ b/drivers/iommu/io-pgtable-arm.c</span>
<span class="p_chunk">@@ -36,9 +36,6 @@</span> <span class="p_context"></span>
 #define io_pgtable_to_data(x)						\
 	container_of((x), struct arm_lpae_io_pgtable, iop)
 
<span class="p_del">-#define io_pgtable_ops_to_pgtable(x)					\</span>
<span class="p_del">-	container_of((x), struct io_pgtable, ops)</span>
<span class="p_del">-</span>
 #define io_pgtable_ops_to_data(x)					\
 	io_pgtable_to_data(io_pgtable_ops_to_pgtable(x))
 
<span class="p_header">diff --git a/drivers/iommu/io-pgtable.c b/drivers/iommu/io-pgtable.c</span>
<span class="p_header">index 6436fe2..14a9b3a 100644</span>
<span class="p_header">--- a/drivers/iommu/io-pgtable.c</span>
<span class="p_header">+++ b/drivers/iommu/io-pgtable.c</span>
<span class="p_chunk">@@ -28,6 +28,7 @@</span> <span class="p_context"> extern struct io_pgtable_init_fns io_pgtable_arm_32_lpae_s1_init_fns;</span>
 extern struct io_pgtable_init_fns io_pgtable_arm_32_lpae_s2_init_fns;
 extern struct io_pgtable_init_fns io_pgtable_arm_64_lpae_s1_init_fns;
 extern struct io_pgtable_init_fns io_pgtable_arm_64_lpae_s2_init_fns;
<span class="p_add">+extern struct io_pgtable_init_fns io_pgtable_arm_short_init_fns;</span>
 
 static const struct io_pgtable_init_fns *
 io_pgtable_init_table[IO_PGTABLE_NUM_FMTS] =
<span class="p_chunk">@@ -38,6 +39,9 @@</span> <span class="p_context"> io_pgtable_init_table[IO_PGTABLE_NUM_FMTS] =</span>
 	[ARM_64_LPAE_S1] = &amp;io_pgtable_arm_64_lpae_s1_init_fns,
 	[ARM_64_LPAE_S2] = &amp;io_pgtable_arm_64_lpae_s2_init_fns,
 #endif
<span class="p_add">+#ifdef CONFIG_IOMMU_IO_PGTABLE_SHORT</span>
<span class="p_add">+	[ARM_SHORT_DESC] = &amp;io_pgtable_arm_short_init_fns,</span>
<span class="p_add">+#endif</span>
 };
 
 struct io_pgtable_ops *alloc_io_pgtable_ops(enum io_pgtable_fmt fmt,
<span class="p_header">diff --git a/drivers/iommu/io-pgtable.h b/drivers/iommu/io-pgtable.h</span>
<span class="p_header">index 10e32f6..7261ada 100644</span>
<span class="p_header">--- a/drivers/iommu/io-pgtable.h</span>
<span class="p_header">+++ b/drivers/iommu/io-pgtable.h</span>
<span class="p_chunk">@@ -9,6 +9,7 @@</span> <span class="p_context"> enum io_pgtable_fmt {</span>
 	ARM_32_LPAE_S2,
 	ARM_64_LPAE_S1,
 	ARM_64_LPAE_S2,
<span class="p_add">+	ARM_SHORT_DESC,</span>
 	IO_PGTABLE_NUM_FMTS,
 };
 
<span class="p_chunk">@@ -44,6 +45,8 @@</span> <span class="p_context"> struct iommu_gather_ops {</span>
  */
 struct io_pgtable_cfg {
 	#define IO_PGTABLE_QUIRK_ARM_NS	(1 &lt;&lt; 0)	/* Set NS bit in PTEs */
<span class="p_add">+	#define IO_PGTABLE_QUIRK_SHORT_SUPERSECTION     BIT(1)</span>
<span class="p_add">+	#define IO_PGTABLE_QUIRK_SHORT_MTK		BIT(2)</span>
 	int				quirks;
 	unsigned long			pgsize_bitmap;
 	unsigned int			ias;
<span class="p_chunk">@@ -62,6 +65,13 @@</span> <span class="p_context"> struct io_pgtable_cfg {</span>
 			u64	vttbr;
 			u64	vtcr;
 		} arm_lpae_s2_cfg;
<span class="p_add">+</span>
<span class="p_add">+		struct {</span>
<span class="p_add">+			u32	ttbr[2];</span>
<span class="p_add">+			u32	tcr;</span>
<span class="p_add">+			u32	nmrr;</span>
<span class="p_add">+			u32	prrr;</span>
<span class="p_add">+		} arm_short_cfg;</span>
 	};
 };
 
<span class="p_chunk">@@ -128,6 +138,9 @@</span> <span class="p_context"> struct io_pgtable {</span>
 	struct io_pgtable_ops	ops;
 };
 
<span class="p_add">+#define io_pgtable_ops_to_pgtable(x)		\</span>
<span class="p_add">+	container_of((x), struct io_pgtable, ops)</span>
<span class="p_add">+</span>
 /**
  * struct io_pgtable_init_fns - Alloc/free a set of page tables for a
  *                              particular format.

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



