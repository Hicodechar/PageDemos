
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v2,3/4] arch/sparc: Optimized memcpy, memset, copy_to_user, copy_from_user for M7/M8 - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v2,3/4] arch/sparc: Optimized memcpy, memset, copy_to_user, copy_from_user for M7/M8</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=146821">Babu Moger</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Aug. 7, 2017, 11:52 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1502149972-61517-4-git-send-email-babu.moger@oracle.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9886321/mbox/"
   >mbox</a>
|
   <a href="/patch/9886321/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9886321/">/patch/9886321/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	DCF30601EB for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  7 Aug 2017 23:54:34 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id CB3B7286E6
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  7 Aug 2017 23:54:34 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id BFF9328726; Mon,  7 Aug 2017 23:54:34 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	UNPARSEABLE_RELAY autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 3C11E286E6
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  7 Aug 2017 23:54:31 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752081AbdHGXxF (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 7 Aug 2017 19:53:05 -0400
Received: from userp1040.oracle.com ([156.151.31.81]:47204 &quot;EHLO
	userp1040.oracle.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1751905AbdHGXxB (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 7 Aug 2017 19:53:01 -0400
Received: from aserv0022.oracle.com (aserv0022.oracle.com [141.146.126.234])
	by userp1040.oracle.com (Sentrion-MTA-4.3.2/Sentrion-MTA-4.3.2)
	with ESMTP id v77NqxJ9020489
	(version=TLSv1.2 cipher=ECDHE-RSA-AES256-GCM-SHA384 bits=256
	verify=OK); Mon, 7 Aug 2017 23:53:00 GMT
Received: from userv0121.oracle.com (userv0121.oracle.com [156.151.31.72])
	by aserv0022.oracle.com (8.14.4/8.14.4) with ESMTP id v77NqwD7017589
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-GCM-SHA384 bits=256
	verify=OK); Mon, 7 Aug 2017 23:52:59 GMT
Received: from abhmp0016.oracle.com (abhmp0016.oracle.com [141.146.116.22])
	by userv0121.oracle.com (8.14.4/8.13.8) with ESMTP id
	v77NqwQu009430; Mon, 7 Aug 2017 23:52:58 GMT
Received: from brm-t84-02.us.oracle.com (/10.80.150.81)
	by default (Oracle Beehive Gateway v4.0)
	with ESMTP ; Mon, 07 Aug 2017 16:52:58 -0700
From: Babu Moger &lt;babu.moger@oracle.com&gt;
To: davem@davemloft.net
Cc: sparclinux@vger.kernel.org, linux-kernel@vger.kernel.org,
	babu.moger@oracle.com
Subject: [PATCH v2 3/4] arch/sparc: Optimized memcpy, memset, copy_to_user,
	copy_from_user for M7/M8
Date: Mon,  7 Aug 2017 17:52:51 -0600
Message-Id: &lt;1502149972-61517-4-git-send-email-babu.moger@oracle.com&gt;
X-Mailer: git-send-email 1.7.1
In-Reply-To: &lt;1502149972-61517-1-git-send-email-babu.moger@oracle.com&gt;
References: &lt;1502149972-61517-1-git-send-email-babu.moger@oracle.com&gt;
X-Source-IP: aserv0022.oracle.com [141.146.126.234]
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=146821">Babu Moger</a> - Aug. 7, 2017, 11:52 p.m.</div>
<pre class="content">
New algorithm that takes advantage of the M7/M8 block init store
ASI, ie, overlapping pipelines and miss buffer filling.
Full details in code comments.
<span class="signed-off-by">
Signed-off-by: Babu Moger &lt;babu.moger@oracle.com&gt;</span>
---
 arch/sparc/kernel/head_64.S       |   16 +-
 arch/sparc/lib/M7copy_from_user.S |   41 ++
 arch/sparc/lib/M7copy_to_user.S   |   51 ++
 arch/sparc/lib/M7memcpy.S         |  923 +++++++++++++++++++++++++++++++++++++
 arch/sparc/lib/M7memset.S         |  352 ++++++++++++++
 arch/sparc/lib/M7patch.S          |   51 ++
 arch/sparc/lib/Makefile           |    3 +
 7 files changed, 1435 insertions(+), 2 deletions(-)
 create mode 100644 arch/sparc/lib/M7copy_from_user.S
 create mode 100644 arch/sparc/lib/M7copy_to_user.S
 create mode 100644 arch/sparc/lib/M7memcpy.S
 create mode 100644 arch/sparc/lib/M7memset.S
 create mode 100644 arch/sparc/lib/M7patch.S
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/sparc/kernel/head_64.S b/arch/sparc/kernel/head_64.S</span>
<span class="p_header">index 78e0211..bf9a5ac 100644</span>
<span class="p_header">--- a/arch/sparc/kernel/head_64.S</span>
<span class="p_header">+++ b/arch/sparc/kernel/head_64.S</span>
<span class="p_chunk">@@ -603,10 +603,10 @@</span> <span class="p_context"> niagara_tlb_fixup:</span>
 	be,pt	%xcc, niagara4_patch
 	 nop
 	cmp	%g1, SUN4V_CHIP_SPARC_M7
<span class="p_del">-	be,pt	%xcc, niagara4_patch</span>
<span class="p_add">+	be,pt	%xcc, sparc_m7_patch</span>
 	 nop
 	cmp	%g1, SUN4V_CHIP_SPARC_M8
<span class="p_del">-	be,pt	%xcc, niagara4_patch</span>
<span class="p_add">+	be,pt	%xcc, sparc_m7_patch</span>
 	 nop
 	cmp	%g1, SUN4V_CHIP_SPARC_SN
 	be,pt	%xcc, niagara4_patch
<span class="p_chunk">@@ -621,6 +621,18 @@</span> <span class="p_context"> niagara_tlb_fixup:</span>
 
 	ba,a,pt	%xcc, 80f
 	 nop
<span class="p_add">+</span>
<span class="p_add">+sparc_m7_patch:</span>
<span class="p_add">+	call	m7_patch_copyops</span>
<span class="p_add">+	 nop</span>
<span class="p_add">+	call	m7_patch_bzero</span>
<span class="p_add">+	 nop</span>
<span class="p_add">+	call	m7_patch_pageops</span>
<span class="p_add">+	 nop</span>
<span class="p_add">+</span>
<span class="p_add">+	ba,a,pt	%xcc, 80f</span>
<span class="p_add">+	 nop</span>
<span class="p_add">+</span>
 niagara4_patch:
 	call	niagara4_patch_copyops
 	 nop
<span class="p_header">diff --git a/arch/sparc/lib/M7copy_from_user.S b/arch/sparc/lib/M7copy_from_user.S</span>
new file mode 100644
<span class="p_header">index 0000000..d0689d7</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/sparc/lib/M7copy_from_user.S</span>
<span class="p_chunk">@@ -0,0 +1,41 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * M7copy_from_user.S: SPARC M7 optimized copy from userspace.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Copyright (c) 2016, Oracle and/or its affiliates. All rights reserved.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+#define EX_LD(x)			\</span>
<span class="p_add">+98:	x;				\</span>
<span class="p_add">+	.section __ex_table,&quot;a&quot;;	\</span>
<span class="p_add">+	.align 4;			\</span>
<span class="p_add">+	.word 98b, __restore_asi;	\</span>
<span class="p_add">+	.text;				\</span>
<span class="p_add">+	.align 4;</span>
<span class="p_add">+</span>
<span class="p_add">+#define EX_LD_FP(x)			\</span>
<span class="p_add">+98:	x;				\</span>
<span class="p_add">+	.section __ex_table,&quot;a&quot;;	\</span>
<span class="p_add">+	.align 4;			\</span>
<span class="p_add">+	.word 98b, __restore_asi_fp;	\</span>
<span class="p_add">+	.text;				\</span>
<span class="p_add">+	.align 4;</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef ASI_AIUS</span>
<span class="p_add">+#define ASI_AIUS	0x11</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#define FUNC_NAME		M7copy_from_user</span>
<span class="p_add">+#define LOAD(type,addr,dest)	type##a [addr] %asi, dest</span>
<span class="p_add">+#define EX_RETVAL(x)		0</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef __KERNEL__</span>
<span class="p_add">+#define PREAMBLE					\</span>
<span class="p_add">+	rd		%asi, %g1;			\</span>
<span class="p_add">+	cmp		%g1, ASI_AIUS;			\</span>
<span class="p_add">+	bne,pn		%icc, raw_copy_in_user;		\</span>
<span class="p_add">+	nop</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#include &quot;M7memcpy.S&quot;</span>
<span class="p_header">diff --git a/arch/sparc/lib/M7copy_to_user.S b/arch/sparc/lib/M7copy_to_user.S</span>
new file mode 100644
<span class="p_header">index 0000000..d3be132</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/sparc/lib/M7copy_to_user.S</span>
<span class="p_chunk">@@ -0,0 +1,51 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * M7copy_to_user.S: SPARC M7 optimized copy to userspace.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Copyright (c) 2016, Oracle and/or its affiliates. All rights reserved.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+#define EX_ST(x)			\</span>
<span class="p_add">+98:	x;				\</span>
<span class="p_add">+	.section __ex_table,&quot;a&quot;;	\</span>
<span class="p_add">+	.align 4;			\</span>
<span class="p_add">+	.word 98b, __restore_asi;	\</span>
<span class="p_add">+	.text;				\</span>
<span class="p_add">+	.align 4;</span>
<span class="p_add">+</span>
<span class="p_add">+#define EX_ST_FP(x)			\</span>
<span class="p_add">+98:	x;				\</span>
<span class="p_add">+	.section __ex_table,&quot;a&quot;;	\</span>
<span class="p_add">+	.align 4;			\</span>
<span class="p_add">+	.word 98b, __restore_asi_fp;	\</span>
<span class="p_add">+	.text;				\</span>
<span class="p_add">+	.align 4;</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef ASI_AIUS</span>
<span class="p_add">+#define ASI_AIUS	0x11</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef ASI_BLK_INIT_QUAD_LDD_AIUS</span>
<span class="p_add">+#define ASI_BLK_INIT_QUAD_LDD_AIUS 0x23</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#define FUNC_NAME		M7copy_to_user</span>
<span class="p_add">+#define STORE(type,src,addr)	type##a src, [addr] %asi</span>
<span class="p_add">+#define STORE_ASI		ASI_BLK_INIT_QUAD_LDD_AIUS</span>
<span class="p_add">+#define	STORE_MRU_ASI		ASI_ST_BLKINIT_MRU_S</span>
<span class="p_add">+#define EX_RETVAL(x)		0</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef __KERNEL__</span>
<span class="p_add">+	/* Writing to %asi is _expensive_ so we hardcode it.</span>
<span class="p_add">+	 * Reading %asi to check for KERNEL_DS is comparatively</span>
<span class="p_add">+	 * cheap.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+#define PREAMBLE					\</span>
<span class="p_add">+	rd		%asi, %g1;			\</span>
<span class="p_add">+	cmp		%g1, ASI_AIUS;			\</span>
<span class="p_add">+	bne,pn		%icc, raw_copy_in_user;		\</span>
<span class="p_add">+	nop</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#include &quot;M7memcpy.S&quot;</span>
<span class="p_header">diff --git a/arch/sparc/lib/M7memcpy.S b/arch/sparc/lib/M7memcpy.S</span>
new file mode 100644
<span class="p_header">index 0000000..0a0421d</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/sparc/lib/M7memcpy.S</span>
<span class="p_chunk">@@ -0,0 +1,923 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * M7memcpy: Optimized SPARC M7 memcpy</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Copyright (c) 2016, Oracle and/or its affiliates. All rights reserved.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+	.file	&quot;M7memcpy.S&quot;</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * memcpy(s1, s2, len)</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Copy s2 to s1, always copy n bytes.</span>
<span class="p_add">+ * Note: this C code does not work for overlapped copies.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Fast assembler language version of the following C-program for memcpy</span>
<span class="p_add">+ * which represents the `standard&#39; for the C-library.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *	void *</span>
<span class="p_add">+ *	memcpy(void *s, const void *s0, size_t n)</span>
<span class="p_add">+ *	{</span>
<span class="p_add">+ *		if (n != 0) {</span>
<span class="p_add">+ *		    char *s1 = s;</span>
<span class="p_add">+ *		    const char *s2 = s0;</span>
<span class="p_add">+ *		    do {</span>
<span class="p_add">+ *			*s1++ = *s2++;</span>
<span class="p_add">+ *		    } while (--n != 0);</span>
<span class="p_add">+ *		}</span>
<span class="p_add">+ *		return (s);</span>
<span class="p_add">+ *	}</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * SPARC T7/M7 Flow :</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * if (count &lt; SMALL_MAX) {</span>
<span class="p_add">+ *   if count &lt; SHORTCOPY              (SHORTCOPY=3)</span>
<span class="p_add">+ *	copy bytes; exit with dst addr</span>
<span class="p_add">+ *   if src &amp; dst aligned on word boundary but not long word boundary,</span>
<span class="p_add">+ *     copy with ldw/stw; branch to finish_up</span>
<span class="p_add">+ *   if src &amp; dst aligned on long word boundary</span>
<span class="p_add">+ *     copy with ldx/stx; branch to finish_up</span>
<span class="p_add">+ *   if src &amp; dst not aligned and length &lt;= SHORTCHECK   (SHORTCHECK=14)</span>
<span class="p_add">+ *     copy bytes; exit with dst addr</span>
<span class="p_add">+ *   move enough bytes to get src to word boundary</span>
<span class="p_add">+ *   if dst now on word boundary</span>
<span class="p_add">+ * move_words:</span>
<span class="p_add">+ *     copy words; branch to finish_up</span>
<span class="p_add">+ *   if dst now on half word boundary</span>
<span class="p_add">+ *     load words, shift half words, store words; branch to finish_up</span>
<span class="p_add">+ *   if dst on byte 1</span>
<span class="p_add">+ *     load words, shift 3 bytes, store words; branch to finish_up</span>
<span class="p_add">+ *   if dst on byte 3</span>
<span class="p_add">+ *     load words, shift 1 byte, store words; branch to finish_up</span>
<span class="p_add">+ * finish_up:</span>
<span class="p_add">+ *     copy bytes; exit with dst addr</span>
<span class="p_add">+ * } else {                                         More than SMALL_MAX bytes</span>
<span class="p_add">+ *   move bytes until dst is on long word boundary</span>
<span class="p_add">+ *   if( src is on long word boundary ) {</span>
<span class="p_add">+ *     if (count &lt; MED_MAX) {</span>
<span class="p_add">+ * finish_long:					   src/dst aligned on 8 bytes</span>
<span class="p_add">+ *       copy with ldx/stx in 8-way unrolled loop;</span>
<span class="p_add">+ *       copy final 0-63 bytes; exit with dst addr</span>
<span class="p_add">+ *     } else {				     src/dst aligned; count &gt; MED_MAX</span>
<span class="p_add">+ *       align dst on 64 byte boundary; for main data movement:</span>
<span class="p_add">+ *       prefetch src data to L2 cache; let HW prefetch move data to L1 cache</span>
<span class="p_add">+ *       Use BIS (block initializing store) to avoid copying store cache</span>
<span class="p_add">+ *       lines from memory. But pre-store first element of each cache line</span>
<span class="p_add">+ *       ST_CHUNK lines in advance of the rest of that cache line. That</span>
<span class="p_add">+ *       gives time for replacement cache lines to be written back without</span>
<span class="p_add">+ *       excess STQ and Miss Buffer filling. Repeat until near the end,</span>
<span class="p_add">+ *       then finish up storing before going to finish_long.</span>
<span class="p_add">+ *     }</span>
<span class="p_add">+ *   } else {                                   src/dst not aligned on 8 bytes</span>
<span class="p_add">+ *     if src is word aligned and count &lt; MED_WMAX</span>
<span class="p_add">+ *       move words in 8-way unrolled loop</span>
<span class="p_add">+ *       move final 0-31 bytes; exit with dst addr</span>
<span class="p_add">+ *     if count &lt; MED_UMAX</span>
<span class="p_add">+ *       use alignaddr/faligndata combined with ldd/std in 8-way</span>
<span class="p_add">+ *       unrolled loop to move data.</span>
<span class="p_add">+ *       go to unalign_done</span>
<span class="p_add">+ *     else</span>
<span class="p_add">+ *       setup alignaddr for faligndata instructions</span>
<span class="p_add">+ *       align dst on 64 byte boundary; prefetch src data to L1 cache</span>
<span class="p_add">+ *       loadx8, falign, block-store, prefetch loop</span>
<span class="p_add">+ *	 (only use block-init-store when src/dst on 8 byte boundaries.)</span>
<span class="p_add">+ * unalign_done:</span>
<span class="p_add">+ *       move remaining bytes for unaligned cases. exit with dst addr.</span>
<span class="p_add">+ * }</span>
<span class="p_add">+ *</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/visasm.h&gt;</span>
<span class="p_add">+#include &lt;asm/asi.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#if !defined(EX_LD) &amp;&amp; !defined(EX_ST)</span>
<span class="p_add">+#define NON_USER_COPY</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef EX_LD</span>
<span class="p_add">+#define EX_LD(x)	x</span>
<span class="p_add">+#endif</span>
<span class="p_add">+#ifndef EX_LD_FP</span>
<span class="p_add">+#define EX_LD_FP(x)	x</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef EX_ST</span>
<span class="p_add">+#define EX_ST(x)	x</span>
<span class="p_add">+#endif</span>
<span class="p_add">+#ifndef EX_ST_FP</span>
<span class="p_add">+#define EX_ST_FP(x)	x</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef EX_RETVAL</span>
<span class="p_add">+#define EX_RETVAL(x)    x</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef LOAD</span>
<span class="p_add">+#define LOAD(type,addr,dest)	type [addr], dest</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef STORE</span>
<span class="p_add">+#define STORE(type,src,addr)	type src, [addr]</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * ASI_BLK_INIT_QUAD_LDD_P/ASI_BLK_INIT_QUAD_LDD_S marks the cache</span>
<span class="p_add">+ * line as &quot;least recently used&quot; which means if many threads are</span>
<span class="p_add">+ * active, it has a high probability of being pushed out of the cache</span>
<span class="p_add">+ * between the first initializing store and the final stores.</span>
<span class="p_add">+ * Thus, we use ASI_ST_BLKINIT_MRU_P/ASI_ST_BLKINIT_MRU_S which</span>
<span class="p_add">+ * marks the cache line as &quot;most recently used&quot; for all</span>
<span class="p_add">+ * but the last cache line</span>
<span class="p_add">+ */</span>
<span class="p_add">+#ifndef STORE_ASI</span>
<span class="p_add">+#ifndef SIMULATE_NIAGARA_ON_NON_NIAGARA</span>
<span class="p_add">+#define STORE_ASI	ASI_BLK_INIT_QUAD_LDD_P</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define STORE_ASI	0x80		/* ASI_P */</span>
<span class="p_add">+#endif</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef STORE_MRU_ASI</span>
<span class="p_add">+#ifndef SIMULATE_NIAGARA_ON_NON_NIAGARA</span>
<span class="p_add">+#define STORE_MRU_ASI	ASI_ST_BLKINIT_MRU_P</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define STORE_MRU_ASI	0x80		/* ASI_P */</span>
<span class="p_add">+#endif</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef STORE_INIT</span>
<span class="p_add">+#define STORE_INIT(src,addr)	stxa src, [addr] STORE_ASI</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef STORE_INIT_MRU</span>
<span class="p_add">+#define STORE_INIT_MRU(src,addr)	stxa src, [addr] STORE_MRU_ASI</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef FUNC_NAME</span>
<span class="p_add">+#define FUNC_NAME	M7memcpy</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef PREAMBLE</span>
<span class="p_add">+#define PREAMBLE</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#define	BLOCK_SIZE	64</span>
<span class="p_add">+#define	SHORTCOPY	3</span>
<span class="p_add">+#define	SHORTCHECK	14</span>
<span class="p_add">+#define	SHORT_LONG	64	/* max copy for short longword-aligned case */</span>
<span class="p_add">+				/* must be at least 64 */</span>
<span class="p_add">+#define	SMALL_MAX	128</span>
<span class="p_add">+#define	MED_UMAX	1024	/* max copy for medium un-aligned case */</span>
<span class="p_add">+#define	MED_WMAX	1024	/* max copy for medium word-aligned case */</span>
<span class="p_add">+#define	MED_MAX		1024	/* max copy for medium longword-aligned case */</span>
<span class="p_add">+#define ST_CHUNK	24	/* ST_CHUNK - block of values for BIS Store */</span>
<span class="p_add">+#define ALIGN_PRE	24	/* distance for aligned prefetch loop */</span>
<span class="p_add">+</span>
<span class="p_add">+	.register	%g2,#scratch</span>
<span class="p_add">+</span>
<span class="p_add">+	.section	&quot;.text&quot;</span>
<span class="p_add">+	.global		FUNC_NAME</span>
<span class="p_add">+	.type		FUNC_NAME, #function</span>
<span class="p_add">+	.align		16</span>
<span class="p_add">+FUNC_NAME:</span>
<span class="p_add">+	srlx            %o2, 31, %g2</span>
<span class="p_add">+	cmp             %g2, 0</span>
<span class="p_add">+	tne             %xcc, 5</span>
<span class="p_add">+	PREAMBLE</span>
<span class="p_add">+	mov		%o0, %g1	! save %o0</span>
<span class="p_add">+	brz,pn          %o2, .Lsmallx</span>
<span class="p_add">+	 cmp            %o2, 3</span>
<span class="p_add">+	ble,pn          %icc, .Ltiny_cp</span>
<span class="p_add">+	 cmp            %o2, 19</span>
<span class="p_add">+	ble,pn          %icc, .Lsmall_cp</span>
<span class="p_add">+	 or             %o0, %o1, %g2</span>
<span class="p_add">+	cmp             %o2, SMALL_MAX</span>
<span class="p_add">+	bl,pn           %icc, .Lmedium_cp</span>
<span class="p_add">+	 nop</span>
<span class="p_add">+</span>
<span class="p_add">+.Lmedium:</span>
<span class="p_add">+	neg	%o0, %o5</span>
<span class="p_add">+	andcc	%o5, 7, %o5		! bytes till DST 8 byte aligned</span>
<span class="p_add">+	brz,pt	%o5, .Ldst_aligned_on_8</span>
<span class="p_add">+</span>
<span class="p_add">+	! %o5 has the bytes to be written in partial store.</span>
<span class="p_add">+	 sub	%o2, %o5, %o2</span>
<span class="p_add">+	sub	%o1, %o0, %o1		! %o1 gets the difference</span>
<span class="p_add">+7:					! dst aligning loop</span>
<span class="p_add">+	add	%o1, %o0, %o4</span>
<span class="p_add">+	EX_LD(LOAD(ldub, %o4, %o4))	! load one byte</span>
<span class="p_add">+	subcc	%o5, 1, %o5</span>
<span class="p_add">+	EX_ST(STORE(stb, %o4, %o0))</span>
<span class="p_add">+	bgu,pt	%ncc, 7b</span>
<span class="p_add">+	 add	%o0, 1, %o0		! advance dst</span>
<span class="p_add">+	add	%o1, %o0, %o1		! restore %o1</span>
<span class="p_add">+.Ldst_aligned_on_8:</span>
<span class="p_add">+	andcc	%o1, 7, %o5</span>
<span class="p_add">+	brnz,pt	%o5, .Lsrc_dst_unaligned_on_8</span>
<span class="p_add">+	 nop</span>
<span class="p_add">+</span>
<span class="p_add">+.Lsrc_dst_aligned_on_8:</span>
<span class="p_add">+	! check if we are copying MED_MAX or more bytes</span>
<span class="p_add">+	set MED_MAX, %o3</span>
<span class="p_add">+	cmp %o2, %o3 			! limit to store buffer size</span>
<span class="p_add">+	bgu,pn	%ncc, .Llarge_align8_copy</span>
<span class="p_add">+	 nop</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Special case for handling when src and dest are both long word aligned</span>
<span class="p_add">+ * and total data to move is less than MED_MAX bytes</span>
<span class="p_add">+ */</span>
<span class="p_add">+.Lmedlong:</span>
<span class="p_add">+	subcc	%o2, 63, %o2		! adjust length to allow cc test</span>
<span class="p_add">+	ble,pn	%ncc, .Lmedl63		! skip big loop if less than 64 bytes</span>
<span class="p_add">+	 nop</span>
<span class="p_add">+.Lmedl64:</span>
<span class="p_add">+	EX_LD(LOAD(ldx, %o1, %o4))	! load</span>
<span class="p_add">+	subcc	%o2, 64, %o2		! decrement length count</span>
<span class="p_add">+	EX_ST(STORE(stx, %o4, %o0))	! and store</span>
<span class="p_add">+	EX_LD(LOAD(ldx, %o1+8, %o3))	! a block of 64 bytes</span>
<span class="p_add">+	EX_ST(STORE(stx, %o3, %o0+8))</span>
<span class="p_add">+	EX_LD(LOAD(ldx, %o1+16, %o4))</span>
<span class="p_add">+	EX_ST(STORE(stx, %o4, %o0+16))</span>
<span class="p_add">+	EX_LD(LOAD(ldx, %o1+24, %o3))</span>
<span class="p_add">+	EX_ST(STORE(stx, %o3, %o0+24))</span>
<span class="p_add">+	EX_LD(LOAD(ldx, %o1+32, %o4))	! load</span>
<span class="p_add">+	EX_ST(STORE(stx, %o4, %o0+32))	! and store</span>
<span class="p_add">+	EX_LD(LOAD(ldx, %o1+40, %o3))	! a block of 64 bytes</span>
<span class="p_add">+	add	%o1, 64, %o1		! increase src ptr by 64</span>
<span class="p_add">+	EX_ST(STORE(stx, %o3, %o0+40))</span>
<span class="p_add">+	EX_LD(LOAD(ldx, %o1-16, %o4))</span>
<span class="p_add">+	add	%o0, 64, %o0		! increase dst ptr by 64</span>
<span class="p_add">+	EX_ST(STORE(stx, %o4, %o0-16))</span>
<span class="p_add">+	EX_LD(LOAD(ldx, %o1-8, %o3))</span>
<span class="p_add">+	bgu,pt	%ncc, .Lmedl64		! repeat if at least 64 bytes left</span>
<span class="p_add">+	 EX_ST(STORE(stx, %o3, %o0-8))</span>
<span class="p_add">+.Lmedl63:</span>
<span class="p_add">+	addcc	%o2, 32, %o2		! adjust remaining count</span>
<span class="p_add">+	ble,pt	%ncc, .Lmedl31		! to skip if 31 or fewer bytes left</span>
<span class="p_add">+	 nop</span>
<span class="p_add">+	EX_LD(LOAD(ldx, %o1, %o4))	! load</span>
<span class="p_add">+	sub	%o2, 32, %o2		! decrement length count</span>
<span class="p_add">+	EX_ST(STORE(stx, %o4, %o0))	! and store</span>
<span class="p_add">+	EX_LD(LOAD(ldx, %o1+8, %o3))	! a block of 32 bytes</span>
<span class="p_add">+	add	%o1, 32, %o1		! increase src ptr by 32</span>
<span class="p_add">+	EX_ST(STORE(stx, %o3, %o0+8))</span>
<span class="p_add">+	EX_LD(LOAD(ldx, %o1-16, %o4))</span>
<span class="p_add">+	add	%o0, 32, %o0		! increase dst ptr by 32</span>
<span class="p_add">+	EX_ST(STORE(stx, %o4, %o0-16))</span>
<span class="p_add">+	EX_LD(LOAD(ldx, %o1-8, %o3))</span>
<span class="p_add">+	EX_ST(STORE(stx, %o3, %o0-8))</span>
<span class="p_add">+.Lmedl31:</span>
<span class="p_add">+	addcc	%o2, 16, %o2		! adjust remaining count</span>
<span class="p_add">+	ble,pt	%ncc, .Lmedl15		! skip if 15 or fewer bytes left</span>
<span class="p_add">+	 nop				!</span>
<span class="p_add">+	EX_LD(LOAD(ldx, %o1, %o4))</span>
<span class="p_add">+	add	%o1, 16, %o1		! increase src ptr by 16</span>
<span class="p_add">+	EX_ST(STORE(stx, %o4, %o0))</span>
<span class="p_add">+	sub	%o2, 16, %o2		! decrease count by 16</span>
<span class="p_add">+	EX_LD(LOAD(ldx, %o1-8, %o3))</span>
<span class="p_add">+	add	%o0, 16, %o0		! increase dst ptr by 16</span>
<span class="p_add">+	EX_ST(STORE(stx, %o3, %o0-8))</span>
<span class="p_add">+.Lmedl15:</span>
<span class="p_add">+	addcc	%o2, 15, %o2		! restore count</span>
<span class="p_add">+	bz,pt	%ncc, .Lsmallx	! exit if finished</span>
<span class="p_add">+	 cmp	%o2, 8</span>
<span class="p_add">+	blt,pt	%ncc, .Lmedw7		! skip if 7 or fewer bytes left</span>
<span class="p_add">+	 tst	%o2</span>
<span class="p_add">+	EX_LD(LOAD(ldx, %o1, %o4))	! load 8 bytes</span>
<span class="p_add">+	add	%o1, 8, %o1		! increase src ptr by 8</span>
<span class="p_add">+	add	%o0, 8, %o0		! increase dst ptr by 8</span>
<span class="p_add">+	subcc	%o2, 8, %o2		! decrease count by 8</span>
<span class="p_add">+	bnz,pn	%ncc, .Lmedw7</span>
<span class="p_add">+	EX_ST(STORE(stx, %o4, %o0-8))	! and store 8 bytes</span>
<span class="p_add">+	retl</span>
<span class="p_add">+	 mov	EX_RETVAL(%g1), %o0	! restore %o0</span>
<span class="p_add">+</span>
<span class="p_add">+	.align 16</span>
<span class="p_add">+.Lsrc_dst_unaligned_on_8:</span>
<span class="p_add">+	! DST is 8-byte aligned, src is not</span>
<span class="p_add">+2:</span>
<span class="p_add">+	andcc	%o1, 0x3, %o5		! test word alignment</span>
<span class="p_add">+	bnz,pt	%ncc, .Lunalignsetup	! branch to skip if not word aligned</span>
<span class="p_add">+	 nop</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Handle all cases where src and dest are aligned on word</span>
<span class="p_add">+ * boundaries. Use unrolled loops for better performance.</span>
<span class="p_add">+ * This option wins over standard large data move when</span>
<span class="p_add">+ * source and destination is in cache for.Lmedium</span>
<span class="p_add">+ * to short data moves.</span>
<span class="p_add">+ */</span>
<span class="p_add">+	set MED_WMAX, %o3</span>
<span class="p_add">+	cmp %o2, %o3 			! limit to store buffer size</span>
<span class="p_add">+	bge,pt	%ncc, .Lunalignrejoin	! otherwise rejoin main loop</span>
<span class="p_add">+	 nop</span>
<span class="p_add">+</span>
<span class="p_add">+	subcc	%o2, 31, %o2		! adjust length to allow cc test</span>
<span class="p_add">+					! for end of loop</span>
<span class="p_add">+	ble,pt	%ncc, .Lmedw31		! skip big loop if less than 16</span>
<span class="p_add">+.Lmedw32:</span>
<span class="p_add">+	EX_LD(LOAD(ld, %o1, %o4))	! move a block of 32 bytes</span>
<span class="p_add">+	sllx	%o4, 32, %o5</span>
<span class="p_add">+	EX_LD(LOAD(ld, %o1+4, %o4))</span>
<span class="p_add">+	or	%o4, %o5, %o5</span>
<span class="p_add">+	EX_ST(STORE(stx, %o5, %o0))</span>
<span class="p_add">+	subcc	%o2, 32, %o2		! decrement length count</span>
<span class="p_add">+	EX_LD(LOAD(ld, %o1+8, %o4))</span>
<span class="p_add">+	sllx	%o4, 32, %o5</span>
<span class="p_add">+	EX_LD(LOAD(ld, %o1+12, %o4))</span>
<span class="p_add">+	or	%o4, %o5, %o5</span>
<span class="p_add">+	EX_ST(STORE(stx, %o5, %o0+8))</span>
<span class="p_add">+	add	%o1, 32, %o1		! increase src ptr by 32</span>
<span class="p_add">+	EX_LD(LOAD(ld, %o1-16, %o4))</span>
<span class="p_add">+	sllx	%o4, 32, %o5</span>
<span class="p_add">+	EX_LD(LOAD(ld, %o1-12, %o4))</span>
<span class="p_add">+	or	%o4, %o5, %o5</span>
<span class="p_add">+	EX_ST(STORE(stx, %o5, %o0+16))</span>
<span class="p_add">+	add	%o0, 32, %o0		! increase dst ptr by 32</span>
<span class="p_add">+	EX_LD(LOAD(ld, %o1-8, %o4))</span>
<span class="p_add">+	sllx	%o4, 32, %o5</span>
<span class="p_add">+	EX_LD(LOAD(ld, %o1-4, %o4))</span>
<span class="p_add">+	or	%o4, %o5, %o5</span>
<span class="p_add">+	bgu,pt	%ncc, .Lmedw32		! repeat if at least 32 bytes left</span>
<span class="p_add">+	 EX_ST(STORE(stx, %o5, %o0-8))</span>
<span class="p_add">+.Lmedw31:</span>
<span class="p_add">+	addcc	%o2, 31, %o2		! restore count</span>
<span class="p_add">+</span>
<span class="p_add">+	bz,pt	%ncc, .Lsmallx	! exit if finished</span>
<span class="p_add">+	 nop</span>
<span class="p_add">+	cmp	%o2, 16</span>
<span class="p_add">+	blt,pt	%ncc, .Lmedw15</span>
<span class="p_add">+	 nop</span>
<span class="p_add">+	EX_LD(LOAD(ld, %o1, %o4))	! move a block of 16 bytes</span>
<span class="p_add">+	sllx	%o4, 32, %o5</span>
<span class="p_add">+	subcc	%o2, 16, %o2		! decrement length count</span>
<span class="p_add">+	EX_LD(LOAD(ld, %o1+4, %o4))</span>
<span class="p_add">+	or	%o4, %o5, %o5</span>
<span class="p_add">+	EX_ST(STORE(stx, %o5, %o0))</span>
<span class="p_add">+	add	%o1, 16, %o1		! increase src ptr by 16</span>
<span class="p_add">+	EX_LD(LOAD(ld, %o1-8, %o4))</span>
<span class="p_add">+	add	%o0, 16, %o0		! increase dst ptr by 16</span>
<span class="p_add">+	sllx	%o4, 32, %o5</span>
<span class="p_add">+	EX_LD(LOAD(ld, %o1-4, %o4))</span>
<span class="p_add">+	or	%o4, %o5, %o5</span>
<span class="p_add">+	EX_ST(STORE(stx, %o5, %o0-8))</span>
<span class="p_add">+.Lmedw15:</span>
<span class="p_add">+	bz,pt	%ncc, .Lsmallx	! exit if finished</span>
<span class="p_add">+	 cmp	%o2, 8</span>
<span class="p_add">+	blt,pn	%ncc, .Lmedw7		! skip if 7 or fewer bytes left</span>
<span class="p_add">+	 tst	%o2</span>
<span class="p_add">+	EX_LD(LOAD(ld, %o1, %o4))	! load 4 bytes</span>
<span class="p_add">+	subcc	%o2, 8, %o2		! decrease count by 8</span>
<span class="p_add">+	EX_ST(STORE(stw, %o4, %o0))	! and store 4 bytes</span>
<span class="p_add">+	add	%o1, 8, %o1		! increase src ptr by 8</span>
<span class="p_add">+	EX_LD(LOAD(ld, %o1-4, %o3))	! load 4 bytes</span>
<span class="p_add">+	add	%o0, 8, %o0		! increase dst ptr by 8</span>
<span class="p_add">+	EX_ST(STORE(stw, %o3, %o0-4))	! and store 4 bytes</span>
<span class="p_add">+	bz,pt	%ncc, .Lsmallx	! exit if finished</span>
<span class="p_add">+.Lmedw7:				! count is ge 1, less than 8</span>
<span class="p_add">+	cmp	%o2, 4			! check for 4 bytes left</span>
<span class="p_add">+	blt,pn	%ncc, .Lsmallleft3	! skip if 3 or fewer bytes left</span>
<span class="p_add">+	 nop				!</span>
<span class="p_add">+	EX_LD(LOAD(ld, %o1, %o4))	! load 4 bytes</span>
<span class="p_add">+	add	%o1, 4, %o1		! increase src ptr by 4</span>
<span class="p_add">+	add	%o0, 4, %o0		! increase dst ptr by 4</span>
<span class="p_add">+	subcc	%o2, 4, %o2		! decrease count by 4</span>
<span class="p_add">+	bnz	.Lsmallleft3</span>
<span class="p_add">+	 EX_ST(STORE(stw, %o4, %o0-4))! and store 4 bytes</span>
<span class="p_add">+	retl</span>
<span class="p_add">+	 mov	EX_RETVAL(%g1), %o0</span>
<span class="p_add">+</span>
<span class="p_add">+	.align 16</span>
<span class="p_add">+.Llarge_align8_copy:			! Src and dst share 8 byte alignment</span>
<span class="p_add">+	! align dst to 64 byte boundary</span>
<span class="p_add">+	andcc	%o0, 0x3f, %o3		! %o3 == 0 means dst is 64 byte aligned</span>
<span class="p_add">+	brz,pn	%o3, .Laligned_to_64</span>
<span class="p_add">+	 andcc	%o0, 8, %o3		! odd long words to move?</span>
<span class="p_add">+	brz,pt	%o3, .Laligned_to_16</span>
<span class="p_add">+	 nop</span>
<span class="p_add">+	EX_LD(LOAD(ldx, %o1, %o4))</span>
<span class="p_add">+	sub	%o2, 8, %o2</span>
<span class="p_add">+	add	%o1, 8, %o1		! increment src ptr</span>
<span class="p_add">+	add	%o0, 8, %o0		! increment dst ptr</span>
<span class="p_add">+	EX_ST(STORE(stx, %o4, %o0-8))</span>
<span class="p_add">+.Laligned_to_16:</span>
<span class="p_add">+	andcc	%o0, 16, %o3		! pair of long words to move?</span>
<span class="p_add">+	brz,pt	%o3, .Laligned_to_32</span>
<span class="p_add">+	 nop</span>
<span class="p_add">+	EX_LD(LOAD(ldx, %o1, %o4))</span>
<span class="p_add">+	sub	%o2, 16, %o2</span>
<span class="p_add">+	EX_ST(STORE(stx, %o4, %o0))</span>
<span class="p_add">+	add	%o1, 16, %o1		! increment src ptr</span>
<span class="p_add">+	EX_LD(LOAD(ldx, %o1-8, %o4))</span>
<span class="p_add">+	add	%o0, 16, %o0		! increment dst ptr</span>
<span class="p_add">+	EX_ST(STORE(stx, %o4, %o0-8))</span>
<span class="p_add">+.Laligned_to_32:</span>
<span class="p_add">+	andcc	%o0, 32, %o3		! four long words to move?</span>
<span class="p_add">+	brz,pt	%o3, .Laligned_to_64</span>
<span class="p_add">+	 nop</span>
<span class="p_add">+	EX_LD(LOAD(ldx, %o1, %o4))</span>
<span class="p_add">+	sub	%o2, 32, %o2</span>
<span class="p_add">+	EX_ST(STORE(stx, %o4, %o0))</span>
<span class="p_add">+	EX_LD(LOAD(ldx, %o1+8, %o4))</span>
<span class="p_add">+	EX_ST(STORE(stx, %o4, %o0+8))</span>
<span class="p_add">+	EX_LD(LOAD(ldx, %o1+16, %o4))</span>
<span class="p_add">+	EX_ST(STORE(stx, %o4, %o0+16))</span>
<span class="p_add">+	add	%o1, 32, %o1		! increment src ptr</span>
<span class="p_add">+	EX_LD(LOAD(ldx, %o1-8, %o4))</span>
<span class="p_add">+	add	%o0, 32, %o0		! increment dst ptr</span>
<span class="p_add">+	EX_ST(STORE(stx, %o4, %o0-8))</span>
<span class="p_add">+.Laligned_to_64:</span>
<span class="p_add">+!</span>
<span class="p_add">+!	Using block init store (BIS) instructions to avoid fetching cache</span>
<span class="p_add">+!	lines from memory. Use ST_CHUNK stores to first element of each cache</span>
<span class="p_add">+!	line (similar to prefetching) to avoid overfilling STQ or miss buffers.</span>
<span class="p_add">+!	Gives existing cache lines time to be moved out of L1/L2/L3 cache.</span>
<span class="p_add">+!	Initial stores using MRU version of BIS to keep cache line in</span>
<span class="p_add">+!	cache until we are ready to store final element of cache line.</span>
<span class="p_add">+!	Then store last element using the LRU version of BIS.</span>
<span class="p_add">+!</span>
<span class="p_add">+	andn	%o2, 0x3f, %o5		! %o5 is multiple of block size</span>
<span class="p_add">+	and	%o2, 0x3f, %o2		! residue bytes in %o2</span>
<span class="p_add">+!</span>
<span class="p_add">+!	We use STORE_MRU_ASI for the first seven stores to each cache line</span>
<span class="p_add">+!	followed by STORE_ASI (mark as LRU) for the last store. That</span>
<span class="p_add">+!	mixed approach reduces the probability that the cache line is removed</span>
<span class="p_add">+!	before we finish setting it, while minimizing the effects on</span>
<span class="p_add">+!	other cached values during a large memcpy</span>
<span class="p_add">+!</span>
<span class="p_add">+!	ST_CHUNK batches up initial BIS operations for several cache lines</span>
<span class="p_add">+!	to allow multiple requests to not be blocked by overflowing the</span>
<span class="p_add">+!	the store miss buffer. Then the matching stores for all those</span>
<span class="p_add">+!	BIS operations are executed.</span>
<span class="p_add">+!</span>
<span class="p_add">+</span>
<span class="p_add">+	sub	%o0, 8, %o0		! adjust %o0 for ASI alignment</span>
<span class="p_add">+.Lalign_loop:</span>
<span class="p_add">+	cmp	%o5, ST_CHUNK*64</span>
<span class="p_add">+	blu,pt	%ncc, .Lalign_loop_fin</span>
<span class="p_add">+	 mov	ST_CHUNK,%o3</span>
<span class="p_add">+.Lalign_loop_start:</span>
<span class="p_add">+	prefetch [%o1 + (ALIGN_PRE * BLOCK_SIZE)], 21</span>
<span class="p_add">+	subcc	%o3, 1, %o3</span>
<span class="p_add">+	EX_LD(LOAD(ldx, %o1, %o4))</span>
<span class="p_add">+	add	%o1, 64, %o1</span>
<span class="p_add">+	add	%o0, 8, %o0</span>
<span class="p_add">+	EX_ST(STORE_INIT_MRU(%o4, %o0))</span>
<span class="p_add">+	bgu	%ncc,.Lalign_loop_start</span>
<span class="p_add">+	 add	%o0, 56, %o0</span>
<span class="p_add">+</span>
<span class="p_add">+	mov	ST_CHUNK,%o3</span>
<span class="p_add">+	sllx	%o3, 6, %o4		! ST_CHUNK*64</span>
<span class="p_add">+	sub	%o1, %o4, %o1		! reset %o1</span>
<span class="p_add">+	sub	%o0, %o4, %o0		! reset %o0</span>
<span class="p_add">+</span>
<span class="p_add">+.Lalign_loop_rest:</span>
<span class="p_add">+	EX_LD(LOAD(ldx, %o1+8, %o4))</span>
<span class="p_add">+	add	%o0, 16, %o0</span>
<span class="p_add">+	EX_ST(STORE_INIT_MRU(%o4, %o0))</span>
<span class="p_add">+	EX_LD(LOAD(ldx, %o1+16, %o4))</span>
<span class="p_add">+	add	%o0, 8, %o0</span>
<span class="p_add">+	EX_ST(STORE_INIT_MRU(%o4, %o0))</span>
<span class="p_add">+	subcc	%o3, 1, %o3</span>
<span class="p_add">+	EX_LD(LOAD(ldx, %o1+24, %o4))</span>
<span class="p_add">+	add	%o0, 8, %o0</span>
<span class="p_add">+	EX_ST(STORE_INIT_MRU(%o4, %o0))</span>
<span class="p_add">+	EX_LD(LOAD(ldx, %o1+32, %o4))</span>
<span class="p_add">+	add	%o0, 8, %o0</span>
<span class="p_add">+	EX_ST(STORE_INIT_MRU(%o4, %o0))</span>
<span class="p_add">+	EX_LD(LOAD(ldx, %o1+40, %o4))</span>
<span class="p_add">+	add	%o0, 8, %o0</span>
<span class="p_add">+	EX_ST(STORE_INIT_MRU(%o4, %o0))</span>
<span class="p_add">+	EX_LD(LOAD(ldx, %o1+48, %o4))</span>
<span class="p_add">+	add	%o1, 64, %o1</span>
<span class="p_add">+	add	%o0, 8, %o0</span>
<span class="p_add">+	EX_ST(STORE_INIT_MRU(%o4, %o0))</span>
<span class="p_add">+	add	%o0, 8, %o0</span>
<span class="p_add">+	EX_LD(LOAD(ldx, %o1-8, %o4))</span>
<span class="p_add">+	sub	%o5, 64, %o5</span>
<span class="p_add">+	bgu	%ncc,.Lalign_loop_rest</span>
<span class="p_add">+	! mark cache line as LRU</span>
<span class="p_add">+	 EX_ST(STORE_INIT(%o4, %o0))</span>
<span class="p_add">+</span>
<span class="p_add">+	cmp	%o5, ST_CHUNK*64</span>
<span class="p_add">+	bgu,pt	%ncc, .Lalign_loop_start</span>
<span class="p_add">+	 mov	ST_CHUNK,%o3</span>
<span class="p_add">+</span>
<span class="p_add">+	cmp	%o5, 0</span>
<span class="p_add">+	beq	.Lalign_done</span>
<span class="p_add">+	 nop</span>
<span class="p_add">+.Lalign_loop_fin:</span>
<span class="p_add">+	EX_LD(LOAD(ldx, %o1, %o4))</span>
<span class="p_add">+	EX_ST(STORE(stx, %o4, %o0+8))</span>
<span class="p_add">+	EX_LD(LOAD(ldx, %o1+8, %o4))</span>
<span class="p_add">+	EX_ST(STORE(stx, %o4, %o0+8+8))</span>
<span class="p_add">+	EX_LD(LOAD(ldx, %o1+16, %o4))</span>
<span class="p_add">+	EX_ST(STORE(stx, %o4, %o0+8+16))</span>
<span class="p_add">+	subcc	%o5, 64, %o5</span>
<span class="p_add">+	EX_LD(LOAD(ldx, %o1+24, %o4))</span>
<span class="p_add">+	EX_ST(STORE(stx, %o4, %o0+8+24))</span>
<span class="p_add">+	EX_LD(LOAD(ldx, %o1+32, %o4))</span>
<span class="p_add">+	EX_ST(STORE(stx, %o4, %o0+8+32))</span>
<span class="p_add">+	EX_LD(LOAD(ldx, %o1+40, %o4))</span>
<span class="p_add">+	EX_ST(STORE(stx, %o4, %o0+8+40))</span>
<span class="p_add">+	EX_LD(LOAD(ldx, %o1+48, %o4))</span>
<span class="p_add">+	add	%o1, 64, %o1</span>
<span class="p_add">+	EX_ST(STORE(stx, %o4, %o0+8+48))</span>
<span class="p_add">+	add	%o0, 64, %o0</span>
<span class="p_add">+	EX_LD(LOAD(ldx, %o1-8, %o4))</span>
<span class="p_add">+	bgu	%ncc,.Lalign_loop_fin</span>
<span class="p_add">+	 EX_ST(STORE(stx, %o4, %o0))</span>
<span class="p_add">+</span>
<span class="p_add">+.Lalign_done:</span>
<span class="p_add">+	add	%o0, 8, %o0		! restore %o0 from ASI alignment</span>
<span class="p_add">+	membar	#StoreStore</span>
<span class="p_add">+	sub	%o2, 63, %o2		! adjust length to allow cc test</span>
<span class="p_add">+	ba	.Lmedl63		! in .Lmedl63</span>
<span class="p_add">+	 nop</span>
<span class="p_add">+</span>
<span class="p_add">+	.align 16</span>
<span class="p_add">+	! Dst is on 8 byte boundary; src is not; remaining count &gt; SMALL_MAX</span>
<span class="p_add">+.Lunalignsetup:</span>
<span class="p_add">+.Lunalignrejoin:</span>
<span class="p_add">+	mov	%g1, %o3	! save %g1 as VISEntryHalf clobbers it</span>
<span class="p_add">+#ifdef NON_USER_COPY</span>
<span class="p_add">+	VISEntryHalfFast(.Lmedium_vis_entry_fail_cp)</span>
<span class="p_add">+#else</span>
<span class="p_add">+	VISEntryHalf</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	mov	%o3, %g1	! restore %g1</span>
<span class="p_add">+</span>
<span class="p_add">+	set MED_UMAX, %o3</span>
<span class="p_add">+	cmp %o2, %o3 		! check for.Lmedium unaligned limit</span>
<span class="p_add">+	bge,pt	%ncc,.Lunalign_large</span>
<span class="p_add">+	 prefetch [%o1 + (4 * BLOCK_SIZE)], 20</span>
<span class="p_add">+	andn	%o2, 0x3f, %o5		! %o5 is multiple of block size</span>
<span class="p_add">+	and	%o2, 0x3f, %o2		! residue bytes in %o2</span>
<span class="p_add">+	cmp	%o2, 8			! Insure we do not load beyond</span>
<span class="p_add">+	bgt	.Lunalign_adjust	! end of source buffer</span>
<span class="p_add">+	 andn	%o1, 0x7, %o4		! %o4 has long word aligned src address</span>
<span class="p_add">+	add	%o2, 64, %o2		! adjust to leave loop</span>
<span class="p_add">+	sub	%o5, 64, %o5		! early if necessary</span>
<span class="p_add">+.Lunalign_adjust:</span>
<span class="p_add">+	alignaddr %o1, %g0, %g0		! generate %gsr</span>
<span class="p_add">+	add	%o1, %o5, %o1		! advance %o1 to after blocks</span>
<span class="p_add">+	EX_LD_FP(LOAD(ldd, %o4, %f0))</span>
<span class="p_add">+.Lunalign_loop:</span>
<span class="p_add">+	EX_LD_FP(LOAD(ldd, %o4+8, %f2))</span>
<span class="p_add">+	faligndata %f0, %f2, %f16</span>
<span class="p_add">+	EX_LD_FP(LOAD(ldd, %o4+16, %f4))</span>
<span class="p_add">+	subcc	%o5, BLOCK_SIZE, %o5</span>
<span class="p_add">+	EX_ST_FP(STORE(std, %f16, %o0))</span>
<span class="p_add">+	faligndata %f2, %f4, %f18</span>
<span class="p_add">+	EX_LD_FP(LOAD(ldd, %o4+24, %f6))</span>
<span class="p_add">+	EX_ST_FP(STORE(std, %f18, %o0+8))</span>
<span class="p_add">+	faligndata %f4, %f6, %f20</span>
<span class="p_add">+	EX_LD_FP(LOAD(ldd, %o4+32, %f8))</span>
<span class="p_add">+	EX_ST_FP(STORE(std, %f20, %o0+16))</span>
<span class="p_add">+	faligndata %f6, %f8, %f22</span>
<span class="p_add">+	EX_LD_FP(LOAD(ldd, %o4+40, %f10))</span>
<span class="p_add">+	EX_ST_FP(STORE(std, %f22, %o0+24))</span>
<span class="p_add">+	faligndata %f8, %f10, %f24</span>
<span class="p_add">+	EX_LD_FP(LOAD(ldd, %o4+48, %f12))</span>
<span class="p_add">+	EX_ST_FP(STORE(std, %f24, %o0+32))</span>
<span class="p_add">+	faligndata %f10, %f12, %f26</span>
<span class="p_add">+	EX_LD_FP(LOAD(ldd, %o4+56, %f14))</span>
<span class="p_add">+	add	%o4, BLOCK_SIZE, %o4</span>
<span class="p_add">+	EX_ST_FP(STORE(std, %f26, %o0+40))</span>
<span class="p_add">+	faligndata %f12, %f14, %f28</span>
<span class="p_add">+	EX_LD_FP(LOAD(ldd, %o4, %f0))</span>
<span class="p_add">+	EX_ST_FP(STORE(std, %f28, %o0+48))</span>
<span class="p_add">+	faligndata %f14, %f0, %f30</span>
<span class="p_add">+	EX_ST_FP(STORE(std, %f30, %o0+56))</span>
<span class="p_add">+	add	%o0, BLOCK_SIZE, %o0</span>
<span class="p_add">+	bgu,pt	%ncc, .Lunalign_loop</span>
<span class="p_add">+	 prefetch [%o4 + (5 * BLOCK_SIZE)], 20</span>
<span class="p_add">+	ba	.Lunalign_done</span>
<span class="p_add">+	 nop</span>
<span class="p_add">+</span>
<span class="p_add">+.Lunalign_large:</span>
<span class="p_add">+	andcc	%o0, 0x3f, %o3		! is dst 64-byte block aligned?</span>
<span class="p_add">+	bz	%ncc, .Lunalignsrc</span>
<span class="p_add">+	 sub	%o3, 64, %o3		! %o3 will be multiple of 8</span>
<span class="p_add">+	neg	%o3			! bytes until dest is 64 byte aligned</span>
<span class="p_add">+	sub	%o2, %o3, %o2		! update cnt with bytes to be moved</span>
<span class="p_add">+	! Move bytes according to source alignment</span>
<span class="p_add">+	andcc	%o1, 0x1, %o5</span>
<span class="p_add">+	bnz	%ncc, .Lunalignbyte	! check for byte alignment</span>
<span class="p_add">+	 nop</span>
<span class="p_add">+	andcc	%o1, 2, %o5		! check for half word alignment</span>
<span class="p_add">+	bnz	%ncc, .Lunalignhalf</span>
<span class="p_add">+	 nop</span>
<span class="p_add">+	! Src is word aligned</span>
<span class="p_add">+.Lunalignword:</span>
<span class="p_add">+	EX_LD_FP(LOAD(ld, %o1, %o4))	! load 4 bytes</span>
<span class="p_add">+	add	%o1, 8, %o1		! increase src ptr by 8</span>
<span class="p_add">+	EX_ST_FP(STORE(stw, %o4, %o0))	! and store 4 bytes</span>
<span class="p_add">+	subcc	%o3, 8, %o3		! decrease count by 8</span>
<span class="p_add">+	EX_LD_FP(LOAD(ld, %o1-4, %o4))	! load 4 bytes</span>
<span class="p_add">+	add	%o0, 8, %o0		! increase dst ptr by 8</span>
<span class="p_add">+	bnz	%ncc, .Lunalignword</span>
<span class="p_add">+	 EX_ST_FP(STORE(stw, %o4, %o0-4))! and store 4 bytes</span>
<span class="p_add">+	ba	.Lunalignsrc</span>
<span class="p_add">+	 nop</span>
<span class="p_add">+</span>
<span class="p_add">+	! Src is half-word aligned</span>
<span class="p_add">+.Lunalignhalf:</span>
<span class="p_add">+	EX_LD_FP(LOAD(lduh, %o1, %o4))	! load 2 bytes</span>
<span class="p_add">+	sllx	%o4, 32, %o5		! shift left</span>
<span class="p_add">+	EX_LD_FP(LOAD(lduw, %o1+2, %o4))</span>
<span class="p_add">+	or	%o4, %o5, %o5</span>
<span class="p_add">+	sllx	%o5, 16, %o5</span>
<span class="p_add">+	EX_LD_FP(LOAD(lduh, %o1+6, %o4))</span>
<span class="p_add">+	or	%o4, %o5, %o5</span>
<span class="p_add">+	EX_ST_FP(STORE(stx, %o5, %o0))</span>
<span class="p_add">+	add	%o1, 8, %o1</span>
<span class="p_add">+	subcc	%o3, 8, %o3</span>
<span class="p_add">+	bnz	%ncc, .Lunalignhalf</span>
<span class="p_add">+	 add	%o0, 8, %o0</span>
<span class="p_add">+	ba	.Lunalignsrc</span>
<span class="p_add">+	 nop</span>
<span class="p_add">+</span>
<span class="p_add">+	! Src is Byte aligned</span>
<span class="p_add">+.Lunalignbyte:</span>
<span class="p_add">+	sub	%o0, %o1, %o0		! share pointer advance</span>
<span class="p_add">+.Lunalignbyte_loop:</span>
<span class="p_add">+	EX_LD_FP(LOAD(ldub, %o1, %o4))</span>
<span class="p_add">+	sllx	%o4, 56, %o5</span>
<span class="p_add">+	EX_LD_FP(LOAD(lduh, %o1+1, %o4))</span>
<span class="p_add">+	sllx	%o4, 40, %o4</span>
<span class="p_add">+	or	%o4, %o5, %o5</span>
<span class="p_add">+	EX_LD_FP(LOAD(lduh, %o1+3, %o4))</span>
<span class="p_add">+	sllx	%o4, 24, %o4</span>
<span class="p_add">+	or	%o4, %o5, %o5</span>
<span class="p_add">+	EX_LD_FP(LOAD(lduh, %o1+5, %o4))</span>
<span class="p_add">+	sllx	%o4,  8, %o4</span>
<span class="p_add">+	or	%o4, %o5, %o5</span>
<span class="p_add">+	EX_LD_FP(LOAD(ldub, %o1+7, %o4))</span>
<span class="p_add">+	or	%o4, %o5, %o5</span>
<span class="p_add">+	add	%o0, %o1, %o0</span>
<span class="p_add">+	EX_ST_FP(STORE(stx, %o5, %o0))</span>
<span class="p_add">+	sub	%o0, %o1, %o0</span>
<span class="p_add">+	subcc	%o3, 8, %o3</span>
<span class="p_add">+	bnz	%ncc, .Lunalignbyte_loop</span>
<span class="p_add">+	 add	%o1, 8, %o1</span>
<span class="p_add">+	add	%o0,%o1, %o0 		! restore pointer</span>
<span class="p_add">+</span>
<span class="p_add">+	! Destination is now block (64 byte aligned)</span>
<span class="p_add">+.Lunalignsrc:</span>
<span class="p_add">+	andn	%o2, 0x3f, %o5		! %o5 is multiple of block size</span>
<span class="p_add">+	and	%o2, 0x3f, %o2		! residue bytes in %o2</span>
<span class="p_add">+	add	%o2, 64, %o2		! Insure we do not load beyond</span>
<span class="p_add">+	sub	%o5, 64, %o5		! end of source buffer</span>
<span class="p_add">+</span>
<span class="p_add">+	andn	%o1, 0x7, %o4		! %o4 has long word aligned src address</span>
<span class="p_add">+	alignaddr %o1, %g0, %g0		! generate %gsr</span>
<span class="p_add">+	add	%o1, %o5, %o1		! advance %o1 to after blocks</span>
<span class="p_add">+</span>
<span class="p_add">+	EX_LD_FP(LOAD(ldd, %o4, %f14))</span>
<span class="p_add">+	add	%o4, 8, %o4</span>
<span class="p_add">+.Lunalign_sloop:</span>
<span class="p_add">+	EX_LD_FP(LOAD(ldd, %o4, %f16))</span>
<span class="p_add">+	faligndata %f14, %f16, %f0</span>
<span class="p_add">+	EX_LD_FP(LOAD(ldd, %o4+8, %f18))</span>
<span class="p_add">+	faligndata %f16, %f18, %f2</span>
<span class="p_add">+	EX_LD_FP(LOAD(ldd, %o4+16, %f20))</span>
<span class="p_add">+	faligndata %f18, %f20, %f4</span>
<span class="p_add">+	EX_ST_FP(STORE(std, %f0, %o0))</span>
<span class="p_add">+	subcc	%o5, 64, %o5</span>
<span class="p_add">+	EX_LD_FP(LOAD(ldd, %o4+24, %f22))</span>
<span class="p_add">+	faligndata %f20, %f22, %f6</span>
<span class="p_add">+	EX_ST_FP(STORE(std, %f2, %o0+8))</span>
<span class="p_add">+	EX_LD_FP(LOAD(ldd, %o4+32, %f24))</span>
<span class="p_add">+	faligndata %f22, %f24, %f8</span>
<span class="p_add">+	EX_ST_FP(STORE(std, %f4, %o0+16))</span>
<span class="p_add">+	EX_LD_FP(LOAD(ldd, %o4+40, %f26))</span>
<span class="p_add">+	faligndata %f24, %f26, %f10</span>
<span class="p_add">+	EX_ST_FP(STORE(std, %f6, %o0+24))</span>
<span class="p_add">+	EX_LD_FP(LOAD(ldd, %o4+48, %f28))</span>
<span class="p_add">+	faligndata %f26, %f28, %f12</span>
<span class="p_add">+	EX_ST_FP(STORE(std, %f8, %o0+32))</span>
<span class="p_add">+	add	%o4, 64, %o4</span>
<span class="p_add">+	EX_LD_FP(LOAD(ldd, %o4-8, %f30))</span>
<span class="p_add">+	faligndata %f28, %f30, %f14</span>
<span class="p_add">+	EX_ST_FP(STORE(std, %f10, %o0+40))</span>
<span class="p_add">+	EX_ST_FP(STORE(std, %f12, %o0+48))</span>
<span class="p_add">+	add	%o0, 64, %o0</span>
<span class="p_add">+	EX_ST_FP(STORE(std, %f14, %o0-8))</span>
<span class="p_add">+	fsrc2	%f30, %f14</span>
<span class="p_add">+	bgu,pt	%ncc, .Lunalign_sloop</span>
<span class="p_add">+	 prefetch [%o4 + (8 * BLOCK_SIZE)], 20</span>
<span class="p_add">+</span>
<span class="p_add">+.Lunalign_done:</span>
<span class="p_add">+	! Handle trailing bytes, 64 to 127</span>
<span class="p_add">+	! Dest long word aligned, Src not long word aligned</span>
<span class="p_add">+	cmp	%o2, 15</span>
<span class="p_add">+	bleu	%ncc, .Lunalign_short</span>
<span class="p_add">+</span>
<span class="p_add">+	 andn	%o2, 0x7, %o5		! %o5 is multiple of 8</span>
<span class="p_add">+	and	%o2, 0x7, %o2		! residue bytes in %o2</span>
<span class="p_add">+	add	%o2, 8, %o2</span>
<span class="p_add">+	sub	%o5, 8, %o5		! insure we do not load past end of src</span>
<span class="p_add">+	andn	%o1, 0x7, %o4		! %o4 has long word aligned src address</span>
<span class="p_add">+	add	%o1, %o5, %o1		! advance %o1 to after multiple of 8</span>
<span class="p_add">+	EX_LD_FP(LOAD(ldd, %o4, %f0))	! fetch partial word</span>
<span class="p_add">+.Lunalign_by8:</span>
<span class="p_add">+	EX_LD_FP(LOAD(ldd, %o4+8, %f2))</span>
<span class="p_add">+	add	%o4, 8, %o4</span>
<span class="p_add">+	faligndata %f0, %f2, %f16</span>
<span class="p_add">+	subcc	%o5, 8, %o5</span>
<span class="p_add">+	EX_ST_FP(STORE(std, %f16, %o0))</span>
<span class="p_add">+	fsrc2	%f2, %f0</span>
<span class="p_add">+	bgu,pt	%ncc, .Lunalign_by8</span>
<span class="p_add">+	 add	%o0, 8, %o0</span>
<span class="p_add">+</span>
<span class="p_add">+.Lunalign_short:</span>
<span class="p_add">+#ifdef NON_USER_COPY</span>
<span class="p_add">+	VISExitHalfFast</span>
<span class="p_add">+#else</span>
<span class="p_add">+	VISExitHalf</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	ba	.Lsmallrest</span>
<span class="p_add">+	 nop</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * This is a special case of nested memcpy. This can happen when kernel</span>
<span class="p_add">+ * calls unaligned memcpy back to back without saving FP registers. We need</span>
<span class="p_add">+ * traps(context switch) to save/restore FP registers. If the kernel calls</span>
<span class="p_add">+ * memcpy without this trap sequence we will hit FP corruption. Let&#39;s use</span>
<span class="p_add">+ * the normal integer load/store method in this case.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef NON_USER_COPY</span>
<span class="p_add">+.Lmedium_vis_entry_fail_cp:</span>
<span class="p_add">+	or	%o0, %o1, %g2</span>
<span class="p_add">+#endif</span>
<span class="p_add">+.Lmedium_cp:</span>
<span class="p_add">+	LOAD(prefetch, %o1 + 0x40, #n_reads_strong)</span>
<span class="p_add">+	andcc	%g2, 0x7, %g0</span>
<span class="p_add">+	bne,pn	%ncc, .Lmedium_unaligned_cp</span>
<span class="p_add">+	 nop</span>
<span class="p_add">+</span>
<span class="p_add">+.Lmedium_noprefetch_cp:</span>
<span class="p_add">+	andncc	%o2, 0x20 - 1, %o5</span>
<span class="p_add">+	be,pn	%ncc, 2f</span>
<span class="p_add">+	 sub	%o2, %o5, %o2</span>
<span class="p_add">+1:	EX_LD(LOAD(ldx, %o1 + 0x00, %o3))</span>
<span class="p_add">+	EX_LD(LOAD(ldx, %o1 + 0x08, %g2))</span>
<span class="p_add">+	EX_LD(LOAD(ldx, %o1 + 0x10, %g7))</span>
<span class="p_add">+	EX_LD(LOAD(ldx, %o1 + 0x18, %o4))</span>
<span class="p_add">+	add	%o1, 0x20, %o1</span>
<span class="p_add">+	subcc	%o5, 0x20, %o5</span>
<span class="p_add">+	EX_ST(STORE(stx, %o3, %o0 + 0x00))</span>
<span class="p_add">+	EX_ST(STORE(stx, %g2, %o0 + 0x08))</span>
<span class="p_add">+	EX_ST(STORE(stx, %g7, %o0 + 0x10))</span>
<span class="p_add">+	EX_ST(STORE(stx, %o4, %o0 + 0x18))</span>
<span class="p_add">+	bne,pt	%ncc, 1b</span>
<span class="p_add">+	 add	%o0, 0x20, %o0</span>
<span class="p_add">+2:	andcc	%o2, 0x18, %o5</span>
<span class="p_add">+	be,pt	%ncc, 3f</span>
<span class="p_add">+	 sub	%o2, %o5, %o2</span>
<span class="p_add">+1:	EX_LD(LOAD(ldx, %o1 + 0x00, %o3))</span>
<span class="p_add">+	add	%o1, 0x08, %o1</span>
<span class="p_add">+	add	%o0, 0x08, %o0</span>
<span class="p_add">+	subcc	%o5, 0x08, %o5</span>
<span class="p_add">+	bne,pt	%ncc, 1b</span>
<span class="p_add">+	 EX_ST(STORE(stx, %o3, %o0 - 0x08))</span>
<span class="p_add">+3:	brz,pt	%o2, .Lexit_cp</span>
<span class="p_add">+	 cmp	%o2, 0x04</span>
<span class="p_add">+	bl,pn	%ncc, .Ltiny_cp</span>
<span class="p_add">+	 nop</span>
<span class="p_add">+	EX_LD(LOAD(lduw, %o1 + 0x00, %o3))</span>
<span class="p_add">+	add	%o1, 0x04, %o1</span>
<span class="p_add">+	add	%o0, 0x04, %o0</span>
<span class="p_add">+	subcc	%o2, 0x04, %o2</span>
<span class="p_add">+	bne,pn	%ncc, .Ltiny_cp</span>
<span class="p_add">+	 EX_ST(STORE(stw, %o3, %o0 - 0x04))</span>
<span class="p_add">+	ba,a,pt	%ncc, .Lexit_cp</span>
<span class="p_add">+</span>
<span class="p_add">+.Lmedium_unaligned_cp:</span>
<span class="p_add">+	/* First get dest 8 byte aligned.  */</span>
<span class="p_add">+	sub	%g0, %o0, %o3</span>
<span class="p_add">+	and	%o3, 0x7, %o3</span>
<span class="p_add">+	brz,pt	%o3, 2f</span>
<span class="p_add">+	 sub	%o2, %o3, %o2</span>
<span class="p_add">+</span>
<span class="p_add">+1:	EX_LD(LOAD(ldub, %o1 + 0x00, %g2))</span>
<span class="p_add">+	add	%o1, 1, %o1</span>
<span class="p_add">+	subcc	%o3, 1, %o3</span>
<span class="p_add">+	add	%o0, 1, %o0</span>
<span class="p_add">+	bne,pt	%ncc, 1b</span>
<span class="p_add">+	 EX_ST(STORE(stb, %g2, %o0 - 0x01))</span>
<span class="p_add">+2:</span>
<span class="p_add">+	and	%o1, 0x7, %o3</span>
<span class="p_add">+	brz,pn	%o3, .Lmedium_noprefetch_cp</span>
<span class="p_add">+	 sll	%o3, 3, %o3</span>
<span class="p_add">+	mov	64, %g2</span>
<span class="p_add">+	sub	%g2, %o3, %g2</span>
<span class="p_add">+	andn	%o1, 0x7, %o1</span>
<span class="p_add">+	EX_LD(LOAD(ldx, %o1 + 0x00, %o4))</span>
<span class="p_add">+	sllx	%o4, %o3, %o4</span>
<span class="p_add">+	andn	%o2, 0x08 - 1, %o5</span>
<span class="p_add">+	sub	%o2, %o5, %o2</span>
<span class="p_add">+</span>
<span class="p_add">+1:	EX_LD(LOAD(ldx, %o1 + 0x08, %g3))</span>
<span class="p_add">+	add	%o1, 0x08, %o1</span>
<span class="p_add">+	subcc	%o5, 0x08, %o5</span>
<span class="p_add">+	srlx	%g3, %g2, %g7</span>
<span class="p_add">+	or	%g7, %o4, %g7</span>
<span class="p_add">+	EX_ST(STORE(stx, %g7, %o0 + 0x00))</span>
<span class="p_add">+	add	%o0, 0x08, %o0</span>
<span class="p_add">+	bne,pt	%ncc, 1b</span>
<span class="p_add">+	 sllx	%g3, %o3, %o4</span>
<span class="p_add">+	srl	%o3, 3, %o3</span>
<span class="p_add">+	add	%o1, %o3, %o1</span>
<span class="p_add">+	brz,pn	%o2, .Lexit_cp</span>
<span class="p_add">+	 nop</span>
<span class="p_add">+	ba,pt	%ncc, .Lsmall_unaligned_cp</span>
<span class="p_add">+</span>
<span class="p_add">+.Ltiny_cp:</span>
<span class="p_add">+	EX_LD(LOAD(ldub, %o1 + 0x00, %o3))</span>
<span class="p_add">+	subcc	%o2, 1, %o2</span>
<span class="p_add">+	be,pn	%ncc, .Lexit_cp</span>
<span class="p_add">+	 EX_ST(STORE(stb, %o3, %o0 + 0x00))</span>
<span class="p_add">+	EX_LD(LOAD(ldub, %o1 + 0x01, %o3))</span>
<span class="p_add">+	subcc	%o2, 1, %o2</span>
<span class="p_add">+	be,pn	%ncc, .Lexit_cp</span>
<span class="p_add">+	 EX_ST(STORE(stb, %o3, %o0 + 0x01))</span>
<span class="p_add">+	EX_LD(LOAD(ldub, %o1 + 0x02, %o3))</span>
<span class="p_add">+	ba,pt	%ncc, .Lexit_cp</span>
<span class="p_add">+	 EX_ST(STORE(stb, %o3, %o0 + 0x02))</span>
<span class="p_add">+</span>
<span class="p_add">+.Lsmall_cp:</span>
<span class="p_add">+	andcc	%g2, 0x3, %g0</span>
<span class="p_add">+	bne,pn	%ncc, .Lsmall_unaligned_cp</span>
<span class="p_add">+	 andn	%o2, 0x4 - 1, %o5</span>
<span class="p_add">+	sub	%o2, %o5, %o2</span>
<span class="p_add">+1:</span>
<span class="p_add">+	EX_LD(LOAD(lduw, %o1 + 0x00, %o3))</span>
<span class="p_add">+	add	%o1, 0x04, %o1</span>
<span class="p_add">+	subcc	%o5, 0x04, %o5</span>
<span class="p_add">+	add	%o0, 0x04, %o0</span>
<span class="p_add">+	bne,pt	%ncc, 1b</span>
<span class="p_add">+	 EX_ST(STORE(stw, %o3, %o0 - 0x04))</span>
<span class="p_add">+	brz,pt	%o2, .Lexit_cp</span>
<span class="p_add">+	 nop</span>
<span class="p_add">+	ba,a,pt	%ncc, .Ltiny_cp</span>
<span class="p_add">+</span>
<span class="p_add">+.Lsmall_unaligned_cp:</span>
<span class="p_add">+1:	EX_LD(LOAD(ldub, %o1 + 0x00, %o3))</span>
<span class="p_add">+	add	%o1, 1, %o1</span>
<span class="p_add">+	add	%o0, 1, %o0</span>
<span class="p_add">+	subcc	%o2, 1, %o2</span>
<span class="p_add">+	bne,pt	%ncc, 1b</span>
<span class="p_add">+	 EX_ST(STORE(stb, %o3, %o0 - 0x01))</span>
<span class="p_add">+	ba,a,pt	%ncc, .Lexit_cp</span>
<span class="p_add">+</span>
<span class="p_add">+.Lsmallrest:</span>
<span class="p_add">+	tst	%o2</span>
<span class="p_add">+	bz,pt	%ncc, .Lsmallx</span>
<span class="p_add">+	 cmp	%o2, 4</span>
<span class="p_add">+	blt,pn	%ncc, .Lsmallleft3</span>
<span class="p_add">+	 nop</span>
<span class="p_add">+	sub	%o2, 3, %o2</span>
<span class="p_add">+.Lsmallnotalign4:</span>
<span class="p_add">+	EX_LD(LOAD(ldub, %o1, %o3))! read byte</span>
<span class="p_add">+	subcc	%o2, 4, %o2		! reduce count by 4</span>
<span class="p_add">+	EX_ST(STORE(stb, %o3, %o0))	! write byte</span>
<span class="p_add">+	EX_LD(LOAD(ldub, %o1+1, %o3))! repeat for total of 4 bytes</span>
<span class="p_add">+	add	%o1, 4, %o1		! advance SRC by 4</span>
<span class="p_add">+	EX_ST(STORE(stb, %o3, %o0+1))</span>
<span class="p_add">+	EX_LD(LOAD(ldub, %o1-2, %o3))</span>
<span class="p_add">+	add	%o0, 4, %o0		! advance DST by 4</span>
<span class="p_add">+	EX_ST(STORE(stb, %o3, %o0-2))</span>
<span class="p_add">+	EX_LD(LOAD(ldub, %o1-1, %o3))</span>
<span class="p_add">+	bgu,pt	%ncc, .Lsmallnotalign4	! loop til 3 or fewer bytes remain</span>
<span class="p_add">+	EX_ST(STORE(stb, %o3, %o0-1))</span>
<span class="p_add">+	addcc	%o2, 3, %o2		! restore count</span>
<span class="p_add">+	bz,pt	%ncc, .Lsmallx</span>
<span class="p_add">+.Lsmallleft3:				! 1, 2, or 3 bytes remain</span>
<span class="p_add">+	subcc	%o2, 1, %o2</span>
<span class="p_add">+	EX_LD(LOAD(ldub, %o1, %o3))	! load one byte</span>
<span class="p_add">+	bz,pt	%ncc, .Lsmallx</span>
<span class="p_add">+	 EX_ST(STORE(stb, %o3, %o0))	! store one byte</span>
<span class="p_add">+	EX_LD(LOAD(ldub, %o1+1, %o3))	! load second byte</span>
<span class="p_add">+	subcc	%o2, 1, %o2</span>
<span class="p_add">+	bz,pt	%ncc, .Lsmallx</span>
<span class="p_add">+	 EX_ST(STORE(stb, %o3, %o0+1))! store second byte</span>
<span class="p_add">+	EX_LD(LOAD(ldub, %o1+2, %o3))	! load third byte</span>
<span class="p_add">+	EX_ST(STORE(stb, %o3, %o0+2))	! store third byte</span>
<span class="p_add">+.Lsmallx:</span>
<span class="p_add">+	retl</span>
<span class="p_add">+	 mov	EX_RETVAL(%g1), %o0</span>
<span class="p_add">+.Lsmallfin:</span>
<span class="p_add">+	tst	%o2</span>
<span class="p_add">+	bnz,pn	%ncc, .Lsmallleft3</span>
<span class="p_add">+	 nop</span>
<span class="p_add">+	retl</span>
<span class="p_add">+	 mov	EX_RETVAL(%g1), %o0	! restore %o0</span>
<span class="p_add">+.Lexit_cp:</span>
<span class="p_add">+	retl</span>
<span class="p_add">+	 mov	EX_RETVAL(%g1), %o0</span>
<span class="p_add">+	.size  FUNC_NAME, .-FUNC_NAME</span>
<span class="p_header">diff --git a/arch/sparc/lib/M7memset.S b/arch/sparc/lib/M7memset.S</span>
new file mode 100644
<span class="p_header">index 0000000..ea88424</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/sparc/lib/M7memset.S</span>
<span class="p_chunk">@@ -0,0 +1,352 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * M7memset.S: SPARC M7 optimized memset.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Copyright (c) 2016, Oracle and/or its affiliates.  All rights reserved.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * M7memset.S: M7 optimized memset.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * char *memset(sp, c, n)</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Set an array of n chars starting at sp to the character c.</span>
<span class="p_add">+ * Return sp.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Fast assembler language version of the following C-program for memset</span>
<span class="p_add">+ * which represents the `standard&#39; for the C-library.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *	void *</span>
<span class="p_add">+ *	memset(void *sp1, int c, size_t n)</span>
<span class="p_add">+ *	{</span>
<span class="p_add">+ *	    if (n != 0) {</span>
<span class="p_add">+ *		char *sp = sp1;</span>
<span class="p_add">+ *		do {</span>
<span class="p_add">+ *		    *sp++ = (char)c;</span>
<span class="p_add">+ *		} while (--n != 0);</span>
<span class="p_add">+ *	    }</span>
<span class="p_add">+ *	    return (sp1);</span>
<span class="p_add">+ *	}</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * The algorithm is as follows :</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *	For small 6 or fewer bytes stores, bytes will be stored.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *	For less than 32 bytes stores, align the address on 4 byte boundary.</span>
<span class="p_add">+ *	Then store as many 4-byte chunks, followed by trailing bytes.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *	For sizes greater than 32 bytes, align the address on 8 byte boundary.</span>
<span class="p_add">+ *	if (count &gt;= 64) {</span>
<span class="p_add">+ *      	store 8-bytes chunks to align the address on 64 byte boundary</span>
<span class="p_add">+ *		if (value to be set is zero &amp;&amp; count &gt;= MIN_ZERO) {</span>
<span class="p_add">+ *              	Using BIS stores, set the first long word of each</span>
<span class="p_add">+ *			64-byte cache line to zero which will also clear the</span>
<span class="p_add">+ *			other seven long words of the cache line.</span>
<span class="p_add">+ *       	}</span>
<span class="p_add">+ *       	else if (count &gt;= MIN_LOOP) {</span>
<span class="p_add">+ *       		Using BIS stores, set the first long word of each of</span>
<span class="p_add">+ *              	ST_CHUNK cache lines (64 bytes each) before the main</span>
<span class="p_add">+ *			loop is entered.</span>
<span class="p_add">+ *              	In the main loop, continue pre-setting the first long</span>
<span class="p_add">+ *              	word of each cache line ST_CHUNK lines in advance while</span>
<span class="p_add">+ *              	setting the other seven long words (56 bytes) of each</span>
<span class="p_add">+ * 			cache line until fewer than ST_CHUNK*64 bytes remain.</span>
<span class="p_add">+ *			Then set the remaining seven long words of each cache</span>
<span class="p_add">+ * 			line that has already had its first long word set.</span>
<span class="p_add">+ *       	}</span>
<span class="p_add">+ *       	store remaining data in 64-byte chunks until less than</span>
<span class="p_add">+ *       	64 bytes remain.</span>
<span class="p_add">+ *       }</span>
<span class="p_add">+ *       Store as many 8-byte chunks, followed by trailing bytes.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * BIS = Block Init Store</span>
<span class="p_add">+ *   Doing the advance store of the first element of the cache line</span>
<span class="p_add">+ *   initiates the displacement of a cache line while only using a single</span>
<span class="p_add">+ *   instruction in the pipeline. That avoids various pipeline delays,</span>
<span class="p_add">+ *   such as filling the miss buffer. The performance effect is</span>
<span class="p_add">+ *   similar to prefetching for normal stores.</span>
<span class="p_add">+ *   The special case for zero fills runs faster and uses fewer instruction</span>
<span class="p_add">+ *   cycles than the normal memset loop.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * We only use BIS for memset of greater than MIN_LOOP bytes because a sequence</span>
<span class="p_add">+ * BIS stores must be followed by a membar #StoreStore. The benefit of</span>
<span class="p_add">+ * the BIS store must be balanced against the cost of the membar operation.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * ASI_STBI_P marks the cache line as &quot;least recently used&quot;</span>
<span class="p_add">+ * which means if many threads are active, it has a high chance</span>
<span class="p_add">+ * of being pushed out of the cache between the first initializing</span>
<span class="p_add">+ * store and the final stores.</span>
<span class="p_add">+ * Thus, we use ASI_STBIMRU_P which marks the cache line as</span>
<span class="p_add">+ * &quot;most recently used&quot; for all but the last store to the cache line.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/asi.h&gt;</span>
<span class="p_add">+#include &lt;asm/page.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#define ASI_STBI_P      ASI_BLK_INIT_QUAD_LDD_P</span>
<span class="p_add">+#define ASI_STBIMRU_P   ASI_ST_BLKINIT_MRU_P</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+#define ST_CHUNK        24   /* multiple of 4 due to loop unrolling */</span>
<span class="p_add">+#define MIN_LOOP        16320</span>
<span class="p_add">+#define MIN_ZERO        512</span>
<span class="p_add">+</span>
<span class="p_add">+	.section	&quot;.text&quot;</span>
<span class="p_add">+	.align		32</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Define clear_page(dest) as memset(dest, 0, PAGE_SIZE)</span>
<span class="p_add">+ * (can create a more optimized version later.)</span>
<span class="p_add">+ */</span>
<span class="p_add">+	.globl		M7clear_page</span>
<span class="p_add">+	.globl		M7clear_user_page</span>
<span class="p_add">+M7clear_page:		/* clear_page(dest) */</span>
<span class="p_add">+M7clear_user_page:</span>
<span class="p_add">+	set	PAGE_SIZE, %o1</span>
<span class="p_add">+	/* fall through into bzero code */</span>
<span class="p_add">+</span>
<span class="p_add">+	.size		M7clear_page,.-M7clear_page</span>
<span class="p_add">+	.size		M7clear_user_page,.-M7clear_user_page</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Define bzero(dest, n) as memset(dest, 0, n)</span>
<span class="p_add">+ * (can create a more optimized version later.)</span>
<span class="p_add">+ */</span>
<span class="p_add">+	.globl		M7bzero</span>
<span class="p_add">+M7bzero:		/* bzero(dest, size) */</span>
<span class="p_add">+	mov	%o1, %o2</span>
<span class="p_add">+	mov	0, %o1</span>
<span class="p_add">+	/* fall through into memset code */</span>
<span class="p_add">+</span>
<span class="p_add">+	.size		M7bzero,.-M7bzero</span>
<span class="p_add">+</span>
<span class="p_add">+	.global		M7memset</span>
<span class="p_add">+	.type		M7memset, #function</span>
<span class="p_add">+	.register	%g3, #scratch</span>
<span class="p_add">+M7memset:</span>
<span class="p_add">+	mov     %o0, %o5                ! copy sp1 before using it</span>
<span class="p_add">+	cmp     %o2, 7                  ! if small counts, just write bytes</span>
<span class="p_add">+	bleu,pn %ncc, .wrchar</span>
<span class="p_add">+	 and     %o1, 0xff, %o1          ! o1 is (char)c</span>
<span class="p_add">+</span>
<span class="p_add">+	sll     %o1, 8, %o3</span>
<span class="p_add">+	or      %o1, %o3, %o1           ! now o1 has 2 bytes of c</span>
<span class="p_add">+	sll     %o1, 16, %o3</span>
<span class="p_add">+	cmp     %o2, 32</span>
<span class="p_add">+	blu,pn  %ncc, .wdalign</span>
<span class="p_add">+	 or      %o1, %o3, %o1           ! now o1 has 4 bytes of c</span>
<span class="p_add">+</span>
<span class="p_add">+	sllx    %o1, 32, %o3</span>
<span class="p_add">+	or      %o1, %o3, %o1           ! now o1 has 8 bytes of c</span>
<span class="p_add">+</span>
<span class="p_add">+.dbalign:</span>
<span class="p_add">+	andcc   %o5, 7, %o3             ! is sp1 aligned on a 8 byte bound?</span>
<span class="p_add">+	bz,pt   %ncc, .blkalign         ! already long word aligned</span>
<span class="p_add">+	 sub     %o3, 8, %o3             ! -(bytes till long word aligned)</span>
<span class="p_add">+</span>
<span class="p_add">+	add     %o2, %o3, %o2           ! update o2 with new count</span>
<span class="p_add">+	! Set -(%o3) bytes till sp1 long word aligned</span>
<span class="p_add">+1:	stb     %o1, [%o5]              ! there is at least 1 byte to set</span>
<span class="p_add">+	inccc   %o3                     ! byte clearing loop</span>
<span class="p_add">+	bl,pt   %ncc, 1b</span>
<span class="p_add">+	 inc     %o5</span>
<span class="p_add">+</span>
<span class="p_add">+	! Now sp1 is long word aligned (sp1 is found in %o5)</span>
<span class="p_add">+.blkalign:</span>
<span class="p_add">+	cmp     %o2, 64                 ! check if there are 64 bytes to set</span>
<span class="p_add">+	blu,pn  %ncc, .wrshort</span>
<span class="p_add">+	 mov     %o2, %o3</span>
<span class="p_add">+</span>
<span class="p_add">+	andcc   %o5, 63, %o3            ! is sp1 block aligned?</span>
<span class="p_add">+	bz,pt   %ncc, .blkwr            ! now block aligned</span>
<span class="p_add">+	 sub     %o3, 64, %o3            ! o3 is -(bytes till block aligned)</span>
<span class="p_add">+	add     %o2, %o3, %o2           ! o2 is the remainder</span>
<span class="p_add">+</span>
<span class="p_add">+	! Store -(%o3) bytes till dst is block (64 byte) aligned.</span>
<span class="p_add">+	! Use long word stores.</span>
<span class="p_add">+	! Recall that dst is already long word aligned</span>
<span class="p_add">+1:</span>
<span class="p_add">+	addcc   %o3, 8, %o3</span>
<span class="p_add">+	stx     %o1, [%o5]</span>
<span class="p_add">+	bl,pt   %ncc, 1b</span>
<span class="p_add">+	 add     %o5, 8, %o5</span>
<span class="p_add">+</span>
<span class="p_add">+	! Now sp1 is block aligned</span>
<span class="p_add">+.blkwr:</span>
<span class="p_add">+	andn    %o2, 63, %o4            ! calculate size of blocks in bytes</span>
<span class="p_add">+	brz,pn  %o1, .wrzero            ! special case if c == 0</span>
<span class="p_add">+	 and     %o2, 63, %o3            ! %o3 = bytes left after blk stores.</span>
<span class="p_add">+</span>
<span class="p_add">+	set     MIN_LOOP, %g1</span>
<span class="p_add">+	cmp     %o4, %g1                ! check there are enough bytes to set</span>
<span class="p_add">+	blu,pn  %ncc, .short_set        ! to justify cost of membar</span>
<span class="p_add">+	                                ! must be &gt; pre-cleared lines</span>
<span class="p_add">+	 nop</span>
<span class="p_add">+</span>
<span class="p_add">+	! initial cache-clearing stores</span>
<span class="p_add">+	! get store pipeline moving</span>
<span class="p_add">+	rd	%asi, %g3		! save %asi to be restored later</span>
<span class="p_add">+	wr     %g0, ASI_STBIMRU_P, %asi</span>
<span class="p_add">+</span>
<span class="p_add">+	! Primary memset loop for large memsets</span>
<span class="p_add">+.wr_loop:</span>
<span class="p_add">+	sub     %o5, 8, %o5		! adjust %o5 for ASI store alignment</span>
<span class="p_add">+	mov     ST_CHUNK, %g1</span>
<span class="p_add">+.wr_loop_start:</span>
<span class="p_add">+	stxa    %o1, [%o5+8]%asi</span>
<span class="p_add">+	subcc   %g1, 4, %g1</span>
<span class="p_add">+	stxa    %o1, [%o5+8+64]%asi</span>
<span class="p_add">+	add     %o5, 256, %o5</span>
<span class="p_add">+	stxa    %o1, [%o5+8-128]%asi</span>
<span class="p_add">+	bgu     %ncc, .wr_loop_start</span>
<span class="p_add">+	 stxa    %o1, [%o5+8-64]%asi</span>
<span class="p_add">+</span>
<span class="p_add">+	sub     %o5, ST_CHUNK*64, %o5	! reset %o5</span>
<span class="p_add">+	mov     ST_CHUNK, %g1</span>
<span class="p_add">+</span>
<span class="p_add">+.wr_loop_rest:</span>
<span class="p_add">+	stxa    %o1, [%o5+8+8]%asi</span>
<span class="p_add">+	sub     %o4, 64, %o4</span>
<span class="p_add">+	stxa    %o1, [%o5+16+8]%asi</span>
<span class="p_add">+	subcc   %g1, 1, %g1</span>
<span class="p_add">+	stxa    %o1, [%o5+24+8]%asi</span>
<span class="p_add">+	stxa    %o1, [%o5+32+8]%asi</span>
<span class="p_add">+	stxa    %o1, [%o5+40+8]%asi</span>
<span class="p_add">+	add     %o5, 64, %o5</span>
<span class="p_add">+	stxa    %o1, [%o5-8]%asi</span>
<span class="p_add">+	bgu     %ncc, .wr_loop_rest</span>
<span class="p_add">+	 stxa    %o1, [%o5]ASI_STBI_P</span>
<span class="p_add">+</span>
<span class="p_add">+	! If more than ST_CHUNK*64 bytes remain to set, continue</span>
<span class="p_add">+	! setting the first long word of each cache line in advance</span>
<span class="p_add">+	! to keep the store pipeline moving.</span>
<span class="p_add">+</span>
<span class="p_add">+	cmp     %o4, ST_CHUNK*64</span>
<span class="p_add">+	bge,pt  %ncc, .wr_loop_start</span>
<span class="p_add">+	 mov     ST_CHUNK, %g1</span>
<span class="p_add">+</span>
<span class="p_add">+	brz,a,pn %o4, .asi_done</span>
<span class="p_add">+	 add     %o5, 8, %o5             ! restore %o5 offset</span>
<span class="p_add">+</span>
<span class="p_add">+.wr_loop_small:</span>
<span class="p_add">+	stxa    %o1, [%o5+8]%asi</span>
<span class="p_add">+	stxa    %o1, [%o5+8+8]%asi</span>
<span class="p_add">+	stxa    %o1, [%o5+16+8]%asi</span>
<span class="p_add">+	stxa    %o1, [%o5+24+8]%asi</span>
<span class="p_add">+	stxa    %o1, [%o5+32+8]%asi</span>
<span class="p_add">+	subcc   %o4, 64, %o4</span>
<span class="p_add">+	stxa    %o1, [%o5+40+8]%asi</span>
<span class="p_add">+	add     %o5, 64, %o5</span>
<span class="p_add">+	stxa    %o1, [%o5-8]%asi</span>
<span class="p_add">+	bgu,pt  %ncc, .wr_loop_small</span>
<span class="p_add">+	 stxa    %o1, [%o5]ASI_STBI_P</span>
<span class="p_add">+</span>
<span class="p_add">+	ba      .asi_done</span>
<span class="p_add">+	 add     %o5, 8, %o5             ! restore %o5 offset</span>
<span class="p_add">+</span>
<span class="p_add">+	! Special case loop for zero fill memsets</span>
<span class="p_add">+	! For each 64 byte cache line, single STBI to first element</span>
<span class="p_add">+	! clears line</span>
<span class="p_add">+.wrzero:</span>
<span class="p_add">+	cmp     %o4, MIN_ZERO           ! check if enough bytes to set</span>
<span class="p_add">+					! to pay %asi + membar cost</span>
<span class="p_add">+	blu     %ncc, .short_set</span>
<span class="p_add">+	 nop</span>
<span class="p_add">+	sub     %o4, 256, %o4</span>
<span class="p_add">+</span>
<span class="p_add">+.wrzero_loop:</span>
<span class="p_add">+	mov     64, %g3</span>
<span class="p_add">+	stxa    %o1, [%o5]ASI_STBI_P</span>
<span class="p_add">+	subcc   %o4, 256, %o4</span>
<span class="p_add">+	stxa    %o1, [%o5+%g3]ASI_STBI_P</span>
<span class="p_add">+	add     %o5, 256, %o5</span>
<span class="p_add">+	sub     %g3, 192, %g3</span>
<span class="p_add">+	stxa    %o1, [%o5+%g3]ASI_STBI_P</span>
<span class="p_add">+	add %g3, 64, %g3</span>
<span class="p_add">+	bge,pt  %ncc, .wrzero_loop</span>
<span class="p_add">+	 stxa    %o1, [%o5+%g3]ASI_STBI_P</span>
<span class="p_add">+	add     %o4, 256, %o4</span>
<span class="p_add">+</span>
<span class="p_add">+	brz,pn  %o4, .bsi_done</span>
<span class="p_add">+	 nop</span>
<span class="p_add">+</span>
<span class="p_add">+.wrzero_small:</span>
<span class="p_add">+	stxa    %o1, [%o5]ASI_STBI_P</span>
<span class="p_add">+	subcc   %o4, 64, %o4</span>
<span class="p_add">+	bgu,pt  %ncc, .wrzero_small</span>
<span class="p_add">+	 add     %o5, 64, %o5</span>
<span class="p_add">+	ba,a	.bsi_done</span>
<span class="p_add">+</span>
<span class="p_add">+.asi_done:</span>
<span class="p_add">+	wr	%g3, 0x0, %asi		! restored saved %asi</span>
<span class="p_add">+.bsi_done:</span>
<span class="p_add">+	membar  #StoreStore             ! required by use of Block Store Init</span>
<span class="p_add">+</span>
<span class="p_add">+.short_set:</span>
<span class="p_add">+	cmp     %o4, 64                 ! check if 64 bytes to set</span>
<span class="p_add">+	blu     %ncc, 5f</span>
<span class="p_add">+	 nop</span>
<span class="p_add">+4:                                      ! set final blocks of 64 bytes</span>
<span class="p_add">+	stx     %o1, [%o5]</span>
<span class="p_add">+	stx     %o1, [%o5+8]</span>
<span class="p_add">+	stx     %o1, [%o5+16]</span>
<span class="p_add">+	stx     %o1, [%o5+24]</span>
<span class="p_add">+	subcc   %o4, 64, %o4</span>
<span class="p_add">+	stx     %o1, [%o5+32]</span>
<span class="p_add">+	stx     %o1, [%o5+40]</span>
<span class="p_add">+	add     %o5, 64, %o5</span>
<span class="p_add">+	stx     %o1, [%o5-16]</span>
<span class="p_add">+	bgu,pt  %ncc, 4b</span>
<span class="p_add">+	 stx     %o1, [%o5-8]</span>
<span class="p_add">+</span>
<span class="p_add">+5:</span>
<span class="p_add">+	! Set the remaining long words</span>
<span class="p_add">+.wrshort:</span>
<span class="p_add">+	subcc   %o3, 8, %o3             ! Can we store any long words?</span>
<span class="p_add">+	blu,pn  %ncc, .wrchars</span>
<span class="p_add">+	 and     %o2, 7, %o2             ! calc bytes left after long words</span>
<span class="p_add">+6:</span>
<span class="p_add">+	subcc   %o3, 8, %o3</span>
<span class="p_add">+	stx     %o1, [%o5]              ! store the long words</span>
<span class="p_add">+	bgeu,pt %ncc, 6b</span>
<span class="p_add">+	 add     %o5, 8, %o5</span>
<span class="p_add">+</span>
<span class="p_add">+.wrchars:                               ! check for extra chars</span>
<span class="p_add">+	brnz    %o2, .wrfin</span>
<span class="p_add">+	 nop</span>
<span class="p_add">+	retl</span>
<span class="p_add">+	 nop</span>
<span class="p_add">+</span>
<span class="p_add">+.wdalign:</span>
<span class="p_add">+	andcc   %o5, 3, %o3             ! is sp1 aligned on a word boundary</span>
<span class="p_add">+	bz,pn   %ncc, .wrword</span>
<span class="p_add">+	 andn    %o2, 3, %o3             ! create word sized count in %o3</span>
<span class="p_add">+</span>
<span class="p_add">+	dec     %o2                     ! decrement count</span>
<span class="p_add">+	stb     %o1, [%o5]              ! clear a byte</span>
<span class="p_add">+	b       .wdalign</span>
<span class="p_add">+	 inc     %o5                     ! next byte</span>
<span class="p_add">+</span>
<span class="p_add">+.wrword:</span>
<span class="p_add">+	subcc   %o3, 4, %o3</span>
<span class="p_add">+	st      %o1, [%o5]              ! 4-byte writing loop</span>
<span class="p_add">+	bnz,pt  %ncc, .wrword</span>
<span class="p_add">+	 add     %o5, 4, %o5</span>
<span class="p_add">+</span>
<span class="p_add">+	and     %o2, 3, %o2             ! leftover count, if any</span>
<span class="p_add">+</span>
<span class="p_add">+.wrchar:</span>
<span class="p_add">+	! Set the remaining bytes, if any</span>
<span class="p_add">+	brz     %o2, .exit</span>
<span class="p_add">+	 nop</span>
<span class="p_add">+.wrfin:</span>
<span class="p_add">+	deccc   %o2</span>
<span class="p_add">+	stb     %o1, [%o5]</span>
<span class="p_add">+	bgu,pt  %ncc, .wrfin</span>
<span class="p_add">+	 inc     %o5</span>
<span class="p_add">+.exit:</span>
<span class="p_add">+	retl                            ! %o0 was preserved</span>
<span class="p_add">+	 nop</span>
<span class="p_add">+</span>
<span class="p_add">+	.size		M7memset,.-M7memset</span>
<span class="p_header">diff --git a/arch/sparc/lib/M7patch.S b/arch/sparc/lib/M7patch.S</span>
new file mode 100644
<span class="p_header">index 0000000..9000b7b</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/sparc/lib/M7patch.S</span>
<span class="p_chunk">@@ -0,0 +1,51 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * M7patch.S: Patch generic routines with M7 variant.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Copyright (c) 2016, Oracle and/or its affiliates.  All rights reserved.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/linkage.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#define BRANCH_ALWAYS	0x10680000</span>
<span class="p_add">+#define NOP		0x01000000</span>
<span class="p_add">+#define NG_DO_PATCH(OLD, NEW)	\</span>
<span class="p_add">+	sethi	%hi(NEW), %g1; \</span>
<span class="p_add">+	or	%g1, %lo(NEW), %g1; \</span>
<span class="p_add">+	sethi	%hi(OLD), %g2; \</span>
<span class="p_add">+	or	%g2, %lo(OLD), %g2; \</span>
<span class="p_add">+	sub	%g1, %g2, %g1; \</span>
<span class="p_add">+	sethi	%hi(BRANCH_ALWAYS), %g3; \</span>
<span class="p_add">+	sll	%g1, 11, %g1; \</span>
<span class="p_add">+	srl	%g1, 11 + 2, %g1; \</span>
<span class="p_add">+	or	%g3, %lo(BRANCH_ALWAYS), %g3; \</span>
<span class="p_add">+	or	%g3, %g1, %g3; \</span>
<span class="p_add">+	stw	%g3, [%g2]; \</span>
<span class="p_add">+	sethi	%hi(NOP), %g3; \</span>
<span class="p_add">+	or	%g3, %lo(NOP), %g3; \</span>
<span class="p_add">+	stw	%g3, [%g2 + 0x4]; \</span>
<span class="p_add">+	flush	%g2;</span>
<span class="p_add">+</span>
<span class="p_add">+ENTRY(m7_patch_copyops)</span>
<span class="p_add">+	NG_DO_PATCH(memcpy, M7memcpy)</span>
<span class="p_add">+	NG_DO_PATCH(raw_copy_from_user, M7copy_from_user)</span>
<span class="p_add">+	NG_DO_PATCH(raw_copy_to_user, M7copy_to_user)</span>
<span class="p_add">+	retl</span>
<span class="p_add">+	 nop</span>
<span class="p_add">+ENDPROC(m7_patch_copyops)</span>
<span class="p_add">+</span>
<span class="p_add">+ENTRY(m7_patch_bzero)</span>
<span class="p_add">+	NG_DO_PATCH(memset, M7memset)</span>
<span class="p_add">+	NG_DO_PATCH(__bzero, M7bzero)</span>
<span class="p_add">+	NG_DO_PATCH(__clear_user, NGclear_user)</span>
<span class="p_add">+	NG_DO_PATCH(tsb_init, NGtsb_init)</span>
<span class="p_add">+	retl</span>
<span class="p_add">+	 nop</span>
<span class="p_add">+ENDPROC(m7_patch_bzero)</span>
<span class="p_add">+</span>
<span class="p_add">+ENTRY(m7_patch_pageops)</span>
<span class="p_add">+	NG_DO_PATCH(copy_user_page, NG4copy_user_page)</span>
<span class="p_add">+	NG_DO_PATCH(_clear_page, M7clear_page)</span>
<span class="p_add">+	NG_DO_PATCH(clear_user_page, M7clear_user_page)</span>
<span class="p_add">+	retl</span>
<span class="p_add">+	 nop</span>
<span class="p_add">+ENDPROC(m7_patch_pageops)</span>
<span class="p_header">diff --git a/arch/sparc/lib/Makefile b/arch/sparc/lib/Makefile</span>
<span class="p_header">index 37930c0..a1a2d39 100644</span>
<span class="p_header">--- a/arch/sparc/lib/Makefile</span>
<span class="p_header">+++ b/arch/sparc/lib/Makefile</span>
<span class="p_chunk">@@ -38,6 +38,9 @@</span> <span class="p_context"> lib-$(CONFIG_SPARC64) +=  NG4patch.o NG4copy_page.o NG4clear_page.o NG4memset.o</span>
 
 lib-$(CONFIG_SPARC64) += Memcpy_utils.o
 
<span class="p_add">+lib-$(CONFIG_SPARC64) += M7memcpy.o M7copy_from_user.o M7copy_to_user.o</span>
<span class="p_add">+lib-$(CONFIG_SPARC64) += M7patch.o M7memset.o</span>
<span class="p_add">+</span>
 lib-$(CONFIG_SPARC64) += GENmemcpy.o GENcopy_from_user.o GENcopy_to_user.o
 lib-$(CONFIG_SPARC64) += GENpatch.o GENpage.o GENbzero.o
 

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



