
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v3,08/10] x86/hyper-v: use hypercall for remote TLB flush - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v3,08/10] x86/hyper-v: use hypercall for remote TLB flush</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=99981">Vitaly Kuznetsov</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>May 19, 2017, 2:09 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20170519140953.1167-9-vkuznets@redhat.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9737367/mbox/"
   >mbox</a>
|
   <a href="/patch/9737367/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9737367/">/patch/9737367/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	8B2F46020B for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 19 May 2017 14:10:58 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 9231328927
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 19 May 2017 14:10:58 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 868232892C; Fri, 19 May 2017 14:10:58 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id E1BF128927
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 19 May 2017 14:10:57 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S932318AbdESOKw (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Fri, 19 May 2017 10:10:52 -0400
Received: from mx1.redhat.com ([209.132.183.28]:34310 &quot;EHLO mx1.redhat.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S932240AbdESOKa (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Fri, 19 May 2017 10:10:30 -0400
Received: from smtp.corp.redhat.com
	(int-mx03.intmail.prod.int.phx2.redhat.com [10.5.11.13])
	(using TLSv1.2 with cipher AECDH-AES256-SHA (256/256 bits))
	(No client certificate requested)
	by mx1.redhat.com (Postfix) with ESMTPS id 70A0A80498;
	Fri, 19 May 2017 14:10:24 +0000 (UTC)
DMARC-Filter: OpenDMARC Filter v1.3.2 mx1.redhat.com 70A0A80498
Authentication-Results: ext-mx04.extmail.prod.ext.phx2.redhat.com;
	dmarc=none (p=none dis=none) header.from=redhat.com
Authentication-Results: ext-mx04.extmail.prod.ext.phx2.redhat.com;
	spf=pass smtp.mailfrom=vkuznets@redhat.com
DKIM-Filter: OpenDKIM Filter v2.11.0 mx1.redhat.com 70A0A80498
Received: from vitty.brq.redhat.com (vitty.brq.redhat.com [10.34.26.3])
	by smtp.corp.redhat.com (Postfix) with ESMTP id CA8145C663;
	Fri, 19 May 2017 14:10:21 +0000 (UTC)
From: Vitaly Kuznetsov &lt;vkuznets@redhat.com&gt;
To: devel@linuxdriverproject.org
Cc: x86@kernel.org, linux-kernel@vger.kernel.org,
	&quot;K. Y. Srinivasan&quot; &lt;kys@microsoft.com&gt;,
	Haiyang Zhang &lt;haiyangz@microsoft.com&gt;,
	Stephen Hemminger &lt;sthemmin@microsoft.com&gt;,
	Thomas Gleixner &lt;tglx@linutronix.de&gt;, Ingo Molnar &lt;mingo@redhat.com&gt;,
	&quot;H. Peter Anvin&quot; &lt;hpa@zytor.com&gt;, Steven Rostedt &lt;rostedt@goodmis.org&gt;,
	Jork Loeser &lt;Jork.Loeser@microsoft.com&gt;,
	Simon Xiao &lt;sixiao@microsoft.com&gt;
Subject: [PATCH v3 08/10] x86/hyper-v: use hypercall for remote TLB flush
Date: Fri, 19 May 2017 16:09:51 +0200
Message-Id: &lt;20170519140953.1167-9-vkuznets@redhat.com&gt;
In-Reply-To: &lt;20170519140953.1167-1-vkuznets@redhat.com&gt;
References: &lt;20170519140953.1167-1-vkuznets@redhat.com&gt;
X-Scanned-By: MIMEDefang 2.79 on 10.5.11.13
X-Greylist: Sender IP whitelisted, not delayed by milter-greylist-4.5.16
	(mx1.redhat.com [10.5.110.28]);
	Fri, 19 May 2017 14:10:25 +0000 (UTC)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=99981">Vitaly Kuznetsov</a> - May 19, 2017, 2:09 p.m.</div>
<pre class="content">
Hyper-V host can suggest us to use hypercall for doing remote TLB flush,
this is supposed to work faster than IPIs.

Implementation details: to do HvFlushVirtualAddress{Space,List} hypercalls
we need to put the input somewhere in memory and we don&#39;t really want to
have memory allocation on each call so we pre-allocate per cpu memory areas
on boot. These areas are of fixes size, limit them with an arbitrary number
of 16 (16 gvas are able to specify 16 * 4096 pages).

pv_ops patching is happening very early so we need to separate
hyperv_setup_mmu_ops() and hyper_alloc_mmu().

It is possible and easy to implement local TLB flushing too and there is
even a hint for that. However, I don&#39;t see a room for optimization on the
host side as both hypercall and native tlb flush will result in vmexit. The
hint is also not set on modern Hyper-V versions.
<span class="signed-off-by">
Signed-off-by: Vitaly Kuznetsov &lt;vkuznets@redhat.com&gt;</span>
<span class="acked-by">Acked-by: K. Y. Srinivasan &lt;kys@microsoft.com&gt;</span>
<span class="tested-by">Tested-by: Simon Xiao &lt;sixiao@microsoft.com&gt;</span>
<span class="tested-by">Tested-by: Srikanth Myakam &lt;v-srm@microsoft.com&gt;</span>
---
 arch/x86/hyperv/Makefile           |   2 +-
 arch/x86/hyperv/hv_init.c          |   2 +
 arch/x86/hyperv/mmu.c              | 117 +++++++++++++++++++++++++++++++++++++
 arch/x86/include/asm/mshyperv.h    |   3 +
 arch/x86/include/uapi/asm/hyperv.h |   7 +++
 arch/x86/kernel/cpu/mshyperv.c     |   1 +
 6 files changed, 131 insertions(+), 1 deletion(-)
 create mode 100644 arch/x86/hyperv/mmu.c
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=125831">Andrew Lutomirski</a> - May 21, 2017, 3:23 a.m.</div>
<pre class="content">
On 05/19/2017 07:09 AM, Vitaly Kuznetsov wrote:
<span class="quote">&gt; Hyper-V host can suggest us to use hypercall for doing remote TLB flush,</span>
<span class="quote">&gt; this is supposed to work faster than IPIs.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Implementation details: to do HvFlushVirtualAddress{Space,List} hypercalls</span>
<span class="quote">&gt; we need to put the input somewhere in memory and we don&#39;t really want to</span>
<span class="quote">&gt; have memory allocation on each call so we pre-allocate per cpu memory areas</span>
<span class="quote">&gt; on boot. These areas are of fixes size, limit them with an arbitrary number</span>
<span class="quote">&gt; of 16 (16 gvas are able to specify 16 * 4096 pages).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; pv_ops patching is happening very early so we need to separate</span>
<span class="quote">&gt; hyperv_setup_mmu_ops() and hyper_alloc_mmu().</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It is possible and easy to implement local TLB flushing too and there is</span>
<span class="quote">&gt; even a hint for that. However, I don&#39;t see a room for optimization on the</span>
<span class="quote">&gt; host side as both hypercall and native tlb flush will result in vmexit. The</span>
<span class="quote">&gt; hint is also not set on modern Hyper-V versions.</span>

Why do local flushes exit?
<span class="quote">
&gt; +static void hyperv_flush_tlb_others(const struct cpumask *cpus,</span>
<span class="quote">&gt; +				    struct mm_struct *mm, unsigned long start,</span>
<span class="quote">&gt; +				    unsigned long end)</span>
<span class="quote">&gt; +{</span>

What tree will this go through?  I&#39;m about to send a signature change 
for this function for tip:x86/mm.

Also, how would this interact with PCID?  I have PCID patches that I&#39;m 
pretty happy with now, and I&#39;m hoping to support PCID in 4.13.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=22291">K. Y. Srinivasan</a> - May 22, 2017, 2:39 p.m.</div>
<pre class="content">
<span class="quote">&gt; -----Original Message-----</span>
<span class="quote">&gt; From: devel [mailto:driverdev-devel-bounces@linuxdriverproject.org] On</span>
<span class="quote">&gt; Behalf Of Vitaly Kuznetsov</span>
<span class="quote">&gt; Sent: Monday, May 22, 2017 3:44 AM</span>
<span class="quote">&gt; To: Andy Lutomirski &lt;luto@kernel.org&gt;</span>
<span class="quote">&gt; Cc: Stephen Hemminger &lt;sthemmin@microsoft.com&gt;; Jork Loeser</span>
<span class="quote">&gt; &lt;Jork.Loeser@microsoft.com&gt;; Haiyang Zhang &lt;haiyangz@microsoft.com&gt;;</span>
<span class="quote">&gt; x86@kernel.org; linux-kernel@vger.kernel.org; Steven Rostedt</span>
<span class="quote">&gt; &lt;rostedt@goodmis.org&gt;; Ingo Molnar &lt;mingo@redhat.com&gt;; H. Peter Anvin</span>
<span class="quote">&gt; &lt;hpa@zytor.com&gt;; devel@linuxdriverproject.org; Thomas Gleixner</span>
<span class="quote">&gt; &lt;tglx@linutronix.de&gt;</span>
<span class="quote">&gt; Subject: Re: [PATCH v3 08/10] x86/hyper-v: use hypercall for remote TLB</span>
<span class="quote">&gt; flush</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Andy Lutomirski &lt;luto@kernel.org&gt; writes:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; On 05/19/2017 07:09 AM, Vitaly Kuznetsov wrote:</span>
<span class="quote">&gt; &gt;&gt; Hyper-V host can suggest us to use hypercall for doing remote TLB flush,</span>
<span class="quote">&gt; &gt;&gt; this is supposed to work faster than IPIs.</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; Implementation details: to do HvFlushVirtualAddress{Space,List}</span>
<span class="quote">&gt; hypercalls</span>
<span class="quote">&gt; &gt;&gt; we need to put the input somewhere in memory and we don&#39;t really</span>
<span class="quote">&gt; want to</span>
<span class="quote">&gt; &gt;&gt; have memory allocation on each call so we pre-allocate per cpu memory</span>
<span class="quote">&gt; areas</span>
<span class="quote">&gt; &gt;&gt; on boot. These areas are of fixes size, limit them with an arbitrary number</span>
<span class="quote">&gt; &gt;&gt; of 16 (16 gvas are able to specify 16 * 4096 pages).</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; pv_ops patching is happening very early so we need to separate</span>
<span class="quote">&gt; &gt;&gt; hyperv_setup_mmu_ops() and hyper_alloc_mmu().</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; It is possible and easy to implement local TLB flushing too and there is</span>
<span class="quote">&gt; &gt;&gt; even a hint for that. However, I don&#39;t see a room for optimization on the</span>
<span class="quote">&gt; &gt;&gt; host side as both hypercall and native tlb flush will result in vmexit. The</span>
<span class="quote">&gt; &gt;&gt; hint is also not set on modern Hyper-V versions.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Why do local flushes exit?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &quot;exist&quot;? I don&#39;t know, to be honest. To me it makes no difference from</span>
<span class="quote">&gt; hypervisor&#39;s point of view as intercepting tlb flushing instructions is</span>
<span class="quote">&gt; not any different from implmenting a hypercall.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Hyper-V gives its guests &#39;hints&#39; to indicate if they need to use</span>
<span class="quote">&gt; hypercalls for remote/locat TLB flush and I don&#39;t remember seeing</span>
<span class="quote">&gt; &#39;local&#39; bit set.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Microsoft folks may probably shed some light on why this was added.</span>

As Vitaly has indicated, these are based on hints from the hypervisor.
Not sure what the perf impact might be for the local flush enlightenment.
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; +static void hyperv_flush_tlb_others(const struct cpumask *cpus,</span>
<span class="quote">&gt; &gt;&gt; +				    struct mm_struct *mm, unsigned long</span>
<span class="quote">&gt; start,</span>
<span class="quote">&gt; &gt;&gt; +				    unsigned long end)</span>
<span class="quote">&gt; &gt;&gt; +{</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; What tree will this go through?  I&#39;m about to send a signature change</span>
<span class="quote">&gt; &gt; for this function for tip:x86/mm.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I think this was going to get through Greg&#39;s char-misc tree but if we</span>
<span class="quote">&gt; need to synchronize I think we can push this through x86.</span>

It will be good to take this through Greg&#39;s tree as that would simplify coordination
with other changes. 
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Also, how would this interact with PCID?  I have PCID patches that I&#39;m</span>
<span class="quote">&gt; &gt; pretty happy with now, and I&#39;m hoping to support PCID in 4.13.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Sorry, I wasn&#39;t following this work closely. .flush_tlb_others() hook is</span>
<span class="quote">&gt; not going away from pv_mmu_ops, right? In think case we can have both in</span>
<span class="quote">&gt; 4.13. Or do you see any other clashes?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; --</span>
<span class="quote">&gt;   Vitaly</span>
<span class="quote">&gt; _______________________________________________</span>
<span class="quote">&gt; devel mailing list</span>
<span class="quote">&gt; devel@linuxdriverproject.org</span>
<span class="quote">&gt; https://na01.safelinks.protection.outlook.com/?url=http%3A%2F%2Fdriverd</span>
<span class="quote">&gt; ev.linuxdriverproject.org%2Fmailman%2Flistinfo%2Fdriverdev-</span>
<span class="quote">&gt; devel&amp;data=02%7C01%7Ckys%40microsoft.com%7Cbdee6af479524fb02db50</span>
<span class="quote">&gt; 8d4a0ff73fe%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C63631046</span>
<span class="quote">&gt; 6477893081&amp;sdata=69mm5horEX93QjLCyhvyFwD8CL%2B0M8kJFaWC9%2BW</span>
<span class="quote">&gt; 18wc%3D&amp;reserved=0</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=125831">Andrew Lutomirski</a> - May 22, 2017, 6:28 p.m.</div>
<pre class="content">
On Mon, May 22, 2017 at 3:43 AM, Vitaly Kuznetsov &lt;vkuznets@redhat.com&gt; wrote:
<span class="quote">&gt; Andy Lutomirski &lt;luto@kernel.org&gt; writes:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; On 05/19/2017 07:09 AM, Vitaly Kuznetsov wrote:</span>
<span class="quote">&gt;&gt;&gt; Hyper-V host can suggest us to use hypercall for doing remote TLB flush,</span>
<span class="quote">&gt;&gt;&gt; this is supposed to work faster than IPIs.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Implementation details: to do HvFlushVirtualAddress{Space,List} hypercalls</span>
<span class="quote">&gt;&gt;&gt; we need to put the input somewhere in memory and we don&#39;t really want to</span>
<span class="quote">&gt;&gt;&gt; have memory allocation on each call so we pre-allocate per cpu memory areas</span>
<span class="quote">&gt;&gt;&gt; on boot. These areas are of fixes size, limit them with an arbitrary number</span>
<span class="quote">&gt;&gt;&gt; of 16 (16 gvas are able to specify 16 * 4096 pages).</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; pv_ops patching is happening very early so we need to separate</span>
<span class="quote">&gt;&gt;&gt; hyperv_setup_mmu_ops() and hyper_alloc_mmu().</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; It is possible and easy to implement local TLB flushing too and there is</span>
<span class="quote">&gt;&gt;&gt; even a hint for that. However, I don&#39;t see a room for optimization on the</span>
<span class="quote">&gt;&gt;&gt; host side as both hypercall and native tlb flush will result in vmexit. The</span>
<span class="quote">&gt;&gt;&gt; hint is also not set on modern Hyper-V versions.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Why do local flushes exit?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; &quot;exist&quot;? I don&#39;t know, to be honest. To me it makes no difference from</span>
<span class="quote">&gt; hypervisor&#39;s point of view as intercepting tlb flushing instructions is</span>
<span class="quote">&gt; not any different from implmenting a hypercall.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Hyper-V gives its guests &#39;hints&#39; to indicate if they need to use</span>
<span class="quote">&gt; hypercalls for remote/locat TLB flush and I don&#39;t remember seeing</span>
<span class="quote">&gt; &#39;local&#39; bit set.</span>

What I meant was: why aren&#39;t local flushes handled directly in the
guest without exiting to the host?  Or are they?  In principle,
INVPCID should just work, right?  Even reading and writing CR3 back
should work if the hypervisor sets up the magic list of allowed CR3
values, right?

I guess on older CPUs there might not be any way to flush the local
TLB without exiting, but I&#39;m not *that* familiar with the details of
the virtualization extensions.
<span class="quote">
&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; +static void hyperv_flush_tlb_others(const struct cpumask *cpus,</span>
<span class="quote">&gt;&gt;&gt; +                                struct mm_struct *mm, unsigned long start,</span>
<span class="quote">&gt;&gt;&gt; +                                unsigned long end)</span>
<span class="quote">&gt;&gt;&gt; +{</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; What tree will this go through?  I&#39;m about to send a signature change</span>
<span class="quote">&gt;&gt; for this function for tip:x86/mm.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I think this was going to get through Greg&#39;s char-misc tree but if we</span>
<span class="quote">&gt; need to synchronize I think we can push this through x86.</span>

Works for me.  Linus can probably resolve the trivial conflict.  But
going through the x86 tree might make sense here if that&#39;s okay with
you.
<span class="quote">
&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Also, how would this interact with PCID?  I have PCID patches that I&#39;m</span>
<span class="quote">&gt;&gt; pretty happy with now, and I&#39;m hoping to support PCID in 4.13.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Sorry, I wasn&#39;t following this work closely. .flush_tlb_others() hook is</span>
<span class="quote">&gt; not going away from pv_mmu_ops, right? In think case we can have both in</span>
<span class="quote">&gt; 4.13. Or do you see any other clashes?</span>
<span class="quote">&gt;</span>

The issue is that I&#39;m changing the whole flush algorithm.  The main
patch that affects this is here:

https://git.kernel.org/pub/scm/linux/kernel/git/luto/linux.git/commit/?h=x86/pcid&amp;id=a67bff42e1e55666fdbaddf233a484a8773688c1

The interactions between that patch and paravirt flush helpers may be
complex, and it&#39;ll need some thought.  PCID makes everything even more
subtle, so just turning off PCID when paravirt flush is involved seems
the safest for now.  Ideally we&#39;d eventually support PCID and paravirt
flushes together (and even eventual native remote flushes assuming
they ever get added).

Also, can you share the benchmark you used for these patches?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=99981">Vitaly Kuznetsov</a> - May 23, 2017, 12:36 p.m.</div>
<pre class="content">
Andy Lutomirski &lt;luto@kernel.org&gt; writes:
<span class="quote">
&gt; On Mon, May 22, 2017 at 3:43 AM, Vitaly Kuznetsov &lt;vkuznets@redhat.com&gt; wrote:</span>
<span class="quote">&gt;&gt; Andy Lutomirski &lt;luto@kernel.org&gt; writes:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; On 05/19/2017 07:09 AM, Vitaly Kuznetsov wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; Hyper-V host can suggest us to use hypercall for doing remote TLB flush,</span>
<span class="quote">&gt;&gt;&gt;&gt; this is supposed to work faster than IPIs.</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; Implementation details: to do HvFlushVirtualAddress{Space,List} hypercalls</span>
<span class="quote">&gt;&gt;&gt;&gt; we need to put the input somewhere in memory and we don&#39;t really want to</span>
<span class="quote">&gt;&gt;&gt;&gt; have memory allocation on each call so we pre-allocate per cpu memory areas</span>
<span class="quote">&gt;&gt;&gt;&gt; on boot. These areas are of fixes size, limit them with an arbitrary number</span>
<span class="quote">&gt;&gt;&gt;&gt; of 16 (16 gvas are able to specify 16 * 4096 pages).</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; pv_ops patching is happening very early so we need to separate</span>
<span class="quote">&gt;&gt;&gt;&gt; hyperv_setup_mmu_ops() and hyper_alloc_mmu().</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; It is possible and easy to implement local TLB flushing too and there is</span>
<span class="quote">&gt;&gt;&gt;&gt; even a hint for that. However, I don&#39;t see a room for optimization on the</span>
<span class="quote">&gt;&gt;&gt;&gt; host side as both hypercall and native tlb flush will result in vmexit. The</span>
<span class="quote">&gt;&gt;&gt;&gt; hint is also not set on modern Hyper-V versions.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Why do local flushes exit?</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; &quot;exist&quot;? I don&#39;t know, to be honest. To me it makes no difference from</span>
<span class="quote">&gt;&gt; hypervisor&#39;s point of view as intercepting tlb flushing instructions is</span>
<span class="quote">&gt;&gt; not any different from implmenting a hypercall.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Hyper-V gives its guests &#39;hints&#39; to indicate if they need to use</span>
<span class="quote">&gt;&gt; hypercalls for remote/locat TLB flush and I don&#39;t remember seeing</span>
<span class="quote">&gt;&gt; &#39;local&#39; bit set.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; What I meant was: why aren&#39;t local flushes handled directly in the</span>
<span class="quote">&gt; guest without exiting to the host?  Or are they?  In principle,</span>
<span class="quote">&gt; INVPCID should just work, right?  Even reading and writing CR3 back</span>
<span class="quote">&gt; should work if the hypervisor sets up the magic list of allowed CR3</span>
<span class="quote">&gt; values, right?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I guess on older CPUs there might not be any way to flush the local</span>
<span class="quote">&gt; TLB without exiting, but I&#39;m not *that* familiar with the details of</span>
<span class="quote">&gt; the virtualization extensions.</span>
<span class="quote">&gt;</span>

Right, local flushes should &#39;just work&#39;. If for whatever reason
hypervisor decides to trap us it&#39;s nothing we can do about it.
<span class="quote">
&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; +static void hyperv_flush_tlb_others(const struct cpumask *cpus,</span>
<span class="quote">&gt;&gt;&gt;&gt; +                                struct mm_struct *mm, unsigned long start,</span>
<span class="quote">&gt;&gt;&gt;&gt; +                                unsigned long end)</span>
<span class="quote">&gt;&gt;&gt;&gt; +{</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; What tree will this go through?  I&#39;m about to send a signature change</span>
<span class="quote">&gt;&gt;&gt; for this function for tip:x86/mm.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; I think this was going to get through Greg&#39;s char-misc tree but if we</span>
<span class="quote">&gt;&gt; need to synchronize I think we can push this through x86.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Works for me.  Linus can probably resolve the trivial conflict.  But</span>
<span class="quote">&gt; going through the x86 tree might make sense here if that&#39;s okay with</span>
<span class="quote">&gt; you.</span>
<span class="quote">&gt;</span>

Definitely fine with me, I&#39;ll leave this decision up to x86 maintainers,
Hyper-V maintainers, and Greg.
<span class="quote">
&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Also, how would this interact with PCID?  I have PCID patches that I&#39;m</span>
<span class="quote">&gt;&gt;&gt; pretty happy with now, and I&#39;m hoping to support PCID in 4.13.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Sorry, I wasn&#39;t following this work closely. .flush_tlb_others() hook is</span>
<span class="quote">&gt;&gt; not going away from pv_mmu_ops, right? In think case we can have both in</span>
<span class="quote">&gt;&gt; 4.13. Or do you see any other clashes?</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; The issue is that I&#39;m changing the whole flush algorithm.  The main</span>
<span class="quote">&gt; patch that affects this is here:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; https://git.kernel.org/pub/scm/linux/kernel/git/luto/linux.git/commit/?h=x86/pcid&amp;id=a67bff42e1e55666fdbaddf233a484a8773688c1</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; The interactions between that patch and paravirt flush helpers may be</span>
<span class="quote">&gt; complex, and it&#39;ll need some thought.  PCID makes everything even more</span>
<span class="quote">&gt; subtle, so just turning off PCID when paravirt flush is involved seems</span>
<span class="quote">&gt; the safest for now.  Ideally we&#39;d eventually support PCID and paravirt</span>
<span class="quote">&gt; flushes together (and even eventual native remote flushes assuming</span>
<span class="quote">&gt; they ever get added).</span>

I see. On Hyper-V HVCALL_FLUSH_VIRTUAL_ADDRESS_LIST hypercall&#39;s
interface is:
1) List of entries to flush. Each entry is a PFN and lower 12 bits are
used to encode the number of pages after this one (defined by the PFN)
we&#39;d like to flush. We can flush up to 509 entries with one
hypercall (can be extended but requires a pre-allocated memory region).

2) Processor mask

3) Address space id (all 64 bits of CR3. Not sure how it&#39;s used within
the hypervisor).

HVCALL_FLUSH_VIRTUAL_ADDRESS_SPACE_EX is more or less the same but we
need more space to specify &gt; 64 vCPUs so we&#39;ll be able to pass less than
509 entries.

The main advantage compared to sending IPIs, as far as I understand, is
that virtual CPUs which are not currently scheduled don&#39;t need flushing
and we can&#39;t know this from within the guest.

I agree that disabling PCID for paravirt flush users for now is a good
option, let&#39;s have it merged and tested without this additional
complexity and make another round after.
<span class="quote">
&gt;</span>
<span class="quote">&gt; Also, can you share the benchmark you used for these patches?</span>

I didn&#39;t do much while writing the patchset, mostly I was running the
attached dumb trasher (32 pthreads doing mmap/munmap). On a 16 vCPU
Hyper-V 2016 guest I get the following (just re-did the test with
4.12-rc1):

Before the patchset:
# time ./pthread_mmap ./randfile 

real	3m33.118s
user	0m3.698s
sys	3m16.624s

After the patchset:
# time ./pthread_mmap ./randfile 

real	2m19.920s
user	0m2.662s
sys	2m9.948s

K. Y.&#39;s guys at Microsoft did additional testing for the patchset on
different Hyper-V deployments including Azure, they may share their
findings too.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=22291">K. Y. Srinivasan</a> - May 23, 2017, 5:50 p.m.</div>
<pre class="content">
<span class="quote">&gt; -----Original Message-----</span>
<span class="quote">&gt; From: devel [mailto:driverdev-devel-bounces@linuxdriverproject.org] On</span>
<span class="quote">&gt; Behalf Of Vitaly Kuznetsov</span>
<span class="quote">&gt; Sent: Tuesday, May 23, 2017 5:37 AM</span>
<span class="quote">&gt; To: Andy Lutomirski &lt;luto@kernel.org&gt;</span>
<span class="quote">&gt; Cc: Stephen Hemminger &lt;sthemmin@microsoft.com&gt;; Jork Loeser</span>
<span class="quote">&gt; &lt;Jork.Loeser@microsoft.com&gt;; Haiyang Zhang &lt;haiyangz@microsoft.com&gt;;</span>
<span class="quote">&gt; X86 ML &lt;x86@kernel.org&gt;; linux-kernel@vger.kernel.org; Steven Rostedt</span>
<span class="quote">&gt; &lt;rostedt@goodmis.org&gt;; Ingo Molnar &lt;mingo@redhat.com&gt;; H. Peter Anvin</span>
<span class="quote">&gt; &lt;hpa@zytor.com&gt;; devel@linuxdriverproject.org; Thomas Gleixner</span>
<span class="quote">&gt; &lt;tglx@linutronix.de&gt;</span>
<span class="quote">&gt; Subject: Re: [PATCH v3 08/10] x86/hyper-v: use hypercall for remote TLB</span>
<span class="quote">&gt; flush</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Andy Lutomirski &lt;luto@kernel.org&gt; writes:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; On Mon, May 22, 2017 at 3:43 AM, Vitaly Kuznetsov</span>
<span class="quote">&gt; &lt;vkuznets@redhat.com&gt; wrote:</span>
<span class="quote">&gt; &gt;&gt; Andy Lutomirski &lt;luto@kernel.org&gt; writes:</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt; On 05/19/2017 07:09 AM, Vitaly Kuznetsov wrote:</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; Hyper-V host can suggest us to use hypercall for doing remote TLB</span>
<span class="quote">&gt; flush,</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; this is supposed to work faster than IPIs.</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; Implementation details: to do HvFlushVirtualAddress{Space,List}</span>
<span class="quote">&gt; hypercalls</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; we need to put the input somewhere in memory and we don&#39;t really</span>
<span class="quote">&gt; want to</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; have memory allocation on each call so we pre-allocate per cpu</span>
<span class="quote">&gt; memory areas</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; on boot. These areas are of fixes size, limit them with an arbitrary</span>
<span class="quote">&gt; number</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; of 16 (16 gvas are able to specify 16 * 4096 pages).</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; pv_ops patching is happening very early so we need to separate</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; hyperv_setup_mmu_ops() and hyper_alloc_mmu().</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; It is possible and easy to implement local TLB flushing too and there is</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; even a hint for that. However, I don&#39;t see a room for optimization on</span>
<span class="quote">&gt; the</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; host side as both hypercall and native tlb flush will result in vmexit. The</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; hint is also not set on modern Hyper-V versions.</span>
<span class="quote">&gt; &gt;&gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt; Why do local flushes exit?</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; &quot;exist&quot;? I don&#39;t know, to be honest. To me it makes no difference from</span>
<span class="quote">&gt; &gt;&gt; hypervisor&#39;s point of view as intercepting tlb flushing instructions is</span>
<span class="quote">&gt; &gt;&gt; not any different from implmenting a hypercall.</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; Hyper-V gives its guests &#39;hints&#39; to indicate if they need to use</span>
<span class="quote">&gt; &gt;&gt; hypercalls for remote/locat TLB flush and I don&#39;t remember seeing</span>
<span class="quote">&gt; &gt;&gt; &#39;local&#39; bit set.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; What I meant was: why aren&#39;t local flushes handled directly in the</span>
<span class="quote">&gt; &gt; guest without exiting to the host?  Or are they?  In principle,</span>
<span class="quote">&gt; &gt; INVPCID should just work, right?  Even reading and writing CR3 back</span>
<span class="quote">&gt; &gt; should work if the hypervisor sets up the magic list of allowed CR3</span>
<span class="quote">&gt; &gt; values, right?</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; I guess on older CPUs there might not be any way to flush the local</span>
<span class="quote">&gt; &gt; TLB without exiting, but I&#39;m not *that* familiar with the details of</span>
<span class="quote">&gt; &gt; the virtualization extensions.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Right, local flushes should &#39;just work&#39;. If for whatever reason</span>
<span class="quote">&gt; hypervisor decides to trap us it&#39;s nothing we can do about it.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +static void hyperv_flush_tlb_others(const struct cpumask *cpus,</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +                                struct mm_struct *mm, unsigned long start,</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +                                unsigned long end)</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +{</span>
<span class="quote">&gt; &gt;&gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt; What tree will this go through?  I&#39;m about to send a signature change</span>
<span class="quote">&gt; &gt;&gt;&gt; for this function for tip:x86/mm.</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; I think this was going to get through Greg&#39;s char-misc tree but if we</span>
<span class="quote">&gt; &gt;&gt; need to synchronize I think we can push this through x86.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Works for me.  Linus can probably resolve the trivial conflict.  But</span>
<span class="quote">&gt; &gt; going through the x86 tree might make sense here if that&#39;s okay with</span>
<span class="quote">&gt; &gt; you.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Definitely fine with me, I&#39;ll leave this decision up to x86 maintainers,</span>
<span class="quote">&gt; Hyper-V maintainers, and Greg.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt; Also, how would this interact with PCID?  I have PCID patches that I&#39;m</span>
<span class="quote">&gt; &gt;&gt;&gt; pretty happy with now, and I&#39;m hoping to support PCID in 4.13.</span>
<span class="quote">&gt; &gt;&gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; Sorry, I wasn&#39;t following this work closely. .flush_tlb_others() hook is</span>
<span class="quote">&gt; &gt;&gt; not going away from pv_mmu_ops, right? In think case we can have both</span>
<span class="quote">&gt; in</span>
<span class="quote">&gt; &gt;&gt; 4.13. Or do you see any other clashes?</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; The issue is that I&#39;m changing the whole flush algorithm.  The main</span>
<span class="quote">&gt; &gt; patch that affects this is here:</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; https://na01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgit.ker</span>
<span class="quote">&gt; nel.org%2Fpub%2Fscm%2Flinux%2Fkernel%2Fgit%2Fluto%2Flinux.git%2Fco</span>
<span class="quote">&gt; mmit%2F%3Fh%3Dx86%2Fpcid%26id%3Da67bff42e1e55666fdbaddf233a484a</span>
<span class="quote">&gt; 8773688c1&amp;data=02%7C01%7Ckys%40microsoft.com%7C88a812b285a741bcd</span>
<span class="quote">&gt; 28d08d4a1d864aa%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C636</span>
<span class="quote">&gt; 311398154677248&amp;sdata=%2BaCK2EW9S%2BdggL168xQ5eiaXXRZY31II6lLle1ys</span>
<span class="quote">&gt; 6Bw%3D&amp;reserved=0</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; The interactions between that patch and paravirt flush helpers may be</span>
<span class="quote">&gt; &gt; complex, and it&#39;ll need some thought.  PCID makes everything even more</span>
<span class="quote">&gt; &gt; subtle, so just turning off PCID when paravirt flush is involved seems</span>
<span class="quote">&gt; &gt; the safest for now.  Ideally we&#39;d eventually support PCID and paravirt</span>
<span class="quote">&gt; &gt; flushes together (and even eventual native remote flushes assuming</span>
<span class="quote">&gt; &gt; they ever get added).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I see. On Hyper-V HVCALL_FLUSH_VIRTUAL_ADDRESS_LIST hypercall&#39;s</span>
<span class="quote">&gt; interface is:</span>
<span class="quote">&gt; 1) List of entries to flush. Each entry is a PFN and lower 12 bits are</span>
<span class="quote">&gt; used to encode the number of pages after this one (defined by the PFN)</span>
<span class="quote">&gt; we&#39;d like to flush. We can flush up to 509 entries with one</span>
<span class="quote">&gt; hypercall (can be extended but requires a pre-allocated memory region).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 2) Processor mask</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 3) Address space id (all 64 bits of CR3. Not sure how it&#39;s used within</span>
<span class="quote">&gt; the hypervisor).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; HVCALL_FLUSH_VIRTUAL_ADDRESS_SPACE_EX is more or less the same but</span>
<span class="quote">&gt; we</span>
<span class="quote">&gt; need more space to specify &gt; 64 vCPUs so we&#39;ll be able to pass less than</span>
<span class="quote">&gt; 509 entries.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The main advantage compared to sending IPIs, as far as I understand, is</span>
<span class="quote">&gt; that virtual CPUs which are not currently scheduled don&#39;t need flushing</span>
<span class="quote">&gt; and we can&#39;t know this from within the guest.</span>

There are other potential advantages as well:
1. When we need to flush with a large CPU mask, the hypercall mechanism can obviously
minimize the number of intercepts.
2. There is no instruction emulation in the hypercall path. 
<span class="quote">&gt; </span>
<span class="quote">&gt; I agree that disabling PCID for paravirt flush users for now is a good</span>
<span class="quote">&gt; option, let&#39;s have it merged and tested without this additional</span>
<span class="quote">&gt; complexity and make another round after.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Also, can you share the benchmark you used for these patches?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I didn&#39;t do much while writing the patchset, mostly I was running the</span>
<span class="quote">&gt; attached dumb trasher (32 pthreads doing mmap/munmap). On a 16 vCPU</span>
<span class="quote">&gt; Hyper-V 2016 guest I get the following (just re-did the test with</span>
<span class="quote">&gt; 4.12-rc1):</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Before the patchset:</span>
<span class="quote">&gt; # time ./pthread_mmap ./randfile</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; real	3m33.118s</span>
<span class="quote">&gt; user	0m3.698s</span>
<span class="quote">&gt; sys	3m16.624s</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; After the patchset:</span>
<span class="quote">&gt; # time ./pthread_mmap ./randfile</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; real	2m19.920s</span>
<span class="quote">&gt; user	0m2.662s</span>
<span class="quote">&gt; sys	2m9.948s</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; K. Y.&#39;s guys at Microsoft did additional testing for the patchset on</span>
<span class="quote">&gt; different Hyper-V deployments including Azure, they may share their</span>
<span class="quote">&gt; findings too.</span>

Our testing was mostly focused on stability and correctness. For the benchmarks we ran
(micro benchmarks for storage and networking) we did see improvements across the board.

Regards,

K. Y
<span class="quote">
&gt; </span>
<span class="quote">&gt; --</span>
<span class="quote">&gt;   Vitaly</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=125831">Andrew Lutomirski</a> - June 27, 2017, 1:36 a.m.</div>
<pre class="content">
On Tue, May 23, 2017 at 5:36 AM, Vitaly Kuznetsov &lt;vkuznets@redhat.com&gt; wrote:
<span class="quote">&gt; Andy Lutomirski &lt;luto@kernel.org&gt; writes:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Also, can you share the benchmark you used for these patches?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I didn&#39;t do much while writing the patchset, mostly I was running the</span>
<span class="quote">&gt; attached dumb trasher (32 pthreads doing mmap/munmap). On a 16 vCPU</span>
<span class="quote">&gt; Hyper-V 2016 guest I get the following (just re-did the test with</span>
<span class="quote">&gt; 4.12-rc1):</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Before the patchset:</span>
<span class="quote">&gt; # time ./pthread_mmap ./randfile</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; real    3m33.118s</span>
<span class="quote">&gt; user    0m3.698s</span>
<span class="quote">&gt; sys     3m16.624s</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; After the patchset:</span>
<span class="quote">&gt; # time ./pthread_mmap ./randfile</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; real    2m19.920s</span>
<span class="quote">&gt; user    0m2.662s</span>
<span class="quote">&gt; sys     2m9.948s</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; K. Y.&#39;s guys at Microsoft did additional testing for the patchset on</span>
<span class="quote">&gt; different Hyper-V deployments including Azure, they may share their</span>
<span class="quote">&gt; findings too.</span>

I ran this benchmark on my big TLB patchset, mainly to make sure I
didn&#39;t regress your test.  I seem to have sped it up by 30% or so
instead.  I need to study this a little bit to figure out why to make
sure that the reason isn&#39;t that I&#39;m failing to do flushes I need to
do.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=99981">Vitaly Kuznetsov</a> - July 13, 2017, 12:46 p.m.</div>
<pre class="content">
Andy Lutomirski &lt;luto@kernel.org&gt; writes:
<span class="quote">
&gt; On Tue, May 23, 2017 at 5:36 AM, Vitaly Kuznetsov &lt;vkuznets@redhat.com&gt; wrote:</span>
<span class="quote">&gt;&gt; Andy Lutomirski &lt;luto@kernel.org&gt; writes:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Also, can you share the benchmark you used for these patches?</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; I didn&#39;t do much while writing the patchset, mostly I was running the</span>
<span class="quote">&gt;&gt; attached dumb trasher (32 pthreads doing mmap/munmap). On a 16 vCPU</span>
<span class="quote">&gt;&gt; Hyper-V 2016 guest I get the following (just re-did the test with</span>
<span class="quote">&gt;&gt; 4.12-rc1):</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Before the patchset:</span>
<span class="quote">&gt;&gt; # time ./pthread_mmap ./randfile</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; real    3m33.118s</span>
<span class="quote">&gt;&gt; user    0m3.698s</span>
<span class="quote">&gt;&gt; sys     3m16.624s</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; After the patchset:</span>
<span class="quote">&gt;&gt; # time ./pthread_mmap ./randfile</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; real    2m19.920s</span>
<span class="quote">&gt;&gt; user    0m2.662s</span>
<span class="quote">&gt;&gt; sys     2m9.948s</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; K. Y.&#39;s guys at Microsoft did additional testing for the patchset on</span>
<span class="quote">&gt;&gt; different Hyper-V deployments including Azure, they may share their</span>
<span class="quote">&gt;&gt; findings too.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I ran this benchmark on my big TLB patchset, mainly to make sure I</span>
<span class="quote">&gt; didn&#39;t regress your test.  I seem to have sped it up by 30% or so</span>
<span class="quote">&gt; instead.  I need to study this a little bit to figure out why to make</span>
<span class="quote">&gt; sure that the reason isn&#39;t that I&#39;m failing to do flushes I need to</span>
<span class="quote">&gt; do.</span>

Got back to this and tested everything on WS2016 Hyper-V guest (24
vCPUs) with my slightly modified benchmark. The numbers are:

1) pre-patch:

real	1m15.775s
user	0m0.850s
sys	1m31.515s

2) your &#39;x86/pcid&#39; series (PCID feature is not passed to the guest so this
is mainly your lazy tlb optimization):

real	0m55.135s
user	0m1.168s
sys	1m3.810s

3) My &#39;pv tlb shootdown&#39; patchset on top of your &#39;x86/pcid&#39; series:

real	0m48.891s
user	0m1.052s
sys	0m52.591s

As far as I understand I need to add
&#39;setup_clear_cpu_cap(X86_FEATURE_PCID)&#39; to my series to make things work
properly if this feature appears in the guest.

Other than that there is an additional room for optimization:
tlb_single_page_flush_ceiling, I&#39;m not sure that with Hyper-V&#39;s PV the
default value of 33 is optimal. But the investigation can be done
separately.

AFAIU with your TLB preparatory work which got into 4.13 our series
become untangled and can go through different trees. I&#39;ll rebase mine
and send it to K. Y. to push through Greg&#39;s char-misc tree.

Is there anything blocking your PCID series from going into 4.14? It
seems to big a huge improvement for some workloads.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=125831">Andrew Lutomirski</a> - July 14, 2017, 10:26 p.m.</div>
<pre class="content">
On Thu, Jul 13, 2017 at 5:46 AM, Vitaly Kuznetsov &lt;vkuznets@redhat.com&gt; wrote:
<span class="quote">&gt; Andy Lutomirski &lt;luto@kernel.org&gt; writes:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; On Tue, May 23, 2017 at 5:36 AM, Vitaly Kuznetsov &lt;vkuznets@redhat.com&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt; Andy Lutomirski &lt;luto@kernel.org&gt; writes:</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; Also, can you share the benchmark you used for these patches?</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; I didn&#39;t do much while writing the patchset, mostly I was running the</span>
<span class="quote">&gt;&gt;&gt; attached dumb trasher (32 pthreads doing mmap/munmap). On a 16 vCPU</span>
<span class="quote">&gt;&gt;&gt; Hyper-V 2016 guest I get the following (just re-did the test with</span>
<span class="quote">&gt;&gt;&gt; 4.12-rc1):</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Before the patchset:</span>
<span class="quote">&gt;&gt;&gt; # time ./pthread_mmap ./randfile</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; real    3m33.118s</span>
<span class="quote">&gt;&gt;&gt; user    0m3.698s</span>
<span class="quote">&gt;&gt;&gt; sys     3m16.624s</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; After the patchset:</span>
<span class="quote">&gt;&gt;&gt; # time ./pthread_mmap ./randfile</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; real    2m19.920s</span>
<span class="quote">&gt;&gt;&gt; user    0m2.662s</span>
<span class="quote">&gt;&gt;&gt; sys     2m9.948s</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; K. Y.&#39;s guys at Microsoft did additional testing for the patchset on</span>
<span class="quote">&gt;&gt;&gt; different Hyper-V deployments including Azure, they may share their</span>
<span class="quote">&gt;&gt;&gt; findings too.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; I ran this benchmark on my big TLB patchset, mainly to make sure I</span>
<span class="quote">&gt;&gt; didn&#39;t regress your test.  I seem to have sped it up by 30% or so</span>
<span class="quote">&gt;&gt; instead.  I need to study this a little bit to figure out why to make</span>
<span class="quote">&gt;&gt; sure that the reason isn&#39;t that I&#39;m failing to do flushes I need to</span>
<span class="quote">&gt;&gt; do.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Got back to this and tested everything on WS2016 Hyper-V guest (24</span>
<span class="quote">&gt; vCPUs) with my slightly modified benchmark. The numbers are:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; 1) pre-patch:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; real    1m15.775s</span>
<span class="quote">&gt; user    0m0.850s</span>
<span class="quote">&gt; sys     1m31.515s</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; 2) your &#39;x86/pcid&#39; series (PCID feature is not passed to the guest so this</span>
<span class="quote">&gt; is mainly your lazy tlb optimization):</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; real    0m55.135s</span>
<span class="quote">&gt; user    0m1.168s</span>
<span class="quote">&gt; sys     1m3.810s</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; 3) My &#39;pv tlb shootdown&#39; patchset on top of your &#39;x86/pcid&#39; series:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; real    0m48.891s</span>
<span class="quote">&gt; user    0m1.052s</span>
<span class="quote">&gt; sys     0m52.591s</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; As far as I understand I need to add</span>
<span class="quote">&gt; &#39;setup_clear_cpu_cap(X86_FEATURE_PCID)&#39; to my series to make things work</span>
<span class="quote">&gt; properly if this feature appears in the guest.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Other than that there is an additional room for optimization:</span>
<span class="quote">&gt; tlb_single_page_flush_ceiling, I&#39;m not sure that with Hyper-V&#39;s PV the</span>
<span class="quote">&gt; default value of 33 is optimal. But the investigation can be done</span>
<span class="quote">&gt; separately.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; AFAIU with your TLB preparatory work which got into 4.13 our series</span>
<span class="quote">&gt; become untangled and can go through different trees. I&#39;ll rebase mine</span>
<span class="quote">&gt; and send it to K. Y. to push through Greg&#39;s char-misc tree.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Is there anything blocking your PCID series from going into 4.14? It</span>
<span class="quote">&gt; seems to big a huge improvement for some workloads.</span>

No.  All but one patch should land in 4.13.

It would also be nifty if someone were to augment by work to allow one
CPU to tell another CPU that it just flushed on that CPU&#39;s behalf.
Basically, a property atomic and/or locked operation that finds a
given ctx_id in the remote cpu&#39;s cpu_tlbstate and, if tlb_gen &lt;= x,
sets tlb_gen to x.  Some read operations might be useful, too.  This
*might* be doable with cmpxchg16b, but spinlocks would be easier.  The
idea would be for paravirt remote flushes to be able to see, for real,
which remote CPUs need flushes, do the flushes, and then update the
remote tlb_gen to record that they&#39;ve been done.

FWIW, I read the HV TLB docs, and it&#39;s entirely unclear to me how it
interacts with PCID or whether PCID is supported at all.  It would be
real nice to get PCID *and* paravirt flush on the major hypervisor
platforms.

--Andy
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/hyperv/Makefile b/arch/x86/hyperv/Makefile</span>
<span class="p_header">index 171ae09..367a820 100644</span>
<span class="p_header">--- a/arch/x86/hyperv/Makefile</span>
<span class="p_header">+++ b/arch/x86/hyperv/Makefile</span>
<span class="p_chunk">@@ -1 +1 @@</span> <span class="p_context"></span>
<span class="p_del">-obj-y		:= hv_init.o</span>
<span class="p_add">+obj-y		:= hv_init.o mmu.o</span>
<span class="p_header">diff --git a/arch/x86/hyperv/hv_init.c b/arch/x86/hyperv/hv_init.c</span>
<span class="p_header">index 7fd9cd3..df3252f 100644</span>
<span class="p_header">--- a/arch/x86/hyperv/hv_init.c</span>
<span class="p_header">+++ b/arch/x86/hyperv/hv_init.c</span>
<span class="p_chunk">@@ -140,6 +140,8 @@</span> <span class="p_context"> void hyperv_init(void)</span>
 	hypercall_msr.guest_physical_address = vmalloc_to_pfn(hv_hypercall_pg);
 	wrmsrl(HV_X64_MSR_HYPERCALL, hypercall_msr.as_uint64);
 
<span class="p_add">+	hyper_alloc_mmu();</span>
<span class="p_add">+</span>
 	/*
 	 * Register Hyper-V specific clocksource.
 	 */
<span class="p_header">diff --git a/arch/x86/hyperv/mmu.c b/arch/x86/hyperv/mmu.c</span>
new file mode 100644
<span class="p_header">index 0000000..e3ab9b9</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/x86/hyperv/mmu.c</span>
<span class="p_chunk">@@ -0,0 +1,117 @@</span> <span class="p_context"></span>
<span class="p_add">+#include &lt;linux/types.h&gt;</span>
<span class="p_add">+#include &lt;linux/hyperv.h&gt;</span>
<span class="p_add">+#include &lt;linux/slab.h&gt;</span>
<span class="p_add">+#include &lt;linux/log2.h&gt;</span>
<span class="p_add">+#include &lt;asm/mshyperv.h&gt;</span>
<span class="p_add">+#include &lt;asm/tlbflush.h&gt;</span>
<span class="p_add">+#include &lt;asm/msr.h&gt;</span>
<span class="p_add">+#include &lt;asm/fpu/api.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+/* HvFlushVirtualAddressSpace, HvFlushVirtualAddressList hypercalls */</span>
<span class="p_add">+struct hv_flush_pcpu {</span>
<span class="p_add">+	__u64 address_space;</span>
<span class="p_add">+	__u64 flags;</span>
<span class="p_add">+	__u64 processor_mask;</span>
<span class="p_add">+	__u64 gva_list[];</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+static struct hv_flush_pcpu __percpu *pcpu_flush;</span>
<span class="p_add">+</span>
<span class="p_add">+static void hyperv_flush_tlb_others(const struct cpumask *cpus,</span>
<span class="p_add">+				    struct mm_struct *mm, unsigned long start,</span>
<span class="p_add">+				    unsigned long end)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct hv_flush_pcpu *flush;</span>
<span class="p_add">+	unsigned long cur, flags;</span>
<span class="p_add">+	u64 status = -1ULL;</span>
<span class="p_add">+	int cpu, vcpu, gva_n, max_gvas;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!pcpu_flush || !hv_hypercall_pg)</span>
<span class="p_add">+		goto do_native;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (cpumask_empty(cpus))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	local_irq_save(flags);</span>
<span class="p_add">+</span>
<span class="p_add">+	flush = this_cpu_ptr(pcpu_flush);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (mm) {</span>
<span class="p_add">+		flush-&gt;address_space = virt_to_phys(mm-&gt;pgd);</span>
<span class="p_add">+		flush-&gt;flags = 0;</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		flush-&gt;address_space = 0;</span>
<span class="p_add">+		flush-&gt;flags = HV_FLUSH_ALL_VIRTUAL_ADDRESS_SPACES;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	flush-&gt;processor_mask = 0;</span>
<span class="p_add">+	if (cpumask_equal(cpus, cpu_present_mask)) {</span>
<span class="p_add">+		flush-&gt;flags |= HV_FLUSH_ALL_PROCESSORS;</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		for_each_cpu(cpu, cpus) {</span>
<span class="p_add">+			vcpu = hv_cpu_number_to_vp_number(cpu);</span>
<span class="p_add">+			if (vcpu != -1 &amp;&amp; vcpu &lt; 64)</span>
<span class="p_add">+				flush-&gt;processor_mask |= 1 &lt;&lt; vcpu;</span>
<span class="p_add">+			else</span>
<span class="p_add">+				goto do_native;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We can flush not more than max_gvas with one hypercall. Flush the</span>
<span class="p_add">+	 * whole address space if we were asked to do more.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	max_gvas = (PAGE_SIZE - sizeof(*flush)) / 8;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (end == TLB_FLUSH_ALL ||</span>
<span class="p_add">+	    (end &amp;&amp; ((end - start)/(PAGE_SIZE*PAGE_SIZE)) &gt; max_gvas)) {</span>
<span class="p_add">+		if (end == TLB_FLUSH_ALL)</span>
<span class="p_add">+			flush-&gt;flags |= HV_FLUSH_NON_GLOBAL_MAPPINGS_ONLY;</span>
<span class="p_add">+		status = hv_do_hypercall(HVCALL_FLUSH_VIRTUAL_ADDRESS_SPACE,</span>
<span class="p_add">+					 flush, NULL);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		cur = start;</span>
<span class="p_add">+		gva_n = 0;</span>
<span class="p_add">+		do {</span>
<span class="p_add">+			flush-&gt;gva_list[gva_n] = cur &amp; PAGE_MASK;</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * Lower 12 bits encode the number of additional</span>
<span class="p_add">+			 * pages to flush (in addition to the &#39;cur&#39; page).</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			if (end &gt;= cur + PAGE_SIZE * PAGE_SIZE)</span>
<span class="p_add">+				flush-&gt;gva_list[gva_n] |= ~PAGE_MASK;</span>
<span class="p_add">+			else if (end &gt; cur)</span>
<span class="p_add">+				flush-&gt;gva_list[gva_n] |=</span>
<span class="p_add">+					(end - cur - 1) &gt;&gt; PAGE_SHIFT;</span>
<span class="p_add">+</span>
<span class="p_add">+			cur += PAGE_SIZE * PAGE_SIZE;</span>
<span class="p_add">+			++gva_n;</span>
<span class="p_add">+</span>
<span class="p_add">+		} while (cur &lt; end);</span>
<span class="p_add">+</span>
<span class="p_add">+		status = hv_do_rep_hypercall(HVCALL_FLUSH_VIRTUAL_ADDRESS_LIST,</span>
<span class="p_add">+					     gva_n, 0, flush, NULL);</span>
<span class="p_add">+</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	local_irq_restore(flags);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!(status &amp; 0xffff))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+do_native:</span>
<span class="p_add">+	native_flush_tlb_others(cpus, mm, start, end);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void hyperv_setup_mmu_ops(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (ms_hyperv.hints &amp; HV_X64_REMOTE_TLB_FLUSH_RECOMMENDED) {</span>
<span class="p_add">+		pr_info(&quot;Hyper-V: Using hypercall for remote TLB flush\n&quot;);</span>
<span class="p_add">+		pv_mmu_ops.flush_tlb_others = hyperv_flush_tlb_others;</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void hyper_alloc_mmu(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (ms_hyperv.hints &amp; HV_X64_REMOTE_TLB_FLUSH_RECOMMENDED)</span>
<span class="p_add">+		pcpu_flush = __alloc_percpu(PAGE_SIZE, PAGE_SIZE);</span>
<span class="p_add">+}</span>
<span class="p_header">diff --git a/arch/x86/include/asm/mshyperv.h b/arch/x86/include/asm/mshyperv.h</span>
<span class="p_header">index eb38da3..359967f 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/mshyperv.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/mshyperv.h</span>
<span class="p_chunk">@@ -308,6 +308,8 @@</span> <span class="p_context"> static inline int hv_cpu_number_to_vp_number(int cpu_number)</span>
 }
 
 void hyperv_init(void);
<span class="p_add">+void hyperv_setup_mmu_ops(void);</span>
<span class="p_add">+void hyper_alloc_mmu(void);</span>
 void hyperv_report_panic(struct pt_regs *regs);
 bool hv_is_hypercall_page_setup(void);
 void hyperv_cleanup(void);
<span class="p_chunk">@@ -318,6 +320,7 @@</span> <span class="p_context"> static inline bool hv_is_hypercall_page_setup(void)</span>
 	return false;
 }
 static inline hyperv_cleanup(void) {}
<span class="p_add">+static inline void hyperv_setup_mmu_ops(void) {}</span>
 #endif /* CONFIG_HYPERV */
 
 #ifdef CONFIG_HYPERV_TSCPAGE
<span class="p_header">diff --git a/arch/x86/include/uapi/asm/hyperv.h b/arch/x86/include/uapi/asm/hyperv.h</span>
<span class="p_header">index c87e900..3d44036 100644</span>
<span class="p_header">--- a/arch/x86/include/uapi/asm/hyperv.h</span>
<span class="p_header">+++ b/arch/x86/include/uapi/asm/hyperv.h</span>
<span class="p_chunk">@@ -239,6 +239,8 @@</span> <span class="p_context"></span>
 		(~((1ull &lt;&lt; HV_X64_MSR_HYPERCALL_PAGE_ADDRESS_SHIFT) - 1))
 
 /* Declare the various hypercall operations. */
<span class="p_add">+#define HVCALL_FLUSH_VIRTUAL_ADDRESS_SPACE	0x0002</span>
<span class="p_add">+#define HVCALL_FLUSH_VIRTUAL_ADDRESS_LIST	0x0003</span>
 #define HVCALL_NOTIFY_LONG_SPIN_WAIT		0x0008
 #define HVCALL_POST_MESSAGE			0x005c
 #define HVCALL_SIGNAL_EVENT			0x005d
<span class="p_chunk">@@ -256,6 +258,11 @@</span> <span class="p_context"></span>
 #define HV_PROCESSOR_POWER_STATE_C2		2
 #define HV_PROCESSOR_POWER_STATE_C3		3
 
<span class="p_add">+#define HV_FLUSH_ALL_PROCESSORS			0x00000001</span>
<span class="p_add">+#define HV_FLUSH_ALL_VIRTUAL_ADDRESS_SPACES	0x00000002</span>
<span class="p_add">+#define HV_FLUSH_NON_GLOBAL_MAPPINGS_ONLY	0x00000004</span>
<span class="p_add">+#define HV_FLUSH_USE_EXTENDED_RANGE_FORMAT	0x00000008</span>
<span class="p_add">+</span>
 /* Hypercall interface */
 union hv_hypercall_input {
 	u64 as_uint64;
<span class="p_header">diff --git a/arch/x86/kernel/cpu/mshyperv.c b/arch/x86/kernel/cpu/mshyperv.c</span>
<span class="p_header">index a8b4765..16a9221 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/mshyperv.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/mshyperv.c</span>
<span class="p_chunk">@@ -240,6 +240,7 @@</span> <span class="p_context"> static void __init ms_hyperv_init_platform(void)</span>
 	 * Setup the hook to get control post apic initialization.
 	 */
 	x86_platform.apic_post_init = hyperv_init;
<span class="p_add">+	hyperv_setup_mmu_ops();</span>
 #endif
 }
 

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



