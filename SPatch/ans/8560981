
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v5,04/14] mm: Add support for PUD-sized transparent hugepages - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v5,04/14] mm: Add support for PUD-sized transparent hugepages</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=66491">Wilcox, Matthew R</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>March 10, 2016, 11:55 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1457654131-4562-5-git-send-email-matthew.r.wilcox@intel.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/8560981/mbox/"
   >mbox</a>
|
   <a href="/patch/8560981/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/8560981/">/patch/8560981/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork1.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork1.web.kernel.org (Postfix) with ESMTP id 47C9C9F38C
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 11 Mar 2016 00:02:14 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 33E2C20328
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 11 Mar 2016 00:02:12 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id CEDAD20377
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 11 Mar 2016 00:02:09 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S933329AbcCKACA (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 10 Mar 2016 19:02:00 -0500
Received: from mga02.intel.com ([134.134.136.20]:61724 &quot;EHLO mga02.intel.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S932910AbcCJXzm (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 10 Mar 2016 18:55:42 -0500
Received: from fmsmga003.fm.intel.com ([10.253.24.29])
	by orsmga101.jf.intel.com with ESMTP; 10 Mar 2016 15:55:38 -0800
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i=&quot;5.24,317,1455004800&quot;; d=&quot;scan&#39;208&quot;;a=&quot;667361533&quot;
Received: from jduan1-mobl2.amr.corp.intel.com (HELO thog.int.wil.cx)
	([10.252.134.213])
	by FMSMGA003.fm.intel.com with SMTP; 10 Mar 2016 15:55:38 -0800
Received: by thog.int.wil.cx (Postfix, from userid 1000)
	id 36D0D5F9EC; Thu, 10 Mar 2016 18:55:36 -0500 (EST)
From: Matthew Wilcox &lt;matthew.r.wilcox@intel.com&gt;
To: Andrew Morton &lt;akpm@linux-foundation.org&gt;
Cc: Matthew Wilcox &lt;willy@linux.intel.com&gt;, linux-mm@kvack.org,
	linux-nvdimm@lists.01.org, linux-fsdevel@vger.kernel.org,
	linux-kernel@vger.kernel.org, x86@kernel.org
Subject: [PATCH v5 04/14] mm: Add support for PUD-sized transparent hugepages
Date: Thu, 10 Mar 2016 18:55:21 -0500
Message-Id: &lt;1457654131-4562-5-git-send-email-matthew.r.wilcox@intel.com&gt;
X-Mailer: git-send-email 2.7.0
In-Reply-To: &lt;1457654131-4562-1-git-send-email-matthew.r.wilcox@intel.com&gt;
References: &lt;1457654131-4562-1-git-send-email-matthew.r.wilcox@intel.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-6.9 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=66491">Wilcox, Matthew R</a> - March 10, 2016, 11:55 p.m.</div>
<pre class="content">
<span class="from">From: Matthew Wilcox &lt;willy@linux.intel.com&gt;</span>

The current transparent hugepage code only supports PMDs.  This patch
adds support for transparent use of PUDs with DAX.  It does not include
support for anonymous pages.

Most of this patch simply parallels the work that was done for huge PMDs.
The only major difference is how the new -&gt;pud_entry method in mm_walk
works.  The -&gt;pmd_entry method replaces the -&gt;pte_entry method, whereas
the -&gt;pud_entry method works along with either -&gt;pmd_entry or -&gt;pte_entry.
The pagewalk code takes care of locking the PUD before calling -&gt;pud_walk,
so handlers do not need to worry whether the PUD is stable.
<span class="signed-off-by">
Signed-off-by: Matthew Wilcox &lt;willy@linux.intel.com&gt;</span>
---
 arch/Kconfig                  |   3 +
 include/asm-generic/pgtable.h |  73 ++++++++++++-
 include/asm-generic/tlb.h     |  14 +++
 include/linux/huge_mm.h       |  84 ++++++++++++++-
 include/linux/mm.h            |  28 +++++
 include/linux/mmu_notifier.h  |  14 +++
 include/linux/pfn_t.h         |   8 ++
 mm/gup.c                      |   7 ++
 mm/huge_memory.c              | 246 ++++++++++++++++++++++++++++++++++++++++++
 mm/memory.c                   |  97 ++++++++++++++++-
 mm/pagewalk.c                 |  19 +++-
 mm/pgtable-generic.c          |  13 +++
 12 files changed, 594 insertions(+), 12 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/Kconfig b/arch/Kconfig</span>
<span class="p_header">index 049f243..4e22842 100644</span>
<span class="p_header">--- a/arch/Kconfig</span>
<span class="p_header">+++ b/arch/Kconfig</span>
<span class="p_chunk">@@ -459,6 +459,9 @@</span> <span class="p_context"> config HAVE_IRQ_TIME_ACCOUNTING</span>
 config HAVE_ARCH_TRANSPARENT_HUGEPAGE
 	bool
 
<span class="p_add">+config HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD</span>
<span class="p_add">+	bool</span>
<span class="p_add">+</span>
 config HAVE_ARCH_HUGE_VMAP
 	bool
 
<span class="p_header">diff --git a/include/asm-generic/pgtable.h b/include/asm-generic/pgtable.h</span>
<span class="p_header">index 9401f48..a72b092 100644</span>
<span class="p_header">--- a/include/asm-generic/pgtable.h</span>
<span class="p_header">+++ b/include/asm-generic/pgtable.h</span>
<span class="p_chunk">@@ -36,6 +36,9 @@</span> <span class="p_context"> extern int ptep_set_access_flags(struct vm_area_struct *vma,</span>
 extern int pmdp_set_access_flags(struct vm_area_struct *vma,
 				 unsigned long address, pmd_t *pmdp,
 				 pmd_t entry, int dirty);
<span class="p_add">+extern int pudp_set_access_flags(struct vm_area_struct *vma,</span>
<span class="p_add">+				 unsigned long address, pud_t *pudp,</span>
<span class="p_add">+				 pud_t entry, int dirty);</span>
 #else
 static inline int pmdp_set_access_flags(struct vm_area_struct *vma,
 					unsigned long address, pmd_t *pmdp,
<span class="p_chunk">@@ -44,6 +47,13 @@</span> <span class="p_context"> static inline int pmdp_set_access_flags(struct vm_area_struct *vma,</span>
 	BUILD_BUG();
 	return 0;
 }
<span class="p_add">+static inline int pudp_set_access_flags(struct vm_area_struct *vma,</span>
<span class="p_add">+					unsigned long address, pud_t *pudp,</span>
<span class="p_add">+					pud_t entry, int dirty)</span>
<span class="p_add">+{</span>
<span class="p_add">+	BUILD_BUG();</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
 #endif /* CONFIG_TRANSPARENT_HUGEPAGE */
 #endif
 
<span class="p_chunk">@@ -121,8 +131,8 @@</span> <span class="p_context"> static inline pte_t ptep_get_and_clear(struct mm_struct *mm,</span>
 }
 #endif
 
<span class="p_del">-#ifndef __HAVE_ARCH_PMDP_HUGE_GET_AND_CLEAR</span>
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE
<span class="p_add">+#ifndef __HAVE_ARCH_PMDP_HUGE_GET_AND_CLEAR</span>
 static inline pmd_t pmdp_huge_get_and_clear(struct mm_struct *mm,
 					    unsigned long address,
 					    pmd_t *pmdp)
<span class="p_chunk">@@ -131,20 +141,39 @@</span> <span class="p_context"> static inline pmd_t pmdp_huge_get_and_clear(struct mm_struct *mm,</span>
 	pmd_clear(pmdp);
 	return pmd;
 }
<span class="p_add">+#endif /* __HAVE_ARCH_PMDP_HUGE_GET_AND_CLEAR */</span>
<span class="p_add">+#ifndef __HAVE_ARCH_PUDP_HUGE_GET_AND_CLEAR</span>
<span class="p_add">+static inline pud_t pudp_huge_get_and_clear(struct mm_struct *mm,</span>
<span class="p_add">+					    unsigned long address,</span>
<span class="p_add">+					    pud_t *pudp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pud_t pud = *pudp;</span>
<span class="p_add">+	pud_clear(pudp);</span>
<span class="p_add">+	return pud;</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif /* __HAVE_ARCH_PUDP_HUGE_GET_AND_CLEAR */</span>
 #endif /* CONFIG_TRANSPARENT_HUGEPAGE */
<span class="p_del">-#endif</span>
 
<span class="p_del">-#ifndef __HAVE_ARCH_PMDP_HUGE_GET_AND_CLEAR_FULL</span>
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE
<span class="p_add">+#ifndef __HAVE_ARCH_PMDP_HUGE_GET_AND_CLEAR_FULL</span>
 static inline pmd_t pmdp_huge_get_and_clear_full(struct mm_struct *mm,
 					    unsigned long address, pmd_t *pmdp,
 					    int full)
 {
 	return pmdp_huge_get_and_clear(mm, address, pmdp);
 }
<span class="p_del">-#endif /* CONFIG_TRANSPARENT_HUGEPAGE */</span>
 #endif
 
<span class="p_add">+#ifndef __HAVE_ARCH_PUDP_HUGE_GET_AND_CLEAR_FULL</span>
<span class="p_add">+static inline pud_t pudp_huge_get_and_clear_full(struct mm_struct *mm,</span>
<span class="p_add">+					    unsigned long address, pud_t *pudp,</span>
<span class="p_add">+					    int full)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return pudp_huge_get_and_clear(mm, address, pudp);</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+#endif /* CONFIG_TRANSPARENT_HUGEPAGE */</span>
<span class="p_add">+</span>
 #ifndef __HAVE_ARCH_PTEP_GET_AND_CLEAR_FULL
 static inline pte_t ptep_get_and_clear_full(struct mm_struct *mm,
 					    unsigned long address, pte_t *ptep,
<span class="p_chunk">@@ -181,6 +210,9 @@</span> <span class="p_context"> extern pte_t ptep_clear_flush(struct vm_area_struct *vma,</span>
 extern pmd_t pmdp_huge_clear_flush(struct vm_area_struct *vma,
 			      unsigned long address,
 			      pmd_t *pmdp);
<span class="p_add">+extern pud_t pudp_huge_clear_flush(struct vm_area_struct *vma,</span>
<span class="p_add">+			      unsigned long address,</span>
<span class="p_add">+			      pud_t *pudp);</span>
 #endif
 
 #ifndef __HAVE_ARCH_PTEP_SET_WRPROTECT
<span class="p_chunk">@@ -208,6 +240,22 @@</span> <span class="p_context"> static inline void pmdp_set_wrprotect(struct mm_struct *mm,</span>
 }
 #endif /* CONFIG_TRANSPARENT_HUGEPAGE */
 #endif
<span class="p_add">+#ifndef __HAVE_ARCH_PUDP_SET_WRPROTECT</span>
<span class="p_add">+#ifdef CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD</span>
<span class="p_add">+static inline void pudp_set_wrprotect(struct mm_struct *mm,</span>
<span class="p_add">+				      unsigned long address, pud_t *pudp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pud_t old_pud = *pudp;</span>
<span class="p_add">+	set_pud_at(mm, address, pudp, pud_wrprotect(old_pud));</span>
<span class="p_add">+}</span>
<span class="p_add">+#else</span>
<span class="p_add">+static inline void pudp_set_wrprotect(struct mm_struct *mm,</span>
<span class="p_add">+				      unsigned long address, pud_t *pudp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	BUILD_BUG();</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif /* CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD */</span>
<span class="p_add">+#endif</span>
 
 #ifndef pmdp_collapse_flush
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE
<span class="p_chunk">@@ -273,12 +321,23 @@</span> <span class="p_context"> static inline int pmd_same(pmd_t pmd_a, pmd_t pmd_b)</span>
 {
 	return pmd_val(pmd_a) == pmd_val(pmd_b);
 }
<span class="p_add">+</span>
<span class="p_add">+static inline int pud_same(pud_t pud_a, pud_t pud_b)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return pud_val(pud_a) == pud_val(pud_b);</span>
<span class="p_add">+}</span>
 #else /* CONFIG_TRANSPARENT_HUGEPAGE */
 static inline int pmd_same(pmd_t pmd_a, pmd_t pmd_b)
 {
 	BUILD_BUG();
 	return 0;
 }
<span class="p_add">+</span>
<span class="p_add">+static inline int pud_same(pud_t pud_a, pud_t pud_b)</span>
<span class="p_add">+{</span>
<span class="p_add">+	BUILD_BUG();</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
 #endif /* CONFIG_TRANSPARENT_HUGEPAGE */
 #endif
 
<span class="p_chunk">@@ -632,6 +691,10 @@</span> <span class="p_context"> static inline int pmd_trans_huge(pmd_t pmd)</span>
 {
 	return 0;
 }
<span class="p_add">+static inline int pud_trans_huge(pud_t pud)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
 #ifndef __HAVE_ARCH_PMD_WRITE
 static inline int pmd_write(pmd_t pmd)
 {
<span class="p_chunk">@@ -795,8 +858,10 @@</span> <span class="p_context"> static inline int pmd_clear_huge(pmd_t *pmd)</span>
  * e.g. see arch/arc: flush_pmd_tlb_range
  */
 #define flush_pmd_tlb_range(vma, addr, end)	flush_tlb_range(vma, addr, end)
<span class="p_add">+#define flush_pud_tlb_range(vma, addr, end)	flush_tlb_range(vma, addr, end)</span>
 #else
 #define flush_pmd_tlb_range(vma, addr, end)	BUILD_BUG()
<span class="p_add">+#define flush_pud_tlb_range(vma, addr, end)	BUILD_BUG()</span>
 #endif
 #endif
 
<span class="p_header">diff --git a/include/asm-generic/tlb.h b/include/asm-generic/tlb.h</span>
<span class="p_header">index 9dbb739..9d310c8 100644</span>
<span class="p_header">--- a/include/asm-generic/tlb.h</span>
<span class="p_header">+++ b/include/asm-generic/tlb.h</span>
<span class="p_chunk">@@ -196,6 +196,20 @@</span> <span class="p_context"> static inline void __tlb_reset_range(struct mmu_gather *tlb)</span>
 		__tlb_remove_pmd_tlb_entry(tlb, pmdp, address);	\
 	} while (0)
 
<span class="p_add">+/**</span>
<span class="p_add">+ * tlb_remove_pud_tlb_entry - remember a pud mapping for later tlb invalidation</span>
<span class="p_add">+ * This is a nop so far, because only x86 needs it.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#ifndef __tlb_remove_pud_tlb_entry</span>
<span class="p_add">+#define __tlb_remove_pud_tlb_entry(tlb, pudp, address) do {} while (0)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#define tlb_remove_pud_tlb_entry(tlb, pudp, address)		\</span>
<span class="p_add">+	do {							\</span>
<span class="p_add">+		__tlb_adjust_range(tlb, address);		\</span>
<span class="p_add">+		__tlb_remove_pud_tlb_entry(tlb, pudp, address);	\</span>
<span class="p_add">+	} while (0)</span>
<span class="p_add">+</span>
 #define pte_free_tlb(tlb, ptep, address)			\
 	do {							\
 		__tlb_adjust_range(tlb, address);		\
<span class="p_header">diff --git a/include/linux/huge_mm.h b/include/linux/huge_mm.h</span>
<span class="p_header">index d257d27..152991b 100644</span>
<span class="p_header">--- a/include/linux/huge_mm.h</span>
<span class="p_header">+++ b/include/linux/huge_mm.h</span>
<span class="p_chunk">@@ -8,10 +8,27 @@</span> <span class="p_context"> extern int do_huge_pmd_anonymous_page(struct mm_struct *mm,</span>
 extern int copy_huge_pmd(struct mm_struct *dst_mm, struct mm_struct *src_mm,
 			 pmd_t *dst_pmd, pmd_t *src_pmd, unsigned long addr,
 			 struct vm_area_struct *vma);
<span class="p_add">+extern int copy_huge_pud(struct mm_struct *dst_mm, struct mm_struct *src_mm,</span>
<span class="p_add">+			 pud_t *dst_pud, pud_t *src_pud, unsigned long addr,</span>
<span class="p_add">+			 struct vm_area_struct *vma);</span>
 extern void huge_pmd_set_accessed(struct mm_struct *mm,
 				  struct vm_area_struct *vma,
 				  unsigned long address, pmd_t *pmd,
 				  pmd_t orig_pmd, int dirty);
<span class="p_add">+#ifdef CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD</span>
<span class="p_add">+extern void huge_pud_set_accessed(struct mm_struct *mm,</span>
<span class="p_add">+				  struct vm_area_struct *vma,</span>
<span class="p_add">+				  unsigned long address, pud_t *pud,</span>
<span class="p_add">+				  pud_t orig_pud, int dirty);</span>
<span class="p_add">+#else</span>
<span class="p_add">+static inline void huge_pud_set_accessed(struct mm_struct *mm,</span>
<span class="p_add">+				  struct vm_area_struct *vma,</span>
<span class="p_add">+				  unsigned long address, pud_t *pud,</span>
<span class="p_add">+				  pud_t orig_pud, int dirty)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 extern int do_huge_pmd_wp_page(struct mm_struct *mm, struct vm_area_struct *vma,
 			       unsigned long address, pmd_t *pmd,
 			       pmd_t orig_pmd);
<span class="p_chunk">@@ -25,6 +42,9 @@</span> <span class="p_context"> extern int madvise_free_huge_pmd(struct mmu_gather *tlb,</span>
 extern int zap_huge_pmd(struct mmu_gather *tlb,
 			struct vm_area_struct *vma,
 			pmd_t *pmd, unsigned long addr);
<span class="p_add">+extern int zap_huge_pud(struct mmu_gather *tlb,</span>
<span class="p_add">+			struct vm_area_struct *vma,</span>
<span class="p_add">+			pud_t *pud, unsigned long addr);</span>
 extern int mincore_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,
 			unsigned long addr, unsigned long end,
 			unsigned char *vec);
<span class="p_chunk">@@ -38,6 +58,8 @@</span> <span class="p_context"> extern int change_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,</span>
 			int prot_numa);
 int vmf_insert_pfn_pmd(struct vm_area_struct *, unsigned long addr, pmd_t *,
 			pfn_t pfn, bool write);
<span class="p_add">+int vmf_insert_pfn_pud(struct vm_area_struct *, unsigned long addr, pud_t *,</span>
<span class="p_add">+			pfn_t pfn, bool write);</span>
 enum transparent_hugepage_flag {
 	TRANSPARENT_HUGEPAGE_FLAG,
 	TRANSPARENT_HUGEPAGE_REQ_MADV_FLAG,
<span class="p_chunk">@@ -55,13 +77,14 @@</span> <span class="p_context"> enum transparent_hugepage_flag {</span>
 #define HPAGE_PMD_NR (1&lt;&lt;HPAGE_PMD_ORDER)
 
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE
<span class="p_del">-struct page *follow_devmap_pmd(struct vm_area_struct *vma, unsigned long addr,</span>
<span class="p_del">-		pmd_t *pmd, int flags);</span>
<span class="p_del">-</span>
 #define HPAGE_PMD_SHIFT PMD_SHIFT
 #define HPAGE_PMD_SIZE	((1UL) &lt;&lt; HPAGE_PMD_SHIFT)
 #define HPAGE_PMD_MASK	(~(HPAGE_PMD_SIZE - 1))
 
<span class="p_add">+#define HPAGE_PUD_SHIFT PUD_SHIFT</span>
<span class="p_add">+#define HPAGE_PUD_SIZE	((1UL) &lt;&lt; HPAGE_PUD_SHIFT)</span>
<span class="p_add">+#define HPAGE_PUD_MASK	(~(HPAGE_PUD_SIZE - 1))</span>
<span class="p_add">+</span>
 extern bool is_vma_temporary_stack(struct vm_area_struct *vma);
 
 #define transparent_hugepage_enabled(__vma)				\
<span class="p_chunk">@@ -107,10 +130,20 @@</span> <span class="p_context"> void __split_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,</span>
 						false);			\
 	}  while (0)
 
<span class="p_del">-</span>
 void split_huge_pmd_address(struct vm_area_struct *vma, unsigned long address,
 		bool freeze);
 
<span class="p_add">+void __split_huge_pud(struct vm_area_struct *vma, pud_t *pud,</span>
<span class="p_add">+		unsigned long address);</span>
<span class="p_add">+</span>
<span class="p_add">+#define split_huge_pud(__vma, __pud, __address)				\</span>
<span class="p_add">+	do {								\</span>
<span class="p_add">+		pud_t *____pud = (__pud);				\</span>
<span class="p_add">+		if (pud_trans_huge(*____pud)				\</span>
<span class="p_add">+					|| pud_devmap(*____pud))	\</span>
<span class="p_add">+			__split_huge_pud(__vma, __pud, __address);	\</span>
<span class="p_add">+	}  while (0)</span>
<span class="p_add">+</span>
 extern int hugepage_madvise(struct vm_area_struct *vma,
 			    unsigned long *vm_flags, int advice);
 extern void vma_adjust_trans_huge(struct vm_area_struct *vma,
<span class="p_chunk">@@ -119,6 +152,8 @@</span> <span class="p_context"> extern void vma_adjust_trans_huge(struct vm_area_struct *vma,</span>
 				    long adjust_next);
 extern spinlock_t *__pmd_trans_huge_lock(pmd_t *pmd,
 		struct vm_area_struct *vma);
<span class="p_add">+extern spinlock_t *__pud_trans_huge_lock(pud_t *pud,</span>
<span class="p_add">+		struct vm_area_struct *vma);</span>
 /* mmap_sem must be held on entry */
 static inline spinlock_t *pmd_trans_huge_lock(pmd_t *pmd,
 		struct vm_area_struct *vma)
<span class="p_chunk">@@ -129,6 +164,15 @@</span> <span class="p_context"> static inline spinlock_t *pmd_trans_huge_lock(pmd_t *pmd,</span>
 	else
 		return false;
 }
<span class="p_add">+static inline spinlock_t *pud_trans_huge_lock(pud_t *pud,</span>
<span class="p_add">+		struct vm_area_struct *vma)</span>
<span class="p_add">+{</span>
<span class="p_add">+	VM_BUG_ON_VMA(!rwsem_is_locked(&amp;vma-&gt;vm_mm-&gt;mmap_sem), vma);</span>
<span class="p_add">+	if (pud_trans_huge(*pud) || pud_devmap(*pud))</span>
<span class="p_add">+		return __pud_trans_huge_lock(pud, vma);</span>
<span class="p_add">+	else</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+}</span>
 static inline int hpage_nr_pages(struct page *page)
 {
 	if (unlikely(PageTransHuge(page)))
<span class="p_chunk">@@ -136,6 +180,11 @@</span> <span class="p_context"> static inline int hpage_nr_pages(struct page *page)</span>
 	return 1;
 }
 
<span class="p_add">+struct page *follow_devmap_pmd(struct vm_area_struct *vma, unsigned long addr,</span>
<span class="p_add">+		pmd_t *pmd, int flags);</span>
<span class="p_add">+struct page *follow_devmap_pud(struct vm_area_struct *vma, unsigned long addr,</span>
<span class="p_add">+		pud_t *pud, int flags);</span>
<span class="p_add">+</span>
 extern int do_huge_pmd_numa_page(struct mm_struct *mm, struct vm_area_struct *vma,
 				unsigned long addr, pmd_t pmd, pmd_t *pmdp);
 
<span class="p_chunk">@@ -151,6 +200,11 @@</span> <span class="p_context"> static inline bool is_huge_zero_pmd(pmd_t pmd)</span>
 	return is_huge_zero_page(pmd_page(pmd));
 }
 
<span class="p_add">+static inline bool is_huge_zero_pud(pud_t pud)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return false;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 struct page *get_huge_zero_page(void);
 
 #else /* CONFIG_TRANSPARENT_HUGEPAGE */
<span class="p_chunk">@@ -158,6 +212,10 @@</span> <span class="p_context"> struct page *get_huge_zero_page(void);</span>
 #define HPAGE_PMD_MASK ({ BUILD_BUG(); 0; })
 #define HPAGE_PMD_SIZE ({ BUILD_BUG(); 0; })
 
<span class="p_add">+#define HPAGE_PUD_SHIFT ({ BUILD_BUG(); 0; })</span>
<span class="p_add">+#define HPAGE_PUD_MASK ({ BUILD_BUG(); 0; })</span>
<span class="p_add">+#define HPAGE_PUD_SIZE ({ BUILD_BUG(); 0; })</span>
<span class="p_add">+</span>
 #define hpage_nr_pages(x) 1
 
 #define transparent_hugepage_enabled(__vma) 0
<span class="p_chunk">@@ -179,6 +237,9 @@</span> <span class="p_context"> static inline void deferred_split_huge_page(struct page *page) {}</span>
 static inline void split_huge_pmd_address(struct vm_area_struct *vma,
 		unsigned long address, bool freeze) {}
 
<span class="p_add">+#define split_huge_pud(__vma, __pmd, __address)	\</span>
<span class="p_add">+	do { } while (0)</span>
<span class="p_add">+</span>
 static inline int hugepage_madvise(struct vm_area_struct *vma,
 				   unsigned long *vm_flags, int advice)
 {
<span class="p_chunk">@@ -196,6 +257,11 @@</span> <span class="p_context"> static inline spinlock_t *pmd_trans_huge_lock(pmd_t *pmd,</span>
 {
 	return NULL;
 }
<span class="p_add">+static inline spinlock_t *pud_trans_huge_lock(pud_t *pud,</span>
<span class="p_add">+		struct vm_area_struct *vma)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return NULL;</span>
<span class="p_add">+}</span>
 
 static inline int do_huge_pmd_numa_page(struct mm_struct *mm, struct vm_area_struct *vma,
 					unsigned long addr, pmd_t pmd, pmd_t *pmdp)
<span class="p_chunk">@@ -208,12 +274,22 @@</span> <span class="p_context"> static inline bool is_huge_zero_page(struct page *page)</span>
 	return false;
 }
 
<span class="p_add">+static inline bool is_huge_zero_pud(pud_t pud)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return false;</span>
<span class="p_add">+}</span>
 
 static inline struct page *follow_devmap_pmd(struct vm_area_struct *vma,
 		unsigned long addr, pmd_t *pmd, int flags)
 {
 	return NULL;
 }
<span class="p_add">+</span>
<span class="p_add">+static inline struct page *follow_devmap_pud(struct vm_area_struct *vma,</span>
<span class="p_add">+		unsigned long addr, pud_t *pud, int flags)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return NULL;</span>
<span class="p_add">+}</span>
 #endif /* CONFIG_TRANSPARENT_HUGEPAGE */
 
 #endif /* _LINUX_HUGE_MM_H */
<span class="p_header">diff --git a/include/linux/mm.h b/include/linux/mm.h</span>
<span class="p_header">index a3d640a..d30d2fb 100644</span>
<span class="p_header">--- a/include/linux/mm.h</span>
<span class="p_header">+++ b/include/linux/mm.h</span>
<span class="p_chunk">@@ -390,6 +390,10 @@</span> <span class="p_context"> static inline int pmd_devmap(pmd_t pmd)</span>
 {
 	return 0;
 }
<span class="p_add">+static inline int pud_devmap(pud_t pud)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
 #endif
 
 /*
<span class="p_chunk">@@ -1158,6 +1162,10 @@</span> <span class="p_context"> void unmap_vmas(struct mmu_gather *tlb, struct vm_area_struct *start_vma,</span>
 
 /**
  * mm_walk - callbacks for walk_page_range
<span class="p_add">+ * @pud_entry: if set, called for each non-empty PUD (2nd-level) entry</span>
<span class="p_add">+ *	       this handler should only handle pud_trans_huge() puds.</span>
<span class="p_add">+ *	       the pmd_entry or pte_entry callbacks will be used for</span>
<span class="p_add">+ *	       regular PUDs.</span>
  * @pmd_entry: if set, called for each non-empty PMD (3rd-level) entry
  *	       this handler is required to be able to handle
  *	       pmd_trans_huge() pmds.  They may simply choose to
<span class="p_chunk">@@ -1177,6 +1185,8 @@</span> <span class="p_context"> void unmap_vmas(struct mmu_gather *tlb, struct vm_area_struct *start_vma,</span>
  * (see the comment on walk_page_range() for more details)
  */
 struct mm_walk {
<span class="p_add">+	int (*pud_entry)(pud_t *pud, unsigned long addr,</span>
<span class="p_add">+			 unsigned long next, struct mm_walk *walk);</span>
 	int (*pmd_entry)(pmd_t *pmd, unsigned long addr,
 			 unsigned long next, struct mm_walk *walk);
 	int (*pte_entry)(pte_t *pte, unsigned long addr,
<span class="p_chunk">@@ -1812,6 +1822,24 @@</span> <span class="p_context"> static inline spinlock_t *pmd_lock(struct mm_struct *mm, pmd_t *pmd)</span>
 	return ptl;
 }
 
<span class="p_add">+/*</span>
<span class="p_add">+ * No scalability reason to split PUD locks yet, but follow the same pattern</span>
<span class="p_add">+ * as the PMD locks to make it easier if we decide to.  The VM should not be</span>
<span class="p_add">+ * considered ready to switch to split PUD locks yet; there may be places</span>
<span class="p_add">+ * which need to be converted from page_table_lock.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline spinlock_t *pud_lockptr(struct mm_struct *mm, pud_t *pud)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return &amp;mm-&gt;page_table_lock;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline spinlock_t *pud_lock(struct mm_struct *mm, pud_t *pud)</span>
<span class="p_add">+{</span>
<span class="p_add">+	spinlock_t *ptl = pud_lockptr(mm, pud);</span>
<span class="p_add">+	spin_lock(ptl);</span>
<span class="p_add">+	return ptl;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 extern void free_area_init(unsigned long * zones_size);
 extern void free_area_init_node(int nid, unsigned long * zones_size,
 		unsigned long zone_start_pfn, unsigned long *zholes_size);
<span class="p_header">diff --git a/include/linux/mmu_notifier.h b/include/linux/mmu_notifier.h</span>
<span class="p_header">index a1a210d..51891fb 100644</span>
<span class="p_header">--- a/include/linux/mmu_notifier.h</span>
<span class="p_header">+++ b/include/linux/mmu_notifier.h</span>
<span class="p_chunk">@@ -381,6 +381,19 @@</span> <span class="p_context"> static inline void mmu_notifier_mm_destroy(struct mm_struct *mm)</span>
 	___pmd;								\
 })
 
<span class="p_add">+#define pudp_huge_clear_flush_notify(__vma, __haddr, __pud)		\</span>
<span class="p_add">+({									\</span>
<span class="p_add">+	unsigned long ___haddr = __haddr &amp; HPAGE_PUD_MASK;		\</span>
<span class="p_add">+	struct mm_struct *___mm = (__vma)-&gt;vm_mm;			\</span>
<span class="p_add">+	pud_t ___pud;							\</span>
<span class="p_add">+									\</span>
<span class="p_add">+	___pud = pudp_huge_clear_flush(__vma, __haddr, __pud);		\</span>
<span class="p_add">+	mmu_notifier_invalidate_range(___mm, ___haddr,			\</span>
<span class="p_add">+				      ___haddr + HPAGE_PUD_SIZE);	\</span>
<span class="p_add">+									\</span>
<span class="p_add">+	___pud;								\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
 #define pmdp_huge_get_and_clear_notify(__mm, __haddr, __pmd)		\
 ({									\
 	unsigned long ___haddr = __haddr &amp; HPAGE_PMD_MASK;		\
<span class="p_chunk">@@ -475,6 +488,7 @@</span> <span class="p_context"> static inline void mmu_notifier_mm_destroy(struct mm_struct *mm)</span>
 #define pmdp_clear_young_notify pmdp_test_and_clear_young
 #define	ptep_clear_flush_notify ptep_clear_flush
 #define pmdp_huge_clear_flush_notify pmdp_huge_clear_flush
<span class="p_add">+#define pudp_huge_clear_flush_notify pudp_huge_clear_flush</span>
 #define pmdp_huge_get_and_clear_notify pmdp_huge_get_and_clear
 #define set_pte_at_notify set_pte_at
 
<span class="p_header">diff --git a/include/linux/pfn_t.h b/include/linux/pfn_t.h</span>
<span class="p_header">index 9499481..10fbd80 100644</span>
<span class="p_header">--- a/include/linux/pfn_t.h</span>
<span class="p_header">+++ b/include/linux/pfn_t.h</span>
<span class="p_chunk">@@ -81,6 +81,13 @@</span> <span class="p_context"> static inline pmd_t pfn_t_pmd(pfn_t pfn, pgprot_t pgprot)</span>
 {
 	return pfn_pmd(pfn_t_to_pfn(pfn), pgprot);
 }
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD</span>
<span class="p_add">+static inline pud_t pfn_t_pud(pfn_t pfn, pgprot_t pgprot)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return pfn_pud(pfn_t_to_pfn(pfn), pgprot);</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
 #endif
 
 #ifdef __HAVE_ARCH_PTE_DEVMAP
<span class="p_chunk">@@ -97,5 +104,6 @@</span> <span class="p_context"> static inline bool pfn_t_devmap(pfn_t pfn)</span>
 }
 pte_t pte_mkdevmap(pte_t pte);
 pmd_t pmd_mkdevmap(pmd_t pmd);
<span class="p_add">+pud_t pud_mkdevmap(pud_t pud);</span>
 #endif
 #endif /* _LINUX_PFN_T_H_ */
<span class="p_header">diff --git a/mm/gup.c b/mm/gup.c</span>
<span class="p_header">index 7f1c4fb..a739b0e 100644</span>
<span class="p_header">--- a/mm/gup.c</span>
<span class="p_header">+++ b/mm/gup.c</span>
<span class="p_chunk">@@ -244,6 +244,13 @@</span> <span class="p_context"> struct page *follow_page_mask(struct vm_area_struct *vma,</span>
 			return page;
 		return no_page_table(vma, flags);
 	}
<span class="p_add">+	if (pud_devmap(*pud)) {</span>
<span class="p_add">+		ptl = pud_lock(mm, pud);</span>
<span class="p_add">+		page = follow_devmap_pud(vma, address, pud, flags);</span>
<span class="p_add">+		spin_unlock(ptl);</span>
<span class="p_add">+		if (page)</span>
<span class="p_add">+			return page;</span>
<span class="p_add">+	}</span>
 	if (unlikely(pud_bad(*pud)))
 		return no_page_table(vma, flags);
 
<span class="p_header">diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="p_header">index f5db20a..ea4e072 100644</span>
<span class="p_header">--- a/mm/huge_memory.c</span>
<span class="p_header">+++ b/mm/huge_memory.c</span>
<span class="p_chunk">@@ -1045,6 +1045,58 @@</span> <span class="p_context"> int vmf_insert_pfn_pmd(struct vm_area_struct *vma, unsigned long addr,</span>
 	return VM_FAULT_NOPAGE;
 }
 
<span class="p_add">+#ifdef CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD</span>
<span class="p_add">+static pud_t maybe_pud_mkwrite(pud_t pud, struct vm_area_struct *vma)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (likely(vma-&gt;vm_flags &amp; VM_WRITE))</span>
<span class="p_add">+		pud = pud_mkwrite(pud);</span>
<span class="p_add">+	return pud;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void insert_pfn_pud(struct vm_area_struct *vma, unsigned long addr,</span>
<span class="p_add">+		pud_t *pud, pfn_t pfn, pgprot_t prot, bool write)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct mm_struct *mm = vma-&gt;vm_mm;</span>
<span class="p_add">+	pud_t entry;</span>
<span class="p_add">+	spinlock_t *ptl;</span>
<span class="p_add">+</span>
<span class="p_add">+	ptl = pud_lock(mm, pud);</span>
<span class="p_add">+	entry = pud_mkhuge(pfn_t_pud(pfn, prot));</span>
<span class="p_add">+	if (pfn_t_devmap(pfn))</span>
<span class="p_add">+		entry = pud_mkdevmap(entry);</span>
<span class="p_add">+	if (write) {</span>
<span class="p_add">+		entry = pud_mkyoung(pud_mkdirty(entry));</span>
<span class="p_add">+		entry = maybe_pud_mkwrite(entry, vma);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	set_pud_at(mm, addr, pud, entry);</span>
<span class="p_add">+	update_mmu_cache_pud(vma, addr, pud);</span>
<span class="p_add">+	spin_unlock(ptl);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+int vmf_insert_pfn_pud(struct vm_area_struct *vma, unsigned long addr,</span>
<span class="p_add">+			pud_t *pud, pfn_t pfn, bool write)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pgprot_t pgprot = vma-&gt;vm_page_prot;</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If we had pud_special, we could avoid all these restrictions,</span>
<span class="p_add">+	 * but we need to be consistent with PTEs and architectures that</span>
<span class="p_add">+	 * can&#39;t support a &#39;special&#39; bit.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	BUG_ON(!(vma-&gt;vm_flags &amp; (VM_PFNMAP|VM_MIXEDMAP)));</span>
<span class="p_add">+	BUG_ON((vma-&gt;vm_flags &amp; (VM_PFNMAP|VM_MIXEDMAP)) ==</span>
<span class="p_add">+						(VM_PFNMAP|VM_MIXEDMAP));</span>
<span class="p_add">+	BUG_ON((vma-&gt;vm_flags &amp; VM_PFNMAP) &amp;&amp; is_cow_mapping(vma-&gt;vm_flags));</span>
<span class="p_add">+	BUG_ON(!pfn_t_devmap(pfn));</span>
<span class="p_add">+</span>
<span class="p_add">+	if (addr &lt; vma-&gt;vm_start || addr &gt;= vma-&gt;vm_end)</span>
<span class="p_add">+		return VM_FAULT_SIGBUS;</span>
<span class="p_add">+	if (track_pfn_insert(vma, &amp;pgprot, pfn))</span>
<span class="p_add">+		return VM_FAULT_SIGBUS;</span>
<span class="p_add">+	insert_pfn_pud(vma, addr, pud, pfn, pgprot, write);</span>
<span class="p_add">+	return VM_FAULT_NOPAGE;</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif /* CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD */</span>
<span class="p_add">+</span>
 static void touch_pmd(struct vm_area_struct *vma, unsigned long addr,
 		pmd_t *pmd)
 {
<span class="p_chunk">@@ -1171,6 +1223,123 @@</span> <span class="p_context"> out:</span>
 	return ret;
 }
 
<span class="p_add">+#ifdef CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD</span>
<span class="p_add">+static void touch_pud(struct vm_area_struct *vma, unsigned long addr,</span>
<span class="p_add">+		pud_t *pud)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pud_t _pud;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We should set the dirty bit only for FOLL_WRITE but for now</span>
<span class="p_add">+	 * the dirty bit in the pud is meaningless.  And if the dirty</span>
<span class="p_add">+	 * bit will become meaningful and we&#39;ll only set it with</span>
<span class="p_add">+	 * FOLL_WRITE, an atomic set_bit will be required on the pud to</span>
<span class="p_add">+	 * set the young bit, instead of the current set_pud_at.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	_pud = pud_mkyoung(pud_mkdirty(*pud));</span>
<span class="p_add">+	if (pudp_set_access_flags(vma, addr &amp; HPAGE_PUD_MASK,</span>
<span class="p_add">+				pud, _pud,  1))</span>
<span class="p_add">+		update_mmu_cache_pud(vma, addr, pud);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+struct page *follow_devmap_pud(struct vm_area_struct *vma, unsigned long addr,</span>
<span class="p_add">+		pud_t *pud, int flags)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long pfn = pud_pfn(*pud);</span>
<span class="p_add">+	struct mm_struct *mm = vma-&gt;vm_mm;</span>
<span class="p_add">+	struct dev_pagemap *pgmap;</span>
<span class="p_add">+	struct page *page;</span>
<span class="p_add">+</span>
<span class="p_add">+	assert_spin_locked(pud_lockptr(mm, pud));</span>
<span class="p_add">+</span>
<span class="p_add">+	if (flags &amp; FOLL_WRITE &amp;&amp; !pud_write(*pud))</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (pud_present(*pud) &amp;&amp; pud_devmap(*pud))</span>
<span class="p_add">+		/* pass */;</span>
<span class="p_add">+	else</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (flags &amp; FOLL_TOUCH)</span>
<span class="p_add">+		touch_pud(vma, addr, pud);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * device mapped pages can only be returned if the</span>
<span class="p_add">+	 * caller will manage the page reference count.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (!(flags &amp; FOLL_GET))</span>
<span class="p_add">+		return ERR_PTR(-EEXIST);</span>
<span class="p_add">+</span>
<span class="p_add">+	pfn += (addr &amp; ~PUD_MASK) &gt;&gt; PAGE_SHIFT;</span>
<span class="p_add">+	pgmap = get_dev_pagemap(pfn, NULL);</span>
<span class="p_add">+	if (!pgmap)</span>
<span class="p_add">+		return ERR_PTR(-EFAULT);</span>
<span class="p_add">+	page = pfn_to_page(pfn);</span>
<span class="p_add">+	get_page(page);</span>
<span class="p_add">+	put_dev_pagemap(pgmap);</span>
<span class="p_add">+</span>
<span class="p_add">+	return page;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+int copy_huge_pud(struct mm_struct *dst_mm, struct mm_struct *src_mm,</span>
<span class="p_add">+		  pud_t *dst_pud, pud_t *src_pud, unsigned long addr,</span>
<span class="p_add">+		  struct vm_area_struct *vma)</span>
<span class="p_add">+{</span>
<span class="p_add">+	spinlock_t *dst_ptl, *src_ptl;</span>
<span class="p_add">+	pud_t pud;</span>
<span class="p_add">+	int ret;</span>
<span class="p_add">+</span>
<span class="p_add">+	dst_ptl = pud_lock(dst_mm, dst_pud);</span>
<span class="p_add">+	src_ptl = pud_lockptr(src_mm, src_pud);</span>
<span class="p_add">+	spin_lock_nested(src_ptl, SINGLE_DEPTH_NESTING);</span>
<span class="p_add">+</span>
<span class="p_add">+	ret = -EAGAIN;</span>
<span class="p_add">+	pud = *src_pud;</span>
<span class="p_add">+	if (unlikely(!pud_trans_huge(pud) &amp;&amp; !pud_devmap(pud)))</span>
<span class="p_add">+		goto out_unlock;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * When page table lock is held, the huge zero pud should not be</span>
<span class="p_add">+	 * under splitting since we don&#39;t split the page itself, only pud to</span>
<span class="p_add">+	 * a page table.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (is_huge_zero_pud(pud)) {</span>
<span class="p_add">+		/* No huge zero pud yet */</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	pudp_set_wrprotect(src_mm, addr, src_pud);</span>
<span class="p_add">+	pud = pud_mkold(pud_wrprotect(pud));</span>
<span class="p_add">+	set_pud_at(dst_mm, addr, dst_pud, pud);</span>
<span class="p_add">+</span>
<span class="p_add">+	ret = 0;</span>
<span class="p_add">+out_unlock:</span>
<span class="p_add">+	spin_unlock(src_ptl);</span>
<span class="p_add">+	spin_unlock(dst_ptl);</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void huge_pud_set_accessed(struct mm_struct *mm, struct vm_area_struct *vma,</span>
<span class="p_add">+			   unsigned long address, pud_t *pud, pud_t orig_pud,</span>
<span class="p_add">+			   int dirty)</span>
<span class="p_add">+{</span>
<span class="p_add">+	spinlock_t *ptl;</span>
<span class="p_add">+	pud_t entry;</span>
<span class="p_add">+	unsigned long haddr;</span>
<span class="p_add">+</span>
<span class="p_add">+	ptl = pud_lock(mm, pud);</span>
<span class="p_add">+	if (unlikely(!pud_same(*pud, orig_pud)))</span>
<span class="p_add">+		goto unlock;</span>
<span class="p_add">+</span>
<span class="p_add">+	entry = pud_mkyoung(orig_pud);</span>
<span class="p_add">+	haddr = address &amp; HPAGE_PMD_MASK;</span>
<span class="p_add">+	if (pudp_set_access_flags(vma, haddr, pud, entry, dirty))</span>
<span class="p_add">+		update_mmu_cache_pud(vma, address, pud);</span>
<span class="p_add">+</span>
<span class="p_add">+unlock:</span>
<span class="p_add">+	spin_unlock(ptl);</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif /* CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD */</span>
<span class="p_add">+</span>
 void huge_pmd_set_accessed(struct mm_struct *mm,
 			   struct vm_area_struct *vma,
 			   unsigned long address,
<span class="p_chunk">@@ -1847,6 +2016,22 @@</span> <span class="p_context"> spinlock_t *__pmd_trans_huge_lock(pmd_t *pmd, struct vm_area_struct *vma)</span>
 	return NULL;
 }
 
<span class="p_add">+/*</span>
<span class="p_add">+ * Returns true if a given pud maps a thp, false otherwise.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Note that if it returns true, this routine returns without unlocking page</span>
<span class="p_add">+ * table lock. So callers must unlock it.</span>
<span class="p_add">+ */</span>
<span class="p_add">+spinlock_t *__pud_trans_huge_lock(pud_t *pud, struct vm_area_struct *vma)</span>
<span class="p_add">+{</span>
<span class="p_add">+	spinlock_t *ptl;</span>
<span class="p_add">+	ptl = pud_lock(vma-&gt;vm_mm, pud);</span>
<span class="p_add">+	if (likely(pud_trans_huge(*pud) || pud_devmap(*pud)))</span>
<span class="p_add">+		return ptl;</span>
<span class="p_add">+	spin_unlock(ptl);</span>
<span class="p_add">+	return NULL;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 #define VM_NO_THP (VM_SPECIAL | VM_HUGETLB | VM_SHARED | VM_MAYSHARE)
 
 int hugepage_madvise(struct vm_area_struct *vma,
<span class="p_chunk">@@ -2917,6 +3102,67 @@</span> <span class="p_context"> static int khugepaged(void *none)</span>
 	return 0;
 }
 
<span class="p_add">+#ifdef CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD</span>
<span class="p_add">+int zap_huge_pud(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
<span class="p_add">+		 pud_t *pud, unsigned long addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pud_t orig_pud;</span>
<span class="p_add">+	spinlock_t *ptl;</span>
<span class="p_add">+</span>
<span class="p_add">+	ptl = __pud_trans_huge_lock(pud, vma);</span>
<span class="p_add">+	if (!ptl)</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * For architectures like ppc64 we look at deposited pgtable</span>
<span class="p_add">+	 * when calling pudp_huge_get_and_clear. So do the</span>
<span class="p_add">+	 * pgtable_trans_huge_withdraw after finishing pudp related</span>
<span class="p_add">+	 * operations.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	orig_pud = pudp_huge_get_and_clear_full(tlb-&gt;mm, addr, pud,</span>
<span class="p_add">+			tlb-&gt;fullmm);</span>
<span class="p_add">+	tlb_remove_pud_tlb_entry(tlb, pud, addr);</span>
<span class="p_add">+	if (vma_is_dax(vma)) {</span>
<span class="p_add">+		spin_unlock(ptl);</span>
<span class="p_add">+		/* No zero page support yet */</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		/* No support for anonymous PUD pages yet */</span>
<span class="p_add">+		BUG();</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return 1;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void __split_huge_pud_locked(struct vm_area_struct *vma, pud_t *pud,</span>
<span class="p_add">+		unsigned long haddr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	VM_BUG_ON(haddr &amp; ~HPAGE_PUD_MASK);</span>
<span class="p_add">+	VM_BUG_ON_VMA(vma-&gt;vm_start &gt; haddr, vma);</span>
<span class="p_add">+	VM_BUG_ON_VMA(vma-&gt;vm_end &lt; haddr + HPAGE_PUD_SIZE, vma);</span>
<span class="p_add">+	VM_BUG_ON(!pud_trans_huge(*pud) &amp;&amp; !pud_devmap(*pud));</span>
<span class="p_add">+</span>
<span class="p_add">+	count_vm_event(THP_SPLIT_PMD);</span>
<span class="p_add">+</span>
<span class="p_add">+	pudp_huge_clear_flush_notify(vma, haddr, pud);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void __split_huge_pud(struct vm_area_struct *vma, pud_t *pud,</span>
<span class="p_add">+		unsigned long address)</span>
<span class="p_add">+{</span>
<span class="p_add">+	spinlock_t *ptl;</span>
<span class="p_add">+	struct mm_struct *mm = vma-&gt;vm_mm;</span>
<span class="p_add">+	unsigned long haddr = address &amp; HPAGE_PUD_MASK;</span>
<span class="p_add">+</span>
<span class="p_add">+	mmu_notifier_invalidate_range_start(mm, haddr, haddr + HPAGE_PUD_SIZE);</span>
<span class="p_add">+	ptl = pud_lock(mm, pud);</span>
<span class="p_add">+	if (unlikely(!pud_trans_huge(*pud) &amp;&amp; !pud_devmap(*pud)))</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+	__split_huge_pud_locked(vma, pud, haddr);</span>
<span class="p_add">+</span>
<span class="p_add">+out:</span>
<span class="p_add">+	spin_unlock(ptl);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_end(mm, haddr, haddr + HPAGE_PUD_SIZE);</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif /* CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD */</span>
<span class="p_add">+</span>
 static void __split_huge_zero_page_pmd(struct vm_area_struct *vma,
 		unsigned long haddr, pmd_t *pmd)
 {
<span class="p_header">diff --git a/mm/memory.c b/mm/memory.c</span>
<span class="p_header">index 3100381..31075a1 100644</span>
<span class="p_header">--- a/mm/memory.c</span>
<span class="p_header">+++ b/mm/memory.c</span>
<span class="p_chunk">@@ -949,7 +949,7 @@</span> <span class="p_context"> static inline int copy_pmd_range(struct mm_struct *dst_mm, struct mm_struct *src</span>
 		next = pmd_addr_end(addr, end);
 		if (pmd_trans_huge(*src_pmd) || pmd_devmap(*src_pmd)) {
 			int err;
<span class="p_del">-			VM_BUG_ON(next-addr != HPAGE_PMD_SIZE);</span>
<span class="p_add">+			VM_BUG_ON_VMA(next-addr != HPAGE_PMD_SIZE, vma);</span>
 			err = copy_huge_pmd(dst_mm, src_mm,
 					    dst_pmd, src_pmd, addr, vma);
 			if (err == -ENOMEM)
<span class="p_chunk">@@ -980,6 +980,17 @@</span> <span class="p_context"> static inline int copy_pud_range(struct mm_struct *dst_mm, struct mm_struct *src</span>
 	src_pud = pud_offset(src_pgd, addr);
 	do {
 		next = pud_addr_end(addr, end);
<span class="p_add">+		if (pud_trans_huge(*src_pud) || pud_devmap(*src_pud)) {</span>
<span class="p_add">+			int err;</span>
<span class="p_add">+			VM_BUG_ON_VMA(next-addr != HPAGE_PUD_SIZE, vma);</span>
<span class="p_add">+			err = copy_huge_pud(dst_mm, src_mm,</span>
<span class="p_add">+					    dst_pud, src_pud, addr, vma);</span>
<span class="p_add">+			if (err == -ENOMEM)</span>
<span class="p_add">+				return -ENOMEM;</span>
<span class="p_add">+			if (!err)</span>
<span class="p_add">+				continue;</span>
<span class="p_add">+			/* fall through */</span>
<span class="p_add">+		}</span>
 		if (pud_none_or_clear_bad(src_pud))
 			continue;
 		if (copy_pmd_range(dst_mm, src_mm, dst_pud, src_pud,
<span class="p_chunk">@@ -1216,9 +1227,19 @@</span> <span class="p_context"> static inline unsigned long zap_pud_range(struct mmu_gather *tlb,</span>
 	pud = pud_offset(pgd, addr);
 	do {
 		next = pud_addr_end(addr, end);
<span class="p_add">+		if (pud_trans_huge(*pud) || pud_devmap(*pud)) {</span>
<span class="p_add">+			if (next - addr != HPAGE_PUD_SIZE) {</span>
<span class="p_add">+				VM_BUG_ON_VMA(!rwsem_is_locked(&amp;tlb-&gt;mm-&gt;mmap_sem), vma);</span>
<span class="p_add">+				split_huge_pud(vma, pud, addr);</span>
<span class="p_add">+			} else if (zap_huge_pud(tlb, vma, pud, addr))</span>
<span class="p_add">+				goto next;</span>
<span class="p_add">+			/* fall through */</span>
<span class="p_add">+		}</span>
 		if (pud_none_or_clear_bad(pud))
 			continue;
 		next = zap_pmd_range(tlb, vma, pud, addr, next, details);
<span class="p_add">+next:</span>
<span class="p_add">+		cond_resched();</span>
 	} while (pud++, addr = next, addr != end);
 
 	return addr;
<span class="p_chunk">@@ -3298,6 +3319,49 @@</span> <span class="p_context"> static int wp_huge_pmd(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 	return VM_FAULT_FALLBACK;
 }
 
<span class="p_add">+static int create_huge_pud(struct mm_struct *mm, struct vm_area_struct *vma,</span>
<span class="p_add">+			unsigned long address, pud_t *pud, unsigned int flags)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifdef CONFIG_TRANSPARENT_HUGEPAGE</span>
<span class="p_add">+	struct vm_fault vmf = {</span>
<span class="p_add">+		.flags = flags | FAULT_FLAG_SIZE_PUD,</span>
<span class="p_add">+		.gfp_mask = __get_fault_gfp_mask(vma),</span>
<span class="p_add">+		.pgoff = linear_page_index(vma, address &amp; HPAGE_PUD_MASK),</span>
<span class="p_add">+		.virtual_address = (void __user *)address,</span>
<span class="p_add">+	};</span>
<span class="p_add">+	vmf.pud = pud;	/* GCC 4.5 and earlier do not allow initialisation */</span>
<span class="p_add">+</span>
<span class="p_add">+	/* No support for anonymous transparent PUD pages yet */</span>
<span class="p_add">+	if (vma_is_anonymous(vma))</span>
<span class="p_add">+		return VM_FAULT_FALLBACK;</span>
<span class="p_add">+	if (vma-&gt;vm_ops-&gt;huge_fault)</span>
<span class="p_add">+		return vma-&gt;vm_ops-&gt;huge_fault(vma, &amp;vmf);</span>
<span class="p_add">+#endif /* CONFIG_TRANSPARENT_HUGEPAGE */</span>
<span class="p_add">+	return VM_FAULT_FALLBACK;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int wp_huge_pud(struct mm_struct *mm, struct vm_area_struct *vma,</span>
<span class="p_add">+			unsigned long address, pud_t *pud, pud_t orig_pud,</span>
<span class="p_add">+			unsigned int flags)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifdef CONFIG_TRANSPARENT_HUGEPAGE</span>
<span class="p_add">+	struct vm_fault vmf = {</span>
<span class="p_add">+		.flags = flags | FAULT_FLAG_SIZE_PUD,</span>
<span class="p_add">+		.gfp_mask = __get_fault_gfp_mask(vma),</span>
<span class="p_add">+		.pgoff = linear_page_index(vma, address &amp; HPAGE_PUD_MASK),</span>
<span class="p_add">+		.virtual_address = (void __user *)address,</span>
<span class="p_add">+	};</span>
<span class="p_add">+	vmf.pud = pud;	/* GCC 4.5 and earlier do not allow initialisation */</span>
<span class="p_add">+</span>
<span class="p_add">+	/* No support for anonymous transparent PUD pages yet */</span>
<span class="p_add">+	if (vma_is_anonymous(vma))</span>
<span class="p_add">+		return VM_FAULT_FALLBACK;</span>
<span class="p_add">+	if (vma-&gt;vm_ops-&gt;huge_fault)</span>
<span class="p_add">+		return vma-&gt;vm_ops-&gt;huge_fault(vma, &amp;vmf);</span>
<span class="p_add">+#endif /* CONFIG_TRANSPARENT_HUGEPAGE */</span>
<span class="p_add">+	return VM_FAULT_FALLBACK;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /*
  * These routines also need to handle stuff like marking pages dirty
  * and/or accessed for architectures that don&#39;t do it in hardware (most
<span class="p_chunk">@@ -3401,6 +3465,32 @@</span> <span class="p_context"> static int __handle_mm_fault(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 	pud = pud_alloc(mm, pgd, address);
 	if (!pud)
 		return VM_FAULT_OOM;
<span class="p_add">+	if (pud_none(*pud) &amp;&amp; transparent_hugepage_enabled(vma)) {</span>
<span class="p_add">+		int ret = create_huge_pud(mm, vma, address, pud, flags);</span>
<span class="p_add">+		if (!(ret &amp; VM_FAULT_FALLBACK))</span>
<span class="p_add">+			return ret;</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		pud_t orig_pud = *pud;</span>
<span class="p_add">+		int ret;</span>
<span class="p_add">+</span>
<span class="p_add">+		barrier();</span>
<span class="p_add">+		if (pud_trans_huge(orig_pud) || pud_devmap(orig_pud)) {</span>
<span class="p_add">+			unsigned int dirty = flags &amp; FAULT_FLAG_WRITE;</span>
<span class="p_add">+</span>
<span class="p_add">+			/* NUMA case for anonymous PUDs would go here */</span>
<span class="p_add">+</span>
<span class="p_add">+			if (dirty &amp;&amp; !pud_write(orig_pud)) {</span>
<span class="p_add">+				ret = wp_huge_pud(mm, vma, address, pud,</span>
<span class="p_add">+							orig_pud, flags);</span>
<span class="p_add">+				if (!(ret &amp; VM_FAULT_FALLBACK))</span>
<span class="p_add">+					return ret;</span>
<span class="p_add">+			} else {</span>
<span class="p_add">+				huge_pud_set_accessed(mm, vma, address, pud,</span>
<span class="p_add">+						      orig_pud, dirty);</span>
<span class="p_add">+				return 0;</span>
<span class="p_add">+			}</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
 	pmd = pmd_alloc(mm, pud, address);
 	if (!pmd)
 		return VM_FAULT_OOM;
<span class="p_chunk">@@ -3538,13 +3628,14 @@</span> <span class="p_context"> int __pud_alloc(struct mm_struct *mm, pgd_t *pgd, unsigned long address)</span>
  */
 int __pmd_alloc(struct mm_struct *mm, pud_t *pud, unsigned long address)
 {
<span class="p_add">+	spinlock_t *ptl;</span>
 	pmd_t *new = pmd_alloc_one(mm, address);
 	if (!new)
 		return -ENOMEM;
 
 	smp_wmb(); /* See comment in __pte_alloc */
 
<span class="p_del">-	spin_lock(&amp;mm-&gt;page_table_lock);</span>
<span class="p_add">+	ptl = pud_lock(mm, pud);</span>
 #ifndef __ARCH_HAS_4LEVEL_HACK
 	if (!pud_present(*pud)) {
 		mm_inc_nr_pmds(mm);
<span class="p_chunk">@@ -3558,7 +3649,7 @@</span> <span class="p_context"> int __pmd_alloc(struct mm_struct *mm, pud_t *pud, unsigned long address)</span>
 	} else /* Another has populated it */
 		pmd_free(mm, new);
 #endif /* __ARCH_HAS_4LEVEL_HACK */
<span class="p_del">-	spin_unlock(&amp;mm-&gt;page_table_lock);</span>
<span class="p_add">+	spin_unlock(ptl);</span>
 	return 0;
 }
 #endif /* __PAGETABLE_PMD_FOLDED */
<span class="p_header">diff --git a/mm/pagewalk.c b/mm/pagewalk.c</span>
<span class="p_header">index 2072444..d6c2e6b 100644</span>
<span class="p_header">--- a/mm/pagewalk.c</span>
<span class="p_header">+++ b/mm/pagewalk.c</span>
<span class="p_chunk">@@ -78,14 +78,31 @@</span> <span class="p_context"> static int walk_pud_range(pgd_t *pgd, unsigned long addr, unsigned long end,</span>
 
 	pud = pud_offset(pgd, addr);
 	do {
<span class="p_add">+ again:</span>
 		next = pud_addr_end(addr, end);
<span class="p_del">-		if (pud_none_or_clear_bad(pud)) {</span>
<span class="p_add">+		if (pud_none(*pud) || !walk-&gt;vma) {</span>
 			if (walk-&gt;pte_hole)
 				err = walk-&gt;pte_hole(addr, next, walk);
 			if (err)
 				break;
 			continue;
 		}
<span class="p_add">+</span>
<span class="p_add">+		if (walk-&gt;pud_entry) {</span>
<span class="p_add">+			spinlock_t *ptl = pud_trans_huge_lock(pud, walk-&gt;vma);</span>
<span class="p_add">+			if (ptl) {</span>
<span class="p_add">+				err = walk-&gt;pud_entry(pud, addr, next, walk);</span>
<span class="p_add">+				spin_unlock(ptl);</span>
<span class="p_add">+				if (err)</span>
<span class="p_add">+					break;</span>
<span class="p_add">+				continue;</span>
<span class="p_add">+			}</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		split_huge_pud(walk-&gt;vma, pud, addr);</span>
<span class="p_add">+		if (pud_none(*pud))</span>
<span class="p_add">+			goto again;</span>
<span class="p_add">+</span>
 		if (walk-&gt;pmd_entry || walk-&gt;pte_entry)
 			err = walk_pmd_range(pud, addr, next, walk);
 		if (err)
<span class="p_header">diff --git a/mm/pgtable-generic.c b/mm/pgtable-generic.c</span>
<span class="p_header">index 71c5f91..d2207a3 100644</span>
<span class="p_header">--- a/mm/pgtable-generic.c</span>
<span class="p_header">+++ b/mm/pgtable-generic.c</span>
<span class="p_chunk">@@ -123,6 +123,19 @@</span> <span class="p_context"> pmd_t pmdp_huge_clear_flush(struct vm_area_struct *vma, unsigned long address,</span>
 	flush_pmd_tlb_range(vma, address, address + HPAGE_PMD_SIZE);
 	return pmd;
 }
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD</span>
<span class="p_add">+pud_t pudp_huge_clear_flush(struct vm_area_struct *vma, unsigned long address,</span>
<span class="p_add">+			    pud_t *pudp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pud_t pud;</span>
<span class="p_add">+	VM_BUG_ON(address &amp; ~HPAGE_PUD_MASK);</span>
<span class="p_add">+	VM_BUG_ON(!pud_trans_huge(*pudp) &amp;&amp; !pud_devmap(*pudp));</span>
<span class="p_add">+	pud = pudp_huge_get_and_clear(vma-&gt;vm_mm, address, pudp);</span>
<span class="p_add">+	flush_pud_tlb_range(vma, address, address + HPAGE_PUD_SIZE);</span>
<span class="p_add">+	return pud;</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
 #endif
 
 #ifndef __HAVE_ARCH_PGTABLE_DEPOSIT

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



