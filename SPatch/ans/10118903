
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[V163,39/51] x86/pti: Put the LDT in its own PGD if PTI is on - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [V163,39/51] x86/pti: Put the LDT in its own PGD if PTI is on</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=107">Thomas Gleixner</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Dec. 18, 2017, 11:42 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20171218115256.826191724@linutronix.de&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10118903/mbox/"
   >mbox</a>
|
   <a href="/patch/10118903/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10118903/">/patch/10118903/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	6B37F60327 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 18 Dec 2017 11:59:06 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 570BD2040D
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 18 Dec 2017 11:59:06 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 4B81528F18; Mon, 18 Dec 2017 11:59:06 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 343612040D
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 18 Dec 2017 11:59:05 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1758613AbdLRL6r (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 18 Dec 2017 06:58:47 -0500
Received: from Galois.linutronix.de ([146.0.238.70]:45373 &quot;EHLO
	Galois.linutronix.de&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S932677AbdLRLyd (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 18 Dec 2017 06:54:33 -0500
Received: from localhost ([127.0.0.1] helo=[127.0.1.1])
	by Galois.linutronix.de with esmtp (Exim 4.80)
	(envelope-from &lt;tglx@linutronix.de&gt;)
	id 1eQtyV-0000uN-OK; Mon, 18 Dec 2017 12:52:51 +0100
Message-Id: &lt;20171218115256.826191724@linutronix.de&gt;
User-Agent: quilt/0.63-1
Date: Mon, 18 Dec 2017 12:42:54 +0100
From: Thomas Gleixner &lt;tglx@linutronix.de&gt;
To: LKML &lt;linux-kernel@vger.kernel.org&gt;
Cc: x86@kernel.org, Linus Torvalds &lt;torvalds@linux-foundation.org&gt;,
	Andy Lutomirsky &lt;luto@kernel.org&gt;, Peter Zijlstra &lt;peterz@infradead.org&gt;,
	Dave Hansen &lt;dave.hansen@intel.com&gt;, Borislav Petkov &lt;bpetkov@suse.de&gt;,
	Greg KH &lt;gregkh@linuxfoundation.org&gt;, keescook@google.com,
	hughd@google.com, Brian Gerst &lt;brgerst@gmail.com&gt;,
	Josh Poimboeuf &lt;jpoimboe@redhat.com&gt;,
	Denys Vlasenko &lt;dvlasenk@redhat.com&gt;, Rik van Riel &lt;riel@redhat.com&gt;,
	Boris Ostrovsky &lt;boris.ostrovsky@oracle.com&gt;,
	Juergen Gross &lt;jgross@suse.com&gt;, David Laight &lt;David.Laight@aculab.com&gt;,
	Eduardo Valentin &lt;eduval@amazon.com&gt;, aliguori@amazon.com,
	Will Deacon &lt;will.deacon@arm.com&gt;, daniel.gruss@iaik.tugraz.at,
	Kees Cook &lt;keescook@chromium.org&gt;, Borislav Petkov &lt;bp@alien8.de&gt;,
	&quot;Kirill A. Shutemov&quot; &lt;kirill@shutemov.name&gt;
Subject: [patch V163 39/51] x86/pti: Put the LDT in its own PGD if PTI is on
References: &lt;20171218114215.239543034@linutronix.de&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=ISO-8859-15
Content-Disposition: inline;
	filename=x86-pti--Put_the_LDT_in_its_own_PGD_if_PTI_is_on.patch
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=107">Thomas Gleixner</a> - Dec. 18, 2017, 11:42 a.m.</div>
<pre class="content">
<span class="from">From: Andy Lutomirski &lt;luto@kernel.org&gt;</span>

With PTI enabled, the LDT must be mapped in the usermode tables somewhere.
The LDT is per process, i.e. per mm.

An earlier approach mapped the LDT on context switch into a fixmap area,
but that&#39;s a big overhead and exhausted the fixmap space when NR_CPUS got
big.

Take advantage of the fact that there is an address space hole which
provides a completely unused pgd. Use this pgd to manage per-mm LDT
mappings.

This has a down side: the LDT isn&#39;t (currently) randomized, and an attack
that can write the LDT is instant root due to call gates (thanks, AMD, for
leaving call gates in AMD64 but designing them wrong so they&#39;re only useful
for exploits).  This can be mitigated by making the LDT read-only or
randomizing the mapping, either of which is strightforward on top of this
patch.

This will significantly slow down LDT users, but that shouldn&#39;t matter for
important workloads -- the LDT is only used by DOSEMU(2), Wine, and very
old libc implementations.

[ tglx: Decrapified it ]
<span class="signed-off-by">
Signed-off-by: Andy Lutomirski &lt;luto@kernel.org&gt;</span>
<span class="signed-off-by">Signed-off-by: Thomas Gleixner &lt;tglx@linutronix.de&gt;</span>
Cc: Kees Cook &lt;keescook@chromium.org&gt;
Cc: Peter Zijlstra &lt;peterz@infradead.org&gt;
Cc: Brian Gerst &lt;brgerst@gmail.com&gt;
Cc: Dave Hansen &lt;dave.hansen@intel.com&gt;
Cc: David Laight &lt;David.Laight@aculab.com&gt;
Cc: Borislav Petkov &lt;bp@alien8.de&gt;
Cc: &quot;Kirill A. Shutemov&quot; &lt;kirill@shutemov.name&gt;

---
 Documentation/x86/x86_64/mm.txt         |    3 
 arch/x86/include/asm/mmu_context.h      |   59 ++++++++++++-
 arch/x86/include/asm/pgtable_64_types.h |    4 
 arch/x86/include/asm/processor.h        |   23 +++--
 arch/x86/kernel/ldt.c                   |  139 +++++++++++++++++++++++++++++++-
 arch/x86/mm/dump_pagetables.c           |   12 ++
 6 files changed, 223 insertions(+), 17 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - Dec. 18, 2017, 1:46 p.m.</div>
<pre class="content">
On Mon, Dec 18, 2017 at 12:42:54PM +0100, Thomas Gleixner wrote:
<span class="quote">
&gt; This will significantly slow down LDT users, but that shouldn&#39;t matter for</span>
<span class="quote">&gt; important workloads -- the LDT is only used by DOSEMU(2), Wine, and very</span>
<span class="quote">&gt; old libc implementations.</span>
<span class="quote">

&gt; +/*</span>
<span class="quote">&gt; + * If PTI is enabled, this maps the LDT into the kernelmode and</span>
<span class="quote">&gt; + * usermode tables for the given mm.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * There is no corresponding unmap function.  Even if the LDT is freed, we</span>
<span class="quote">&gt; + * leave the PTEs around until the slot is reused or the mm is destroyed.</span>
<span class="quote">&gt; + * This is harmless: the LDT is always in ordinary memory, and no one will</span>
<span class="quote">&gt; + * access the freed slot.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * If we wanted to unmap freed LDTs, we&#39;d also need to do a flush to make</span>
<span class="quote">&gt; + * it useful, and the flush would slow down modify_ldt().</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +static int</span>
<span class="quote">&gt; +map_ldt_struct(struct mm_struct *mm, struct ldt_struct *ldt, int slot)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="quote">&gt; +	bool is_vmalloc, had_top_level_entry;</span>
<span class="quote">&gt; +	unsigned long va;</span>
<span class="quote">&gt; +	spinlock_t *ptl;</span>
<span class="quote">&gt; +	pgd_t *pgd;</span>
<span class="quote">&gt; +	int i;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (!static_cpu_has(X86_FEATURE_PTI))</span>
<span class="quote">&gt; +		return 0;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * Any given ldt_struct should have map_ldt_struct() called at most</span>
<span class="quote">&gt; +	 * once.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	WARN_ON(ldt-&gt;slot != -1);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * Did we already have the top level entry allocated?  We can&#39;t</span>
<span class="quote">&gt; +	 * use pgd_none() for this because it doens&#39;t do anything on</span>
<span class="quote">&gt; +	 * 4-level page table kernels.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	pgd = pgd_offset(mm, LDT_BASE_ADDR);</span>
<span class="quote">&gt; +	had_top_level_entry = (pgd-&gt;pgd != 0);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	is_vmalloc = is_vmalloc_addr(ldt-&gt;entries);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	for (i = 0; i * PAGE_SIZE &lt; ldt-&gt;nr_entries * LDT_ENTRY_SIZE; i++) {</span>
<span class="quote">&gt; +		unsigned long offset = i &lt;&lt; PAGE_SHIFT;</span>
<span class="quote">&gt; +		const void *src = (char *)ldt-&gt;entries + offset;</span>
<span class="quote">&gt; +		unsigned long pfn;</span>
<span class="quote">&gt; +		pte_t pte, *ptep;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		va = (unsigned long)ldt_slot_va(slot) + offset;</span>
<span class="quote">&gt; +		pfn = is_vmalloc ? vmalloc_to_pfn(src) :</span>
<span class="quote">&gt; +			page_to_pfn(virt_to_page(src));</span>
<span class="quote">&gt; +		/*</span>
<span class="quote">&gt; +		 * Treat the PTI LDT range as a *userspace* range.</span>
<span class="quote">&gt; +		 * get_locked_pte() will allocate all needed pagetables</span>
<span class="quote">&gt; +		 * and account for them in this mm.</span>
<span class="quote">&gt; +		 */</span>
<span class="quote">&gt; +		ptep = get_locked_pte(mm, va, &amp;ptl);</span>
<span class="quote">&gt; +		if (!ptep)</span>
<span class="quote">&gt; +			return -ENOMEM;</span>
<span class="quote">&gt; +		pte = pfn_pte(pfn, __pgprot(__PAGE_KERNEL &amp; ~_PAGE_GLOBAL));</span>
<span class="quote">&gt; +		set_pte_at(mm, va, ptep, pte);</span>
<span class="quote">&gt; +		pte_unmap_unlock(ptep, ptl);</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (mm-&gt;context.ldt) {</span>
<span class="quote">&gt; +		/*</span>
<span class="quote">&gt; +		 * We already had an LDT.  The top-level entry should already</span>
<span class="quote">&gt; +		 * have been allocated and synchronized with the usermode</span>
<span class="quote">&gt; +		 * tables.</span>
<span class="quote">&gt; +		 */</span>
<span class="quote">&gt; +		WARN_ON(!had_top_level_entry);</span>
<span class="quote">&gt; +		if (static_cpu_has(X86_FEATURE_PTI))</span>
<span class="quote">&gt; +			WARN_ON(!kernel_to_user_pgdp(pgd)-&gt;pgd);</span>
<span class="quote">&gt; +	} else {</span>
<span class="quote">&gt; +		/*</span>
<span class="quote">&gt; +		 * This is the first time we&#39;re mapping an LDT for this process.</span>
<span class="quote">&gt; +		 * Sync the pgd to the usermode tables.</span>
<span class="quote">&gt; +		 */</span>
<span class="quote">&gt; +		WARN_ON(had_top_level_entry);</span>
<span class="quote">&gt; +		if (static_cpu_has(X86_FEATURE_PTI)) {</span>
<span class="quote">&gt; +			WARN_ON(kernel_to_user_pgdp(pgd)-&gt;pgd);</span>
<span class="quote">&gt; +			set_pgd(kernel_to_user_pgdp(pgd), *pgd);</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	va = (unsigned long)ldt_slot_va(slot);</span>
<span class="quote">&gt; +	flush_tlb_mm_range(mm, va, va + LDT_SLOT_STRIDE, 0);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	ldt-&gt;slot = slot;</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; +	return 0;</span>
<span class="quote">&gt; +}</span>

So if I understand this right, we need to remap+tlb-flush every time we
flip LDTs because this still uses the old vmalloc-a-new-ldt thing?

Should we base this on top of the ldt_mapping stuff tglx did? Such that
we keep the 2 arrays of pages immutable and don&#39;t need no tlb flushes?
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">--- a/Documentation/x86/x86_64/mm.txt</span>
<span class="p_header">+++ b/Documentation/x86/x86_64/mm.txt</span>
<span class="p_chunk">@@ -12,6 +12,7 @@</span> <span class="p_context"> ffffea0000000000 - ffffeaffffffffff (=40</span>
 ... unused hole ...
 ffffec0000000000 - fffffbffffffffff (=44 bits) kasan shadow memory (16TB)
 ... unused hole ...
<span class="p_add">+fffffe8000000000 - fffffeffffffffff (=39 bits) LDT remap for PTI</span>
 ffffff0000000000 - ffffff7fffffffff (=39 bits) %esp fixup stacks
 ... unused hole ...
 ffffffef00000000 - fffffffeffffffff (=64 GB) EFI region mapping space
<span class="p_chunk">@@ -28,7 +29,7 @@</span> <span class="p_context"> ffffffffffe00000 - ffffffffffffffff (=2</span>
 hole caused by [56:63] sign extension
 ff00000000000000 - ff0fffffffffffff (=52 bits) guard hole, reserved for hypervisor
 ff10000000000000 - ff8fffffffffffff (=55 bits) direct mapping of all phys. memory
<span class="p_del">-ff90000000000000 - ff9fffffffffffff (=52 bits) hole</span>
<span class="p_add">+ff90000000000000 - ff9fffffffffffff (=52 bits) LDT remap for PTI</span>
 ffa0000000000000 - ffd1ffffffffffff (=54 bits) vmalloc/ioremap space (12800 TB)
 ffd2000000000000 - ffd3ffffffffffff (=49 bits) hole
 ffd4000000000000 - ffd5ffffffffffff (=49 bits) virtual memory map (512TB)
<span class="p_header">--- a/arch/x86/include/asm/mmu_context.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/mmu_context.h</span>
<span class="p_chunk">@@ -50,10 +50,33 @@</span> <span class="p_context"> struct ldt_struct {</span>
 	 * call gates.  On native, we could merge the ldt_struct and LDT
 	 * allocations, but it&#39;s not worth trying to optimize.
 	 */
<span class="p_del">-	struct desc_struct *entries;</span>
<span class="p_del">-	unsigned int nr_entries;</span>
<span class="p_add">+	struct desc_struct	*entries;</span>
<span class="p_add">+	unsigned int		nr_entries;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If PTI is in use, then the entries array is not mapped while we&#39;re</span>
<span class="p_add">+	 * in user mode.  The whole array will be aliased at the addressed</span>
<span class="p_add">+	 * given by ldt_slot_va(slot).  We use two slots so that we can allocate</span>
<span class="p_add">+	 * and map, and enable a new LDT without invalidating the mapping</span>
<span class="p_add">+	 * of an older, still-in-use LDT.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * slot will be -1 if this LDT doesn&#39;t have an alias mapping.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	int			slot;</span>
 };
 
<span class="p_add">+/* This is a multiple of PAGE_SIZE. */</span>
<span class="p_add">+#define LDT_SLOT_STRIDE (LDT_ENTRIES * LDT_ENTRY_SIZE)</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void *ldt_slot_va(int slot)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifdef CONFIG_X86_64</span>
<span class="p_add">+	return (void *)(LDT_BASE_ADDR + LDT_SLOT_STRIDE * slot);</span>
<span class="p_add">+#else</span>
<span class="p_add">+	BUG();</span>
<span class="p_add">+#endif</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /*
  * Used for LDT copy/destruction.
  */
<span class="p_chunk">@@ -64,6 +87,7 @@</span> <span class="p_context"> static inline void init_new_context_ldt(</span>
 }
 int ldt_dup_context(struct mm_struct *oldmm, struct mm_struct *mm);
 void destroy_context_ldt(struct mm_struct *mm);
<span class="p_add">+void ldt_arch_exit_mmap(struct mm_struct *mm);</span>
 #else	/* CONFIG_MODIFY_LDT_SYSCALL */
 static inline void init_new_context_ldt(struct mm_struct *mm) { }
 static inline int ldt_dup_context(struct mm_struct *oldmm,
<span class="p_chunk">@@ -71,7 +95,8 @@</span> <span class="p_context"> static inline int ldt_dup_context(struct</span>
 {
 	return 0;
 }
<span class="p_del">-static inline void destroy_context_ldt(struct mm_struct *mm) {}</span>
<span class="p_add">+static inline void destroy_context_ldt(struct mm_struct *mm) { }</span>
<span class="p_add">+static inline void ldt_arch_exit_mmap(struct mm_struct *mm) { }</span>
 #endif
 
 static inline void load_mm_ldt(struct mm_struct *mm)
<span class="p_chunk">@@ -96,10 +121,31 @@</span> <span class="p_context"> static inline void load_mm_ldt(struct mm</span>
 	 * that we can see.
 	 */
 
<span class="p_del">-	if (unlikely(ldt))</span>
<span class="p_del">-		set_ldt(ldt-&gt;entries, ldt-&gt;nr_entries);</span>
<span class="p_del">-	else</span>
<span class="p_add">+	if (unlikely(ldt)) {</span>
<span class="p_add">+		if (static_cpu_has(X86_FEATURE_PTI)) {</span>
<span class="p_add">+			if (WARN_ON_ONCE((unsigned long)ldt-&gt;slot &gt; 1)) {</span>
<span class="p_add">+				/*</span>
<span class="p_add">+				 * Whoops -- either the new LDT isn&#39;t mapped</span>
<span class="p_add">+				 * (if slot == -1) or is mapped into a bogus</span>
<span class="p_add">+				 * slot (if slot &gt; 1).</span>
<span class="p_add">+				 */</span>
<span class="p_add">+				clear_LDT();</span>
<span class="p_add">+				return;</span>
<span class="p_add">+			}</span>
<span class="p_add">+</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * If page table isolation is enabled, ldt-&gt;entries</span>
<span class="p_add">+			 * will not be mapped in the userspace pagetables.</span>
<span class="p_add">+			 * Tell the CPU to access the LDT through the alias</span>
<span class="p_add">+			 * at ldt_slot_va(ldt-&gt;slot).</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			set_ldt(ldt_slot_va(ldt-&gt;slot), ldt-&gt;nr_entries);</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			set_ldt(ldt-&gt;entries, ldt-&gt;nr_entries);</span>
<span class="p_add">+		}</span>
<span class="p_add">+	} else {</span>
 		clear_LDT();
<span class="p_add">+	}</span>
 #else
 	clear_LDT();
 #endif
<span class="p_chunk">@@ -194,6 +240,7 @@</span> <span class="p_context"> static inline int arch_dup_mmap(struct m</span>
 static inline void arch_exit_mmap(struct mm_struct *mm)
 {
 	paravirt_arch_exit_mmap(mm);
<span class="p_add">+	ldt_arch_exit_mmap(mm);</span>
 }
 
 #ifdef CONFIG_X86_64
<span class="p_header">--- a/arch/x86/include/asm/pgtable_64_types.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/pgtable_64_types.h</span>
<span class="p_chunk">@@ -81,10 +81,14 @@</span> <span class="p_context"> typedef struct { pteval_t pte; } pte_t;</span>
 #define VMALLOC_SIZE_TB _AC(12800, UL)
 #define __VMALLOC_BASE	_AC(0xffa0000000000000, UL)
 #define __VMEMMAP_BASE	_AC(0xffd4000000000000, UL)
<span class="p_add">+#define LDT_PGD_ENTRY _AC(-112, UL)</span>
<span class="p_add">+#define LDT_BASE_ADDR (LDT_PGD_ENTRY &lt;&lt; PGDIR_SHIFT)</span>
 #else
 #define VMALLOC_SIZE_TB	_AC(32, UL)
 #define __VMALLOC_BASE	_AC(0xffffc90000000000, UL)
 #define __VMEMMAP_BASE	_AC(0xffffea0000000000, UL)
<span class="p_add">+#define LDT_PGD_ENTRY _AC(-3, UL)</span>
<span class="p_add">+#define LDT_BASE_ADDR (LDT_PGD_ENTRY &lt;&lt; PGDIR_SHIFT)</span>
 #endif
 #ifdef CONFIG_RANDOMIZE_MEMORY
 #define VMALLOC_START	vmalloc_base
<span class="p_header">--- a/arch/x86/include/asm/processor.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/processor.h</span>
<span class="p_chunk">@@ -851,13 +851,22 @@</span> <span class="p_context"> static inline void spin_lock_prefetch(co</span>
 
 #else
 /*
<span class="p_del">- * User space process size. 47bits minus one guard page.  The guard</span>
<span class="p_del">- * page is necessary on Intel CPUs: if a SYSCALL instruction is at</span>
<span class="p_del">- * the highest possible canonical userspace address, then that</span>
<span class="p_del">- * syscall will enter the kernel with a non-canonical return</span>
<span class="p_del">- * address, and SYSRET will explode dangerously.  We avoid this</span>
<span class="p_del">- * particular problem by preventing anything from being mapped</span>
<span class="p_del">- * at the maximum canonical address.</span>
<span class="p_add">+ * User space process size.  This is the first address outside the user range.</span>
<span class="p_add">+ * There are a few constraints that determine this:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * On Intel CPUs, if a SYSCALL instruction is at the highest canonical</span>
<span class="p_add">+ * address, then that syscall will enter the kernel with a</span>
<span class="p_add">+ * non-canonical return address, and SYSRET will explode dangerously.</span>
<span class="p_add">+ * We avoid this particular problem by preventing anything executable</span>
<span class="p_add">+ * from being mapped at the maximum canonical address.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * On AMD CPUs in the Ryzen family, there&#39;s a nasty bug in which the</span>
<span class="p_add">+ * CPUs malfunction if they execute code from the highest canonical page.</span>
<span class="p_add">+ * They&#39;ll speculate right off the end of the canonical space, and</span>
<span class="p_add">+ * bad things happen.  This is worked around in the same way as the</span>
<span class="p_add">+ * Intel problem.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * With page table isolation enabled, we map the LDT in ... [stay tuned]</span>
  */
 #define TASK_SIZE_MAX	((1UL &lt;&lt; __VIRTUAL_MASK_SHIFT) - PAGE_SIZE)
 
<span class="p_header">--- a/arch/x86/kernel/ldt.c</span>
<span class="p_header">+++ b/arch/x86/kernel/ldt.c</span>
<span class="p_chunk">@@ -24,6 +24,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/uaccess.h&gt;
 
 #include &lt;asm/ldt.h&gt;
<span class="p_add">+#include &lt;asm/tlb.h&gt;</span>
 #include &lt;asm/desc.h&gt;
 #include &lt;asm/mmu_context.h&gt;
 #include &lt;asm/syscalls.h&gt;
<span class="p_chunk">@@ -51,13 +52,11 @@</span> <span class="p_context"> static void refresh_ldt_segments(void)</span>
 static void flush_ldt(void *__mm)
 {
 	struct mm_struct *mm = __mm;
<span class="p_del">-	mm_context_t *pc;</span>
 
 	if (this_cpu_read(cpu_tlbstate.loaded_mm) != mm)
 		return;
 
<span class="p_del">-	pc = &amp;mm-&gt;context;</span>
<span class="p_del">-	set_ldt(pc-&gt;ldt-&gt;entries, pc-&gt;ldt-&gt;nr_entries);</span>
<span class="p_add">+	load_mm_ldt(mm);</span>
 
 	refresh_ldt_segments();
 }
<span class="p_chunk">@@ -94,10 +93,121 @@</span> <span class="p_context"> static struct ldt_struct *alloc_ldt_stru</span>
 		return NULL;
 	}
 
<span class="p_add">+	/* The new LDT isn&#39;t aliased for PTI yet. */</span>
<span class="p_add">+	new_ldt-&gt;slot = -1;</span>
<span class="p_add">+</span>
 	new_ldt-&gt;nr_entries = num_entries;
 	return new_ldt;
 }
 
<span class="p_add">+/*</span>
<span class="p_add">+ * If PTI is enabled, this maps the LDT into the kernelmode and</span>
<span class="p_add">+ * usermode tables for the given mm.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * There is no corresponding unmap function.  Even if the LDT is freed, we</span>
<span class="p_add">+ * leave the PTEs around until the slot is reused or the mm is destroyed.</span>
<span class="p_add">+ * This is harmless: the LDT is always in ordinary memory, and no one will</span>
<span class="p_add">+ * access the freed slot.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * If we wanted to unmap freed LDTs, we&#39;d also need to do a flush to make</span>
<span class="p_add">+ * it useful, and the flush would slow down modify_ldt().</span>
<span class="p_add">+ */</span>
<span class="p_add">+static int</span>
<span class="p_add">+map_ldt_struct(struct mm_struct *mm, struct ldt_struct *ldt, int slot)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+	bool is_vmalloc, had_top_level_entry;</span>
<span class="p_add">+	unsigned long va;</span>
<span class="p_add">+	spinlock_t *ptl;</span>
<span class="p_add">+	pgd_t *pgd;</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!static_cpu_has(X86_FEATURE_PTI))</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Any given ldt_struct should have map_ldt_struct() called at most</span>
<span class="p_add">+	 * once.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	WARN_ON(ldt-&gt;slot != -1);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Did we already have the top level entry allocated?  We can&#39;t</span>
<span class="p_add">+	 * use pgd_none() for this because it doens&#39;t do anything on</span>
<span class="p_add">+	 * 4-level page table kernels.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	pgd = pgd_offset(mm, LDT_BASE_ADDR);</span>
<span class="p_add">+	had_top_level_entry = (pgd-&gt;pgd != 0);</span>
<span class="p_add">+</span>
<span class="p_add">+	is_vmalloc = is_vmalloc_addr(ldt-&gt;entries);</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i * PAGE_SIZE &lt; ldt-&gt;nr_entries * LDT_ENTRY_SIZE; i++) {</span>
<span class="p_add">+		unsigned long offset = i &lt;&lt; PAGE_SHIFT;</span>
<span class="p_add">+		const void *src = (char *)ldt-&gt;entries + offset;</span>
<span class="p_add">+		unsigned long pfn;</span>
<span class="p_add">+		pte_t pte, *ptep;</span>
<span class="p_add">+</span>
<span class="p_add">+		va = (unsigned long)ldt_slot_va(slot) + offset;</span>
<span class="p_add">+		pfn = is_vmalloc ? vmalloc_to_pfn(src) :</span>
<span class="p_add">+			page_to_pfn(virt_to_page(src));</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Treat the PTI LDT range as a *userspace* range.</span>
<span class="p_add">+		 * get_locked_pte() will allocate all needed pagetables</span>
<span class="p_add">+		 * and account for them in this mm.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		ptep = get_locked_pte(mm, va, &amp;ptl);</span>
<span class="p_add">+		if (!ptep)</span>
<span class="p_add">+			return -ENOMEM;</span>
<span class="p_add">+		pte = pfn_pte(pfn, __pgprot(__PAGE_KERNEL &amp; ~_PAGE_GLOBAL));</span>
<span class="p_add">+		set_pte_at(mm, va, ptep, pte);</span>
<span class="p_add">+		pte_unmap_unlock(ptep, ptl);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (mm-&gt;context.ldt) {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * We already had an LDT.  The top-level entry should already</span>
<span class="p_add">+		 * have been allocated and synchronized with the usermode</span>
<span class="p_add">+		 * tables.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		WARN_ON(!had_top_level_entry);</span>
<span class="p_add">+		if (static_cpu_has(X86_FEATURE_PTI))</span>
<span class="p_add">+			WARN_ON(!kernel_to_user_pgdp(pgd)-&gt;pgd);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * This is the first time we&#39;re mapping an LDT for this process.</span>
<span class="p_add">+		 * Sync the pgd to the usermode tables.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		WARN_ON(had_top_level_entry);</span>
<span class="p_add">+		if (static_cpu_has(X86_FEATURE_PTI)) {</span>
<span class="p_add">+			WARN_ON(kernel_to_user_pgdp(pgd)-&gt;pgd);</span>
<span class="p_add">+			set_pgd(kernel_to_user_pgdp(pgd), *pgd);</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	va = (unsigned long)ldt_slot_va(slot);</span>
<span class="p_add">+	flush_tlb_mm_range(mm, va, va + LDT_SLOT_STRIDE, 0);</span>
<span class="p_add">+</span>
<span class="p_add">+	ldt-&gt;slot = slot;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void free_ldt_pgtables(struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+	struct mmu_gather tlb;</span>
<span class="p_add">+	unsigned long start = LDT_BASE_ADDR;</span>
<span class="p_add">+	unsigned long end = start + (1UL &lt;&lt; PGDIR_SHIFT);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!static_cpu_has(X86_FEATURE_PTI))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	tlb_gather_mmu(&amp;tlb, mm, start, end);</span>
<span class="p_add">+	free_pgd_range(&amp;tlb, start, end, start, end);</span>
<span class="p_add">+	tlb_finish_mmu(&amp;tlb, start, end);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /* After calling this, the LDT is immutable. */
 static void finalize_ldt_struct(struct ldt_struct *ldt)
 {
<span class="p_chunk">@@ -156,6 +266,12 @@</span> <span class="p_context"> int ldt_dup_context(struct mm_struct *ol</span>
 	       new_ldt-&gt;nr_entries * LDT_ENTRY_SIZE);
 	finalize_ldt_struct(new_ldt);
 
<span class="p_add">+	retval = map_ldt_struct(mm, new_ldt, 0);</span>
<span class="p_add">+	if (retval) {</span>
<span class="p_add">+		free_ldt_pgtables(mm);</span>
<span class="p_add">+		free_ldt_struct(new_ldt);</span>
<span class="p_add">+		goto out_unlock;</span>
<span class="p_add">+	}</span>
 	mm-&gt;context.ldt = new_ldt;
 
 out_unlock:
<span class="p_chunk">@@ -174,6 +290,11 @@</span> <span class="p_context"> void destroy_context_ldt(struct mm_struc</span>
 	mm-&gt;context.ldt = NULL;
 }
 
<span class="p_add">+void ldt_arch_exit_mmap(struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+	free_ldt_pgtables(mm);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static int read_ldt(void __user *ptr, unsigned long bytecount)
 {
 	struct mm_struct *mm = current-&gt;mm;
<span class="p_chunk">@@ -287,6 +408,18 @@</span> <span class="p_context"> static int write_ldt(void __user *ptr, u</span>
 	new_ldt-&gt;entries[ldt_info.entry_number] = ldt;
 	finalize_ldt_struct(new_ldt);
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If we are using PTI, map the new LDT into the userspace pagetables.</span>
<span class="p_add">+	 * If there is already an LDT, use the other slot so that other CPUs</span>
<span class="p_add">+	 * will continue to use the old LDT until install_ldt() switches</span>
<span class="p_add">+	 * them over to the new LDT.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	error = map_ldt_struct(mm, new_ldt, old_ldt ? !old_ldt-&gt;slot : 0);</span>
<span class="p_add">+	if (error) {</span>
<span class="p_add">+		free_ldt_struct(old_ldt);</span>
<span class="p_add">+		goto out_unlock;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	install_ldt(mm, new_ldt);
 	free_ldt_struct(old_ldt);
 	error = 0;
<span class="p_header">--- a/arch/x86/mm/dump_pagetables.c</span>
<span class="p_header">+++ b/arch/x86/mm/dump_pagetables.c</span>
<span class="p_chunk">@@ -50,12 +50,18 @@</span> <span class="p_context"> enum address_markers_idx {</span>
 #ifdef CONFIG_X86_64
 	KERNEL_SPACE_NR,
 	LOW_KERNEL_NR,
<span class="p_add">+#if defined(CONFIG_MODIFY_LDT_SYSCALL) &amp;&amp; defined(CONFIG_X86_5LEVEL)</span>
<span class="p_add">+	LDT_NR,</span>
<span class="p_add">+#endif</span>
 	VMALLOC_START_NR,
 	VMEMMAP_START_NR,
 #ifdef CONFIG_KASAN
 	KASAN_SHADOW_START_NR,
 	KASAN_SHADOW_END_NR,
 #endif
<span class="p_add">+#if defined(CONFIG_MODIFY_LDT_SYSCALL) &amp;&amp; !defined(CONFIG_X86_5LEVEL)</span>
<span class="p_add">+	LDT_NR,</span>
<span class="p_add">+#endif</span>
 # ifdef CONFIG_X86_ESPFIX64
 	ESPFIX_START_NR,
 # endif
<span class="p_chunk">@@ -79,12 +85,18 @@</span> <span class="p_context"> static struct addr_marker address_marker</span>
 #ifdef CONFIG_X86_64
 	{ 0x8000000000000000UL, &quot;Kernel Space&quot; },
 	{ 0/* PAGE_OFFSET */,   &quot;Low Kernel Mapping&quot; },
<span class="p_add">+#if defined(CONFIG_MODIFY_LDT_SYSCALL) &amp;&amp; defined(CONFIG_X86_5LEVEL)</span>
<span class="p_add">+	{ LDT_BASE_ADDR,	&quot;LDT remap&quot; },</span>
<span class="p_add">+#endif</span>
 	{ 0/* VMALLOC_START */, &quot;vmalloc() Area&quot; },
 	{ 0/* VMEMMAP_START */, &quot;Vmemmap&quot; },
 #ifdef CONFIG_KASAN
 	{ KASAN_SHADOW_START,	&quot;KASAN shadow&quot; },
 	{ KASAN_SHADOW_END,	&quot;KASAN shadow end&quot; },
 #endif
<span class="p_add">+#if defined(CONFIG_MODIFY_LDT_SYSCALL) &amp;&amp; !defined(CONFIG_X86_5LEVEL)</span>
<span class="p_add">+	{ LDT_BASE_ADDR,	&quot;LDT remap&quot; },</span>
<span class="p_add">+#endif</span>
 # ifdef CONFIG_X86_ESPFIX64
 	{ ESPFIX_BASE_ADDR,	&quot;ESPfix Area&quot;, 16 },
 # endif

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



