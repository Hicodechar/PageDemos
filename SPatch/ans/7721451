
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v5,12/12] mm: don&#39;t split THP page when syscall is called - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v5,12/12] mm: don&#39;t split THP page when syscall is called</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=36811">Minchan Kim</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Nov. 30, 2015, 6:39 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1448865583-2446-13-git-send-email-minchan@kernel.org&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/7721451/mbox/"
   >mbox</a>
|
   <a href="/patch/7721451/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/7721451/">/patch/7721451/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork1.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork1.web.kernel.org (Postfix) with ESMTP id 964359F1C2
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 30 Nov 2015 06:40:58 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 8DF382065E
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 30 Nov 2015 06:40:57 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id 254CC204FC
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 30 Nov 2015 06:40:56 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1753485AbbK3GkK (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 30 Nov 2015 01:40:10 -0500
Received: from LGEAMRELO13.lge.com ([156.147.23.53]:34767 &quot;EHLO
	lgeamrelo13.lge.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1753145AbbK3Gja (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 30 Nov 2015 01:39:30 -0500
Received: from unknown (HELO lgeamrelo01.lge.com) (156.147.1.125)
	by 156.147.23.53 with ESMTP; 30 Nov 2015 15:39:29 +0900
X-Original-SENDERIP: 156.147.1.125
X-Original-MAILFROM: minchan@kernel.org
Received: from unknown (HELO localhost.localdomain) (10.177.223.161)
	by 156.147.1.125 with ESMTP; 30 Nov 2015 15:39:29 +0900
X-Original-SENDERIP: 10.177.223.161
X-Original-MAILFROM: minchan@kernel.org
From: Minchan Kim &lt;minchan@kernel.org&gt;
To: Andrew Morton &lt;akpm@linux-foundation.org&gt;
Cc: linux-kernel@vger.kernel.org, linux-mm@kvack.org,
	Michael Kerrisk &lt;mtk.manpages@gmail.com&gt;,
	linux-api@vger.kernel.org, Hugh Dickins &lt;hughd@google.com&gt;,
	Johannes Weiner &lt;hannes@cmpxchg.org&gt;,
	Rik van Riel &lt;riel@redhat.com&gt;, Mel Gorman &lt;mgorman@suse.de&gt;,
	KOSAKI Motohiro &lt;kosaki.motohiro@jp.fujitsu.com&gt;,
	Jason Evans &lt;je@fb.com&gt;, Daniel Micay &lt;danielmicay@gmail.com&gt;,
	&quot;Kirill A. Shutemov&quot; &lt;kirill@shutemov.name&gt;,
	Shaohua Li &lt;shli@kernel.org&gt;, Michal Hocko &lt;mhocko@suse.cz&gt;,
	yalin.wang2010@gmail.com, Andy Lutomirski &lt;luto@amacapital.net&gt;,
	Minchan Kim &lt;minchan@kernel.org&gt;, Andrea Arcangeli &lt;aarcange@redhat.com&gt;
Subject: [PATCH v5 12/12] mm: don&#39;t split THP page when syscall is called
Date: Mon, 30 Nov 2015 15:39:43 +0900
Message-Id: &lt;1448865583-2446-13-git-send-email-minchan@kernel.org&gt;
X-Mailer: git-send-email 1.9.1
In-Reply-To: &lt;1448865583-2446-1-git-send-email-minchan@kernel.org&gt;
References: &lt;1448865583-2446-1-git-send-email-minchan@kernel.org&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-6.9 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	T_RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36811">Minchan Kim</a> - Nov. 30, 2015, 6:39 a.m.</div>
<pre class="content">
We don&#39;t need to split THP page when MADV_FREE syscall is called
if [start, len] is aligned with THP size. The split could be done
when VM decide to free it in reclaim path if memory pressure is
heavy. With that, we could avoid unnecessary THP split.

For the feature, this patch changes pte dirtness marking logic of THP.
Now, it marks every ptes of pages dirty unconditionally in splitting,
which makes MADV_FREE void. So, instead, this patch propagates pmd
dirtiness to all pages via PG_dirty and restores pte dirtiness from
PG_dirty. With this, if pmd is clean(ie, MADV_FREEed) when split
happens(e,g, shrink_page_list), all of pages are clean too so we
could discard them.

Cc: Kirill A. Shutemov &lt;kirill@shutemov.name&gt;
Cc: Hugh Dickins &lt;hughd@google.com&gt;
Cc: Andrea Arcangeli &lt;aarcange@redhat.com&gt;
<span class="signed-off-by">Signed-off-by: Minchan Kim &lt;minchan@kernel.org&gt;</span>
---
 include/linux/huge_mm.h |  3 ++
 mm/huge_memory.c        | 87 ++++++++++++++++++++++++++++++++++++++++++++++---
 mm/madvise.c            |  8 ++++-
 3 files changed, 92 insertions(+), 6 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/include/linux/huge_mm.h b/include/linux/huge_mm.h</span>
<span class="p_header">index 72cd942edb22..0160201993d4 100644</span>
<span class="p_header">--- a/include/linux/huge_mm.h</span>
<span class="p_header">+++ b/include/linux/huge_mm.h</span>
<span class="p_chunk">@@ -19,6 +19,9 @@</span> <span class="p_context"> extern struct page *follow_trans_huge_pmd(struct vm_area_struct *vma,</span>
 					  unsigned long addr,
 					  pmd_t *pmd,
 					  unsigned int flags);
<span class="p_add">+extern int madvise_free_huge_pmd(struct mmu_gather *tlb,</span>
<span class="p_add">+			struct vm_area_struct *vma,</span>
<span class="p_add">+			pmd_t *pmd, unsigned long addr, unsigned long next);</span>
 extern int zap_huge_pmd(struct mmu_gather *tlb,
 			struct vm_area_struct *vma,
 			pmd_t *pmd, unsigned long addr);
<span class="p_header">diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="p_header">index b41793b12a2d..2aa28cbe7263 100644</span>
<span class="p_header">--- a/mm/huge_memory.c</span>
<span class="p_header">+++ b/mm/huge_memory.c</span>
<span class="p_chunk">@@ -1530,6 +1530,77 @@</span> <span class="p_context"> int do_huge_pmd_numa_page(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 	return 0;
 }
 
<span class="p_add">+int madvise_free_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
<span class="p_add">+		pmd_t *pmd, unsigned long addr, unsigned long next)</span>
<span class="p_add">+</span>
<span class="p_add">+{</span>
<span class="p_add">+	spinlock_t *ptl;</span>
<span class="p_add">+	pmd_t orig_pmd;</span>
<span class="p_add">+	struct page *page;</span>
<span class="p_add">+	struct mm_struct *mm = tlb-&gt;mm;</span>
<span class="p_add">+	int ret = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!pmd_trans_huge_lock(pmd, vma, &amp;ptl))</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+</span>
<span class="p_add">+	orig_pmd = *pmd;</span>
<span class="p_add">+	if (is_huge_zero_pmd(orig_pmd)) {</span>
<span class="p_add">+		ret = 1;</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	page = pmd_page(orig_pmd);</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If other processes are mapping this page, we couldn&#39;t discard</span>
<span class="p_add">+	 * the page unless they all do MADV_FREE so let&#39;s skip the page.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (page_mapcount(page) != 1)</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!trylock_page(page))</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If user want to discard part-pages of THP, split it so MADV_FREE</span>
<span class="p_add">+	 * will deactivate only them.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (next - addr != HPAGE_PMD_SIZE) {</span>
<span class="p_add">+		get_page(page);</span>
<span class="p_add">+		spin_unlock(ptl);</span>
<span class="p_add">+		if (split_huge_page(page)) {</span>
<span class="p_add">+			put_page(page);</span>
<span class="p_add">+			unlock_page(page);</span>
<span class="p_add">+			goto out_unlocked;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		put_page(page);</span>
<span class="p_add">+		unlock_page(page);</span>
<span class="p_add">+		ret = 1;</span>
<span class="p_add">+		goto out_unlocked;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (PageDirty(page))</span>
<span class="p_add">+		ClearPageDirty(page);</span>
<span class="p_add">+	unlock_page(page);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (PageActive(page))</span>
<span class="p_add">+		deactivate_page(page);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (pmd_young(orig_pmd) || pmd_dirty(orig_pmd)) {</span>
<span class="p_add">+		orig_pmd = pmdp_huge_get_and_clear_full(tlb-&gt;mm, addr, pmd,</span>
<span class="p_add">+			tlb-&gt;fullmm);</span>
<span class="p_add">+		orig_pmd = pmd_mkold(orig_pmd);</span>
<span class="p_add">+		orig_pmd = pmd_mkclean(orig_pmd);</span>
<span class="p_add">+</span>
<span class="p_add">+		set_pmd_at(mm, addr, pmd, orig_pmd);</span>
<span class="p_add">+		tlb_remove_pmd_tlb_entry(tlb, pmd, addr);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	ret = 1;</span>
<span class="p_add">+out:</span>
<span class="p_add">+	spin_unlock(ptl);</span>
<span class="p_add">+out_unlocked:</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 int zap_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,
 		 pmd_t *pmd, unsigned long addr)
 {
<span class="p_chunk">@@ -2784,7 +2855,7 @@</span> <span class="p_context"> static void __split_huge_pmd_locked(struct vm_area_struct *vma, pmd_t *pmd,</span>
 	struct page *page;
 	pgtable_t pgtable;
 	pmd_t _pmd;
<span class="p_del">-	bool young, write;</span>
<span class="p_add">+	bool young, write, dirty;</span>
 	int i;
 
 	VM_BUG_ON(haddr &amp; ~HPAGE_PMD_MASK);
<span class="p_chunk">@@ -2808,6 +2879,7 @@</span> <span class="p_context"> static void __split_huge_pmd_locked(struct vm_area_struct *vma, pmd_t *pmd,</span>
 	atomic_add(HPAGE_PMD_NR - 1, &amp;page-&gt;_count);
 	write = pmd_write(*pmd);
 	young = pmd_young(*pmd);
<span class="p_add">+	dirty = pmd_dirty(*pmd);</span>
 
 	pgtable = pgtable_trans_huge_withdraw(mm, pmd);
 	pmd_populate(mm, &amp;_pmd, pgtable);
<span class="p_chunk">@@ -2825,12 +2897,14 @@</span> <span class="p_context"> static void __split_huge_pmd_locked(struct vm_area_struct *vma, pmd_t *pmd,</span>
 			entry = swp_entry_to_pte(swp_entry);
 		} else {
 			entry = mk_pte(page + i, vma-&gt;vm_page_prot);
<span class="p_del">-			entry = maybe_mkwrite(pte_mkdirty(entry), vma);</span>
<span class="p_add">+			entry = maybe_mkwrite(entry, vma);</span>
 			if (!write)
 				entry = pte_wrprotect(entry);
 			if (!young)
 				entry = pte_mkold(entry);
 		}
<span class="p_add">+		if (dirty)</span>
<span class="p_add">+			SetPageDirty(page + i);</span>
 		pte = pte_offset_map(&amp;_pmd, haddr);
 		BUG_ON(!pte_none(*pte));
 		set_pte_at(mm, haddr, pte, entry);
<span class="p_chunk">@@ -3028,6 +3102,8 @@</span> <span class="p_context"> static void freeze_page_vma(struct vm_area_struct *vma, struct page *page,</span>
 			continue;
 		flush_cache_page(vma, address, page_to_pfn(page));
 		entry = ptep_clear_flush(vma, address, pte + i);
<span class="p_add">+		if (pte_dirty(entry))</span>
<span class="p_add">+			SetPageDirty(page);</span>
 		swp_entry = make_migration_entry(page, pte_write(entry));
 		swp_pte = swp_entry_to_pte(swp_entry);
 		if (pte_soft_dirty(entry))
<span class="p_chunk">@@ -3086,7 +3162,8 @@</span> <span class="p_context"> static void unfreeze_page_vma(struct vm_area_struct *vma, struct page *page,</span>
 		page_add_anon_rmap(page, vma, address, false);
 
 		entry = pte_mkold(mk_pte(page, vma-&gt;vm_page_prot));
<span class="p_del">-		entry = pte_mkdirty(entry);</span>
<span class="p_add">+		if (PageDirty(page))</span>
<span class="p_add">+			entry = pte_mkdirty(entry);</span>
 		if (is_write_migration_entry(swp_entry))
 			entry = maybe_mkwrite(entry, vma);
 
<span class="p_chunk">@@ -3147,8 +3224,8 @@</span> <span class="p_context"> static int __split_huge_page_tail(struct page *head, int tail,</span>
 			 (1L &lt;&lt; PG_uptodate) |
 			 (1L &lt;&lt; PG_active) |
 			 (1L &lt;&lt; PG_locked) |
<span class="p_del">-			 (1L &lt;&lt; PG_unevictable)));</span>
<span class="p_del">-	page_tail-&gt;flags |= (1L &lt;&lt; PG_dirty);</span>
<span class="p_add">+			 (1L &lt;&lt; PG_unevictable) |</span>
<span class="p_add">+			 (1L &lt;&lt; PG_dirty)));</span>
 
 	/*
 	 * After clearing PageTail the gup refcount can be released.
<span class="p_header">diff --git a/mm/madvise.c b/mm/madvise.c</span>
<span class="p_header">index 975e24e4c134..563d6c145d75 100644</span>
<span class="p_header">--- a/mm/madvise.c</span>
<span class="p_header">+++ b/mm/madvise.c</span>
<span class="p_chunk">@@ -271,8 +271,13 @@</span> <span class="p_context"> static int madvise_free_pte_range(pmd_t *pmd, unsigned long addr,</span>
 	pte_t *orig_pte, *pte, ptent;
 	struct page *page;
 	int nr_swap = 0;
<span class="p_add">+	unsigned long next;</span>
<span class="p_add">+</span>
<span class="p_add">+	next = pmd_addr_end(addr, end);</span>
<span class="p_add">+	if (pmd_trans_huge(*pmd))</span>
<span class="p_add">+		if (madvise_free_huge_pmd(tlb, vma, pmd, addr, next))</span>
<span class="p_add">+			goto next;</span>
 
<span class="p_del">-	split_huge_pmd(vma, pmd, addr);</span>
 	if (pmd_trans_unstable(pmd))
 		return 0;
 
<span class="p_chunk">@@ -381,6 +386,7 @@</span> <span class="p_context"> static int madvise_free_pte_range(pmd_t *pmd, unsigned long addr,</span>
 	arch_leave_lazy_mmu_mode();
 	pte_unmap_unlock(orig_pte, ptl);
 	cond_resched();
<span class="p_add">+next:</span>
 	return 0;
 }
 

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



