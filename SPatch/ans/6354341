
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[V2,1/2] mm/thp: Split out pmd collpase flush into a seperate functions - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [V2,1/2] mm/thp: Split out pmd collpase flush into a seperate functions</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=1141">Aneesh Kumar K.V</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>May 7, 2015, 7:23 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1430983408-24924-1-git-send-email-aneesh.kumar@linux.vnet.ibm.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/6354341/mbox/"
   >mbox</a>
|
   <a href="/patch/6354341/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/6354341/">/patch/6354341/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork2.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork2.web.kernel.org (Postfix) with ESMTP id F3AA7BEEE1
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  7 May 2015 07:23:48 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id E48F4203AC
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  7 May 2015 07:23:47 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id B7C86203AF
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  7 May 2015 07:23:46 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1751967AbbEGHXk (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 7 May 2015 03:23:40 -0400
Received: from e28smtp04.in.ibm.com ([122.248.162.4]:53953 &quot;EHLO
	e28smtp04.in.ibm.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1751337AbbEGHXi (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 7 May 2015 03:23:38 -0400
Received: from /spool/local
	by e28smtp04.in.ibm.com with IBM ESMTP SMTP Gateway: Authorized Use
	Only! Violators will be prosecuted
	for &lt;linux-kernel@vger.kernel.org&gt; from
	&lt;aneesh.kumar@linux.vnet.ibm.com&gt;; Thu, 7 May 2015 12:53:35 +0530
Received: from d28dlp01.in.ibm.com (9.184.220.126)
	by e28smtp04.in.ibm.com (192.168.1.134) with IBM ESMTP SMTP Gateway:
	Authorized Use Only! Violators will be prosecuted; 
	Thu, 7 May 2015 12:53:34 +0530
Received: from d28relay01.in.ibm.com (d28relay01.in.ibm.com [9.184.220.58])
	by d28dlp01.in.ibm.com (Postfix) with ESMTP id EE279E004C
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Thu,  7 May 2015 12:56:26 +0530 (IST)
Received: from d28av02.in.ibm.com (d28av02.in.ibm.com [9.184.220.64])
	by d28relay01.in.ibm.com (8.14.9/8.14.9/NCO v10.0) with ESMTP id
	t477NXBo62193728
	for &lt;linux-kernel@vger.kernel.org&gt;; Thu, 7 May 2015 12:53:33 +0530
Received: from d28av02.in.ibm.com (localhost [127.0.0.1])
	by d28av02.in.ibm.com (8.14.4/8.14.4/NCO v10.0 AVout) with ESMTP id
	t476nOGV003640
	for &lt;linux-kernel@vger.kernel.org&gt;; Thu, 7 May 2015 12:19:26 +0530
Received: from skywalker.in.ibm.com ([9.79.208.4])
	by d28av02.in.ibm.com (8.14.4/8.14.4/NCO v10.0 AVin) with ESMTP id
	t476nOkF003635; Thu, 7 May 2015 12:19:24 +0530
From: &quot;Aneesh Kumar K.V&quot; &lt;aneesh.kumar@linux.vnet.ibm.com&gt;
To: akpm@linux-foundation.org, mpe@ellerman.id.au, paulus@samba.org,
	benh@kernel.crashing.org, kirill.shutemov@linux.intel.com,
	aarcange@redhat.com
Cc: linux-mm@kvack.org, linux-kernel@vger.kernel.org,
	linuxppc-dev@lists.ozlabs.org,
	&quot;Aneesh Kumar K.V&quot; &lt;aneesh.kumar@linux.vnet.ibm.com&gt;
Subject: [PATCH V2 1/2] mm/thp: Split out pmd collpase flush into a seperate
	functions
Date: Thu,  7 May 2015 12:53:27 +0530
Message-Id: &lt;1430983408-24924-1-git-send-email-aneesh.kumar@linux.vnet.ibm.com&gt;
X-Mailer: git-send-email 2.1.4
X-TM-AS-MML: disable
X-Content-Scanned: Fidelis XPS MAILER
x-cbid: 15050707-0013-0000-0000-00000508ACEC
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-6.9 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	T_RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=1141">Aneesh Kumar K.V</a> - May 7, 2015, 7:23 a.m.</div>
<pre class="content">
After this patch pmdp_* functions operate only on hugepage pte,
and not on regular pmd_t values pointing to page table.
<span class="signed-off-by">
Signed-off-by: Aneesh Kumar K.V &lt;aneesh.kumar@linux.vnet.ibm.com&gt;</span>
---
 arch/powerpc/include/asm/pgtable-ppc64.h |  4 ++
 arch/powerpc/mm/pgtable_64.c             | 76 +++++++++++++++++---------------
 include/asm-generic/pgtable.h            | 19 ++++++++
 mm/huge_memory.c                         |  2 +-
 4 files changed, 65 insertions(+), 36 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=874">Kirill A. Shutemov</a> - May 7, 2015, 9:20 a.m.</div>
<pre class="content">
On Thu, May 07, 2015 at 12:53:27PM +0530, Aneesh Kumar K.V wrote:
<span class="quote">&gt; After this patch pmdp_* functions operate only on hugepage pte,</span>
<span class="quote">&gt; and not on regular pmd_t values pointing to page table.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Aneesh Kumar K.V &lt;aneesh.kumar@linux.vnet.ibm.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  arch/powerpc/include/asm/pgtable-ppc64.h |  4 ++</span>
<span class="quote">&gt;  arch/powerpc/mm/pgtable_64.c             | 76 +++++++++++++++++---------------</span>
<span class="quote">&gt;  include/asm-generic/pgtable.h            | 19 ++++++++</span>
<span class="quote">&gt;  mm/huge_memory.c                         |  2 +-</span>
<span class="quote">&gt;  4 files changed, 65 insertions(+), 36 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/arch/powerpc/include/asm/pgtable-ppc64.h b/arch/powerpc/include/asm/pgtable-ppc64.h</span>
<span class="quote">&gt; index 43e6ad424c7f..50830c9a2116 100644</span>
<span class="quote">&gt; --- a/arch/powerpc/include/asm/pgtable-ppc64.h</span>
<span class="quote">&gt; +++ b/arch/powerpc/include/asm/pgtable-ppc64.h</span>
<span class="quote">&gt; @@ -576,6 +576,10 @@ static inline void pmdp_set_wrprotect(struct mm_struct *mm, unsigned long addr,</span>
<span class="quote">&gt;  extern void pmdp_splitting_flush(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  				 unsigned long address, pmd_t *pmdp);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +#define __HAVE_ARCH_PMDP_COLLAPSE_FLUSH</span>
<span class="quote">&gt; +extern pmd_t pmdp_collapse_flush(struct vm_area_struct *vma,</span>
<span class="quote">&gt; +				 unsigned long address, pmd_t *pmdp);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  #define __HAVE_ARCH_PGTABLE_DEPOSIT</span>
<span class="quote">&gt;  extern void pgtable_trans_huge_deposit(struct mm_struct *mm, pmd_t *pmdp,</span>
<span class="quote">&gt;  				       pgtable_t pgtable);</span>
<span class="quote">&gt; diff --git a/arch/powerpc/mm/pgtable_64.c b/arch/powerpc/mm/pgtable_64.c</span>
<span class="quote">&gt; index 59daa5eeec25..9171c1a37290 100644</span>
<span class="quote">&gt; --- a/arch/powerpc/mm/pgtable_64.c</span>
<span class="quote">&gt; +++ b/arch/powerpc/mm/pgtable_64.c</span>
<span class="quote">&gt; @@ -560,41 +560,47 @@ pmd_t pmdp_clear_flush(struct vm_area_struct *vma, unsigned long address,</span>
<span class="quote">&gt;  	pmd_t pmd;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	VM_BUG_ON(address &amp; ~HPAGE_PMD_MASK);</span>
<span class="quote">&gt; -	if (pmd_trans_huge(*pmdp)) {</span>
<span class="quote">&gt; -		pmd = pmdp_get_and_clear(vma-&gt;vm_mm, address, pmdp);</span>
<span class="quote">&gt; -	} else {</span>
<span class="quote">&gt; -		/*</span>
<span class="quote">&gt; -		 * khugepaged calls this for normal pmd</span>
<span class="quote">&gt; -		 */</span>
<span class="quote">&gt; -		pmd = *pmdp;</span>
<span class="quote">&gt; -		pmd_clear(pmdp);</span>
<span class="quote">&gt; -		/*</span>
<span class="quote">&gt; -		 * Wait for all pending hash_page to finish. This is needed</span>
<span class="quote">&gt; -		 * in case of subpage collapse. When we collapse normal pages</span>
<span class="quote">&gt; -		 * to hugepage, we first clear the pmd, then invalidate all</span>
<span class="quote">&gt; -		 * the PTE entries. The assumption here is that any low level</span>
<span class="quote">&gt; -		 * page fault will see a none pmd and take the slow path that</span>
<span class="quote">&gt; -		 * will wait on mmap_sem. But we could very well be in a</span>
<span class="quote">&gt; -		 * hash_page with local ptep pointer value. Such a hash page</span>
<span class="quote">&gt; -		 * can result in adding new HPTE entries for normal subpages.</span>
<span class="quote">&gt; -		 * That means we could be modifying the page content as we</span>
<span class="quote">&gt; -		 * copy them to a huge page. So wait for parallel hash_page</span>
<span class="quote">&gt; -		 * to finish before invalidating HPTE entries. We can do this</span>
<span class="quote">&gt; -		 * by sending an IPI to all the cpus and executing a dummy</span>
<span class="quote">&gt; -		 * function there.</span>
<span class="quote">&gt; -		 */</span>
<span class="quote">&gt; -		kick_all_cpus_sync();</span>
<span class="quote">&gt; -		/*</span>
<span class="quote">&gt; -		 * Now invalidate the hpte entries in the range</span>
<span class="quote">&gt; -		 * covered by pmd. This make sure we take a</span>
<span class="quote">&gt; -		 * fault and will find the pmd as none, which will</span>
<span class="quote">&gt; -		 * result in a major fault which takes mmap_sem and</span>
<span class="quote">&gt; -		 * hence wait for collapse to complete. Without this</span>
<span class="quote">&gt; -		 * the __collapse_huge_page_copy can result in copying</span>
<span class="quote">&gt; -		 * the old content.</span>
<span class="quote">&gt; -		 */</span>
<span class="quote">&gt; -		flush_tlb_pmd_range(vma-&gt;vm_mm, &amp;pmd, address);</span>
<span class="quote">&gt; -	}</span>
<span class="quote">&gt; +	VM_BUG_ON(!pmd_trans_huge(*pmdp));</span>
<span class="quote">&gt; +	pmd = pmdp_get_and_clear(vma-&gt;vm_mm, address, pmdp);</span>
<span class="quote">&gt; +	return pmd;</span>

The patches are in reverse order: you need to change pmdp_get_and_clear
first otherwise you break bisectability.
Or better merge patches together.
<span class="quote">
&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +pmd_t pmdp_collapse_flush(struct vm_area_struct *vma, unsigned long address,</span>
<span class="quote">&gt; +			  pmd_t *pmdp)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	pmd_t pmd;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	VM_BUG_ON(address &amp; ~HPAGE_PMD_MASK);</span>
<span class="quote">&gt; +	VM_BUG_ON(pmd_trans_huge(*pmdp));</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	pmd = *pmdp;</span>
<span class="quote">&gt; +	pmd_clear(pmdp);</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * Wait for all pending hash_page to finish. This is needed</span>
<span class="quote">&gt; +	 * in case of subpage collapse. When we collapse normal pages</span>
<span class="quote">&gt; +	 * to hugepage, we first clear the pmd, then invalidate all</span>
<span class="quote">&gt; +	 * the PTE entries. The assumption here is that any low level</span>
<span class="quote">&gt; +	 * page fault will see a none pmd and take the slow path that</span>
<span class="quote">&gt; +	 * will wait on mmap_sem. But we could very well be in a</span>
<span class="quote">&gt; +	 * hash_page with local ptep pointer value. Such a hash page</span>
<span class="quote">&gt; +	 * can result in adding new HPTE entries for normal subpages.</span>
<span class="quote">&gt; +	 * That means we could be modifying the page content as we</span>
<span class="quote">&gt; +	 * copy them to a huge page. So wait for parallel hash_page</span>
<span class="quote">&gt; +	 * to finish before invalidating HPTE entries. We can do this</span>
<span class="quote">&gt; +	 * by sending an IPI to all the cpus and executing a dummy</span>
<span class="quote">&gt; +	 * function there.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	kick_all_cpus_sync();</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * Now invalidate the hpte entries in the range</span>
<span class="quote">&gt; +	 * covered by pmd. This make sure we take a</span>
<span class="quote">&gt; +	 * fault and will find the pmd as none, which will</span>
<span class="quote">&gt; +	 * result in a major fault which takes mmap_sem and</span>
<span class="quote">&gt; +	 * hence wait for collapse to complete. Without this</span>
<span class="quote">&gt; +	 * the __collapse_huge_page_copy can result in copying</span>
<span class="quote">&gt; +	 * the old content.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	flush_tlb_pmd_range(vma-&gt;vm_mm, &amp;pmd, address);</span>
<span class="quote">&gt;  	return pmd;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/include/asm-generic/pgtable.h b/include/asm-generic/pgtable.h</span>
<span class="quote">&gt; index 39f1d6a2b04d..80e6d415cd57 100644</span>
<span class="quote">&gt; --- a/include/asm-generic/pgtable.h</span>
<span class="quote">&gt; +++ b/include/asm-generic/pgtable.h</span>
<span class="quote">&gt; @@ -189,6 +189,25 @@ extern void pmdp_splitting_flush(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  				 unsigned long address, pmd_t *pmdp);</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +#ifndef __HAVE_ARCH_PMDP_COLLAPSE_FLUSH</span>
<span class="quote">&gt; +#ifdef CONFIG_TRANSPARENT_HUGEPAGE</span>
<span class="quote">&gt; +static inline pmd_t pmdp_collapse_flush(struct vm_area_struct *vma,</span>
<span class="quote">&gt; +				       unsigned long address,</span>
<span class="quote">&gt; +				       pmd_t *pmdp)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return pmdp_clear_flush(vma, address, pmdp);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +#else</span>
<span class="quote">&gt; +static inline pmd_t pmdp_collapse_flush(struct vm_area_struct *vma,</span>
<span class="quote">&gt; +				       unsigned long address,</span>
<span class="quote">&gt; +				       pmd_t *pmdp)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	BUILD_BUG();</span>
<span class="quote">&gt; +	return __pmd(0);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +#endif /* CONFIG_TRANSPARENT_HUGEPAGE */</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  #ifndef __HAVE_ARCH_PGTABLE_DEPOSIT</span>
<span class="quote">&gt;  extern void pgtable_trans_huge_deposit(struct mm_struct *mm, pmd_t *pmdp,</span>
<span class="quote">&gt;  				       pgtable_t pgtable);</span>
<span class="quote">&gt; diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="quote">&gt; index 078832cf3636..88f695a4e38b 100644</span>
<span class="quote">&gt; --- a/mm/huge_memory.c</span>
<span class="quote">&gt; +++ b/mm/huge_memory.c</span>
<span class="quote">&gt; @@ -2499,7 +2499,7 @@ static void collapse_huge_page(struct mm_struct *mm,</span>
<span class="quote">&gt;  	 * huge and small TLB entries for the same virtual address</span>
<span class="quote">&gt;  	 * to avoid the risk of CPU bugs in that area.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt; -	_pmd = pmdp_clear_flush(vma, address, pmd);</span>
<span class="quote">&gt; +	_pmd = pmdp_collapse_flush(vma, address, pmd);</span>

Why? pmdp_clear_flush() does kick_all_cpus_sync() already.
<span class="quote">
&gt;  	spin_unlock(pmd_ptl);</span>
<span class="quote">&gt;  	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -- </span>
<span class="quote">&gt; 2.1.4</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; --</span>
<span class="quote">&gt; To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in</span>
<span class="quote">&gt; the body of a message to majordomo@vger.kernel.org</span>
<span class="quote">&gt; More majordomo info at  http://vger.kernel.org/majordomo-info.html</span>
<span class="quote">&gt; Please read the FAQ at  http://www.tux.org/lkml/</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=1141">Aneesh Kumar K.V</a> - May 7, 2015, 11:18 a.m.</div>
<pre class="content">
&quot;Kirill A. Shutemov&quot; &lt;kirill@shutemov.name&gt; writes:
<span class="quote">
&gt; On Thu, May 07, 2015 at 12:53:27PM +0530, Aneesh Kumar K.V wrote:</span>
<span class="quote">&gt;&gt; After this patch pmdp_* functions operate only on hugepage pte,</span>
<span class="quote">&gt;&gt; and not on regular pmd_t values pointing to page table.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; Signed-off-by: Aneesh Kumar K.V &lt;aneesh.kumar@linux.vnet.ibm.com&gt;</span>
<span class="quote">&gt;&gt; ---</span>
<span class="quote">&gt;&gt;  arch/powerpc/include/asm/pgtable-ppc64.h |  4 ++</span>
<span class="quote">&gt;&gt;  arch/powerpc/mm/pgtable_64.c             | 76 +++++++++++++++++---------------</span>
<span class="quote">&gt;&gt;  include/asm-generic/pgtable.h            | 19 ++++++++</span>
<span class="quote">&gt;&gt;  mm/huge_memory.c                         |  2 +-</span>
<span class="quote">&gt;&gt;  4 files changed, 65 insertions(+), 36 deletions(-)</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; diff --git a/arch/powerpc/include/asm/pgtable-ppc64.h b/arch/powerpc/include/asm/pgtable-ppc64.h</span>
<span class="quote">&gt;&gt; index 43e6ad424c7f..50830c9a2116 100644</span>
<span class="quote">&gt;&gt; --- a/arch/powerpc/include/asm/pgtable-ppc64.h</span>
<span class="quote">&gt;&gt; +++ b/arch/powerpc/include/asm/pgtable-ppc64.h</span>
<span class="quote">&gt;&gt; @@ -576,6 +576,10 @@ static inline void pmdp_set_wrprotect(struct mm_struct *mm, unsigned long addr,</span>
<span class="quote">&gt;&gt;  extern void pmdp_splitting_flush(struct vm_area_struct *vma,</span>
<span class="quote">&gt;&gt;  				 unsigned long address, pmd_t *pmdp);</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; +#define __HAVE_ARCH_PMDP_COLLAPSE_FLUSH</span>
<span class="quote">&gt;&gt; +extern pmd_t pmdp_collapse_flush(struct vm_area_struct *vma,</span>
<span class="quote">&gt;&gt; +				 unsigned long address, pmd_t *pmdp);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt;  #define __HAVE_ARCH_PGTABLE_DEPOSIT</span>
<span class="quote">&gt;&gt;  extern void pgtable_trans_huge_deposit(struct mm_struct *mm, pmd_t *pmdp,</span>
<span class="quote">&gt;&gt;  				       pgtable_t pgtable);</span>
<span class="quote">&gt;&gt; diff --git a/arch/powerpc/mm/pgtable_64.c b/arch/powerpc/mm/pgtable_64.c</span>
<span class="quote">&gt;&gt; index 59daa5eeec25..9171c1a37290 100644</span>
<span class="quote">&gt;&gt; --- a/arch/powerpc/mm/pgtable_64.c</span>
<span class="quote">&gt;&gt; +++ b/arch/powerpc/mm/pgtable_64.c</span>
<span class="quote">&gt;&gt; @@ -560,41 +560,47 @@ pmd_t pmdp_clear_flush(struct vm_area_struct *vma, unsigned long address,</span>
<span class="quote">&gt;&gt;  	pmd_t pmd;</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  	VM_BUG_ON(address &amp; ~HPAGE_PMD_MASK);</span>
<span class="quote">&gt;&gt; -	if (pmd_trans_huge(*pmdp)) {</span>
<span class="quote">&gt;&gt; -		pmd = pmdp_get_and_clear(vma-&gt;vm_mm, address, pmdp);</span>
<span class="quote">&gt;&gt; -	} else {</span>
<span class="quote">&gt;&gt; -		/*</span>
<span class="quote">&gt;&gt; -		 * khugepaged calls this for normal pmd</span>
<span class="quote">&gt;&gt; -		 */</span>
<span class="quote">&gt;&gt; -		pmd = *pmdp;</span>
<span class="quote">&gt;&gt; -		pmd_clear(pmdp);</span>
<span class="quote">&gt;&gt; -		/*</span>
<span class="quote">&gt;&gt; -		 * Wait for all pending hash_page to finish. This is needed</span>
<span class="quote">&gt;&gt; -		 * in case of subpage collapse. When we collapse normal pages</span>
<span class="quote">&gt;&gt; -		 * to hugepage, we first clear the pmd, then invalidate all</span>
<span class="quote">&gt;&gt; -		 * the PTE entries. The assumption here is that any low level</span>
<span class="quote">&gt;&gt; -		 * page fault will see a none pmd and take the slow path that</span>
<span class="quote">&gt;&gt; -		 * will wait on mmap_sem. But we could very well be in a</span>
<span class="quote">&gt;&gt; -		 * hash_page with local ptep pointer value. Such a hash page</span>
<span class="quote">&gt;&gt; -		 * can result in adding new HPTE entries for normal subpages.</span>
<span class="quote">&gt;&gt; -		 * That means we could be modifying the page content as we</span>
<span class="quote">&gt;&gt; -		 * copy them to a huge page. So wait for parallel hash_page</span>
<span class="quote">&gt;&gt; -		 * to finish before invalidating HPTE entries. We can do this</span>
<span class="quote">&gt;&gt; -		 * by sending an IPI to all the cpus and executing a dummy</span>
<span class="quote">&gt;&gt; -		 * function there.</span>
<span class="quote">&gt;&gt; -		 */</span>
<span class="quote">&gt;&gt; -		kick_all_cpus_sync();</span>
<span class="quote">&gt;&gt; -		/*</span>
<span class="quote">&gt;&gt; -		 * Now invalidate the hpte entries in the range</span>
<span class="quote">&gt;&gt; -		 * covered by pmd. This make sure we take a</span>
<span class="quote">&gt;&gt; -		 * fault and will find the pmd as none, which will</span>
<span class="quote">&gt;&gt; -		 * result in a major fault which takes mmap_sem and</span>
<span class="quote">&gt;&gt; -		 * hence wait for collapse to complete. Without this</span>
<span class="quote">&gt;&gt; -		 * the __collapse_huge_page_copy can result in copying</span>
<span class="quote">&gt;&gt; -		 * the old content.</span>
<span class="quote">&gt;&gt; -		 */</span>
<span class="quote">&gt;&gt; -		flush_tlb_pmd_range(vma-&gt;vm_mm, &amp;pmd, address);</span>
<span class="quote">&gt;&gt; -	}</span>
<span class="quote">&gt;&gt; +	VM_BUG_ON(!pmd_trans_huge(*pmdp));</span>
<span class="quote">&gt;&gt; +	pmd = pmdp_get_and_clear(vma-&gt;vm_mm, address, pmdp);</span>
<span class="quote">&gt;&gt; +	return pmd;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; The patches are in reverse order: you need to change pmdp_get_and_clear</span>
<span class="quote">&gt; first otherwise you break bisectability.</span>
<span class="quote">&gt; Or better merge patches together.</span>

The first patch is really a cleanup and should not result in code
changes. It just make sure that we use pmdp_* functions only on hugepage
ptes and not on regular pmd_t pointers to pgtable. It avoid the not so
nice if (pmd_trans_huge()) check in the code and allows us to do the
VM_BUG_ON(!pmd_trans_huge(*pmdp)) there. That is really important on
archs like ppc64 where regular pmd format is different from hugepage pte
format.
<span class="quote">

&gt;</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +pmd_t pmdp_collapse_flush(struct vm_area_struct *vma, unsigned long address,</span>
<span class="quote">&gt;&gt; +			  pmd_t *pmdp)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	pmd_t pmd;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	VM_BUG_ON(address &amp; ~HPAGE_PMD_MASK);</span>
<span class="quote">&gt;&gt; +	VM_BUG_ON(pmd_trans_huge(*pmdp));</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	pmd = *pmdp;</span>
<span class="quote">&gt;&gt; +	pmd_clear(pmdp);</span>
<span class="quote">&gt;&gt; +	/*</span>
<span class="quote">&gt;&gt; +	 * Wait for all pending hash_page to finish. This is needed</span>
<span class="quote">&gt;&gt; +	 * in case of subpage collapse. When we collapse normal pages</span>
<span class="quote">&gt;&gt; +	 * to hugepage, we first clear the pmd, then invalidate all</span>
<span class="quote">&gt;&gt; +	 * the PTE entries. The assumption here is that any low level</span>
<span class="quote">&gt;&gt; +	 * page fault will see a none pmd and take the slow path that</span>
<span class="quote">&gt;&gt; +	 * will wait on mmap_sem. But we could very well be in a</span>
<span class="quote">&gt;&gt; +	 * hash_page with local ptep pointer value. Such a hash page</span>
<span class="quote">&gt;&gt; +	 * can result in adding new HPTE entries for normal subpages.</span>
<span class="quote">&gt;&gt; +	 * That means we could be modifying the page content as we</span>
<span class="quote">&gt;&gt; +	 * copy them to a huge page. So wait for parallel hash_page</span>
<span class="quote">&gt;&gt; +	 * to finish before invalidating HPTE entries. We can do this</span>
<span class="quote">&gt;&gt; +	 * by sending an IPI to all the cpus and executing a dummy</span>
<span class="quote">&gt;&gt; +	 * function there.</span>
<span class="quote">&gt;&gt; +	 */</span>
<span class="quote">&gt;&gt; +	kick_all_cpus_sync();</span>
<span class="quote">&gt;&gt; +	/*</span>
<span class="quote">&gt;&gt; +	 * Now invalidate the hpte entries in the range</span>
<span class="quote">&gt;&gt; +	 * covered by pmd. This make sure we take a</span>
<span class="quote">&gt;&gt; +	 * fault and will find the pmd as none, which will</span>
<span class="quote">&gt;&gt; +	 * result in a major fault which takes mmap_sem and</span>
<span class="quote">&gt;&gt; +	 * hence wait for collapse to complete. Without this</span>
<span class="quote">&gt;&gt; +	 * the __collapse_huge_page_copy can result in copying</span>
<span class="quote">&gt;&gt; +	 * the old content.</span>
<span class="quote">&gt;&gt; +	 */</span>
<span class="quote">&gt;&gt; +	flush_tlb_pmd_range(vma-&gt;vm_mm, &amp;pmd, address);</span>
<span class="quote">&gt;&gt;  	return pmd;</span>
<span class="quote">&gt;&gt;  }</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; diff --git a/include/asm-generic/pgtable.h b/include/asm-generic/pgtable.h</span>
<span class="quote">&gt;&gt; index 39f1d6a2b04d..80e6d415cd57 100644</span>
<span class="quote">&gt;&gt; --- a/include/asm-generic/pgtable.h</span>
<span class="quote">&gt;&gt; +++ b/include/asm-generic/pgtable.h</span>
<span class="quote">&gt;&gt; @@ -189,6 +189,25 @@ extern void pmdp_splitting_flush(struct vm_area_struct *vma,</span>
<span class="quote">&gt;&gt;  				 unsigned long address, pmd_t *pmdp);</span>
<span class="quote">&gt;&gt;  #endif</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; +#ifndef __HAVE_ARCH_PMDP_COLLAPSE_FLUSH</span>
<span class="quote">&gt;&gt; +#ifdef CONFIG_TRANSPARENT_HUGEPAGE</span>
<span class="quote">&gt;&gt; +static inline pmd_t pmdp_collapse_flush(struct vm_area_struct *vma,</span>
<span class="quote">&gt;&gt; +				       unsigned long address,</span>
<span class="quote">&gt;&gt; +				       pmd_t *pmdp)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	return pmdp_clear_flush(vma, address, pmdp);</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +#else</span>
<span class="quote">&gt;&gt; +static inline pmd_t pmdp_collapse_flush(struct vm_area_struct *vma,</span>
<span class="quote">&gt;&gt; +				       unsigned long address,</span>
<span class="quote">&gt;&gt; +				       pmd_t *pmdp)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	BUILD_BUG();</span>
<span class="quote">&gt;&gt; +	return __pmd(0);</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +#endif /* CONFIG_TRANSPARENT_HUGEPAGE */</span>
<span class="quote">&gt;&gt; +#endif</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt;  #ifndef __HAVE_ARCH_PGTABLE_DEPOSIT</span>
<span class="quote">&gt;&gt;  extern void pgtable_trans_huge_deposit(struct mm_struct *mm, pmd_t *pmdp,</span>
<span class="quote">&gt;&gt;  				       pgtable_t pgtable);</span>
<span class="quote">&gt;&gt; diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="quote">&gt;&gt; index 078832cf3636..88f695a4e38b 100644</span>
<span class="quote">&gt;&gt; --- a/mm/huge_memory.c</span>
<span class="quote">&gt;&gt; +++ b/mm/huge_memory.c</span>
<span class="quote">&gt;&gt; @@ -2499,7 +2499,7 @@ static void collapse_huge_page(struct mm_struct *mm,</span>
<span class="quote">&gt;&gt;  	 * huge and small TLB entries for the same virtual address</span>
<span class="quote">&gt;&gt;  	 * to avoid the risk of CPU bugs in that area.</span>
<span class="quote">&gt;&gt;  	 */</span>
<span class="quote">&gt;&gt; -	_pmd = pmdp_clear_flush(vma, address, pmd);</span>
<span class="quote">&gt;&gt; +	_pmd = pmdp_collapse_flush(vma, address, pmd);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Why? pmdp_clear_flush() does kick_all_cpus_sync() already.</span>

Here we are clearing the regular pmd_t and for ppc64 that means we need
to invalidate all the normal page pte mappings we already have inserted
in the hardware hash page table. But before doing that we need to make
sure there are no parallel hash page table insert going on. So we need
to do a kick_all_cpus_sync() before flushing the older hash table
entries. By moving this to a seperate function we capture these details
nicely.

-aneesh

--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=41">Andrew Morton</a> - May 8, 2015, 10:24 p.m.</div>
<pre class="content">
On Thu,  7 May 2015 12:53:27 +0530 &quot;Aneesh Kumar K.V&quot; &lt;aneesh.kumar@linux.vnet.ibm.com&gt; wrote:
<span class="quote">
&gt; After this patch pmdp_* functions operate only on hugepage pte,</span>
<span class="quote">&gt; and not on regular pmd_t values pointing to page table.</span>
<span class="quote">&gt; </span>

The patch looks like a pretty safe no-op for non-powerpc?
<span class="quote">
&gt; --- a/arch/powerpc/include/asm/pgtable-ppc64.h</span>
<span class="quote">&gt; +++ b/arch/powerpc/include/asm/pgtable-ppc64.h</span>
<span class="quote">&gt; @@ -576,6 +576,10 @@ static inline void pmdp_set_wrprotect(struct mm_struct *mm, unsigned long addr,</span>
<span class="quote">&gt;  extern void pmdp_splitting_flush(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  				 unsigned long address, pmd_t *pmdp);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +#define __HAVE_ARCH_PMDP_COLLAPSE_FLUSH</span>
<span class="quote">&gt; +extern pmd_t pmdp_collapse_flush(struct vm_area_struct *vma,</span>
<span class="quote">&gt; +				 unsigned long address, pmd_t *pmdp);</span>
<span class="quote">&gt; +</span>

The fashionable way of doing this is

extern pmd_t pmdp_collapse_flush(struct vm_area_struct *vma,
				 unsigned long address, pmd_t *pmdp);
#define pmdp_collapse_flush pmdp_collapse_flush

then, elsewhere,

#ifndef pmdp_collapse_flush
static inline pmd_t pmdp_collapse_flush(...) {}
#define pmdp_collapse_flush pmdp_collapse_flush
#endif

It avoids introducing a second (ugly) symbol into the kernel.

--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=1141">Aneesh Kumar K.V</a> - May 11, 2015, 6:32 a.m.</div>
<pre class="content">
Andrew Morton &lt;akpm@linux-foundation.org&gt; writes:
<span class="quote">
&gt; On Thu,  7 May 2015 12:53:27 +0530 &quot;Aneesh Kumar K.V&quot; &lt;aneesh.kumar@linux.vnet.ibm.com&gt; wrote:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; After this patch pmdp_* functions operate only on hugepage pte,</span>
<span class="quote">&gt;&gt; and not on regular pmd_t values pointing to page table.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;</span>
<span class="quote">&gt; The patch looks like a pretty safe no-op for non-powerpc?</span>

That is correct. I also updated the commit message

    mm/thp: Split out pmd collpase flush into a seperate functions
    
    Architectures like ppc64 [1] need to do special things while clearing
    pmd before a collapse. For them this operation is largely different
    from a normal hugepage pte clear. Hence add a separate function
    to clear pmd before collapse. After this patch pmdp_* functions
    operate only on hugepage pte, and not on regular pmd_t values
    pointing to page table.
    
    [1] ppc64 needs to invalidate all the normal page pte mappings we
    already have inserted in the hardware hash page table. But before
    doing that we need to make sure there are no parallel hash page
    table insert going on. So we need to do a kick_all_cpus_sync()
    before flushing the older hash table entries. By moving this to
    a separate function we capture these details and mention how it
    is different from a hugepage pte clear.
<span class="quote">

&gt;</span>
<span class="quote">&gt;&gt; --- a/arch/powerpc/include/asm/pgtable-ppc64.h</span>
<span class="quote">&gt;&gt; +++ b/arch/powerpc/include/asm/pgtable-ppc64.h</span>
<span class="quote">&gt;&gt; @@ -576,6 +576,10 @@ static inline void pmdp_set_wrprotect(struct mm_struct *mm, unsigned long addr,</span>
<span class="quote">&gt;&gt;  extern void pmdp_splitting_flush(struct vm_area_struct *vma,</span>
<span class="quote">&gt;&gt;  				 unsigned long address, pmd_t *pmdp);</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; +#define __HAVE_ARCH_PMDP_COLLAPSE_FLUSH</span>
<span class="quote">&gt;&gt; +extern pmd_t pmdp_collapse_flush(struct vm_area_struct *vma,</span>
<span class="quote">&gt;&gt; +				 unsigned long address, pmd_t *pmdp);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; The fashionable way of doing this is</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; extern pmd_t pmdp_collapse_flush(struct vm_area_struct *vma,</span>
<span class="quote">&gt; 				 unsigned long address, pmd_t *pmdp);</span>
<span class="quote">&gt; #define pmdp_collapse_flush pmdp_collapse_flush</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; then, elsewhere,</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; #ifndef pmdp_collapse_flush</span>
<span class="quote">&gt; static inline pmd_t pmdp_collapse_flush(...) {}</span>
<span class="quote">&gt; #define pmdp_collapse_flush pmdp_collapse_flush</span>
<span class="quote">&gt; #endif</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; It avoids introducing a second (ugly) symbol into the kernel.</span>

Ok updated to the above style. The reason I used the earlier style was
because of similar usages in asm-generic/pgtable.h


-aneesh

--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/powerpc/include/asm/pgtable-ppc64.h b/arch/powerpc/include/asm/pgtable-ppc64.h</span>
<span class="p_header">index 43e6ad424c7f..50830c9a2116 100644</span>
<span class="p_header">--- a/arch/powerpc/include/asm/pgtable-ppc64.h</span>
<span class="p_header">+++ b/arch/powerpc/include/asm/pgtable-ppc64.h</span>
<span class="p_chunk">@@ -576,6 +576,10 @@</span> <span class="p_context"> static inline void pmdp_set_wrprotect(struct mm_struct *mm, unsigned long addr,</span>
 extern void pmdp_splitting_flush(struct vm_area_struct *vma,
 				 unsigned long address, pmd_t *pmdp);
 
<span class="p_add">+#define __HAVE_ARCH_PMDP_COLLAPSE_FLUSH</span>
<span class="p_add">+extern pmd_t pmdp_collapse_flush(struct vm_area_struct *vma,</span>
<span class="p_add">+				 unsigned long address, pmd_t *pmdp);</span>
<span class="p_add">+</span>
 #define __HAVE_ARCH_PGTABLE_DEPOSIT
 extern void pgtable_trans_huge_deposit(struct mm_struct *mm, pmd_t *pmdp,
 				       pgtable_t pgtable);
<span class="p_header">diff --git a/arch/powerpc/mm/pgtable_64.c b/arch/powerpc/mm/pgtable_64.c</span>
<span class="p_header">index 59daa5eeec25..9171c1a37290 100644</span>
<span class="p_header">--- a/arch/powerpc/mm/pgtable_64.c</span>
<span class="p_header">+++ b/arch/powerpc/mm/pgtable_64.c</span>
<span class="p_chunk">@@ -560,41 +560,47 @@</span> <span class="p_context"> pmd_t pmdp_clear_flush(struct vm_area_struct *vma, unsigned long address,</span>
 	pmd_t pmd;
 
 	VM_BUG_ON(address &amp; ~HPAGE_PMD_MASK);
<span class="p_del">-	if (pmd_trans_huge(*pmdp)) {</span>
<span class="p_del">-		pmd = pmdp_get_and_clear(vma-&gt;vm_mm, address, pmdp);</span>
<span class="p_del">-	} else {</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * khugepaged calls this for normal pmd</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		pmd = *pmdp;</span>
<span class="p_del">-		pmd_clear(pmdp);</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * Wait for all pending hash_page to finish. This is needed</span>
<span class="p_del">-		 * in case of subpage collapse. When we collapse normal pages</span>
<span class="p_del">-		 * to hugepage, we first clear the pmd, then invalidate all</span>
<span class="p_del">-		 * the PTE entries. The assumption here is that any low level</span>
<span class="p_del">-		 * page fault will see a none pmd and take the slow path that</span>
<span class="p_del">-		 * will wait on mmap_sem. But we could very well be in a</span>
<span class="p_del">-		 * hash_page with local ptep pointer value. Such a hash page</span>
<span class="p_del">-		 * can result in adding new HPTE entries for normal subpages.</span>
<span class="p_del">-		 * That means we could be modifying the page content as we</span>
<span class="p_del">-		 * copy them to a huge page. So wait for parallel hash_page</span>
<span class="p_del">-		 * to finish before invalidating HPTE entries. We can do this</span>
<span class="p_del">-		 * by sending an IPI to all the cpus and executing a dummy</span>
<span class="p_del">-		 * function there.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		kick_all_cpus_sync();</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * Now invalidate the hpte entries in the range</span>
<span class="p_del">-		 * covered by pmd. This make sure we take a</span>
<span class="p_del">-		 * fault and will find the pmd as none, which will</span>
<span class="p_del">-		 * result in a major fault which takes mmap_sem and</span>
<span class="p_del">-		 * hence wait for collapse to complete. Without this</span>
<span class="p_del">-		 * the __collapse_huge_page_copy can result in copying</span>
<span class="p_del">-		 * the old content.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		flush_tlb_pmd_range(vma-&gt;vm_mm, &amp;pmd, address);</span>
<span class="p_del">-	}</span>
<span class="p_add">+	VM_BUG_ON(!pmd_trans_huge(*pmdp));</span>
<span class="p_add">+	pmd = pmdp_get_and_clear(vma-&gt;vm_mm, address, pmdp);</span>
<span class="p_add">+	return pmd;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+pmd_t pmdp_collapse_flush(struct vm_area_struct *vma, unsigned long address,</span>
<span class="p_add">+			  pmd_t *pmdp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pmd_t pmd;</span>
<span class="p_add">+</span>
<span class="p_add">+	VM_BUG_ON(address &amp; ~HPAGE_PMD_MASK);</span>
<span class="p_add">+	VM_BUG_ON(pmd_trans_huge(*pmdp));</span>
<span class="p_add">+</span>
<span class="p_add">+	pmd = *pmdp;</span>
<span class="p_add">+	pmd_clear(pmdp);</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Wait for all pending hash_page to finish. This is needed</span>
<span class="p_add">+	 * in case of subpage collapse. When we collapse normal pages</span>
<span class="p_add">+	 * to hugepage, we first clear the pmd, then invalidate all</span>
<span class="p_add">+	 * the PTE entries. The assumption here is that any low level</span>
<span class="p_add">+	 * page fault will see a none pmd and take the slow path that</span>
<span class="p_add">+	 * will wait on mmap_sem. But we could very well be in a</span>
<span class="p_add">+	 * hash_page with local ptep pointer value. Such a hash page</span>
<span class="p_add">+	 * can result in adding new HPTE entries for normal subpages.</span>
<span class="p_add">+	 * That means we could be modifying the page content as we</span>
<span class="p_add">+	 * copy them to a huge page. So wait for parallel hash_page</span>
<span class="p_add">+	 * to finish before invalidating HPTE entries. We can do this</span>
<span class="p_add">+	 * by sending an IPI to all the cpus and executing a dummy</span>
<span class="p_add">+	 * function there.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	kick_all_cpus_sync();</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Now invalidate the hpte entries in the range</span>
<span class="p_add">+	 * covered by pmd. This make sure we take a</span>
<span class="p_add">+	 * fault and will find the pmd as none, which will</span>
<span class="p_add">+	 * result in a major fault which takes mmap_sem and</span>
<span class="p_add">+	 * hence wait for collapse to complete. Without this</span>
<span class="p_add">+	 * the __collapse_huge_page_copy can result in copying</span>
<span class="p_add">+	 * the old content.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	flush_tlb_pmd_range(vma-&gt;vm_mm, &amp;pmd, address);</span>
 	return pmd;
 }
 
<span class="p_header">diff --git a/include/asm-generic/pgtable.h b/include/asm-generic/pgtable.h</span>
<span class="p_header">index 39f1d6a2b04d..80e6d415cd57 100644</span>
<span class="p_header">--- a/include/asm-generic/pgtable.h</span>
<span class="p_header">+++ b/include/asm-generic/pgtable.h</span>
<span class="p_chunk">@@ -189,6 +189,25 @@</span> <span class="p_context"> extern void pmdp_splitting_flush(struct vm_area_struct *vma,</span>
 				 unsigned long address, pmd_t *pmdp);
 #endif
 
<span class="p_add">+#ifndef __HAVE_ARCH_PMDP_COLLAPSE_FLUSH</span>
<span class="p_add">+#ifdef CONFIG_TRANSPARENT_HUGEPAGE</span>
<span class="p_add">+static inline pmd_t pmdp_collapse_flush(struct vm_area_struct *vma,</span>
<span class="p_add">+				       unsigned long address,</span>
<span class="p_add">+				       pmd_t *pmdp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return pmdp_clear_flush(vma, address, pmdp);</span>
<span class="p_add">+}</span>
<span class="p_add">+#else</span>
<span class="p_add">+static inline pmd_t pmdp_collapse_flush(struct vm_area_struct *vma,</span>
<span class="p_add">+				       unsigned long address,</span>
<span class="p_add">+				       pmd_t *pmdp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	BUILD_BUG();</span>
<span class="p_add">+	return __pmd(0);</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif /* CONFIG_TRANSPARENT_HUGEPAGE */</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 #ifndef __HAVE_ARCH_PGTABLE_DEPOSIT
 extern void pgtable_trans_huge_deposit(struct mm_struct *mm, pmd_t *pmdp,
 				       pgtable_t pgtable);
<span class="p_header">diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="p_header">index 078832cf3636..88f695a4e38b 100644</span>
<span class="p_header">--- a/mm/huge_memory.c</span>
<span class="p_header">+++ b/mm/huge_memory.c</span>
<span class="p_chunk">@@ -2499,7 +2499,7 @@</span> <span class="p_context"> static void collapse_huge_page(struct mm_struct *mm,</span>
 	 * huge and small TLB entries for the same virtual address
 	 * to avoid the risk of CPU bugs in that area.
 	 */
<span class="p_del">-	_pmd = pmdp_clear_flush(vma, address, pmd);</span>
<span class="p_add">+	_pmd = pmdp_collapse_flush(vma, address, pmd);</span>
 	spin_unlock(pmd_ptl);
 	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);
 

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



