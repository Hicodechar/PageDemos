
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v3] mm/hugetlb: fix memory offline with hugepage size &gt; memory block size - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v3] mm/hugetlb: fix memory offline with hugepage size &gt; memory block size</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=2816">Gerald Schaefer</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Sept. 22, 2016, 4:29 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20160922182937.38af9d0e@thinkpad&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9345799/mbox/"
   >mbox</a>
|
   <a href="/patch/9345799/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9345799/">/patch/9345799/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	C1CC360757 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 22 Sep 2016 16:29:54 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id B29AB2ABC0
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 22 Sep 2016 16:29:54 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id A6ECB2ABC2; Thu, 22 Sep 2016 16:29:54 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 2D2AB2ABC0
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 22 Sep 2016 16:29:54 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1757739AbcIVQ3t (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 22 Sep 2016 12:29:49 -0400
Received: from mx0a-001b2d01.pphosted.com ([148.163.156.1]:47912 &quot;EHLO
	mx0a-001b2d01.pphosted.com&quot; rhost-flags-OK-OK-OK-OK)
	by vger.kernel.org with ESMTP id S1751580AbcIVQ3s (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 22 Sep 2016 12:29:48 -0400
Received: from pps.filterd (m0098410.ppops.net [127.0.0.1])
	by mx0a-001b2d01.pphosted.com (8.16.0.17/8.16.0.17) with SMTP id
	u8MGSLsh043677
	for &lt;linux-kernel@vger.kernel.org&gt;; Thu, 22 Sep 2016 12:29:47 -0400
Received: from e06smtp12.uk.ibm.com (e06smtp12.uk.ibm.com [195.75.94.108])
	by mx0a-001b2d01.pphosted.com with ESMTP id 25ktabetg9-1
	(version=TLSv1.2 cipher=AES256-SHA bits=256 verify=NOT)
	for &lt;linux-kernel@vger.kernel.org&gt;; Thu, 22 Sep 2016 12:29:47 -0400
Received: from localhost
	by e06smtp12.uk.ibm.com with IBM ESMTP SMTP Gateway: Authorized Use
	Only! Violators will be prosecuted
	for &lt;linux-kernel@vger.kernel.org&gt; from &lt;gerald.schaefer@de.ibm.com&gt;; 
	Thu, 22 Sep 2016 17:29:43 +0100
Received: from d06dlp01.portsmouth.uk.ibm.com (9.149.20.13)
	by e06smtp12.uk.ibm.com (192.168.101.142) with IBM ESMTP SMTP
	Gateway: Authorized Use Only! Violators will be prosecuted; 
	Thu, 22 Sep 2016 17:29:41 +0100
Received: from b06cxnps4076.portsmouth.uk.ibm.com
	(d06relay13.portsmouth.uk.ibm.com [9.149.109.198])
	by d06dlp01.portsmouth.uk.ibm.com (Postfix) with ESMTP id
	13DE917D8024 for &lt;linux-kernel@vger.kernel.org&gt;;
	Thu, 22 Sep 2016 17:31:40 +0100 (BST)
Received: from d06av08.portsmouth.uk.ibm.com (d06av08.portsmouth.uk.ibm.com
	[9.149.37.249])
	by b06cxnps4076.portsmouth.uk.ibm.com (8.14.9/8.14.9/NCO v10.0) with
	ESMTP id u8MGTeDx14745956
	for &lt;linux-kernel@vger.kernel.org&gt;; Thu, 22 Sep 2016 16:29:40 GMT
Received: from d06av08.portsmouth.uk.ibm.com (localhost [127.0.0.1])
	by d06av08.portsmouth.uk.ibm.com (8.14.4/8.14.4/NCO v10.0 AVout) with
	ESMTP id u8MGTcXS018454
	for &lt;linux-kernel@vger.kernel.org&gt;; Thu, 22 Sep 2016 10:29:40 -0600
Received: from thinkpad (dyn-9-152-212-72.boeblingen.de.ibm.com
	[9.152.212.72])
	by d06av08.portsmouth.uk.ibm.com (8.14.4/8.14.4/NCO v10.0 AVin) with
	ESMTP id u8MGTc1t018413; Thu, 22 Sep 2016 10:29:38 -0600
Date: Thu, 22 Sep 2016 18:29:37 +0200
From: Gerald Schaefer &lt;gerald.schaefer@de.ibm.com&gt;
To: Andrew Morton &lt;akpm@linux-foundation.org&gt;
Cc: Gerald Schaefer &lt;gerald.schaefer@de.ibm.com&gt;,
	Michal Hocko &lt;mhocko@kernel.org&gt;,
	Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;,
	Hillf Danton &lt;hillf.zj@alibaba-inc.com&gt;, linux-mm@kvack.org,
	linux-kernel@vger.kernel.org,
	&quot;Kirill A . Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt;,
	Vlastimil Babka &lt;vbabka@suse.cz&gt;, Mike Kravetz &lt;mike.kravetz@oracle.com&gt;,
	&quot;Aneesh Kumar K . V&quot; &lt;aneesh.kumar@linux.vnet.ibm.com&gt;,
	Martin Schwidefsky &lt;schwidefsky@de.ibm.com&gt;,
	Heiko Carstens &lt;heiko.carstens@de.ibm.com&gt;,
	Dave Hansen &lt;dave.hansen@linux.intel.com&gt;,
	Rui Teng &lt;rui.teng@linux.vnet.ibm.com&gt;
Subject: [PATCH v3] mm/hugetlb: fix memory offline with hugepage size &gt;
	memory block size
In-Reply-To: &lt;20160922154549.483ee313@thinkpad&gt;
References: &lt;20160920155354.54403-1-gerald.schaefer@de.ibm.com&gt;
	&lt;20160920155354.54403-2-gerald.schaefer@de.ibm.com&gt;
	&lt;05d701d213d1$7fb70880$7f251980$@alibaba-inc.com&gt;
	&lt;20160921143534.0dd95fe7@thinkpad&gt;
	&lt;20160922095137.GC11875@dhcp22.suse.cz&gt;
	&lt;20160922154549.483ee313@thinkpad&gt;
Organization: IBM Deutschland Research &amp; Development GmbH / Vorsitzende des
	Aufsichtsrats: Martina Koederitz / Geschaeftsfuehrung: Dirk
	Wittkopp / Sitz
	der Gesellschaft: Boeblingen / Registergericht: Amtsgericht Stuttgart,
	HRB 243294
X-Mailer: Claws Mail 3.9.0 (GTK+ 2.24.23; x86_64-redhat-linux-gnu)
Mime-Version: 1.0
Content-Type: text/plain; charset=US-ASCII
Content-Transfer-Encoding: 7bit
X-TM-AS-MML: disable
X-Content-Scanned: Fidelis XPS MAILER
x-cbid: 16092216-0008-0000-0000-000002D0B19B
X-IBM-AV-DETECTION: SAVI=unused REMOTE=unused XFE=unused
x-cbparentid: 16092216-0009-0000-0000-00001A11343A
Message-Id: &lt;20160922182937.38af9d0e@thinkpad&gt;
X-Proofpoint-Virus-Version: vendor=fsecure engine=2.50.10432:, ,
	definitions=2016-09-22_08:, , signatures=0
X-Proofpoint-Spam-Details: rule=outbound_notspam policy=outbound score=0
	spamscore=0 suspectscore=2
	malwarescore=0 phishscore=0 adultscore=0 bulkscore=0 classifier=spam
	adjust=0 reason=mlx scancount=1 engine=8.0.1-1609020000
	definitions=main-1609220294
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2816">Gerald Schaefer</a> - Sept. 22, 2016, 4:29 p.m.</div>
<pre class="content">
dissolve_free_huge_pages() will either run into the VM_BUG_ON() or a
list corruption and addressing exception when trying to set a memory
block offline that is part (but not the first part) of a &quot;gigantic&quot;
hugetlb page with a size &gt; memory block size.

When no other smaller hugetlb page sizes are present, the VM_BUG_ON()
will trigger directly. In the other case we will run into an addressing
exception later, because dissolve_free_huge_page() will not work on the
head page of the compound hugetlb page which will result in a NULL
hstate from page_hstate().

To fix this, first remove the VM_BUG_ON() because it is wrong, and then
use the compound head page in dissolve_free_huge_page(). This means that
an unused pre-allocated gigantic page that has any part of itself inside
the memory block that is going offline will be dissolved completely.
Losing the gigantic hugepage is preferable to failing the memory offline,
for example in the situation where a (possibly faulty) memory DIMM needs
to go offline.

Also move the PageHuge() and page_count() checks out of
dissolve_free_huge_page() in order to only take the spin_lock when
actually removing a hugepage.

Fixes: c8721bbb (&quot;mm: memory-hotplug: enable memory hotplug to handle hugepage&quot;)
Cc: &lt;stable@vger.kernel.org&gt;
<span class="signed-off-by">Signed-off-by: Gerald Schaefer &lt;gerald.schaefer@de.ibm.com&gt;</span>
---
Changes in v3:
- Add Fixes: c8721bbb
- Add Cc: stable
- Elaborate on losing the gigantic page vs. failing memory offline
- Move page_count() check out of dissolve_free_huge_page()

Changes in v2:
- Update comment in dissolve_free_huge_pages()
- Change locking in dissolve_free_huge_page()

 mm/hugetlb.c | 34 +++++++++++++++++++---------------
 1 file changed, 19 insertions(+), 15 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=64071">Dave Hansen</a> - Sept. 22, 2016, 6:12 p.m.</div>
<pre class="content">
On 09/22/2016 09:29 AM, Gerald Schaefer wrote:
<span class="quote">&gt;  static void dissolve_free_huge_page(struct page *page)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; +	struct page *head = compound_head(page);</span>
<span class="quote">&gt; +	struct hstate *h = page_hstate(head);</span>
<span class="quote">&gt; +	int nid = page_to_nid(head);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	spin_lock(&amp;hugetlb_lock);</span>
<span class="quote">&gt; -	if (PageHuge(page) &amp;&amp; !page_count(page)) {</span>
<span class="quote">&gt; -		struct hstate *h = page_hstate(page);</span>
<span class="quote">&gt; -		int nid = page_to_nid(page);</span>
<span class="quote">&gt; -		list_del(&amp;page-&gt;lru);</span>
<span class="quote">&gt; -		h-&gt;free_huge_pages--;</span>
<span class="quote">&gt; -		h-&gt;free_huge_pages_node[nid]--;</span>
<span class="quote">&gt; -		h-&gt;max_huge_pages--;</span>
<span class="quote">&gt; -		update_and_free_page(h, page);</span>
<span class="quote">&gt; -	}</span>
<span class="quote">&gt; +	list_del(&amp;head-&gt;lru);</span>
<span class="quote">&gt; +	h-&gt;free_huge_pages--;</span>
<span class="quote">&gt; +	h-&gt;free_huge_pages_node[nid]--;</span>
<span class="quote">&gt; +	h-&gt;max_huge_pages--;</span>
<span class="quote">&gt; +	update_and_free_page(h, head);</span>
<span class="quote">&gt;  	spin_unlock(&amp;hugetlb_lock);</span>
<span class="quote">&gt;  }</span>

Do you need to revalidate anything once you acquire the lock?  Can this,
for instance, race with another thread doing vm.nr_hugepages=0?  Or a
thread faulting in and allocating the large page that&#39;s being dissolved?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=124511">Mike Kravetz</a> - Sept. 22, 2016, 7:13 p.m.</div>
<pre class="content">
On 09/22/2016 11:12 AM, Dave Hansen wrote:
<span class="quote">&gt; On 09/22/2016 09:29 AM, Gerald Schaefer wrote:</span>
<span class="quote">&gt;&gt;  static void dissolve_free_huge_page(struct page *page)</span>
<span class="quote">&gt;&gt;  {</span>
<span class="quote">&gt;&gt; +	struct page *head = compound_head(page);</span>
<span class="quote">&gt;&gt; +	struct hstate *h = page_hstate(head);</span>
<span class="quote">&gt;&gt; +	int nid = page_to_nid(head);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt;  	spin_lock(&amp;hugetlb_lock);</span>
<span class="quote">&gt;&gt; -	if (PageHuge(page) &amp;&amp; !page_count(page)) {</span>
<span class="quote">&gt;&gt; -		struct hstate *h = page_hstate(page);</span>
<span class="quote">&gt;&gt; -		int nid = page_to_nid(page);</span>
<span class="quote">&gt;&gt; -		list_del(&amp;page-&gt;lru);</span>
<span class="quote">&gt;&gt; -		h-&gt;free_huge_pages--;</span>
<span class="quote">&gt;&gt; -		h-&gt;free_huge_pages_node[nid]--;</span>
<span class="quote">&gt;&gt; -		h-&gt;max_huge_pages--;</span>
<span class="quote">&gt;&gt; -		update_and_free_page(h, page);</span>
<span class="quote">&gt;&gt; -	}</span>
<span class="quote">&gt;&gt; +	list_del(&amp;head-&gt;lru);</span>
<span class="quote">&gt;&gt; +	h-&gt;free_huge_pages--;</span>
<span class="quote">&gt;&gt; +	h-&gt;free_huge_pages_node[nid]--;</span>
<span class="quote">&gt;&gt; +	h-&gt;max_huge_pages--;</span>
<span class="quote">&gt;&gt; +	update_and_free_page(h, head);</span>
<span class="quote">&gt;&gt;  	spin_unlock(&amp;hugetlb_lock);</span>
<span class="quote">&gt;&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Do you need to revalidate anything once you acquire the lock?  Can this,</span>
<span class="quote">&gt; for instance, race with another thread doing vm.nr_hugepages=0?  Or a</span>
<span class="quote">&gt; thread faulting in and allocating the large page that&#39;s being dissolved?</span>

I originally suggested the locking change, but this is not quite right.
The page count for huge pages is adjusted while holding hugetlb_lock.
So, that check or a revalidation needs to be done while holding the lock.

That question made me think about huge page reservations.  I don&#39;t think
the offline code takes this into account.  But, you would not want your
huge page count to drop below the reserved huge page count
(resv_huge_pages).
So, shouldn&#39;t this be another condition to check before allowing the huge
page to be dissolved?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2816">Gerald Schaefer</a> - Sept. 23, 2016, 10:36 a.m.</div>
<pre class="content">
On Thu, 22 Sep 2016 11:12:06 -0700
Dave Hansen &lt;dave.hansen@linux.intel.com&gt; wrote:
<span class="quote">
&gt; On 09/22/2016 09:29 AM, Gerald Schaefer wrote:</span>
<span class="quote">&gt; &gt;  static void dissolve_free_huge_page(struct page *page)</span>
<span class="quote">&gt; &gt;  {</span>
<span class="quote">&gt; &gt; +	struct page *head = compound_head(page);</span>
<span class="quote">&gt; &gt; +	struct hstate *h = page_hstate(head);</span>
<span class="quote">&gt; &gt; +	int nid = page_to_nid(head);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt;  	spin_lock(&amp;hugetlb_lock);</span>
<span class="quote">&gt; &gt; -	if (PageHuge(page) &amp;&amp; !page_count(page)) {</span>
<span class="quote">&gt; &gt; -		struct hstate *h = page_hstate(page);</span>
<span class="quote">&gt; &gt; -		int nid = page_to_nid(page);</span>
<span class="quote">&gt; &gt; -		list_del(&amp;page-&gt;lru);</span>
<span class="quote">&gt; &gt; -		h-&gt;free_huge_pages--;</span>
<span class="quote">&gt; &gt; -		h-&gt;free_huge_pages_node[nid]--;</span>
<span class="quote">&gt; &gt; -		h-&gt;max_huge_pages--;</span>
<span class="quote">&gt; &gt; -		update_and_free_page(h, page);</span>
<span class="quote">&gt; &gt; -	}</span>
<span class="quote">&gt; &gt; +	list_del(&amp;head-&gt;lru);</span>
<span class="quote">&gt; &gt; +	h-&gt;free_huge_pages--;</span>
<span class="quote">&gt; &gt; +	h-&gt;free_huge_pages_node[nid]--;</span>
<span class="quote">&gt; &gt; +	h-&gt;max_huge_pages--;</span>
<span class="quote">&gt; &gt; +	update_and_free_page(h, head);</span>
<span class="quote">&gt; &gt;  	spin_unlock(&amp;hugetlb_lock);</span>
<span class="quote">&gt; &gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Do you need to revalidate anything once you acquire the lock?  Can this,</span>
<span class="quote">&gt; for instance, race with another thread doing vm.nr_hugepages=0?  Or a</span>
<span class="quote">&gt; thread faulting in and allocating the large page that&#39;s being dissolved?</span>
<span class="quote">&gt; </span>

Yes, good point. I was relying on the range being isolated, but that only
seems to be checked in dequeue_huge_page_node(), as introduced with the
original commit. So this would only protect against anyone allocating the
hugepage at this point. This is also somehow expected, since we already
are beyond the &quot;point of no return&quot; in offline_pages().

vm.nr_hugepages=0 seems to be an issue though, as set_max_hugepages()
will not care about isolation, and so I guess we could have a race here
and double-free the hugepage. Revalidation of at least PageHuge() after
taking the lock should protect from that, not sure about page_count(),
but I think I&#39;ll just check both which will give the same behaviour as
before.

Will send v4, after thinking a bit more on the page reservation point
brought up by Mike.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="p_header">index 87e11d8..29e10a2 100644</span>
<span class="p_header">--- a/mm/hugetlb.c</span>
<span class="p_header">+++ b/mm/hugetlb.c</span>
<span class="p_chunk">@@ -1436,39 +1436,43 @@</span> <span class="p_context"> static int free_pool_huge_page(struct hstate *h, nodemask_t *nodes_allowed,</span>
 }
 
 /*
<span class="p_del">- * Dissolve a given free hugepage into free buddy pages. This function does</span>
<span class="p_del">- * nothing for in-use (including surplus) hugepages.</span>
<span class="p_add">+ * Dissolve a given free hugepage into free buddy pages.</span>
  */
 static void dissolve_free_huge_page(struct page *page)
 {
<span class="p_add">+	struct page *head = compound_head(page);</span>
<span class="p_add">+	struct hstate *h = page_hstate(head);</span>
<span class="p_add">+	int nid = page_to_nid(head);</span>
<span class="p_add">+</span>
 	spin_lock(&amp;hugetlb_lock);
<span class="p_del">-	if (PageHuge(page) &amp;&amp; !page_count(page)) {</span>
<span class="p_del">-		struct hstate *h = page_hstate(page);</span>
<span class="p_del">-		int nid = page_to_nid(page);</span>
<span class="p_del">-		list_del(&amp;page-&gt;lru);</span>
<span class="p_del">-		h-&gt;free_huge_pages--;</span>
<span class="p_del">-		h-&gt;free_huge_pages_node[nid]--;</span>
<span class="p_del">-		h-&gt;max_huge_pages--;</span>
<span class="p_del">-		update_and_free_page(h, page);</span>
<span class="p_del">-	}</span>
<span class="p_add">+	list_del(&amp;head-&gt;lru);</span>
<span class="p_add">+	h-&gt;free_huge_pages--;</span>
<span class="p_add">+	h-&gt;free_huge_pages_node[nid]--;</span>
<span class="p_add">+	h-&gt;max_huge_pages--;</span>
<span class="p_add">+	update_and_free_page(h, head);</span>
 	spin_unlock(&amp;hugetlb_lock);
 }
 
 /*
  * Dissolve free hugepages in a given pfn range. Used by memory hotplug to
  * make specified memory blocks removable from the system.
<span class="p_del">- * Note that start_pfn should aligned with (minimum) hugepage size.</span>
<span class="p_add">+ * Note that this will dissolve a free gigantic hugepage completely, if any</span>
<span class="p_add">+ * part of it lies within the given range.</span>
<span class="p_add">+ * This function does nothing for in-use (including surplus) hugepages.</span>
  */
 void dissolve_free_huge_pages(unsigned long start_pfn, unsigned long end_pfn)
 {
 	unsigned long pfn;
<span class="p_add">+	struct page *page;</span>
 
 	if (!hugepages_supported())
 		return;
 
<span class="p_del">-	VM_BUG_ON(!IS_ALIGNED(start_pfn, 1 &lt;&lt; minimum_order));</span>
<span class="p_del">-	for (pfn = start_pfn; pfn &lt; end_pfn; pfn += 1 &lt;&lt; minimum_order)</span>
<span class="p_del">-		dissolve_free_huge_page(pfn_to_page(pfn));</span>
<span class="p_add">+	for (pfn = start_pfn; pfn &lt; end_pfn; pfn += 1 &lt;&lt; minimum_order) {</span>
<span class="p_add">+		page = pfn_to_page(pfn);</span>
<span class="p_add">+		if (PageHuge(page) &amp;&amp; !page_count(page))</span>
<span class="p_add">+			dissolve_free_huge_page(page);</span>
<span class="p_add">+	}</span>
 }
 
 /*

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



