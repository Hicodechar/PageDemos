
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v3,06/16] iommu/vt-d: add svm/sva invalidate function - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v3,06/16] iommu/vt-d: add svm/sva invalidate function</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=7040">Jacob Pan</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Nov. 17, 2017, 6:55 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1510944914-54430-7-git-send-email-jacob.jun.pan@linux.intel.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10063375/mbox/"
   >mbox</a>
|
   <a href="/patch/10063375/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10063375/">/patch/10063375/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	A7DB0601D3 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 17 Nov 2017 18:55:25 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id A11C220408
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 17 Nov 2017 18:55:25 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 956382AD7A; Fri, 17 Nov 2017 18:55:25 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id A22492AD51
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 17 Nov 2017 18:55:24 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S966314AbdKQSzW (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Fri, 17 Nov 2017 13:55:22 -0500
Received: from mga03.intel.com ([134.134.136.65]:54137 &quot;EHLO mga03.intel.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S934565AbdKQSy2 (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Fri, 17 Nov 2017 13:54:28 -0500
Received: from orsmga003.jf.intel.com ([10.7.209.27])
	by orsmga103.jf.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384;
	17 Nov 2017 10:54:15 -0800
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i=&quot;5.44,410,1505804400&quot;; d=&quot;scan&#39;208&quot;;a=&quot;3461178&quot;
Received: from jacob-builder.jf.intel.com ([10.7.199.155])
	by orsmga003.jf.intel.com with ESMTP; 17 Nov 2017 10:54:15 -0800
From: Jacob Pan &lt;jacob.jun.pan@linux.intel.com&gt;
To: iommu@lists.linux-foundation.org, LKML &lt;linux-kernel@vger.kernel.org&gt;,
	Joerg Roedel &lt;joro@8bytes.org&gt;, David Woodhouse &lt;dwmw2@infradead.org&gt;,
	Greg Kroah-Hartman &lt;gregkh@linuxfoundation.org&gt;,
	Rafael Wysocki &lt;rafael.j.wysocki@intel.com&gt;,
	Alex Williamson &lt;alex.williamson@redhat.com&gt;
Cc: &quot;Liu, Yi L&quot; &lt;yi.l.liu@intel.com&gt;, Lan Tianyu &lt;tianyu.lan@intel.com&gt;,
	&quot;Tian, Kevin&quot; &lt;kevin.tian@intel.com&gt;, Raj Ashok &lt;ashok.raj@intel.com&gt;,
	Jean Delvare &lt;khali@linux-fr.org&gt;,
	&quot;Christoph Hellwig&quot; &lt;hch@infradead.org&gt;,
	Jacob Pan &lt;jacob.jun.pan@linux.intel.com&gt;, Liu@vger.kernel.org,
	Yi L &lt;yi.l.liu@linux.intel.com&gt;
Subject: [PATCH v3 06/16] iommu/vt-d: add svm/sva invalidate function
Date: Fri, 17 Nov 2017 10:55:04 -0800
Message-Id: &lt;1510944914-54430-7-git-send-email-jacob.jun.pan@linux.intel.com&gt;
X-Mailer: git-send-email 2.7.4
In-Reply-To: &lt;1510944914-54430-1-git-send-email-jacob.jun.pan@linux.intel.com&gt;
References: &lt;1510944914-54430-1-git-send-email-jacob.jun.pan@linux.intel.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=7040">Jacob Pan</a> - Nov. 17, 2017, 6:55 p.m.</div>
<pre class="content">
This patch adds Intel VT-d specific function to implement
iommu passdown invalidate API for shared virtual address.

The use case is for supporting caching structure invalidation
of assigned SVM capable devices. Emulated IOMMU exposes queue
invalidation capability and passes down all descriptors from the guest
to the physical IOMMU.

The assumption is that guest to host device ID mapping should be
resolved prior to calling IOMMU driver. Based on the device handle,
host IOMMU driver can replace certain fields before submit to the
invalidation queue.
<span class="signed-off-by">
Signed-off-by: Liu, Yi L &lt;yi.l.liu@linux.intel.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Jacob Pan &lt;jacob.jun.pan@linux.intel.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Ashok Raj &lt;ashok.raj@intel.com&gt;</span>
---
 drivers/iommu/intel-iommu.c | 200 +++++++++++++++++++++++++++++++++++++++++++-
 include/linux/intel-iommu.h |  17 +++-
 2 files changed, 211 insertions(+), 6 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=99681">Lu Baolu</a> - Dec. 5, 2017, 5:43 a.m.</div>
<pre class="content">
Hi,

On 11/18/2017 02:55 AM, Jacob Pan wrote:
<span class="quote">&gt; This patch adds Intel VT-d specific function to implement</span>
<span class="quote">&gt; iommu passdown invalidate API for shared virtual address.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; The use case is for supporting caching structure invalidation</span>
<span class="quote">&gt; of assigned SVM capable devices. Emulated IOMMU exposes queue</span>
<span class="quote">&gt; invalidation capability and passes down all descriptors from the guest</span>
<span class="quote">&gt; to the physical IOMMU.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; The assumption is that guest to host device ID mapping should be</span>
<span class="quote">&gt; resolved prior to calling IOMMU driver. Based on the device handle,</span>
<span class="quote">&gt; host IOMMU driver can replace certain fields before submit to the</span>
<span class="quote">&gt; invalidation queue.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Signed-off-by: Liu, Yi L &lt;yi.l.liu@linux.intel.com&gt;</span>
<span class="quote">&gt; Signed-off-by: Jacob Pan &lt;jacob.jun.pan@linux.intel.com&gt;</span>
<span class="quote">&gt; Signed-off-by: Ashok Raj &lt;ashok.raj@intel.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  drivers/iommu/intel-iommu.c | 200 +++++++++++++++++++++++++++++++++++++++++++-</span>
<span class="quote">&gt;  include/linux/intel-iommu.h |  17 +++-</span>
<span class="quote">&gt;  2 files changed, 211 insertions(+), 6 deletions(-)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; diff --git a/drivers/iommu/intel-iommu.c b/drivers/iommu/intel-iommu.c</span>
<span class="quote">&gt; index 556bdd2..000b2b3 100644</span>
<span class="quote">&gt; --- a/drivers/iommu/intel-iommu.c</span>
<span class="quote">&gt; +++ b/drivers/iommu/intel-iommu.c</span>
<span class="quote">&gt; @@ -4981,6 +4981,183 @@ static void intel_iommu_detach_device(struct iommu_domain *domain,</span>
<span class="quote">&gt;  	dmar_remove_one_dev_info(to_dmar_domain(domain), dev);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * 3D array for converting IOMMU generic type-granularity to VT-d granularity</span>
<span class="quote">&gt; + * X indexed by enum iommu_inv_type</span>
<span class="quote">&gt; + * Y indicates request without and with PASID</span>
<span class="quote">&gt; + * Z indexed by enum enum iommu_inv_granularity</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * For an example, if we want to find the VT-d granularity encoding for IOTLB</span>
<span class="quote">&gt; + * type, DMA request with PASID, and page selective. The look up indices are:</span>
<span class="quote">&gt; + * [1][1][8], where</span>
<span class="quote">&gt; + * 1: IOMMU_INV_TYPE_TLB</span>
<span class="quote">&gt; + * 1: with PASID</span>
<span class="quote">&gt; + * 8: IOMMU_INV_GRANU_PAGE_PASID</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +const static int inv_type_granu_map[IOMMU_INV_NR_TYPE][2][IOMMU_INV_NR_GRANU] = {</span>
<span class="quote">&gt; +	/* extended dev IOTLBs, for dev-IOTLB, only global is valid,</span>
<span class="quote">&gt; +	   for dev-EXIOTLB, two valid granu */</span>
<span class="quote">&gt; +	{</span>
<span class="quote">&gt; +		{1},</span>
<span class="quote">&gt; +		{0, 0, 0, 0, 1, 1, 0, 0, 0}</span>
<span class="quote">&gt; +	},</span>
<span class="quote">&gt; +	/* IOTLB and EIOTLB */</span>
<span class="quote">&gt; +	{</span>
<span class="quote">&gt; +		{1, 1, 0, 1, 0, 0, 0, 0, 0},</span>
<span class="quote">&gt; +		{0, 0, 0, 0, 1, 0, 1, 1, 1}</span>
<span class="quote">&gt; +	},</span>
<span class="quote">&gt; +	/* PASID cache */</span>
<span class="quote">&gt; +	{</span>
<span class="quote">&gt; +		{0},</span>
<span class="quote">&gt; +		{0, 0, 0, 0, 1, 1, 0, 0, 0}</span>
<span class="quote">&gt; +	},</span>
<span class="quote">&gt; +	/* context cache */</span>
<span class="quote">&gt; +	{</span>
<span class="quote">&gt; +		{1, 1, 1}</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +};</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +const static u64 inv_type_granu_table[IOMMU_INV_NR_TYPE][2][IOMMU_INV_NR_GRANU] = {</span>
<span class="quote">&gt; +	/* extended dev IOTLBs, only global is valid */</span>
<span class="quote">&gt; +	{</span>
<span class="quote">&gt; +		{QI_DEV_IOTLB_GRAN_ALL},</span>
<span class="quote">&gt; +		{0, 0, 0, 0, QI_DEV_IOTLB_GRAN_ALL, QI_DEV_IOTLB_GRAN_PASID_SEL, 0, 0, 0}</span>
<span class="quote">&gt; +	},</span>
<span class="quote">&gt; +	/* IOTLB and EIOTLB */</span>
<span class="quote">&gt; +	{</span>
<span class="quote">&gt; +		{DMA_TLB_GLOBAL_FLUSH, DMA_TLB_DSI_FLUSH, 0, DMA_TLB_PSI_FLUSH},</span>
<span class="quote">&gt; +		{0, 0, 0, 0, QI_GRAN_ALL_ALL, 0, QI_GRAN_NONG_ALL, QI_GRAN_NONG_PASID, QI_GRAN_PSI_PASID}</span>
<span class="quote">&gt; +	},</span>
<span class="quote">&gt; +	/* PASID cache */</span>
<span class="quote">&gt; +	{</span>
<span class="quote">&gt; +		{0},</span>
<span class="quote">&gt; +		{0, 0, 0, 0, QI_PC_ALL_PASIDS, QI_PC_PASID_SEL}</span>
<span class="quote">&gt; +	},</span>
<span class="quote">&gt; +	/* context cache */</span>
<span class="quote">&gt; +	{</span>
<span class="quote">&gt; +		{DMA_CCMD_GLOBAL_INVL, DMA_CCMD_DOMAIN_INVL, DMA_CCMD_DEVICE_INVL}</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +};</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline int to_vtd_granularity(int type, int granu, int with_pasid, u64 *vtd_granu)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	if (type &gt;= IOMMU_INV_NR_TYPE || granu &gt;= IOMMU_INV_NR_GRANU || with_pasid &gt; 1)</span>
<span class="quote">&gt; +		return -EINVAL;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (inv_type_granu_map[type][with_pasid][granu] == 0)</span>
<span class="quote">&gt; +		return -EINVAL;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	*vtd_granu = inv_type_granu_table[type][with_pasid][granu];</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return 0;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static int intel_iommu_sva_invalidate(struct iommu_domain *domain,</span>
<span class="quote">&gt; +		struct device *dev, struct tlb_invalidate_info *inv_info)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct intel_iommu *iommu;</span>
<span class="quote">&gt; +	struct dmar_domain *dmar_domain = to_dmar_domain(domain);</span>
<span class="quote">&gt; +	struct device_domain_info *info;</span>
<span class="quote">&gt; +	struct pci_dev *pdev;</span>
<span class="quote">&gt; +	u16 did, sid, pfsid;</span>
<span class="quote">&gt; +	u8 bus, devfn;</span>
<span class="quote">&gt; +	int ret = 0;</span>
<span class="quote">&gt; +	u64 granu;</span>
<span class="quote">&gt; +	unsigned long flags;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (!inv_info || !dmar_domain)</span>
<span class="quote">&gt; +		return -EINVAL;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	iommu = device_to_iommu(dev, &amp;bus, &amp;devfn);</span>
<span class="quote">&gt; +	if (!iommu)</span>
<span class="quote">&gt; +		return -ENODEV;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (!dev || !dev_is_pci(dev))</span>
<span class="quote">&gt; +		return -ENODEV;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	did = dmar_domain-&gt;iommu_did[iommu-&gt;seq_id];</span>
<span class="quote">&gt; +	sid = PCI_DEVID(bus, devfn);</span>
<span class="quote">&gt; +	ret = to_vtd_granularity(inv_info-&gt;hdr.type, inv_info-&gt;granularity,</span>
<span class="quote">&gt; +				!!(inv_info-&gt;flags &amp; IOMMU_INVALIDATE_PASID_TAGGED), &amp;granu);</span>
<span class="quote">&gt; +	if (ret) {</span>
<span class="quote">&gt; +		pr_err(&quot;Invalid range type %d, granu %d\n&quot;, inv_info-&gt;hdr.type,</span>
<span class="quote">&gt; +			inv_info-&gt;granularity);</span>
<span class="quote">&gt; +		return ret;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	spin_lock(&amp;iommu-&gt;lock);</span>
<span class="quote">&gt; +	spin_lock_irqsave(&amp;device_domain_lock, flags);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	switch (inv_info-&gt;hdr.type) {</span>
<span class="quote">&gt; +	case IOMMU_INV_TYPE_CONTEXT:</span>
<span class="quote">&gt; +		iommu-&gt;flush.flush_context(iommu, did, sid,</span>
<span class="quote">&gt; +					DMA_CCMD_MASK_NOBIT, granu);</span>
<span class="quote">&gt; +		break;</span>
<span class="quote">&gt; +	case IOMMU_INV_TYPE_TLB:</span>
<span class="quote">&gt; +		/* We need to deal with two scenarios:</span>
<span class="quote">&gt; +		 * - IOTLB for request w/o PASID</span>
<span class="quote">&gt; +		 * - extended IOTLB for request with PASID.</span>
<span class="quote">&gt; +		 */</span>
<span class="quote">&gt; +		if (inv_info-&gt;size &amp;&amp;</span>
<span class="quote">&gt; +			(inv_info-&gt;addr &amp; ((1 &lt;&lt; (VTD_PAGE_SHIFT + inv_info-&gt;size)) - 1))) {</span>
<span class="quote">&gt; +			pr_err(&quot;Addr out of range, addr 0x%llx, size order %d\n&quot;,</span>
<span class="quote">&gt; +				inv_info-&gt;addr, inv_info-&gt;size);</span>
<span class="quote">&gt; +			ret = -ERANGE;</span>
<span class="quote">&gt; +			goto out_unlock;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		if (inv_info-&gt;flags &amp; IOMMU_INVALIDATE_PASID_TAGGED)</span>
<span class="quote">&gt; +			qi_flush_eiotlb(iommu, did, mm_to_dma_pfn(inv_info-&gt;addr),</span>
<span class="quote">&gt; +					inv_info-&gt;pasid,</span>
<span class="quote">&gt; +					inv_info-&gt;size, granu,</span>
<span class="quote">&gt; +					inv_info-&gt;flags &amp; IOMMU_INVALIDATE_GLOBAL_PAGE);</span>
<span class="quote">&gt; +		else</span>
<span class="quote">&gt; +			qi_flush_iotlb(iommu, did, mm_to_dma_pfn(inv_info-&gt;addr),</span>
<span class="quote">&gt; +				inv_info-&gt;size, granu);</span>
<span class="quote">&gt; +		/* For SRIOV VF, invalidation of device IOTLB requires PFSID */</span>
<span class="quote">&gt; +		pdev = to_pci_dev(dev);</span>
<span class="quote">&gt; +		if (pdev &amp;&amp; pdev-&gt;is_virtfn)</span>
<span class="quote">&gt; +			pfsid = PCI_DEVID(pdev-&gt;physfn-&gt;bus-&gt;number, pdev-&gt;physfn-&gt;devfn);</span>
<span class="quote">&gt; +		else</span>
<span class="quote">&gt; +			pfsid = sid;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		/**</span>
<span class="quote">&gt; +		 * Always flush device IOTLB if ATS is enabled since guest</span>
<span class="quote">&gt; +		 * vIOMMU exposes CM = 1, no device IOTLB flush will be passed</span>
<span class="quote">&gt; +		 * down.</span>
<span class="quote">&gt; +		 * TODO: check if device is VF, use PF ATS data if spec does not require</span>
<span class="quote">&gt; +		 * VF to include all PF capabilities,  VF qdep and VF ats_enabled.</span>
<span class="quote">&gt; +		 */</span>
<span class="quote">&gt; +		info = iommu_support_dev_iotlb(dmar_domain, iommu, bus, devfn);</span>
<span class="quote">&gt; +		if (info &amp;&amp; info-&gt;ats_enabled) {</span>
<span class="quote">&gt; +			if (inv_info-&gt;flags &amp; IOMMU_INVALIDATE_PASID_TAGGED)</span>
<span class="quote">&gt; +				qi_flush_dev_eiotlb(iommu, sid, info-&gt;pfsid,</span>
<span class="quote">&gt; +						inv_info-&gt;pasid, info-&gt;ats_qdep,</span>
<span class="quote">&gt; +						inv_info-&gt;addr, inv_info-&gt;size,</span>
<span class="quote">&gt; +						granu);</span>
<span class="quote">&gt; +			else</span>
<span class="quote">&gt; +				qi_flush_dev_iotlb(iommu, sid, info-&gt;pfsid,</span>
<span class="quote">&gt; +						info-&gt;ats_qdep, inv_info-&gt;addr,</span>
<span class="quote">&gt; +						inv_info-&gt;size);</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +		break;</span>
<span class="quote">&gt; +	case IOMMU_INV_TYPE_PASID:</span>
<span class="quote">&gt; +		qi_flush_pasid(iommu, did, granu, inv_info-&gt;pasid);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		break;</span>
<span class="quote">&gt; +	default:</span>
<span class="quote">&gt; +		dev_err(dev, &quot;Unknown IOMMU invalidation type %d\n&quot;,</span>
<span class="quote">&gt; +			inv_info-&gt;hdr.type);</span>
<span class="quote">&gt; +		ret = -EINVAL;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +out_unlock:</span>
<span class="quote">&gt; +	spin_unlock(&amp;iommu-&gt;lock);</span>
<span class="quote">&gt; +	spin_unlock_irqrestore(&amp;device_domain_lock, flags);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return ret;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  static int intel_iommu_map(struct iommu_domain *domain,</span>
<span class="quote">&gt;  			   unsigned long iova, phys_addr_t hpa,</span>
<span class="quote">&gt;  			   size_t size, int iommu_prot)</span>
<span class="quote">&gt; @@ -5304,7 +5481,7 @@ static int intel_iommu_bind_pasid_table(struct iommu_domain *domain,</span>
<span class="quote">&gt;  	iommu = device_to_iommu(dev, &amp;bus, &amp;devfn);</span>
<span class="quote">&gt;  	if (!iommu)</span>
<span class="quote">&gt;  		return -ENODEV;</span>
<span class="quote">&gt; -	/* VT-d spec 9.4 says pasid table size is encoded as 2^(x+5) */</span>
<span class="quote">&gt; +	/* VT-d spec section 9.4 says pasid table size is encoded as 2^(x+5) */</span>
<span class="quote">&gt;  	host_table_pasid_bits = intel_iommu_get_pts(iommu) + MIN_NR_PASID_BITS;</span>
<span class="quote">&gt;  	if (!pasidt_binfo || pasidt_binfo-&gt;pasid_bits &gt; host_table_pasid_bits ||</span>
<span class="quote">&gt;  		pasidt_binfo-&gt;pasid_bits &lt; MIN_NR_PASID_BITS) {</span>
<span class="quote">&gt; @@ -5313,7 +5490,11 @@ static int intel_iommu_bind_pasid_table(struct iommu_domain *domain,</span>
<span class="quote">&gt;  			MIN_NR_PASID_BITS, host_table_pasid_bits);</span>
<span class="quote">&gt;  		return -ERANGE;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; +	if (!ecap_nest(iommu-&gt;ecap)) {</span>
<span class="quote">&gt; +		dev_err(dev, &quot;Cannot bind PASID table, no nested translation\n&quot;);</span>
<span class="quote">&gt; +		ret = -EINVAL;</span>
<span class="quote">&gt; +		goto out;</span>
<span class="quote">&gt; +	}</span>

This and some of below changes could be included in patch 02/16.

Best regards,
Lu Baolu
<span class="quote">
&gt;  	pdev = to_pci_dev(dev);</span>
<span class="quote">&gt;  	sid = PCI_DEVID(bus, devfn);</span>
<span class="quote">&gt;  	info = dev-&gt;archdata.iommu;</span>
<span class="quote">&gt; @@ -5323,6 +5504,11 @@ static int intel_iommu_bind_pasid_table(struct iommu_domain *domain,</span>
<span class="quote">&gt;  		ret = -EINVAL;</span>
<span class="quote">&gt;  		goto out;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; +	if (info-&gt;pasid_table_bound) {</span>
<span class="quote">&gt; +		dev_err(dev, &quot;Device PASID table already bound\n&quot;);</span>
<span class="quote">&gt; +		ret = -EBUSY;</span>
<span class="quote">&gt; +		goto out;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt;  	if (!info-&gt;pasid_enabled) {</span>
<span class="quote">&gt;  		ret = pci_enable_pasid(pdev, info-&gt;pasid_supported &amp; ~1);</span>
<span class="quote">&gt;  		if (ret) {</span>
<span class="quote">&gt; @@ -5363,7 +5549,7 @@ static int intel_iommu_bind_pasid_table(struct iommu_domain *domain,</span>
<span class="quote">&gt;  				DMA_CCMD_MASK_NOBIT,</span>
<span class="quote">&gt;  				DMA_CCMD_DEVICE_INVL);</span>
<span class="quote">&gt;  	iommu-&gt;flush.flush_iotlb(iommu, did, 0, 0, DMA_TLB_DSI_FLUSH);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; +	info-&gt;pasid_table_bound = 1;</span>
<span class="quote">&gt;  out_unlock:</span>
<span class="quote">&gt;  	spin_unlock_irqrestore(&amp;iommu-&gt;lock, flags);</span>
<span class="quote">&gt;  out:</span>
<span class="quote">&gt; @@ -5375,8 +5561,14 @@ static void intel_iommu_unbind_pasid_table(struct iommu_domain *domain,</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct intel_iommu *iommu;</span>
<span class="quote">&gt;  	struct dmar_domain *dmar_domain = to_dmar_domain(domain);</span>
<span class="quote">&gt; +	struct device_domain_info *info;</span>
<span class="quote">&gt;  	u8 bus, devfn;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +	info = dev-&gt;archdata.iommu;</span>
<span class="quote">&gt; +	if (!info) {</span>
<span class="quote">&gt; +		dev_err(dev, &quot;Invalid device domain info\n&quot;);</span>
<span class="quote">&gt; +		return;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt;  	assert_spin_locked(&amp;device_domain_lock);</span>
<span class="quote">&gt;  	iommu = device_to_iommu(dev, &amp;bus, &amp;devfn);</span>
<span class="quote">&gt;  	if (!iommu) {</span>
<span class="quote">&gt; @@ -5387,6 +5579,7 @@ static void intel_iommu_unbind_pasid_table(struct iommu_domain *domain,</span>
<span class="quote">&gt;  	domain_context_clear(iommu, dev);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	domain_context_mapping_one(dmar_domain, iommu, bus, devfn);</span>
<span class="quote">&gt; +	info-&gt;pasid_table_bound = 0;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  #endif /* CONFIG_INTEL_IOMMU_SVM */</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -5399,6 +5592,7 @@ const struct iommu_ops intel_iommu_ops = {</span>
<span class="quote">&gt;  #ifdef CONFIG_INTEL_IOMMU_SVM</span>
<span class="quote">&gt;  	.bind_pasid_table	= intel_iommu_bind_pasid_table,</span>
<span class="quote">&gt;  	.unbind_pasid_table	= intel_iommu_unbind_pasid_table,</span>
<span class="quote">&gt; +	.sva_invalidate		= intel_iommu_sva_invalidate,</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt;  	.map			= intel_iommu_map,</span>
<span class="quote">&gt;  	.unmap			= intel_iommu_unmap,</span>
<span class="quote">&gt; diff --git a/include/linux/intel-iommu.h b/include/linux/intel-iommu.h</span>
<span class="quote">&gt; index 3c83f7e..7f05e36 100644</span>
<span class="quote">&gt; --- a/include/linux/intel-iommu.h</span>
<span class="quote">&gt; +++ b/include/linux/intel-iommu.h</span>
<span class="quote">&gt; @@ -258,6 +258,10 @@ enum {</span>
<span class="quote">&gt;  #define QI_PGRP_RESP_TYPE	0x9</span>
<span class="quote">&gt;  #define QI_PSTRM_RESP_TYPE	0xa</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +#define QI_DID(did)		(((u64)did &amp; 0xffff) &lt;&lt; 16)</span>
<span class="quote">&gt; +#define QI_DID_MASK		GENMASK(31, 16)</span>
<span class="quote">&gt; +#define QI_TYPE_MASK		GENMASK(3, 0)</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  #define QI_IEC_SELECTIVE	(((u64)1) &lt;&lt; 4)</span>
<span class="quote">&gt;  #define QI_IEC_IIDEX(idx)	(((u64)(idx &amp; 0xffff) &lt;&lt; 32))</span>
<span class="quote">&gt;  #define QI_IEC_IM(m)		(((u64)(m &amp; 0x1f) &lt;&lt; 27))</span>
<span class="quote">&gt; @@ -288,8 +292,9 @@ enum {</span>
<span class="quote">&gt;  #define QI_PC_DID(did)		(((u64)did) &lt;&lt; 16)</span>
<span class="quote">&gt;  #define QI_PC_GRAN(gran)	(((u64)gran) &lt;&lt; 4)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -#define QI_PC_ALL_PASIDS	(QI_PC_TYPE | QI_PC_GRAN(0))</span>
<span class="quote">&gt; -#define QI_PC_PASID_SEL		(QI_PC_TYPE | QI_PC_GRAN(1))</span>
<span class="quote">&gt; +/* PC inv granu */</span>
<span class="quote">&gt; +#define QI_PC_ALL_PASIDS	0</span>
<span class="quote">&gt; +#define QI_PC_PASID_SEL		1</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #define QI_EIOTLB_ADDR(addr)	((u64)(addr) &amp; VTD_PAGE_MASK)</span>
<span class="quote">&gt;  #define QI_EIOTLB_GL(gl)	(((u64)gl) &lt;&lt; 7)</span>
<span class="quote">&gt; @@ -299,6 +304,10 @@ enum {</span>
<span class="quote">&gt;  #define QI_EIOTLB_DID(did)	(((u64)did) &lt;&lt; 16)</span>
<span class="quote">&gt;  #define QI_EIOTLB_GRAN(gran) 	(((u64)gran) &lt;&lt; 4)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +/* QI Dev-IOTLB inv granu */</span>
<span class="quote">&gt; +#define QI_DEV_IOTLB_GRAN_ALL		0</span>
<span class="quote">&gt; +#define QI_DEV_IOTLB_GRAN_PASID_SEL	1</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  #define QI_DEV_EIOTLB_ADDR(a)	((u64)(a) &amp; VTD_PAGE_MASK)</span>
<span class="quote">&gt;  #define QI_DEV_EIOTLB_SIZE	(((u64)1) &lt;&lt; 11)</span>
<span class="quote">&gt;  #define QI_DEV_EIOTLB_GLOB(g)	((u64)g)</span>
<span class="quote">&gt; @@ -327,6 +336,7 @@ enum {</span>
<span class="quote">&gt;  #define QI_RESP_INVALID		0x1</span>
<span class="quote">&gt;  #define QI_RESP_FAILURE		0xf</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +/* QI EIOTLB inv granu */</span>
<span class="quote">&gt;  #define QI_GRAN_ALL_ALL			0</span>
<span class="quote">&gt;  #define QI_GRAN_NONG_ALL		1</span>
<span class="quote">&gt;  #define QI_GRAN_NONG_PASID		2</span>
<span class="quote">&gt; @@ -471,6 +481,7 @@ struct device_domain_info {</span>
<span class="quote">&gt;  	u8 pri_enabled:1;</span>
<span class="quote">&gt;  	u8 ats_supported:1;</span>
<span class="quote">&gt;  	u8 ats_enabled:1;</span>
<span class="quote">&gt; +	u8 pasid_table_bound:1;</span>
<span class="quote">&gt;  	u8 ats_qdep;</span>
<span class="quote">&gt;  	u64 fault_mask;	/* selected IOMMU faults to be reported */</span>
<span class="quote">&gt;  	struct device *dev; /* it&#39;s NULL for PCIe-to-PCI bridge */</span>
<span class="quote">&gt; @@ -502,7 +513,7 @@ extern void qi_flush_eiotlb(struct intel_iommu *iommu, u16 did, u64 addr,</span>
<span class="quote">&gt;  extern void qi_flush_dev_iotlb(struct intel_iommu *iommu, u16 sid, u16 pfsid,</span>
<span class="quote">&gt;  			u16 qdep, u64 addr, unsigned mask);</span>
<span class="quote">&gt;  extern void qi_flush_dev_eiotlb(struct intel_iommu *iommu, u16 sid, u16 pfsid,</span>
<span class="quote">&gt; -				u32 pasid, u16 qdep, u64 addr, unsigned size);</span>
<span class="quote">&gt; +			u32 pasid, u16 qdep, u64 addr, unsigned size, u64 granu);</span>
<span class="quote">&gt;  extern void qi_flush_pasid(struct intel_iommu *iommu, u16 did, u64 granu, int pasid);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  extern int qi_submit_sync(struct qi_desc *desc, struct intel_iommu *iommu);</span>
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/drivers/iommu/intel-iommu.c b/drivers/iommu/intel-iommu.c</span>
<span class="p_header">index 556bdd2..000b2b3 100644</span>
<span class="p_header">--- a/drivers/iommu/intel-iommu.c</span>
<span class="p_header">+++ b/drivers/iommu/intel-iommu.c</span>
<span class="p_chunk">@@ -4981,6 +4981,183 @@</span> <span class="p_context"> static void intel_iommu_detach_device(struct iommu_domain *domain,</span>
 	dmar_remove_one_dev_info(to_dmar_domain(domain), dev);
 }
 
<span class="p_add">+/*</span>
<span class="p_add">+ * 3D array for converting IOMMU generic type-granularity to VT-d granularity</span>
<span class="p_add">+ * X indexed by enum iommu_inv_type</span>
<span class="p_add">+ * Y indicates request without and with PASID</span>
<span class="p_add">+ * Z indexed by enum enum iommu_inv_granularity</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * For an example, if we want to find the VT-d granularity encoding for IOTLB</span>
<span class="p_add">+ * type, DMA request with PASID, and page selective. The look up indices are:</span>
<span class="p_add">+ * [1][1][8], where</span>
<span class="p_add">+ * 1: IOMMU_INV_TYPE_TLB</span>
<span class="p_add">+ * 1: with PASID</span>
<span class="p_add">+ * 8: IOMMU_INV_GRANU_PAGE_PASID</span>
<span class="p_add">+ *</span>
<span class="p_add">+ */</span>
<span class="p_add">+const static int inv_type_granu_map[IOMMU_INV_NR_TYPE][2][IOMMU_INV_NR_GRANU] = {</span>
<span class="p_add">+	/* extended dev IOTLBs, for dev-IOTLB, only global is valid,</span>
<span class="p_add">+	   for dev-EXIOTLB, two valid granu */</span>
<span class="p_add">+	{</span>
<span class="p_add">+		{1},</span>
<span class="p_add">+		{0, 0, 0, 0, 1, 1, 0, 0, 0}</span>
<span class="p_add">+	},</span>
<span class="p_add">+	/* IOTLB and EIOTLB */</span>
<span class="p_add">+	{</span>
<span class="p_add">+		{1, 1, 0, 1, 0, 0, 0, 0, 0},</span>
<span class="p_add">+		{0, 0, 0, 0, 1, 0, 1, 1, 1}</span>
<span class="p_add">+	},</span>
<span class="p_add">+	/* PASID cache */</span>
<span class="p_add">+	{</span>
<span class="p_add">+		{0},</span>
<span class="p_add">+		{0, 0, 0, 0, 1, 1, 0, 0, 0}</span>
<span class="p_add">+	},</span>
<span class="p_add">+	/* context cache */</span>
<span class="p_add">+	{</span>
<span class="p_add">+		{1, 1, 1}</span>
<span class="p_add">+	}</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+const static u64 inv_type_granu_table[IOMMU_INV_NR_TYPE][2][IOMMU_INV_NR_GRANU] = {</span>
<span class="p_add">+	/* extended dev IOTLBs, only global is valid */</span>
<span class="p_add">+	{</span>
<span class="p_add">+		{QI_DEV_IOTLB_GRAN_ALL},</span>
<span class="p_add">+		{0, 0, 0, 0, QI_DEV_IOTLB_GRAN_ALL, QI_DEV_IOTLB_GRAN_PASID_SEL, 0, 0, 0}</span>
<span class="p_add">+	},</span>
<span class="p_add">+	/* IOTLB and EIOTLB */</span>
<span class="p_add">+	{</span>
<span class="p_add">+		{DMA_TLB_GLOBAL_FLUSH, DMA_TLB_DSI_FLUSH, 0, DMA_TLB_PSI_FLUSH},</span>
<span class="p_add">+		{0, 0, 0, 0, QI_GRAN_ALL_ALL, 0, QI_GRAN_NONG_ALL, QI_GRAN_NONG_PASID, QI_GRAN_PSI_PASID}</span>
<span class="p_add">+	},</span>
<span class="p_add">+	/* PASID cache */</span>
<span class="p_add">+	{</span>
<span class="p_add">+		{0},</span>
<span class="p_add">+		{0, 0, 0, 0, QI_PC_ALL_PASIDS, QI_PC_PASID_SEL}</span>
<span class="p_add">+	},</span>
<span class="p_add">+	/* context cache */</span>
<span class="p_add">+	{</span>
<span class="p_add">+		{DMA_CCMD_GLOBAL_INVL, DMA_CCMD_DOMAIN_INVL, DMA_CCMD_DEVICE_INVL}</span>
<span class="p_add">+	}</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int to_vtd_granularity(int type, int granu, int with_pasid, u64 *vtd_granu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (type &gt;= IOMMU_INV_NR_TYPE || granu &gt;= IOMMU_INV_NR_GRANU || with_pasid &gt; 1)</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (inv_type_granu_map[type][with_pasid][granu] == 0)</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+</span>
<span class="p_add">+	*vtd_granu = inv_type_granu_table[type][with_pasid][granu];</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int intel_iommu_sva_invalidate(struct iommu_domain *domain,</span>
<span class="p_add">+		struct device *dev, struct tlb_invalidate_info *inv_info)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct intel_iommu *iommu;</span>
<span class="p_add">+	struct dmar_domain *dmar_domain = to_dmar_domain(domain);</span>
<span class="p_add">+	struct device_domain_info *info;</span>
<span class="p_add">+	struct pci_dev *pdev;</span>
<span class="p_add">+	u16 did, sid, pfsid;</span>
<span class="p_add">+	u8 bus, devfn;</span>
<span class="p_add">+	int ret = 0;</span>
<span class="p_add">+	u64 granu;</span>
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!inv_info || !dmar_domain)</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+</span>
<span class="p_add">+	iommu = device_to_iommu(dev, &amp;bus, &amp;devfn);</span>
<span class="p_add">+	if (!iommu)</span>
<span class="p_add">+		return -ENODEV;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!dev || !dev_is_pci(dev))</span>
<span class="p_add">+		return -ENODEV;</span>
<span class="p_add">+</span>
<span class="p_add">+	did = dmar_domain-&gt;iommu_did[iommu-&gt;seq_id];</span>
<span class="p_add">+	sid = PCI_DEVID(bus, devfn);</span>
<span class="p_add">+	ret = to_vtd_granularity(inv_info-&gt;hdr.type, inv_info-&gt;granularity,</span>
<span class="p_add">+				!!(inv_info-&gt;flags &amp; IOMMU_INVALIDATE_PASID_TAGGED), &amp;granu);</span>
<span class="p_add">+	if (ret) {</span>
<span class="p_add">+		pr_err(&quot;Invalid range type %d, granu %d\n&quot;, inv_info-&gt;hdr.type,</span>
<span class="p_add">+			inv_info-&gt;granularity);</span>
<span class="p_add">+		return ret;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	spin_lock(&amp;iommu-&gt;lock);</span>
<span class="p_add">+	spin_lock_irqsave(&amp;device_domain_lock, flags);</span>
<span class="p_add">+</span>
<span class="p_add">+	switch (inv_info-&gt;hdr.type) {</span>
<span class="p_add">+	case IOMMU_INV_TYPE_CONTEXT:</span>
<span class="p_add">+		iommu-&gt;flush.flush_context(iommu, did, sid,</span>
<span class="p_add">+					DMA_CCMD_MASK_NOBIT, granu);</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case IOMMU_INV_TYPE_TLB:</span>
<span class="p_add">+		/* We need to deal with two scenarios:</span>
<span class="p_add">+		 * - IOTLB for request w/o PASID</span>
<span class="p_add">+		 * - extended IOTLB for request with PASID.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (inv_info-&gt;size &amp;&amp;</span>
<span class="p_add">+			(inv_info-&gt;addr &amp; ((1 &lt;&lt; (VTD_PAGE_SHIFT + inv_info-&gt;size)) - 1))) {</span>
<span class="p_add">+			pr_err(&quot;Addr out of range, addr 0x%llx, size order %d\n&quot;,</span>
<span class="p_add">+				inv_info-&gt;addr, inv_info-&gt;size);</span>
<span class="p_add">+			ret = -ERANGE;</span>
<span class="p_add">+			goto out_unlock;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		if (inv_info-&gt;flags &amp; IOMMU_INVALIDATE_PASID_TAGGED)</span>
<span class="p_add">+			qi_flush_eiotlb(iommu, did, mm_to_dma_pfn(inv_info-&gt;addr),</span>
<span class="p_add">+					inv_info-&gt;pasid,</span>
<span class="p_add">+					inv_info-&gt;size, granu,</span>
<span class="p_add">+					inv_info-&gt;flags &amp; IOMMU_INVALIDATE_GLOBAL_PAGE);</span>
<span class="p_add">+		else</span>
<span class="p_add">+			qi_flush_iotlb(iommu, did, mm_to_dma_pfn(inv_info-&gt;addr),</span>
<span class="p_add">+				inv_info-&gt;size, granu);</span>
<span class="p_add">+		/* For SRIOV VF, invalidation of device IOTLB requires PFSID */</span>
<span class="p_add">+		pdev = to_pci_dev(dev);</span>
<span class="p_add">+		if (pdev &amp;&amp; pdev-&gt;is_virtfn)</span>
<span class="p_add">+			pfsid = PCI_DEVID(pdev-&gt;physfn-&gt;bus-&gt;number, pdev-&gt;physfn-&gt;devfn);</span>
<span class="p_add">+		else</span>
<span class="p_add">+			pfsid = sid;</span>
<span class="p_add">+</span>
<span class="p_add">+		/**</span>
<span class="p_add">+		 * Always flush device IOTLB if ATS is enabled since guest</span>
<span class="p_add">+		 * vIOMMU exposes CM = 1, no device IOTLB flush will be passed</span>
<span class="p_add">+		 * down.</span>
<span class="p_add">+		 * TODO: check if device is VF, use PF ATS data if spec does not require</span>
<span class="p_add">+		 * VF to include all PF capabilities,  VF qdep and VF ats_enabled.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		info = iommu_support_dev_iotlb(dmar_domain, iommu, bus, devfn);</span>
<span class="p_add">+		if (info &amp;&amp; info-&gt;ats_enabled) {</span>
<span class="p_add">+			if (inv_info-&gt;flags &amp; IOMMU_INVALIDATE_PASID_TAGGED)</span>
<span class="p_add">+				qi_flush_dev_eiotlb(iommu, sid, info-&gt;pfsid,</span>
<span class="p_add">+						inv_info-&gt;pasid, info-&gt;ats_qdep,</span>
<span class="p_add">+						inv_info-&gt;addr, inv_info-&gt;size,</span>
<span class="p_add">+						granu);</span>
<span class="p_add">+			else</span>
<span class="p_add">+				qi_flush_dev_iotlb(iommu, sid, info-&gt;pfsid,</span>
<span class="p_add">+						info-&gt;ats_qdep, inv_info-&gt;addr,</span>
<span class="p_add">+						inv_info-&gt;size);</span>
<span class="p_add">+		}</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case IOMMU_INV_TYPE_PASID:</span>
<span class="p_add">+		qi_flush_pasid(iommu, did, granu, inv_info-&gt;pasid);</span>
<span class="p_add">+</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	default:</span>
<span class="p_add">+		dev_err(dev, &quot;Unknown IOMMU invalidation type %d\n&quot;,</span>
<span class="p_add">+			inv_info-&gt;hdr.type);</span>
<span class="p_add">+		ret = -EINVAL;</span>
<span class="p_add">+	}</span>
<span class="p_add">+out_unlock:</span>
<span class="p_add">+	spin_unlock(&amp;iommu-&gt;lock);</span>
<span class="p_add">+	spin_unlock_irqrestore(&amp;device_domain_lock, flags);</span>
<span class="p_add">+</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static int intel_iommu_map(struct iommu_domain *domain,
 			   unsigned long iova, phys_addr_t hpa,
 			   size_t size, int iommu_prot)
<span class="p_chunk">@@ -5304,7 +5481,7 @@</span> <span class="p_context"> static int intel_iommu_bind_pasid_table(struct iommu_domain *domain,</span>
 	iommu = device_to_iommu(dev, &amp;bus, &amp;devfn);
 	if (!iommu)
 		return -ENODEV;
<span class="p_del">-	/* VT-d spec 9.4 says pasid table size is encoded as 2^(x+5) */</span>
<span class="p_add">+	/* VT-d spec section 9.4 says pasid table size is encoded as 2^(x+5) */</span>
 	host_table_pasid_bits = intel_iommu_get_pts(iommu) + MIN_NR_PASID_BITS;
 	if (!pasidt_binfo || pasidt_binfo-&gt;pasid_bits &gt; host_table_pasid_bits ||
 		pasidt_binfo-&gt;pasid_bits &lt; MIN_NR_PASID_BITS) {
<span class="p_chunk">@@ -5313,7 +5490,11 @@</span> <span class="p_context"> static int intel_iommu_bind_pasid_table(struct iommu_domain *domain,</span>
 			MIN_NR_PASID_BITS, host_table_pasid_bits);
 		return -ERANGE;
 	}
<span class="p_del">-</span>
<span class="p_add">+	if (!ecap_nest(iommu-&gt;ecap)) {</span>
<span class="p_add">+		dev_err(dev, &quot;Cannot bind PASID table, no nested translation\n&quot;);</span>
<span class="p_add">+		ret = -EINVAL;</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+	}</span>
 	pdev = to_pci_dev(dev);
 	sid = PCI_DEVID(bus, devfn);
 	info = dev-&gt;archdata.iommu;
<span class="p_chunk">@@ -5323,6 +5504,11 @@</span> <span class="p_context"> static int intel_iommu_bind_pasid_table(struct iommu_domain *domain,</span>
 		ret = -EINVAL;
 		goto out;
 	}
<span class="p_add">+	if (info-&gt;pasid_table_bound) {</span>
<span class="p_add">+		dev_err(dev, &quot;Device PASID table already bound\n&quot;);</span>
<span class="p_add">+		ret = -EBUSY;</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+	}</span>
 	if (!info-&gt;pasid_enabled) {
 		ret = pci_enable_pasid(pdev, info-&gt;pasid_supported &amp; ~1);
 		if (ret) {
<span class="p_chunk">@@ -5363,7 +5549,7 @@</span> <span class="p_context"> static int intel_iommu_bind_pasid_table(struct iommu_domain *domain,</span>
 				DMA_CCMD_MASK_NOBIT,
 				DMA_CCMD_DEVICE_INVL);
 	iommu-&gt;flush.flush_iotlb(iommu, did, 0, 0, DMA_TLB_DSI_FLUSH);
<span class="p_del">-</span>
<span class="p_add">+	info-&gt;pasid_table_bound = 1;</span>
 out_unlock:
 	spin_unlock_irqrestore(&amp;iommu-&gt;lock, flags);
 out:
<span class="p_chunk">@@ -5375,8 +5561,14 @@</span> <span class="p_context"> static void intel_iommu_unbind_pasid_table(struct iommu_domain *domain,</span>
 {
 	struct intel_iommu *iommu;
 	struct dmar_domain *dmar_domain = to_dmar_domain(domain);
<span class="p_add">+	struct device_domain_info *info;</span>
 	u8 bus, devfn;
 
<span class="p_add">+	info = dev-&gt;archdata.iommu;</span>
<span class="p_add">+	if (!info) {</span>
<span class="p_add">+		dev_err(dev, &quot;Invalid device domain info\n&quot;);</span>
<span class="p_add">+		return;</span>
<span class="p_add">+	}</span>
 	assert_spin_locked(&amp;device_domain_lock);
 	iommu = device_to_iommu(dev, &amp;bus, &amp;devfn);
 	if (!iommu) {
<span class="p_chunk">@@ -5387,6 +5579,7 @@</span> <span class="p_context"> static void intel_iommu_unbind_pasid_table(struct iommu_domain *domain,</span>
 	domain_context_clear(iommu, dev);
 
 	domain_context_mapping_one(dmar_domain, iommu, bus, devfn);
<span class="p_add">+	info-&gt;pasid_table_bound = 0;</span>
 }
 #endif /* CONFIG_INTEL_IOMMU_SVM */
 
<span class="p_chunk">@@ -5399,6 +5592,7 @@</span> <span class="p_context"> const struct iommu_ops intel_iommu_ops = {</span>
 #ifdef CONFIG_INTEL_IOMMU_SVM
 	.bind_pasid_table	= intel_iommu_bind_pasid_table,
 	.unbind_pasid_table	= intel_iommu_unbind_pasid_table,
<span class="p_add">+	.sva_invalidate		= intel_iommu_sva_invalidate,</span>
 #endif
 	.map			= intel_iommu_map,
 	.unmap			= intel_iommu_unmap,
<span class="p_header">diff --git a/include/linux/intel-iommu.h b/include/linux/intel-iommu.h</span>
<span class="p_header">index 3c83f7e..7f05e36 100644</span>
<span class="p_header">--- a/include/linux/intel-iommu.h</span>
<span class="p_header">+++ b/include/linux/intel-iommu.h</span>
<span class="p_chunk">@@ -258,6 +258,10 @@</span> <span class="p_context"> enum {</span>
 #define QI_PGRP_RESP_TYPE	0x9
 #define QI_PSTRM_RESP_TYPE	0xa
 
<span class="p_add">+#define QI_DID(did)		(((u64)did &amp; 0xffff) &lt;&lt; 16)</span>
<span class="p_add">+#define QI_DID_MASK		GENMASK(31, 16)</span>
<span class="p_add">+#define QI_TYPE_MASK		GENMASK(3, 0)</span>
<span class="p_add">+</span>
 #define QI_IEC_SELECTIVE	(((u64)1) &lt;&lt; 4)
 #define QI_IEC_IIDEX(idx)	(((u64)(idx &amp; 0xffff) &lt;&lt; 32))
 #define QI_IEC_IM(m)		(((u64)(m &amp; 0x1f) &lt;&lt; 27))
<span class="p_chunk">@@ -288,8 +292,9 @@</span> <span class="p_context"> enum {</span>
 #define QI_PC_DID(did)		(((u64)did) &lt;&lt; 16)
 #define QI_PC_GRAN(gran)	(((u64)gran) &lt;&lt; 4)
 
<span class="p_del">-#define QI_PC_ALL_PASIDS	(QI_PC_TYPE | QI_PC_GRAN(0))</span>
<span class="p_del">-#define QI_PC_PASID_SEL		(QI_PC_TYPE | QI_PC_GRAN(1))</span>
<span class="p_add">+/* PC inv granu */</span>
<span class="p_add">+#define QI_PC_ALL_PASIDS	0</span>
<span class="p_add">+#define QI_PC_PASID_SEL		1</span>
 
 #define QI_EIOTLB_ADDR(addr)	((u64)(addr) &amp; VTD_PAGE_MASK)
 #define QI_EIOTLB_GL(gl)	(((u64)gl) &lt;&lt; 7)
<span class="p_chunk">@@ -299,6 +304,10 @@</span> <span class="p_context"> enum {</span>
 #define QI_EIOTLB_DID(did)	(((u64)did) &lt;&lt; 16)
 #define QI_EIOTLB_GRAN(gran) 	(((u64)gran) &lt;&lt; 4)
 
<span class="p_add">+/* QI Dev-IOTLB inv granu */</span>
<span class="p_add">+#define QI_DEV_IOTLB_GRAN_ALL		0</span>
<span class="p_add">+#define QI_DEV_IOTLB_GRAN_PASID_SEL	1</span>
<span class="p_add">+</span>
 #define QI_DEV_EIOTLB_ADDR(a)	((u64)(a) &amp; VTD_PAGE_MASK)
 #define QI_DEV_EIOTLB_SIZE	(((u64)1) &lt;&lt; 11)
 #define QI_DEV_EIOTLB_GLOB(g)	((u64)g)
<span class="p_chunk">@@ -327,6 +336,7 @@</span> <span class="p_context"> enum {</span>
 #define QI_RESP_INVALID		0x1
 #define QI_RESP_FAILURE		0xf
 
<span class="p_add">+/* QI EIOTLB inv granu */</span>
 #define QI_GRAN_ALL_ALL			0
 #define QI_GRAN_NONG_ALL		1
 #define QI_GRAN_NONG_PASID		2
<span class="p_chunk">@@ -471,6 +481,7 @@</span> <span class="p_context"> struct device_domain_info {</span>
 	u8 pri_enabled:1;
 	u8 ats_supported:1;
 	u8 ats_enabled:1;
<span class="p_add">+	u8 pasid_table_bound:1;</span>
 	u8 ats_qdep;
 	u64 fault_mask;	/* selected IOMMU faults to be reported */
 	struct device *dev; /* it&#39;s NULL for PCIe-to-PCI bridge */
<span class="p_chunk">@@ -502,7 +513,7 @@</span> <span class="p_context"> extern void qi_flush_eiotlb(struct intel_iommu *iommu, u16 did, u64 addr,</span>
 extern void qi_flush_dev_iotlb(struct intel_iommu *iommu, u16 sid, u16 pfsid,
 			u16 qdep, u64 addr, unsigned mask);
 extern void qi_flush_dev_eiotlb(struct intel_iommu *iommu, u16 sid, u16 pfsid,
<span class="p_del">-				u32 pasid, u16 qdep, u64 addr, unsigned size);</span>
<span class="p_add">+			u32 pasid, u16 qdep, u64 addr, unsigned size, u64 granu);</span>
 extern void qi_flush_pasid(struct intel_iommu *iommu, u16 did, u64 granu, int pasid);
 
 extern int qi_submit_sync(struct qi_desc *desc, struct intel_iommu *iommu);

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



