
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v2,01/10] mm/hugetlb: add cache of descriptors to resv_map for region_add - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v2,01/10] mm/hugetlb: add cache of descriptors to resv_map for region_add</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=124511">Mike Kravetz</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>July 9, 2015, 12:21 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1436401301-18839-2-git-send-email-mike.kravetz@oracle.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/6752101/mbox/"
   >mbox</a>
|
   <a href="/patch/6752101/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/6752101/">/patch/6752101/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork2.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork2.web.kernel.org (Postfix) with ESMTP id CB6DDC05AC
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  9 Jul 2015 00:23:19 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 82E432046F
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  9 Jul 2015 00:23:18 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id 5536C204F6
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  9 Jul 2015 00:23:16 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752666AbbGIAXE (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Wed, 8 Jul 2015 20:23:04 -0400
Received: from aserp1040.oracle.com ([141.146.126.69]:46882 &quot;EHLO
	aserp1040.oracle.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1751941AbbGIAWk (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Wed, 8 Jul 2015 20:22:40 -0400
Received: from userv0021.oracle.com (userv0021.oracle.com [156.151.31.71])
	by aserp1040.oracle.com (Sentrion-MTA-4.3.2/Sentrion-MTA-4.3.2) with
	ESMTP id t690M3N6017830
	(version=TLSv1 cipher=DHE-RSA-AES256-SHA bits=256 verify=OK);
	Thu, 9 Jul 2015 00:22:04 GMT
Received: from userv0122.oracle.com (userv0122.oracle.com [156.151.31.75])
	by userv0021.oracle.com (8.13.8/8.13.8) with ESMTP id t690M32G021531
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-SHA bits=256 verify=FAIL); 
	Thu, 9 Jul 2015 00:22:03 GMT
Received: from abhmp0017.oracle.com (abhmp0017.oracle.com [141.146.116.23])
	by userv0122.oracle.com (8.13.8/8.13.8) with ESMTP id
	t690M2bT015562; Thu, 9 Jul 2015 00:22:03 GMT
Received: from monkey.oracle.com (/50.53.81.168)
	by default (Oracle Beehive Gateway v4.0)
	with ESMTP ; Wed, 08 Jul 2015 17:22:02 -0700
From: Mike Kravetz &lt;mike.kravetz@oracle.com&gt;
To: linux-mm@kvack.org, linux-kernel@vger.kernel.org,
	linux-api@vger.kernel.org
Cc: Dave Hansen &lt;dave.hansen@linux.intel.com&gt;,
	Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;,
	David Rientjes &lt;rientjes@google.com&gt;, Hugh Dickins &lt;hughd@google.com&gt;,
	Davidlohr Bueso &lt;dave@stgolabs.net&gt;,
	Aneesh Kumar &lt;aneesh.kumar@linux.vnet.ibm.com&gt;,
	Hillf Danton &lt;hillf.zj@alibaba-inc.com&gt;,
	Christoph Hellwig &lt;hch@infradead.org&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;, Michal Hocko &lt;mhocko@suse.cz&gt;,
	Mike Kravetz &lt;mike.kravetz@oracle.com&gt;
Subject: [PATCH v2 01/10] mm/hugetlb: add cache of descriptors to resv_map
	for region_add
Date: Wed,  8 Jul 2015 17:21:32 -0700
Message-Id: &lt;1436401301-18839-2-git-send-email-mike.kravetz@oracle.com&gt;
X-Mailer: git-send-email 2.1.0
In-Reply-To: &lt;1436401301-18839-1-git-send-email-mike.kravetz@oracle.com&gt;
References: &lt;1436401301-18839-1-git-send-email-mike.kravetz@oracle.com&gt;
X-Source-IP: userv0021.oracle.com [156.151.31.71]
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-7.6 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=124511">Mike Kravetz</a> - July 9, 2015, 12:21 a.m.</div>
<pre class="content">
fallocate hole punch will want to remove a specific range of
pages.  When pages are removed, their associated entries in
the region/reserve map will also be removed.  This will break
an assumption in the region_chg/region_add calling sequence.
If a new region descriptor must be allocated, it is done as
part of the region_chg processing.  In this way, region_add
can not fail because it does not need to attempt an allocation.

To prepare for fallocate hole punch, create a &quot;cache&quot; of
descriptors that can be used by region_add if necessary.
region_chg will ensure there are sufficient entries in the
cache.  It will be necessary to track the number of in progress
add operations to know a sufficient number of descriptors
reside in the cache.  A new routine region_abort is added to
adjust this in progress count when add operations are aborted.
vma_abort_reservation is also added for callers creating
reservations with vma_needs_reservation/vma_commit_reservation.
<span class="signed-off-by">
Signed-off-by: Mike Kravetz &lt;mike.kravetz@oracle.com&gt;</span>
---
 include/linux/hugetlb.h |   3 +
 mm/hugetlb.c            | 168 ++++++++++++++++++++++++++++++++++++++++++------
 2 files changed, 152 insertions(+), 19 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=124511">Mike Kravetz</a> - July 9, 2015, 12:43 a.m.</div>
<pre class="content">
On 07/08/2015 05:21 PM, Mike Kravetz wrote:
<span class="quote">&gt; fallocate hole punch will want to remove a specific range of</span>
<span class="quote">&gt; pages.  When pages are removed, their associated entries in</span>
<span class="quote">&gt; the region/reserve map will also be removed.  This will break</span>
<span class="quote">&gt; an assumption in the region_chg/region_add calling sequence.</span>
<span class="quote">&gt; If a new region descriptor must be allocated, it is done as</span>
<span class="quote">&gt; part of the region_chg processing.  In this way, region_add</span>
<span class="quote">&gt; can not fail because it does not need to attempt an allocation.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; To prepare for fallocate hole punch, create a &quot;cache&quot; of</span>
<span class="quote">&gt; descriptors that can be used by region_add if necessary.</span>
<span class="quote">&gt; region_chg will ensure there are sufficient entries in the</span>
<span class="quote">&gt; cache.  It will be necessary to track the number of in progress</span>
<span class="quote">&gt; add operations to know a sufficient number of descriptors</span>
<span class="quote">&gt; reside in the cache.  A new routine region_abort is added to</span>
<span class="quote">&gt; adjust this in progress count when add operations are aborted.</span>
<span class="quote">&gt; vma_abort_reservation is also added for callers creating</span>
<span class="quote">&gt; reservations with vma_needs_reservation/vma_commit_reservation.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Signed-off-by: Mike Kravetz &lt;mike.kravetz@oracle.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;   include/linux/hugetlb.h |   3 +</span>
<span class="quote">&gt;   mm/hugetlb.c            | 168 ++++++++++++++++++++++++++++++++++++++++++------</span>
<span class="quote">&gt;   2 files changed, 152 insertions(+), 19 deletions(-)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h</span>
<span class="quote">&gt; index d891f94..667cf44 100644</span>
<span class="quote">&gt; --- a/include/linux/hugetlb.h</span>
<span class="quote">&gt; +++ b/include/linux/hugetlb.h</span>
<span class="quote">&gt; @@ -35,6 +35,9 @@ struct resv_map {</span>
<span class="quote">&gt;   	struct kref refs;</span>
<span class="quote">&gt;   	spinlock_t lock;</span>
<span class="quote">&gt;   	struct list_head regions;</span>
<span class="quote">&gt; +	long adds_in_progress;</span>
<span class="quote">&gt; +	struct list_head rgn_cache;</span>
<span class="quote">&gt; +	long rgn_cache_count;</span>
<span class="quote">&gt;   };</span>
<span class="quote">&gt;   extern struct resv_map *resv_map_alloc(void);</span>
<span class="quote">&gt;   void resv_map_release(struct kref *ref);</span>
<span class="quote">&gt; diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="quote">&gt; index a8c3087..de03374 100644</span>
<span class="quote">&gt; --- a/mm/hugetlb.c</span>
<span class="quote">&gt; +++ b/mm/hugetlb.c</span>
<span class="quote">&gt; @@ -240,11 +240,14 @@ struct file_region {</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;   /*</span>
<span class="quote">&gt;    * Add the huge page range represented by [f, t) to the reserve</span>
<span class="quote">&gt; - * map.  Existing regions will be expanded to accommodate the</span>
<span class="quote">&gt; - * specified range.  We know only existing regions need to be</span>
<span class="quote">&gt; - * expanded, because region_add is only called after region_chg</span>
<span class="quote">&gt; - * with the same range.  If a new file_region structure must</span>
<span class="quote">&gt; - * be allocated, it is done in region_chg.</span>
<span class="quote">&gt; + * map.  In the normal case, existing regions will be expanded</span>
<span class="quote">&gt; + * to accommodate the specified range.  Sufficient regions should</span>
<span class="quote">&gt; + * exist for expansion due to the previous call to region_chg</span>
<span class="quote">&gt; + * with the same range.  However, it is possible that region_del</span>
<span class="quote">&gt; + * could have been called after region_chg and modifed the map</span>
<span class="quote">&gt; + * in such a way that no region exists to be expanded.  In this</span>
<span class="quote">&gt; + * case, pull a region descriptor from the cache associated with</span>
<span class="quote">&gt; + * the map and use that for the new range.</span>
<span class="quote">&gt;    *</span>
<span class="quote">&gt;    * Return the number of new huge pages added to the map.  This</span>
<span class="quote">&gt;    * number is greater than or equal to zero.</span>
<span class="quote">&gt; @@ -261,6 +264,27 @@ static long region_add(struct resv_map *resv, long f, long t)</span>
<span class="quote">&gt;   		if (f &lt;= rg-&gt;to)</span>
<span class="quote">&gt;   			break;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; +	if (&amp;rg-&gt;link == head || t &lt; rg-&gt;from) {</span>
<span class="quote">&gt; +		/*</span>
<span class="quote">&gt; +		 * No region exists which can be expanded to include the</span>
<span class="quote">&gt; +		 * specified range.  Pull a region descriptor from the</span>
<span class="quote">&gt; +		 * cache, and use it for this range.</span>
<span class="quote">&gt; +		 */</span>
<span class="quote">&gt; +		VM_BUG_ON(!resv-&gt;rgn_cache_count);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		resv-&gt;rgn_cache_count--;</span>
<span class="quote">&gt; +		nrg = list_first_entry(&amp;resv-&gt;rgn_cache, struct file_region,</span>
<span class="quote">&gt; +					link);</span>
<span class="quote">&gt; +		list_del(&amp;nrg-&gt;link);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		nrg-&gt;from = f;</span>
<span class="quote">&gt; +		nrg-&gt;to = t;</span>
<span class="quote">&gt; +		list_add(&amp;nrg-&gt;link, rg-&gt;link.prev);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		add += t - f;</span>
<span class="quote">&gt; +		goto out_locked;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;   	/* Round our left edge to the current segment if it encloses us. */</span>
<span class="quote">&gt;   	if (f &gt; rg-&gt;from)</span>
<span class="quote">&gt;   		f = rg-&gt;from;</span>
<span class="quote">&gt; @@ -294,6 +318,8 @@ static long region_add(struct resv_map *resv, long f, long t)</span>
<span class="quote">&gt;   	add += t - nrg-&gt;to;		/* Added to end of region */</span>
<span class="quote">&gt;   	nrg-&gt;to = t;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; +out_locked:</span>
<span class="quote">&gt; +	resv-&gt;adds_in_progress--;</span>
<span class="quote">&gt;   	spin_unlock(&amp;resv-&gt;lock);</span>
<span class="quote">&gt;   	VM_BUG_ON(add &lt; 0);</span>
<span class="quote">&gt;   	return add;</span>
<span class="quote">&gt; @@ -312,11 +338,16 @@ static long region_add(struct resv_map *resv, long f, long t)</span>
<span class="quote">&gt;    * so that the subsequent region_add call will have all the</span>
<span class="quote">&gt;    * regions it needs and will not fail.</span>
<span class="quote">&gt;    *</span>
<span class="quote">&gt; + * Upon entry, region_chg will also examine the cache of</span>
<span class="quote">&gt; + * region descriptors associated with the map.  If there</span>
<span class="quote">&gt; + * not enough descriptors cached, one will be allocated</span>
<span class="quote">&gt; + * for the in progress add operation.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt;    * Returns the number of huge pages that need to be added</span>
<span class="quote">&gt;    * to the existing reservation map for the range [f, t).</span>
<span class="quote">&gt;    * This number is greater or equal to zero.  -ENOMEM is</span>
<span class="quote">&gt; - * returned if a new file_region structure is needed and can</span>
<span class="quote">&gt; - * not be allocated.</span>
<span class="quote">&gt; + * returned if a new file_region structure or cache entry</span>
<span class="quote">&gt; + * is needed and can not be allocated.</span>
<span class="quote">&gt;    */</span>
<span class="quote">&gt;   static long region_chg(struct resv_map *resv, long f, long t)</span>
<span class="quote">&gt;   {</span>
<span class="quote">&gt; @@ -326,6 +357,30 @@ static long region_chg(struct resv_map *resv, long f, long t)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;   retry:</span>
<span class="quote">&gt;   	spin_lock(&amp;resv-&gt;lock);</span>
<span class="quote">&gt; +	resv-&gt;adds_in_progress++;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * Check for sufficient descriptors in the cache to accommodate</span>
<span class="quote">&gt; +	 * the number of in progress add operations.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	if (resv-&gt;adds_in_progress &gt; resv-&gt;rgn_cache_count) {</span>
<span class="quote">&gt; +		struct file_region *trg;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		VM_BUG_ON(resv-&gt;adds_in_progress - resv-&gt;rgn_cache_count &gt; 1);</span>
<span class="quote">&gt; +		/* Must drop lock to allocate a new descriptor. */</span>
<span class="quote">&gt; +		resv-&gt;adds_in_progress--;</span>
<span class="quote">&gt; +		spin_unlock(&amp;resv-&gt;lock);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		trg = kmalloc(sizeof(*trg), GFP_KERNEL);</span>
<span class="quote">&gt; +		if (!trg)</span>
<span class="quote">&gt; +			return -ENOMEM;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		spin_lock(&amp;resv-&gt;lock);</span>
<span class="quote">&gt; +		resv-&gt;adds_in_progress++;</span>
<span class="quote">&gt; +		list_add(&amp;trg-&gt;link, &amp;resv-&gt;rgn_cache);</span>
<span class="quote">&gt; +		resv-&gt;rgn_cache_count++;</span>

Doh!  I noticed shortly after sending that this needs to go back and
check the condition (sufficient descriptors) after acquiring the lock.

Apologies for the obvious bug.  More comments are welcome.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h</span>
<span class="p_header">index d891f94..667cf44 100644</span>
<span class="p_header">--- a/include/linux/hugetlb.h</span>
<span class="p_header">+++ b/include/linux/hugetlb.h</span>
<span class="p_chunk">@@ -35,6 +35,9 @@</span> <span class="p_context"> struct resv_map {</span>
 	struct kref refs;
 	spinlock_t lock;
 	struct list_head regions;
<span class="p_add">+	long adds_in_progress;</span>
<span class="p_add">+	struct list_head rgn_cache;</span>
<span class="p_add">+	long rgn_cache_count;</span>
 };
 extern struct resv_map *resv_map_alloc(void);
 void resv_map_release(struct kref *ref);
<span class="p_header">diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="p_header">index a8c3087..de03374 100644</span>
<span class="p_header">--- a/mm/hugetlb.c</span>
<span class="p_header">+++ b/mm/hugetlb.c</span>
<span class="p_chunk">@@ -240,11 +240,14 @@</span> <span class="p_context"> struct file_region {</span>
 
 /*
  * Add the huge page range represented by [f, t) to the reserve
<span class="p_del">- * map.  Existing regions will be expanded to accommodate the</span>
<span class="p_del">- * specified range.  We know only existing regions need to be</span>
<span class="p_del">- * expanded, because region_add is only called after region_chg</span>
<span class="p_del">- * with the same range.  If a new file_region structure must</span>
<span class="p_del">- * be allocated, it is done in region_chg.</span>
<span class="p_add">+ * map.  In the normal case, existing regions will be expanded</span>
<span class="p_add">+ * to accommodate the specified range.  Sufficient regions should</span>
<span class="p_add">+ * exist for expansion due to the previous call to region_chg</span>
<span class="p_add">+ * with the same range.  However, it is possible that region_del</span>
<span class="p_add">+ * could have been called after region_chg and modifed the map</span>
<span class="p_add">+ * in such a way that no region exists to be expanded.  In this</span>
<span class="p_add">+ * case, pull a region descriptor from the cache associated with</span>
<span class="p_add">+ * the map and use that for the new range.</span>
  *
  * Return the number of new huge pages added to the map.  This
  * number is greater than or equal to zero.
<span class="p_chunk">@@ -261,6 +264,27 @@</span> <span class="p_context"> static long region_add(struct resv_map *resv, long f, long t)</span>
 		if (f &lt;= rg-&gt;to)
 			break;
 
<span class="p_add">+	if (&amp;rg-&gt;link == head || t &lt; rg-&gt;from) {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * No region exists which can be expanded to include the</span>
<span class="p_add">+		 * specified range.  Pull a region descriptor from the</span>
<span class="p_add">+		 * cache, and use it for this range.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		VM_BUG_ON(!resv-&gt;rgn_cache_count);</span>
<span class="p_add">+</span>
<span class="p_add">+		resv-&gt;rgn_cache_count--;</span>
<span class="p_add">+		nrg = list_first_entry(&amp;resv-&gt;rgn_cache, struct file_region,</span>
<span class="p_add">+					link);</span>
<span class="p_add">+		list_del(&amp;nrg-&gt;link);</span>
<span class="p_add">+</span>
<span class="p_add">+		nrg-&gt;from = f;</span>
<span class="p_add">+		nrg-&gt;to = t;</span>
<span class="p_add">+		list_add(&amp;nrg-&gt;link, rg-&gt;link.prev);</span>
<span class="p_add">+</span>
<span class="p_add">+		add += t - f;</span>
<span class="p_add">+		goto out_locked;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	/* Round our left edge to the current segment if it encloses us. */
 	if (f &gt; rg-&gt;from)
 		f = rg-&gt;from;
<span class="p_chunk">@@ -294,6 +318,8 @@</span> <span class="p_context"> static long region_add(struct resv_map *resv, long f, long t)</span>
 	add += t - nrg-&gt;to;		/* Added to end of region */
 	nrg-&gt;to = t;
 
<span class="p_add">+out_locked:</span>
<span class="p_add">+	resv-&gt;adds_in_progress--;</span>
 	spin_unlock(&amp;resv-&gt;lock);
 	VM_BUG_ON(add &lt; 0);
 	return add;
<span class="p_chunk">@@ -312,11 +338,16 @@</span> <span class="p_context"> static long region_add(struct resv_map *resv, long f, long t)</span>
  * so that the subsequent region_add call will have all the
  * regions it needs and will not fail.
  *
<span class="p_add">+ * Upon entry, region_chg will also examine the cache of</span>
<span class="p_add">+ * region descriptors associated with the map.  If there</span>
<span class="p_add">+ * not enough descriptors cached, one will be allocated</span>
<span class="p_add">+ * for the in progress add operation.</span>
<span class="p_add">+ *</span>
  * Returns the number of huge pages that need to be added
  * to the existing reservation map for the range [f, t).
  * This number is greater or equal to zero.  -ENOMEM is
<span class="p_del">- * returned if a new file_region structure is needed and can</span>
<span class="p_del">- * not be allocated.</span>
<span class="p_add">+ * returned if a new file_region structure or cache entry</span>
<span class="p_add">+ * is needed and can not be allocated.</span>
  */
 static long region_chg(struct resv_map *resv, long f, long t)
 {
<span class="p_chunk">@@ -326,6 +357,30 @@</span> <span class="p_context"> static long region_chg(struct resv_map *resv, long f, long t)</span>
 
 retry:
 	spin_lock(&amp;resv-&gt;lock);
<span class="p_add">+	resv-&gt;adds_in_progress++;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Check for sufficient descriptors in the cache to accommodate</span>
<span class="p_add">+	 * the number of in progress add operations.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (resv-&gt;adds_in_progress &gt; resv-&gt;rgn_cache_count) {</span>
<span class="p_add">+		struct file_region *trg;</span>
<span class="p_add">+</span>
<span class="p_add">+		VM_BUG_ON(resv-&gt;adds_in_progress - resv-&gt;rgn_cache_count &gt; 1);</span>
<span class="p_add">+		/* Must drop lock to allocate a new descriptor. */</span>
<span class="p_add">+		resv-&gt;adds_in_progress--;</span>
<span class="p_add">+		spin_unlock(&amp;resv-&gt;lock);</span>
<span class="p_add">+</span>
<span class="p_add">+		trg = kmalloc(sizeof(*trg), GFP_KERNEL);</span>
<span class="p_add">+		if (!trg)</span>
<span class="p_add">+			return -ENOMEM;</span>
<span class="p_add">+</span>
<span class="p_add">+		spin_lock(&amp;resv-&gt;lock);</span>
<span class="p_add">+		resv-&gt;adds_in_progress++;</span>
<span class="p_add">+		list_add(&amp;trg-&gt;link, &amp;resv-&gt;rgn_cache);</span>
<span class="p_add">+		resv-&gt;rgn_cache_count++;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	/* Locate the region we are before or in. */
 	list_for_each_entry(rg, head, link)
 		if (f &lt;= rg-&gt;to)
<span class="p_chunk">@@ -336,6 +391,7 @@</span> <span class="p_context"> retry:</span>
 	 * size such that we can guarantee to record the reservation. */
 	if (&amp;rg-&gt;link == head || t &lt; rg-&gt;from) {
 		if (!nrg) {
<span class="p_add">+			resv-&gt;adds_in_progress--;</span>
 			spin_unlock(&amp;resv-&gt;lock);
 			nrg = kmalloc(sizeof(*nrg), GFP_KERNEL);
 			if (!nrg)
<span class="p_chunk">@@ -385,6 +441,25 @@</span> <span class="p_context"> out_nrg:</span>
 }
 
 /*
<span class="p_add">+ * Abort the in progress add operation.  The adds_in_progress field</span>
<span class="p_add">+ * of the resv_map keeps track of the operations in progress between</span>
<span class="p_add">+ * calls to region_chg and region_add.  Operations are sometimes</span>
<span class="p_add">+ * aborted after the call to region_chg.  In such cases, region_abort</span>
<span class="p_add">+ * is called to decrement the adds_in_progress counter.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * NOTE: The range arguments [f, t) are not needed or used in this</span>
<span class="p_add">+ * routine.  They are kept to make reading the calling code easier as</span>
<span class="p_add">+ * arguments will match the associated region_chg call.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static void region_abort(struct resv_map *resv, long f, long t)</span>
<span class="p_add">+{</span>
<span class="p_add">+	spin_lock(&amp;resv-&gt;lock);</span>
<span class="p_add">+	VM_BUG_ON(!resv-&gt;rgn_cache_count);</span>
<span class="p_add">+	resv-&gt;adds_in_progress--;</span>
<span class="p_add">+	spin_unlock(&amp;resv-&gt;lock);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
  * Truncate the reserve map at index &#39;end&#39;.  Modify/truncate any
  * region which contains end.  Delete any regions past end.
  * Return the number of huge pages removed from the map.
<span class="p_chunk">@@ -544,22 +619,44 @@</span> <span class="p_context"> static void set_vma_private_data(struct vm_area_struct *vma,</span>
 struct resv_map *resv_map_alloc(void)
 {
 	struct resv_map *resv_map = kmalloc(sizeof(*resv_map), GFP_KERNEL);
<span class="p_del">-	if (!resv_map)</span>
<span class="p_add">+	struct file_region *rg = kmalloc(sizeof(*rg), GFP_KERNEL);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!resv_map || !rg) {</span>
<span class="p_add">+		kfree(resv_map);</span>
<span class="p_add">+		kfree(rg);</span>
 		return NULL;
<span class="p_add">+	}</span>
 
 	kref_init(&amp;resv_map-&gt;refs);
 	spin_lock_init(&amp;resv_map-&gt;lock);
 	INIT_LIST_HEAD(&amp;resv_map-&gt;regions);
 
<span class="p_add">+	resv_map-&gt;adds_in_progress = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	INIT_LIST_HEAD(&amp;resv_map-&gt;rgn_cache);</span>
<span class="p_add">+	list_add(&amp;rg-&gt;link, &amp;resv_map-&gt;rgn_cache);</span>
<span class="p_add">+	resv_map-&gt;rgn_cache_count = 1;</span>
<span class="p_add">+</span>
 	return resv_map;
 }
 
 void resv_map_release(struct kref *ref)
 {
 	struct resv_map *resv_map = container_of(ref, struct resv_map, refs);
<span class="p_add">+	struct list_head *head = &amp;resv_map-&gt;rgn_cache;</span>
<span class="p_add">+	struct file_region *rg, *trg;</span>
 
 	/* Clear out any active regions before we release the map. */
 	region_truncate(resv_map, 0);
<span class="p_add">+</span>
<span class="p_add">+	/* ... and any entries left in the cache */</span>
<span class="p_add">+	list_for_each_entry_safe(rg, trg, head, link) {</span>
<span class="p_add">+		list_del(&amp;rg-&gt;link);</span>
<span class="p_add">+		kfree(rg);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	VM_BUG_ON(resv_map-&gt;adds_in_progress);</span>
<span class="p_add">+</span>
 	kfree(resv_map);
 }
 
<span class="p_chunk">@@ -1473,16 +1570,18 @@</span> <span class="p_context"> static void return_unused_surplus_pages(struct hstate *h,</span>
 	}
 }
 
<span class="p_add">+</span>
 /*
<span class="p_del">- * vma_needs_reservation and vma_commit_reservation are used by the huge</span>
<span class="p_del">- * page allocation routines to manage reservations.</span>
<span class="p_add">+ * vma_needs_reservation, vma_commit_reservation and vma_abort_reservation</span>
<span class="p_add">+ * are used by the huge page allocation routines to manage reservations.</span>
  *
  * vma_needs_reservation is called to determine if the huge page at addr
  * within the vma has an associated reservation.  If a reservation is
  * needed, the value 1 is returned.  The caller is then responsible for
  * managing the global reservation and subpool usage counts.  After
  * the huge page has been allocated, vma_commit_reservation is called
<span class="p_del">- * to add the page to the reservation map.</span>
<span class="p_add">+ * to add the page to the reservation map.  If the reservation must be</span>
<span class="p_add">+ * aborted instead of committed, vma_abort_reservation is called.</span>
  *
  * In the normal case, vma_commit_reservation returns the same value
  * as the preceding vma_needs_reservation call.  The only time this
<span class="p_chunk">@@ -1490,9 +1589,14 @@</span> <span class="p_context"> static void return_unused_surplus_pages(struct hstate *h,</span>
  * is the responsibility of the caller to notice the difference and
  * take appropriate action.
  */
<span class="p_add">+enum vma_resv_mode {</span>
<span class="p_add">+	VMA_NEEDS_RESV,</span>
<span class="p_add">+	VMA_COMMIT_RESV,</span>
<span class="p_add">+	VMA_ABORT_RESV,</span>
<span class="p_add">+};</span>
 static long __vma_reservation_common(struct hstate *h,
 				struct vm_area_struct *vma, unsigned long addr,
<span class="p_del">-				bool commit)</span>
<span class="p_add">+				enum vma_resv_mode mode)</span>
 {
 	struct resv_map *resv;
 	pgoff_t idx;
<span class="p_chunk">@@ -1503,10 +1607,20 @@</span> <span class="p_context"> static long __vma_reservation_common(struct hstate *h,</span>
 		return 1;
 
 	idx = vma_hugecache_offset(h, vma, addr);
<span class="p_del">-	if (commit)</span>
<span class="p_del">-		ret = region_add(resv, idx, idx + 1);</span>
<span class="p_del">-	else</span>
<span class="p_add">+	switch (mode) {</span>
<span class="p_add">+	case VMA_NEEDS_RESV:</span>
 		ret = region_chg(resv, idx, idx + 1);
<span class="p_add">+		break;</span>
<span class="p_add">+	case VMA_COMMIT_RESV:</span>
<span class="p_add">+		ret = region_add(resv, idx, idx + 1);</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case VMA_ABORT_RESV:</span>
<span class="p_add">+		region_abort(resv, idx, idx + 1);</span>
<span class="p_add">+		ret = 0;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	default:</span>
<span class="p_add">+		BUG();</span>
<span class="p_add">+	}</span>
 
 	if (vma-&gt;vm_flags &amp; VM_MAYSHARE)
 		return ret;
<span class="p_chunk">@@ -1517,13 +1631,19 @@</span> <span class="p_context"> static long __vma_reservation_common(struct hstate *h,</span>
 static long vma_needs_reservation(struct hstate *h,
 			struct vm_area_struct *vma, unsigned long addr)
 {
<span class="p_del">-	return __vma_reservation_common(h, vma, addr, false);</span>
<span class="p_add">+	return __vma_reservation_common(h, vma, addr, VMA_NEEDS_RESV);</span>
 }
 
 static long vma_commit_reservation(struct hstate *h,
 			struct vm_area_struct *vma, unsigned long addr)
 {
<span class="p_del">-	return __vma_reservation_common(h, vma, addr, true);</span>
<span class="p_add">+	return __vma_reservation_common(h, vma, addr, VMA_COMMIT_RESV);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void vma_abort_reservation(struct hstate *h,</span>
<span class="p_add">+			struct vm_area_struct *vma, unsigned long addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	(void)__vma_reservation_common(h, vma, addr, VMA_ABORT_RESV);</span>
 }
 
 static struct page *alloc_huge_page(struct vm_area_struct *vma,
<span class="p_chunk">@@ -1549,8 +1669,10 @@</span> <span class="p_context"> static struct page *alloc_huge_page(struct vm_area_struct *vma,</span>
 	if (chg &lt; 0)
 		return ERR_PTR(-ENOMEM);
 	if (chg || avoid_reserve)
<span class="p_del">-		if (hugepage_subpool_get_pages(spool, 1) &lt; 0)</span>
<span class="p_add">+		if (hugepage_subpool_get_pages(spool, 1) &lt; 0) {</span>
<span class="p_add">+			vma_abort_reservation(h, vma, addr);</span>
 			return ERR_PTR(-ENOSPC);
<span class="p_add">+		}</span>
 
 	ret = hugetlb_cgroup_charge_cgroup(idx, pages_per_huge_page(h), &amp;h_cg);
 	if (ret)
<span class="p_chunk">@@ -1596,6 +1718,7 @@</span> <span class="p_context"> out_uncharge_cgroup:</span>
 out_subpool_put:
 	if (chg || avoid_reserve)
 		hugepage_subpool_put_pages(spool, 1);
<span class="p_add">+	vma_abort_reservation(h, vma, addr);</span>
 	return ERR_PTR(-ENOSPC);
 }
 
<span class="p_chunk">@@ -3236,11 +3359,14 @@</span> <span class="p_context"> retry:</span>
 	 * any allocations necessary to record that reservation occur outside
 	 * the spinlock.
 	 */
<span class="p_del">-	if ((flags &amp; FAULT_FLAG_WRITE) &amp;&amp; !(vma-&gt;vm_flags &amp; VM_SHARED))</span>
<span class="p_add">+	if ((flags &amp; FAULT_FLAG_WRITE) &amp;&amp; !(vma-&gt;vm_flags &amp; VM_SHARED)) {</span>
 		if (vma_needs_reservation(h, vma, address) &lt; 0) {
 			ret = VM_FAULT_OOM;
 			goto backout_unlocked;
 		}
<span class="p_add">+		/* Just decrements count, does not deallocate */</span>
<span class="p_add">+		vma_abort_reservation(h, vma, address);</span>
<span class="p_add">+	}</span>
 
 	ptl = huge_pte_lockptr(h, mm, ptep);
 	spin_lock(ptl);
<span class="p_chunk">@@ -3387,6 +3513,8 @@</span> <span class="p_context"> int hugetlb_fault(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 			ret = VM_FAULT_OOM;
 			goto out_mutex;
 		}
<span class="p_add">+		/* Just decrements count, does not deallocate */</span>
<span class="p_add">+		vma_abort_reservation(h, vma, address);</span>
 
 		if (!(vma-&gt;vm_flags &amp; VM_MAYSHARE))
 			pagecache_page = hugetlbfs_pagecache_page(h,
<span class="p_chunk">@@ -3726,6 +3854,8 @@</span> <span class="p_context"> int hugetlb_reserve_pages(struct inode *inode,</span>
 	}
 	return 0;
 out_err:
<span class="p_add">+	if (!vma || vma-&gt;vm_flags &amp; VM_MAYSHARE)</span>
<span class="p_add">+		region_abort(resv_map, from, to);</span>
 	if (vma &amp;&amp; is_vma_resv_set(vma, HPAGE_RESV_OWNER))
 		kref_put(&amp;resv_map-&gt;refs, resv_map_release);
 	return ret;

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



