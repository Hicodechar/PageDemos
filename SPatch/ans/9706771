
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[RFC] hugetlbfs &#39;noautofill&#39; mount option - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [RFC] hugetlbfs &#39;noautofill&#39; mount option</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=174063">Prakash Sangappa</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>May 1, 2017, 6 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;b0efc671-0d7a-0aef-5646-a635478c31b0@oracle.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9706771/mbox/"
   >mbox</a>
|
   <a href="/patch/9706771/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9706771/">/patch/9706771/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	DCFF660387 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  1 May 2017 18:00:20 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id D2789208C2
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  1 May 2017 18:00:20 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id C5F9D27D4D; Mon,  1 May 2017 18:00:20 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	UNPARSEABLE_RELAY autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 4314F208C2
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  1 May 2017 18:00:20 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1750849AbdEASAK (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 1 May 2017 14:00:10 -0400
Received: from userp1040.oracle.com ([156.151.31.81]:39815 &quot;EHLO
	userp1040.oracle.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1750803AbdEASAI (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 1 May 2017 14:00:08 -0400
Received: from aserv0022.oracle.com (aserv0022.oracle.com [141.146.126.234])
	by userp1040.oracle.com (Sentrion-MTA-4.3.2/Sentrion-MTA-4.3.2)
	with ESMTP id v41I06Lv012718
	(version=TLSv1.2 cipher=ECDHE-RSA-AES256-GCM-SHA384 bits=256
	verify=OK); Mon, 1 May 2017 18:00:06 GMT
Received: from userv0121.oracle.com (userv0121.oracle.com [156.151.31.72])
	by aserv0022.oracle.com (8.14.4/8.14.4) with ESMTP id v41I05Zw029140
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-GCM-SHA384 bits=256
	verify=OK); Mon, 1 May 2017 18:00:06 GMT
Received: from abhmp0008.oracle.com (abhmp0008.oracle.com [141.146.116.14])
	by userv0121.oracle.com (8.14.4/8.13.8) with ESMTP id
	v41I04h0022750; Mon, 1 May 2017 18:00:05 GMT
Received: from Prakash-Sangappas-MacBook-Pro-2.local (/73.71.24.91)
	by default (Oracle Beehive Gateway v4.0)
	with ESMTP ; Mon, 01 May 2017 11:00:04 -0700
Subject: [PATCH RFC] hugetlbfs &#39;noautofill&#39; mount option
References: &lt;326e38dd-b4a8-e0ca-6ff7-af60e8045c74@oracle.com&gt;
From: Prakash Sangappa &lt;prakash.sangappa@oracle.com&gt;
To: linux-kernel@vger.kernel.org, linux-mm@kvack.org
X-Forwarded-Message-Id: &lt;326e38dd-b4a8-e0ca-6ff7-af60e8045c74@oracle.com&gt;
Message-ID: &lt;b0efc671-0d7a-0aef-5646-a635478c31b0@oracle.com&gt;
Date: Mon, 1 May 2017 11:00:09 -0700
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.11; rv:45.0)
	Gecko/20100101 Thunderbird/45.8.0
MIME-Version: 1.0
In-Reply-To: &lt;326e38dd-b4a8-e0ca-6ff7-af60e8045c74@oracle.com&gt;
Content-Type: text/plain; charset=utf-8; format=flowed
Content-Transfer-Encoding: 7bit
X-Source-IP: aserv0022.oracle.com [141.146.126.234]
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=174063">Prakash Sangappa</a> - May 1, 2017, 6 p.m.</div>
<pre class="content">
Some applications like a database use hugetblfs for performance
reasons. Files on hugetlbfs filesystem are created and huge pages
allocated using fallocate() API. Pages are deallocated/freed using
fallocate() hole punching support that has been added to hugetlbfs.
These files are mmapped and accessed by many processes as shared memory.
Such applications keep track of which offsets in the hugetlbfs file have
pages allocated.

Any access to mapped address over holes in the file, which can occur due
to bugs in the application, is considered invalid and expect the process
to simply receive a SIGBUS.  However, currently when a hole in the file is
accessed via the mapped address, kernel/mm attempts to automatically
allocate a page at page fault time, resulting in implicitly filling the hole
in the file. This may not be the desired behavior for applications like the
database that want to explicitly manage page allocations of hugetlbfs files.

This patch adds a new hugetlbfs mount option &#39;noautofill&#39;, to indicate that
pages should not be allocated at page fault time when accessed thru mmapped
address.
<span class="signed-off-by">
Signed-off-by: Prakash &lt;prakash.sangappa@oracle.com&gt;</span>
---
fs/hugetlbfs/inode.c    | 11 ++++++++++-
  include/linux/hugetlb.h |  1 +
  mm/hugetlb.c            |  5 +++++
  3 files changed, 16 insertions(+), 1 deletion(-)

              ret = PTR_ERR(page);
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36302">Anshuman Khandual</a> - May 2, 2017, 10:53 a.m.</div>
<pre class="content">
On 05/01/2017 11:30 PM, Prakash Sangappa wrote:
<span class="quote">&gt; Some applications like a database use hugetblfs for performance</span>
<span class="quote">&gt; reasons. Files on hugetlbfs filesystem are created and huge pages</span>
<span class="quote">&gt; allocated using fallocate() API. Pages are deallocated/freed using</span>
<span class="quote">&gt; fallocate() hole punching support that has been added to hugetlbfs.</span>
<span class="quote">&gt; These files are mmapped and accessed by many processes as shared memory.</span>
<span class="quote">&gt; Such applications keep track of which offsets in the hugetlbfs file have</span>
<span class="quote">&gt; pages allocated.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Any access to mapped address over holes in the file, which can occur due</span>

s/mapped/unmapped/ ^ ?
<span class="quote">
&gt; to bugs in the application, is considered invalid and expect the process</span>
<span class="quote">&gt; to simply receive a SIGBUS.  However, currently when a hole in the file is</span>
<span class="quote">&gt; accessed via the mapped address, kernel/mm attempts to automatically</span>
<span class="quote">&gt; allocate a page at page fault time, resulting in implicitly filling the</span>
<span class="quote">&gt; hole</span>

But this is expected when you try to control the file allocation from
a mapped address. Any changes while walking past or writing the range
in the memory mapped should reflect exactly in the file on the disk.
Why its not a valid behavior ?
<span class="quote">
&gt; in the file. This may not be the desired behavior for applications like the</span>
<span class="quote">&gt; database that want to explicitly manage page allocations of hugetlbfs</span>
<span class="quote">&gt; files.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This patch adds a new hugetlbfs mount option &#39;noautofill&#39;, to indicate that</span>
<span class="quote">&gt; pages should not be allocated at page fault time when accessed thru mmapped</span>
<span class="quote">&gt; address.</span>

When the page should be allocated for mapping ?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=174063">Prakash Sangappa</a> - May 2, 2017, 4:07 p.m.</div>
<pre class="content">
On 5/2/17 3:53 AM, Anshuman Khandual wrote:
<span class="quote">&gt; On 05/01/2017 11:30 PM, Prakash Sangappa wrote:</span>
<span class="quote">&gt;&gt; Some applications like a database use hugetblfs for performance</span>
<span class="quote">&gt;&gt; reasons. Files on hugetlbfs filesystem are created and huge pages</span>
<span class="quote">&gt;&gt; allocated using fallocate() API. Pages are deallocated/freed using</span>
<span class="quote">&gt;&gt; fallocate() hole punching support that has been added to hugetlbfs.</span>
<span class="quote">&gt;&gt; These files are mmapped and accessed by many processes as shared memory.</span>
<span class="quote">&gt;&gt; Such applications keep track of which offsets in the hugetlbfs file have</span>
<span class="quote">&gt;&gt; pages allocated.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Any access to mapped address over holes in the file, which can occur due</span>
<span class="quote">&gt; s/mapped/unmapped/ ^ ?</span>

It is &#39;mapped&#39; address.
<span class="quote">
&gt;</span>
<span class="quote">&gt;&gt; to bugs in the application, is considered invalid and expect the process</span>
<span class="quote">&gt;&gt; to simply receive a SIGBUS.  However, currently when a hole in the file is</span>
<span class="quote">&gt;&gt; accessed via the mapped address, kernel/mm attempts to automatically</span>
<span class="quote">&gt;&gt; allocate a page at page fault time, resulting in implicitly filling the</span>
<span class="quote">&gt;&gt; hole</span>
<span class="quote">&gt; But this is expected when you try to control the file allocation from</span>
<span class="quote">&gt; a mapped address. Any changes while walking past or writing the range</span>
<span class="quote">&gt; in the memory mapped should reflect exactly in the file on the disk.</span>
<span class="quote">&gt; Why its not a valid behavior ?</span>
Sure, that is a valid behavior. However, hugetlbfs is a pesudo filesystem
and the purpose is for applications to use hugepage memory. The contents
of these filesystem are not backed by disk nor are they swapped out.

The proposed new behavior is only applicable for hugetlbfs filesystem 
mounted
with the new &#39;noautofill&#39; mount option. The file&#39;s page allocation/free 
are managed
using the &#39;fallocate()&#39; API.

For hugetlbfs filesystems mounted without this option, there is no 
change in behavior.
<span class="quote">
&gt;&gt; in the file. This may not be the desired behavior for applications like the</span>
<span class="quote">&gt;&gt; database that want to explicitly manage page allocations of hugetlbfs</span>
<span class="quote">&gt;&gt; files.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; This patch adds a new hugetlbfs mount option &#39;noautofill&#39;, to indicate that</span>
<span class="quote">&gt;&gt; pages should not be allocated at page fault time when accessed thru mmapped</span>
<span class="quote">&gt;&gt; address.</span>
<span class="quote">&gt; When the page should be allocated for mapping ?</span>
The application would allocate/free file pages using the fallocate() API.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=65121">Dave Hansen</a> - May 2, 2017, 9:32 p.m.</div>
<pre class="content">
On 05/01/2017 11:00 AM, Prakash Sangappa wrote:
<span class="quote">&gt; This patch adds a new hugetlbfs mount option &#39;noautofill&#39;, to indicate that</span>
<span class="quote">&gt; pages should not be allocated at page fault time when accessed thru mmapped</span>
<span class="quote">&gt; address.</span>

I think the main argument against doing something like this is further
specializing hugetlbfs.  I was really hoping that userfaultfd would be
usable for your needs here.

Could you elaborate on other options that you considered?  Did you look
at userfaultfd?  What about an madvise() option that disallows backing
allocations?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=174063">Prakash Sangappa</a> - May 2, 2017, 11:34 p.m.</div>
<pre class="content">
On 5/2/17 2:32 PM, Dave Hansen wrote:
<span class="quote">&gt; On 05/01/2017 11:00 AM, Prakash Sangappa wrote:</span>
<span class="quote">&gt;&gt; This patch adds a new hugetlbfs mount option &#39;noautofill&#39;, to indicate that</span>
<span class="quote">&gt;&gt; pages should not be allocated at page fault time when accessed thru mmapped</span>
<span class="quote">&gt;&gt; address.</span>
<span class="quote">&gt; I think the main argument against doing something like this is further</span>
<span class="quote">&gt; specializing hugetlbfs.  I was really hoping that userfaultfd would be</span>
<span class="quote">&gt; usable for your needs here.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Could you elaborate on other options that you considered?  Did you look</span>
<span class="quote">&gt; at userfaultfd?  What about an madvise() option that disallows backing</span>
<span class="quote">&gt; allocations?</span>


Yes, we did consider userfaultfd and madvise(). The use case in mind is 
the database.

With a database, large number of single threaded processes are involved 
which
will map hugetlbfs file and use it for shared memory. The concern with 
using
userfaultfd is the overhead of setup and having an additional thread per 
process
to monitor the userfaultfd. Even if the additional thread can be 
avoided, by using
an external monitor process and  each process sending the userfaultfd to 
this
monitor process, setup overhead exists.

Similarly, a madvise() option also requires additional system call by every
process mapping the file, this is considered a overhead for the database.

If we do consider a new madvise() option, will it be acceptable since 
this will be
specifically for hugetlbfs file mappings? If so, would a new flag to mmap()
call itself be acceptable, which would define the proposed behavior?. 
That way
no additional system calls need to be made. Again this mmap flag would be
applicable  specifically to hugetlbfs file mappings

With the proposed mount option, it would enforce one consistent behavior
and the application using this filesystem would not have to take additional
steps as with userfaultfd or madvise().
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=65121">Dave Hansen</a> - May 2, 2017, 11:43 p.m.</div>
<pre class="content">
On 05/02/2017 04:34 PM, Prakash Sangappa wrote:
<span class="quote">&gt; Similarly, a madvise() option also requires additional system call by every</span>
<span class="quote">&gt; process mapping the file, this is considered a overhead for the database.</span>

How long-lived are these processes?  For a database, I&#39;d assume that
this would happen a single time, or a single time per mmap() at process
startup time.  Such a syscall would be doing something on the order of
taking mmap_sem, walking the VMA tree, setting a bit per VMA, and
unlocking.  That&#39;s a pretty cheap one-time cost...
<span class="quote">
&gt; If we do consider a new madvise() option, will it be acceptable</span>
<span class="quote">&gt; since this will be specifically for hugetlbfs file mappings?</span>

Ideally, it would be something that is *not* specifically for hugetlbfs.
 MADV_NOAUTOFILL, for instance, could be defined to SIGSEGV whenever
memory is touched that was not populated with MADV_WILLNEED, mlock(), etc...
<span class="quote">
&gt; If so,</span>
<span class="quote">&gt; would a new flag to mmap() call itself be acceptable, which would</span>
<span class="quote">&gt; define the proposed behavior?. That way no additional system calls</span>
<span class="quote">&gt; need to be made.</span>

I don&#39;t feel super strongly about it, but I guess an mmap() flag could
work too.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=174063">Prakash Sangappa</a> - May 3, 2017, 7:02 p.m.</div>
<pre class="content">
On 5/2/17 4:43 PM, Dave Hansen wrote:
<span class="quote">
&gt; On 05/02/2017 04:34 PM, Prakash Sangappa wrote:</span>
<span class="quote">&gt;&gt; Similarly, a madvise() option also requires additional system call by every</span>
<span class="quote">&gt;&gt; process mapping the file, this is considered a overhead for the database.</span>
<span class="quote">&gt; How long-lived are these processes?  For a database, I&#39;d assume that</span>
<span class="quote">&gt; this would happen a single time, or a single time per mmap() at process</span>
<span class="quote">&gt; startup time.  Such a syscall would be doing something on the order of</span>
<span class="quote">&gt; taking mmap_sem, walking the VMA tree, setting a bit per VMA, and</span>
<span class="quote">&gt; unlocking.  That&#39;s a pretty cheap one-time cost...</span>
Plus a call into the filesystem (a_ops?) to check if the underlying 
filesystem
supports not filling holes to mapped access before setting the bit per vma.
Although the overhead may not be that bad.

Database processes can exit and new once started, for instance, depending on
database activity.
<span class="quote">

&gt;&gt; If we do consider a new madvise() option, will it be acceptable</span>
<span class="quote">&gt;&gt; since this will be specifically for hugetlbfs file mappings?</span>
<span class="quote">&gt; Ideally, it would be something that is *not* specifically for hugetlbfs.</span>
<span class="quote">&gt;   MADV_NOAUTOFILL, for instance, could be defined to SIGSEGV whenever</span>
<span class="quote">&gt; memory is touched that was not populated with MADV_WILLNEED, mlock(), etc...</span>

If this is a generic advice type, necessary support will have to be 
implemented
in various filesystems which can support this.

The proposed behavior for &#39;noautofill&#39; was to not fill holes in 
files(like sparse files).
In the page fault path, mm would not know if the mmapped address on which
the fault occurred, is over a hole in the file or just that the page is 
not available
in the page cache. The underlying filesystem would be called and it 
determines
if it is a hole and that is where it would fail and not fill the hole, 
if this support is added.
Normally, filesystem which support sparse files(holes in file) 
automatically fill the hole
when accessed. Then there is the issue of file system block size and 
page size. If the
block sizes are smaller then page size, it could mean the noautofill 
would only work
if the hole size is equal to  or a multiple of, page size?

In case of hugetlbfs it is much straight forward. Since this filesystem 
is not like a normal
filesystems and and the file sizes are multiple of huge pages. The hole 
will be a multiple
of the huge page size. For this reason then should the advise be 
specific to hugetlbfs?
<span class="quote">

&gt;</span>
<span class="quote">&gt;&gt; If so,</span>
<span class="quote">&gt;&gt; would a new flag to mmap() call itself be acceptable, which would</span>
<span class="quote">&gt;&gt; define the proposed behavior?. That way no additional system calls</span>
<span class="quote">&gt;&gt; need to be made.</span>
<span class="quote">&gt; I don&#39;t feel super strongly about it, but I guess an mmap() flag could</span>
<span class="quote">&gt; work too.</span>
<span class="quote">&gt;</span>

Same goes with the mmap call, if it is a generic flag.

-Prakash.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=174063">Prakash Sangappa</a> - May 8, 2017, 5:57 a.m.</div>
<pre class="content">
On 5/3/17 12:02 PM, Prakash Sangappa wrote:
<span class="quote">&gt; On 5/2/17 4:43 PM, Dave Hansen wrote:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; Ideally, it would be something that is *not* specifically for hugetlbfs.</span>
<span class="quote">&gt;&gt;   MADV_NOAUTOFILL, for instance, could be defined to SIGSEGV whenever</span>
<span class="quote">&gt;&gt; memory is touched that was not populated with MADV_WILLNEED, mlock(), </span>
<span class="quote">&gt;&gt; etc...</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; If this is a generic advice type, necessary support will have to be </span>
<span class="quote">&gt; implemented</span>
<span class="quote">&gt; in various filesystems which can support this.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; The proposed behavior for &#39;noautofill&#39; was to not fill holes in </span>
<span class="quote">&gt; files(like sparse files).</span>
<span class="quote">&gt; In the page fault path, mm would not know if the mmapped address on which</span>
<span class="quote">&gt; the fault occurred, is over a hole in the file or just that the page </span>
<span class="quote">&gt; is not available</span>
<span class="quote">&gt; in the page cache. The underlying filesystem would be called and it </span>
<span class="quote">&gt; determines</span>
<span class="quote">&gt; if it is a hole and that is where it would fail and not fill the hole, </span>
<span class="quote">&gt; if this support is added.</span>
<span class="quote">&gt; Normally, filesystem which support sparse files(holes in file) </span>
<span class="quote">&gt; automatically fill the hole</span>
<span class="quote">&gt; when accessed. Then there is the issue of file system block size and </span>
<span class="quote">&gt; page size. If the</span>
<span class="quote">&gt; block sizes are smaller then page size, it could mean the noautofill </span>
<span class="quote">&gt; would only work</span>
<span class="quote">&gt; if the hole size is equal to  or a multiple of, page size?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; In case of hugetlbfs it is much straight forward. Since this </span>
<span class="quote">&gt; filesystem is not like a normal</span>
<span class="quote">&gt; filesystems and and the file sizes are multiple of huge pages. The </span>
<span class="quote">&gt; hole will be a multiple</span>
<span class="quote">&gt; of the huge page size. For this reason then should the advise be </span>
<span class="quote">&gt; specific to hugetlbfs?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;</span>


Any further comments? I think introducing a general madvise option or a 
mmap flag applicable to all filesystems, may not be required. The 
&#39;noautofill&#39; behavior would be specifically useful in hugetlbfs filesystem.

So, if it is specific to hugetlbfs, will the mount option be ok? 
Otherwise adding a madvise / mmap option specific to hugetlbfs, be 
preferred?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=65121">Dave Hansen</a> - May 8, 2017, 3:58 p.m.</div>
<pre class="content">
On 05/03/2017 12:02 PM, Prakash Sangappa wrote:
<span class="quote">&gt;&gt;&gt; If we do consider a new madvise()option, will it be acceptable</span>
<span class="quote">&gt;&gt;&gt; since this will be specifically for hugetlbfs file mappings?</span>
<span class="quote">&gt;&gt; Ideally, it would be something that is *not* specifically for</span>
<span class="quote">&gt;&gt; hugetlbfs. MADV_NOAUTOFILL, for instance, could be defined to</span>
<span class="quote">&gt;&gt; SIGSEGV whenever memory is touched that was not populated with</span>
<span class="quote">&gt;&gt; MADV_WILLNEED, mlock(), etc...</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; If this is a generic advice type, necessary support will have to be </span>
<span class="quote">&gt; implemented in various filesystems which can support this.</span>

Yep.
<span class="quote">
&gt; The proposed behavior for &#39;noautofill&#39; was to not fill holes in </span>
<span class="quote">&gt; files(like sparse files). In the page fault path, mm would not know</span>
<span class="quote">&gt; if the mmapped address on which the fault occurred, is over a hole in</span>
<span class="quote">&gt; the file or just that the page is not available in the page cache.</span>

It depends on how you define the feature.  I think you have three choices:

1. &quot;Error&quot; on page fault.  Require all access to be pre-faulted.
2. Allow faults, but &quot;Error&quot; if page cache has to be allocated
3. Allow faults and page cache allocations, but error on filesystem
   backing storage allocation.

All of those are useful in some cases.  But the implementations probably
happen in different places:

#1 can be implemented in core mm code
#2 can be implemented in the VFS
#3 needs filesystem involvement
<span class="quote">
&gt; The underlying filesystem would be called and it determines if it is</span>
<span class="quote">&gt; a hole and that is where it would fail and not fill the hole, if this</span>
<span class="quote">&gt; support is added. Normally, filesystem which support sparse</span>
<span class="quote">&gt; files(holes in file) automatically fill the hole when accessed. Then</span>
<span class="quote">&gt; there is the issue of file system block size and page size. If the </span>
<span class="quote">&gt; block sizes are smaller then page size, it could mean the noautofill </span>
<span class="quote">&gt; would only work if the hole size is equal to or a multiple of, page</span>
<span class="quote">&gt; size?</span>

It depends on how you define the feature whether this is true.
<span class="quote">
&gt; In case of hugetlbfs it is much straight forward. Since this</span>
<span class="quote">&gt; filesystem is not like a normal filesystems and and the file sizes</span>
<span class="quote">&gt; are multiple of huge pages. The hole will be a multiple of the huge</span>
<span class="quote">&gt; page size. For this reason then should the advise be specific to</span>
<span class="quote">&gt; hugetlbfs?</span>

Let me paraphrase: it&#39;s simpler to implement it if it&#39;s specific to
hugetlbfs, thus we should implement it only for hugetlbfs, and keep it
specific to hugetlbfs.

The bigger question is: do we want to continue adding to the complexity
of hugetlbfs and increase its divergence from the core mm?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=174063">Prakash Sangappa</a> - May 8, 2017, 10:12 p.m.</div>
<pre class="content">
On 05/08/2017 08:58 AM, Dave Hansen wrote:
<span class="quote">&gt;</span>
<span class="quote">&gt; It depends on how you define the feature.  I think you have three choices:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; 1. &quot;Error&quot; on page fault.  Require all access to be pre-faulted.</span>
<span class="quote">&gt; 2. Allow faults, but &quot;Error&quot; if page cache has to be allocated</span>
<span class="quote">&gt; 3. Allow faults and page cache allocations, but error on filesystem</span>
<span class="quote">&gt;     backing storage allocation.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; All of those are useful in some cases.  But the implementations probably</span>
<span class="quote">&gt; happen in different places:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; #1 can be implemented in core mm code</span>
<span class="quote">&gt; #2 can be implemented in the VFS</span>
<span class="quote">&gt; #3 needs filesystem involvement</span>


Sure it will depend on how we define the feature.
However, I am not clear about how useful are #1 &amp; #2
as a general feature with respect to filesystems, but I
assume we could find some use cases for them.

Regarding #3 as a general feature, do we want to
consider this and the complexity associated with the
implementation?
<span class="quote">

&gt;</span>
<span class="quote">&gt;&gt; In case of hugetlbfs it is much straight forward. Since this</span>
<span class="quote">&gt;&gt; filesystem is not like a normal filesystems and and the file sizes</span>
<span class="quote">&gt;&gt; are multiple of huge pages. The hole will be a multiple of the huge</span>
<span class="quote">&gt;&gt; page size. For this reason then should the advise be specific to</span>
<span class="quote">&gt;&gt; hugetlbfs?</span>
<span class="quote">&gt; Let me paraphrase: it&#39;s simpler to implement it if it&#39;s specific to</span>
<span class="quote">&gt; hugetlbfs, thus we should implement it only for hugetlbfs, and keep it</span>
<span class="quote">&gt; specific to hugetlbfs.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; The bigger question is: do we want to continue adding to the complexity</span>
<span class="quote">&gt; of hugetlbfs and increase its divergence from the core mm?</span>

Ok,

The purpose of hugetlbfs is to enable applications to be
able to use memory in huge page sizes. Expecting that there
will be no change to its purpose other then this. The filesystem
API fallocate(), with the recent addition for hole punching support
to free pages, allows  explicit control on page
allocation  / deallocation which is useful.

It seems that the &#39;noautofill&#39; feature is what is missing, with
regards to applications having explicit control on memory page
allocations using hugetlbfs. Even though the description for this
feature is not to fill holes in files, given it is filesystem semantic, but
actually the intent is to indicate not allocating pages implicitly as
the application is primarily dealing with memory allocation and
deallocation here. Is this a good enough justification?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=101">Christoph Hellwig</a> - May 9, 2017, 8:58 a.m.</div>
<pre class="content">
On Mon, May 08, 2017 at 03:12:42PM -0700, prakash.sangappa wrote:
<span class="quote">&gt; Regarding #3 as a general feature, do we want to</span>
<span class="quote">&gt; consider this and the complexity associated with the</span>
<span class="quote">&gt; implementation?</span>

We have to.  Given that no one has exclusive access to hugetlbfs
a mount option is fundamentally the wrong interface.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=174063">Prakash Sangappa</a> - May 9, 2017, 8:59 p.m.</div>
<pre class="content">
On 5/9/17 1:58 AM, Christoph Hellwig wrote:
<span class="quote">&gt; On Mon, May 08, 2017 at 03:12:42PM -0700, prakash.sangappa wrote:</span>
<span class="quote">&gt;&gt; Regarding #3 as a general feature, do we want to</span>
<span class="quote">&gt;&gt; consider this and the complexity associated with the</span>
<span class="quote">&gt;&gt; implementation?</span>
<span class="quote">&gt; We have to.  Given that no one has exclusive access to hugetlbfs</span>
<span class="quote">&gt; a mount option is fundamentally the wrong interface.</span>


A hugetlbfs filesystem may need to be mounted for exclusive use by
an application. Note, recently the &#39;min_size&#39; mount option was added
to hugetlbfs, which would reserve minimum number of huge pages
for that filesystem for use by an application. If the filesystem with
min size specified, is not setup for exclusive use by an application,
then the purpose of reserving huge pages is defeated.  The
min_size option was for use by applications like the database.

Also, I am investigating enabling hugetlbfs mounts within user
namespace&#39;s mount namespace. That would allow an application
to mount a hugetlbfs filesystem inside a namespace exclusively for
its use, running as a non root user. For this it seems like the &#39;min_size&#39;
should be subject to some user limits. Anyways, mounting inside
user namespaces is  a different discussion.

So, if a filesystem has to be setup for exclusive use by an application,
then different mount options can be used for that filesystem.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=174063">Prakash Sangappa</a> - May 16, 2017, 4:51 p.m.</div>
<pre class="content">
On 5/9/17 1:59 PM, Prakash Sangappa wrote:
<span class="quote">&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; On 5/9/17 1:58 AM, Christoph Hellwig wrote:</span>
<span class="quote">&gt;&gt; On Mon, May 08, 2017 at 03:12:42PM -0700, prakash.sangappa wrote:</span>
<span class="quote">&gt;&gt;&gt; Regarding #3 as a general feature, do we want to</span>
<span class="quote">&gt;&gt;&gt; consider this and the complexity associated with the</span>
<span class="quote">&gt;&gt;&gt; implementation?</span>
<span class="quote">&gt;&gt; We have to.  Given that no one has exclusive access to hugetlbfs</span>
<span class="quote">&gt;&gt; a mount option is fundamentally the wrong interface.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; A hugetlbfs filesystem may need to be mounted for exclusive use by</span>
<span class="quote">&gt; an application. Note, recently the &#39;min_size&#39; mount option was added</span>
<span class="quote">&gt; to hugetlbfs, which would reserve minimum number of huge pages</span>
<span class="quote">&gt; for that filesystem for use by an application. If the filesystem with</span>
<span class="quote">&gt; min size specified, is not setup for exclusive use by an application,</span>
<span class="quote">&gt; then the purpose of reserving huge pages is defeated.  The</span>
<span class="quote">&gt; min_size option was for use by applications like the database.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Also, I am investigating enabling hugetlbfs mounts within user</span>
<span class="quote">&gt; namespace&#39;s mount namespace. That would allow an application</span>
<span class="quote">&gt; to mount a hugetlbfs filesystem inside a namespace exclusively for</span>
<span class="quote">&gt; its use, running as a non root user. For this it seems like the </span>
<span class="quote">&gt; &#39;min_size&#39;</span>
<span class="quote">&gt; should be subject to some user limits. Anyways, mounting inside</span>
<span class="quote">&gt; user namespaces is  a different discussion.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; So, if a filesystem has to be setup for exclusive use by an application,</span>
<span class="quote">&gt; then different mount options can be used for that filesystem.</span>
<span class="quote">&gt;</span>

Any further comments?

Cc&#39;ing Andrea as we had discussed this requirement for the Database.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=965">Andrea Arcangeli</a> - June 16, 2017, 1:15 p.m.</div>
<pre class="content">
Hello Prakash,

On Tue, May 09, 2017 at 01:59:34PM -0700, Prakash Sangappa wrote:
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; On 5/9/17 1:58 AM, Christoph Hellwig wrote:</span>
<span class="quote">&gt; &gt; On Mon, May 08, 2017 at 03:12:42PM -0700, prakash.sangappa wrote:</span>
<span class="quote">&gt; &gt;&gt; Regarding #3 as a general feature, do we want to</span>
<span class="quote">&gt; &gt;&gt; consider this and the complexity associated with the</span>
<span class="quote">&gt; &gt;&gt; implementation?</span>
<span class="quote">&gt; &gt; We have to.  Given that no one has exclusive access to hugetlbfs</span>
<span class="quote">&gt; &gt; a mount option is fundamentally the wrong interface.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; A hugetlbfs filesystem may need to be mounted for exclusive use by</span>
<span class="quote">&gt; an application. Note, recently the &#39;min_size&#39; mount option was added</span>
<span class="quote">&gt; to hugetlbfs, which would reserve minimum number of huge pages</span>
<span class="quote">&gt; for that filesystem for use by an application. If the filesystem with</span>
<span class="quote">&gt; min size specified, is not setup for exclusive use by an application,</span>
<span class="quote">&gt; then the purpose of reserving huge pages is defeated.  The</span>
<span class="quote">&gt; min_size option was for use by applications like the database.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Also, I am investigating enabling hugetlbfs mounts within user</span>
<span class="quote">&gt; namespace&#39;s mount namespace. That would allow an application</span>
<span class="quote">&gt; to mount a hugetlbfs filesystem inside a namespace exclusively for</span>
<span class="quote">&gt; its use, running as a non root user. For this it seems like the &#39;min_size&#39;</span>
<span class="quote">&gt; should be subject to some user limits. Anyways, mounting inside</span>
<span class="quote">&gt; user namespaces is  a different discussion.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So, if a filesystem has to be setup for exclusive use by an application,</span>
<span class="quote">&gt; then different mount options can be used for that filesystem.</span>

Before userfaultfd I used a madvise that triggered SIGBUS. Aside from
performance that is much lower than userfaultfd because of the return
to userland, SIGBUS handling and new enter kernel to communicate
through a pipe with a memory manager, it couldn&#39;t work reliably
because you&#39;re not going to get exact information on the virtual
address that triggered the fault if the SIGBUS triggers in some random
in a copy-user of some random syscall, depending on the syscall some
random error will be returned. So it couldn&#39;t work transparently to
the app as far as syscalls and get_user_pages drivers were concerned.

With your solution if you pass a corrupted pointer to a random read()
syscall you&#39;re going to get a error, but supposedly you already handle
any syscall error and stop the app.

This is a special case because you don&#39;t care about performance and
you don&#39;t care about not returning random EFAULT errors from syscalls
like read().

This mount option seems non intrusive enough and hugetlbfs is quite
special already, so I&#39;m not particularly concerned by the fact it&#39;s
one more special tweak.

If it would be enough to convert the SIGBUS into a (killable) process
hang, you could still use uffd and there would be no need to send the
uffd to a manager. You&#39;d find the corrupting buggy process stuck in
handle_userfault().

As an alternative to the mount option we could consider adding
UFFD_FEATURE_SIGBUS that tells the handle_userfault() to simply return
VM_FAULT_SIGBUS in presence of a pagefault event. You&#39;d still get
weird EFAULT or erratic retvals from syscalls so it would only be
usable in for your robustness feature. Then you could use UFFDIO_COPY
too to fill the memory atomically which runs faster than a page fault
(fallocate punch hole still required to zap it).

Adding a single if (ctx-&gt;feature &amp; UFFD_FEATURE_SIGBUS) goto out,
branch for this corner case to handle_userfault() isn&#39;t great and the
hugetlbfs mount option is absolutely zero cost to the handle_userfault
which is primarily why I&#39;m not against it.. although it&#39;s not going to
be measurable so it would be ok also to add such feature.

Thanks,
Andrea
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=174063">Prakash Sangappa</a> - June 20, 2017, 11:35 p.m.</div>
<pre class="content">
On 6/16/17 6:15 AM, Andrea Arcangeli wrote:
<span class="quote">&gt; Hello Prakash,</span>


Thanks for you response. Comments inline.
<span class="quote">
&gt;</span>
<span class="quote">&gt; On Tue, May 09, 2017 at 01:59:34PM -0700, Prakash Sangappa wrote:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; On 5/9/17 1:58 AM, Christoph Hellwig wrote:</span>
<span class="quote">&gt;&gt;&gt; On Mon, May 08, 2017 at 03:12:42PM -0700, prakash.sangappa wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; Regarding #3 as a general feature, do we want to</span>
<span class="quote">&gt;&gt;&gt;&gt; consider this and the complexity associated with the</span>
<span class="quote">&gt;&gt;&gt;&gt; implementation?</span>
<span class="quote">&gt;&gt;&gt; We have to.  Given that no one has exclusive access to hugetlbfs</span>
<span class="quote">&gt;&gt;&gt; a mount option is fundamentally the wrong interface.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; A hugetlbfs filesystem may need to be mounted for exclusive use by</span>
<span class="quote">&gt;&gt; an application. Note, recently the &#39;min_size&#39; mount option was added</span>
<span class="quote">&gt;&gt; to hugetlbfs, which would reserve minimum number of huge pages</span>
<span class="quote">&gt;&gt; for that filesystem for use by an application. If the filesystem with</span>
<span class="quote">&gt;&gt; min size specified, is not setup for exclusive use by an application,</span>
<span class="quote">&gt;&gt; then the purpose of reserving huge pages is defeated.  The</span>
<span class="quote">&gt;&gt; min_size option was for use by applications like the database.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Also, I am investigating enabling hugetlbfs mounts within user</span>
<span class="quote">&gt;&gt; namespace&#39;s mount namespace. That would allow an application</span>
<span class="quote">&gt;&gt; to mount a hugetlbfs filesystem inside a namespace exclusively for</span>
<span class="quote">&gt;&gt; its use, running as a non root user. For this it seems like the &#39;min_size&#39;</span>
<span class="quote">&gt;&gt; should be subject to some user limits. Anyways, mounting inside</span>
<span class="quote">&gt;&gt; user namespaces is  a different discussion.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; So, if a filesystem has to be setup for exclusive use by an application,</span>
<span class="quote">&gt;&gt; then different mount options can be used for that filesystem.</span>
<span class="quote">&gt; Before userfaultfd I used a madvise that triggered SIGBUS. Aside from</span>
<span class="quote">&gt; performance that is much lower than userfaultfd because of the return</span>
<span class="quote">&gt; to userland, SIGBUS handling and new enter kernel to communicate</span>
<span class="quote">&gt; through a pipe with a memory manager, it couldn&#39;t work reliably</span>
<span class="quote">&gt; because you&#39;re not going to get exact information on the virtual</span>
<span class="quote">&gt; address that triggered the fault if the SIGBUS triggers in some random</span>
<span class="quote">&gt; in a copy-user of some random syscall, depending on the syscall some</span>
<span class="quote">&gt; random error will be returned. So it couldn&#39;t work transparently to</span>
<span class="quote">&gt; the app as far as syscalls and get_user_pages drivers were concerned.</span>


Sure, seems like that would be the case if an application wants to take 
some action as a result of the fault.
<span class="quote">
&gt;</span>
<span class="quote">&gt; With your solution if you pass a corrupted pointer to a random read()</span>
<span class="quote">&gt; syscall you&#39;re going to get a error, but supposedly you already handle</span>
<span class="quote">&gt; any syscall error and stop the app.</span>


Yes, the expectation is  that the application will handle the error and 
stop. This would be similar to an application passing an invalid  
address to a system call.

So, in the use case for this feature,  accessing the mapped address over 
a hole in hugetlbfs file is invalid. The application will keep track of 
the valid regions.
<span class="quote">

&gt;</span>
<span class="quote">&gt; This is a special case because you don&#39;t care about performance and</span>
<span class="quote">&gt; you don&#39;t care about not returning random EFAULT errors from syscalls</span>
<span class="quote">&gt; like read().</span>


Exactly.
<span class="quote">
&gt;</span>
<span class="quote">&gt; This mount option seems non intrusive enough and hugetlbfs is quite</span>
<span class="quote">&gt; special already, so I&#39;m not particularly concerned by the fact it&#39;s</span>
<span class="quote">&gt; one more special tweak.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; If it would be enough to convert the SIGBUS into a (killable) process</span>
<span class="quote">&gt; hang, you could still use uffd and there would be no need to send the</span>
<span class="quote">&gt; uffd to a manager. You&#39;d find the corrupting buggy process stuck in</span>
<span class="quote">&gt; handle_userfault().</span>


This could be a useful feature in debug mode.  However, In the normal 
mode the application should exit/die.
<span class="quote">
&gt;</span>
<span class="quote">&gt; As an alternative to the mount option we could consider adding</span>
<span class="quote">&gt; UFFD_FEATURE_SIGBUS that tells the handle_userfault() to simply return</span>
<span class="quote">&gt; VM_FAULT_SIGBUS in presence of a pagefault event. You&#39;d still get</span>
<span class="quote">&gt; weird EFAULT or erratic retvals from syscalls so it would only be</span>
<span class="quote">&gt; usable in for your robustness feature. Then you could use UFFDIO_COPY</span>
<span class="quote">&gt; too to fill the memory atomically which runs faster than a page fault</span>
<span class="quote">&gt; (fallocate punch hole still required to zap it).</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Adding a single if (ctx-&gt;feature &amp; UFFD_FEATURE_SIGBUS) goto out,</span>
<span class="quote">&gt; branch for this corner case to handle_userfault() isn&#39;t great and the</span>
<span class="quote">&gt; hugetlbfs mount option is absolutely zero cost to the handle_userfault</span>
<span class="quote">&gt; which is primarily why I&#39;m not against it.. although it&#39;s not going to</span>
<span class="quote">&gt; be measurable so it would be ok also to add such feature.</span>


Sure, UFFD_FEATURE_SIGBUS would address the use case for the database 
using hugetlbfs. This could be a generic API and so could be useful in 
other cases as well maybe?

However for this, the userfaultfd(2)  has to be opened to register. This 
fd has to remain opened. Is this ok? Also, even though  a monitor thread 
will not be required for this particular feature, hopefully it will not 
hinder future  enhancements to userfaultfd.

Expectation is that the overhead of registering UFFD_FEATURE_SIGBUS is 
minimal, and the registration will be done by the application ones after 
every mmap() call as required, hopefully this is not required to be done 
frequently. In the database use case, the registration  will mainly be 
done once in the beginning when mapping hugetlbfs files, so should be ok.

The mount option proposed, would give one consistent behavior for the 
filesystem and will not require the application to take any additional 
steps.

If implementing UFFD_FEATURE_SIGBUS is preferred instead of the mount 
option, I could look into that.


Thanks,
-Prakash.
<span class="quote">
&gt;</span>
<span class="quote">&gt; Thanks,</span>
<span class="quote">&gt; Andrea</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=174063">Prakash Sangappa</a> - June 27, 2017, 8:57 p.m.</div>
<pre class="content">
On 6/20/17 4:35 PM, Prakash Sangappa wrote:
<span class="quote">&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; On 6/16/17 6:15 AM, Andrea Arcangeli wrote:</span>
<span class="quote">&gt;&gt; Adding a single if (ctx-&gt;feature &amp; UFFD_FEATURE_SIGBUS) goto out,</span>
<span class="quote">&gt;&gt; branch for this corner case to handle_userfault() isn&#39;t great and the</span>
<span class="quote">&gt;&gt; hugetlbfs mount option is absolutely zero cost to the handle_userfault</span>
<span class="quote">&gt;&gt; which is primarily why I&#39;m not against it.. although it&#39;s not going to</span>
<span class="quote">&gt;&gt; be measurable so it would be ok also to add such feature.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; If implementing UFFD_FEATURE_SIGBUS is preferred instead of the mount </span>
<span class="quote">&gt; option, I could look into that.</span>
<span class="quote">&gt;</span>
Implementing UFFD_FEATURE_SIGBUS seems reasonable.

I wanted to note here on this thread that I sent out a seperate
RFC patch review for adding UFFD_FEATURE_SIGBUS.

See,
http://marc.info/?l=linux-mm&amp;m=149857975906880&amp;w=2
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/fs/hugetlbfs/inode.c b/fs/hugetlbfs/inode.c</span>
<span class="p_header">index 8f96461..8342ee9 100644</span>
<span class="p_header">--- a/fs/hugetlbfs/inode.c</span>
<span class="p_header">+++ b/fs/hugetlbfs/inode.c</span>
<span class="p_chunk">@@ -53,6 +53,7 @@</span> <span class="p_context"> struct hugetlbfs_config {</span>
      long    nr_inodes;
      struct hstate *hstate;
      long    min_hpages;
<span class="p_add">+    long    noautofill;</span>
  };

  struct hugetlbfs_inode_info {
<span class="p_chunk">@@ -71,7 +72,7 @@</span> <span class="p_context"> enum {</span>
      Opt_size, Opt_nr_inodes,
      Opt_mode, Opt_uid, Opt_gid,
      Opt_pagesize, Opt_min_size,
<span class="p_del">-    Opt_err,</span>
<span class="p_add">+    Opt_noautofill, Opt_err,</span>
  };

  static const match_table_t tokens = {
<span class="p_chunk">@@ -82,6 +83,7 @@</span> <span class="p_context"> static const match_table_t tokens = {</span>
      {Opt_gid,    &quot;gid=%u&quot;},
      {Opt_pagesize,    &quot;pagesize=%s&quot;},
      {Opt_min_size,    &quot;min_size=%s&quot;},
<span class="p_add">+    {Opt_noautofill,    &quot;noautofill&quot;},</span>
      {Opt_err,    NULL},
  };

<span class="p_chunk">@@ -1109,6 +1111,11 @@</span> <span class="p_context"> hugetlbfs_parse_options(char *options, struct</span>
hugetlbfs_config *pconfig)
              break;
          }

<span class="p_add">+        case Opt_noautofill: {</span>
<span class="p_add">+            pconfig-&gt;noautofill = 1;</span>
<span class="p_add">+            break;</span>
<span class="p_add">+        }</span>
<span class="p_add">+</span>
          default:
              pr_err(&quot;Bad mount option: \&quot;%s\&quot;\n&quot;, p);
              return -EINVAL;
<span class="p_chunk">@@ -1157,6 +1164,7 @@</span> <span class="p_context"> hugetlbfs_fill_super(struct super_block *sb, void</span>
*data, int silent)
      config.mode = 0755;
      config.hstate = &amp;default_hstate;
      config.min_hpages = -1; /* No default minimum size */
<span class="p_add">+    config.noautofill = 0;</span>
      ret = hugetlbfs_parse_options(data, &amp;config);
      if (ret)
          return ret;
<span class="p_chunk">@@ -1170,6 +1178,7 @@</span> <span class="p_context"> hugetlbfs_fill_super(struct super_block *sb, void</span>
*data, int silent)
      sbinfo-&gt;max_inodes = config.nr_inodes;
      sbinfo-&gt;free_inodes = config.nr_inodes;
      sbinfo-&gt;spool = NULL;
<span class="p_add">+    sbinfo-&gt;noautofill = config.noautofill;</span>
      /*
       * Allocate and initialize subpool if maximum or minimum size is
       * specified.  Any needed reservations (for minimim size) are taken
<span class="p_header">diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h</span>
<span class="p_header">index 503099d..2f37e0c 100644</span>
<span class="p_header">--- a/include/linux/hugetlb.h</span>
<span class="p_header">+++ b/include/linux/hugetlb.h</span>
<span class="p_chunk">@@ -259,6 +259,7 @@</span> <span class="p_context"> struct hugetlbfs_sb_info {</span>
      spinlock_t    stat_lock;
      struct hstate *hstate;
      struct hugepage_subpool *spool;
<span class="p_add">+    int    noautofill; /* don&#39;t allocate page to fill hole at fault time */</span>
  };

  static inline struct hugetlbfs_sb_info *HUGETLBFS_SB(struct
super_block *sb)
<span class="p_header">diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="p_header">index a7aa811..11655ef 100644</span>
<span class="p_header">--- a/mm/hugetlb.c</span>
<span class="p_header">+++ b/mm/hugetlb.c</span>
<span class="p_chunk">@@ -3715,6 +3715,11 @@</span> <span class="p_context"> static int hugetlb_no_page(struct mm_struct *mm,</span>
struct vm_area_struct *vma,
              goto out;
          }

<span class="p_add">+        if (HUGETLBFS_SB(mapping-&gt;host-&gt;i_sb)-&gt;noautofill) {</span>
<span class="p_add">+            ret = VM_FAULT_SIGBUS;</span>
<span class="p_add">+            goto out;</span>
<span class="p_add">+        }</span>
<span class="p_add">+</span>
          page = alloc_huge_page(vma, address, 0);
          if (IS_ERR(page)) {

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



