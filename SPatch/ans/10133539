
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[RFC,v2,1/2] vfio/type1: Adopt fast IOTLB flush interface when unmap IOVAs - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [RFC,v2,1/2] vfio/type1: Adopt fast IOTLB flush interface when unmap IOVAs</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=2908">Suthikulpanit, Suravee</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Dec. 27, 2017, 9:20 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1514366435-12723-2-git-send-email-suravee.suthikulpanit@amd.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10133539/mbox/"
   >mbox</a>
|
   <a href="/patch/10133539/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10133539/">/patch/10133539/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	919AF6037D for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 27 Dec 2017 09:21:22 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 9B8E52DB5B
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 27 Dec 2017 09:21:22 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 8DDF62DB4E; Wed, 27 Dec 2017 09:21:22 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	DKIM_VALID,RCVD_IN_DNSWL_HI autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id D07242DB4E
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 27 Dec 2017 09:21:21 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1751925AbdL0JVF (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Wed, 27 Dec 2017 04:21:05 -0500
Received: from mail-sn1nam02on0046.outbound.protection.outlook.com
	([104.47.36.46]:43692
	&quot;EHLO NAM02-SN1-obe.outbound.protection.outlook.com&quot;
	rhost-flags-OK-OK-OK-FAIL) by vger.kernel.org with ESMTP
	id S1751888AbdL0JVA (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Wed, 27 Dec 2017 04:21:00 -0500
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=amdcloud.onmicrosoft.com; s=selector1-amd-com;
	h=From:Date:Subject:Message-ID:Content-Type:MIME-Version;
	bh=c77zZD4WUU84M2snVHut2GI3Uk3JzUpA1b4JP4arEGo=;
	b=vG2KlSutdbyPs+nzXg7Rm18jV0HVAhEbRnDU57KmujReUiCePo2Qi45CNvmRN4UrCsSA4XAw11AFAdGVrYQJEKNtGcqWXgeNfFNvkZHE+fROhszPCl5IZ3kmGlNTdw/PEG8kEB94S3x6owFwNQbdDo7nGD/sqVskUUFw1e8SilY=
Authentication-Results: spf=none (sender IP is )
	smtp.mailfrom=Suravee.Suthikulpanit@amd.com; 
Received: from ssuthiku-rhel74.localdomain (114.109.128.54) by
	MWHPR12MB1742.namprd12.prod.outlook.com (10.175.55.13) with Microsoft
	SMTP Server (version=TLS1_2,
	cipher=TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA384_P256) id
	15.20.366.8; Wed, 27 Dec 2017 09:20:56 +0000
From: Suravee Suthikulpanit &lt;suravee.suthikulpanit@amd.com&gt;
To: linux-kernel@vger.kernel.org, iommu@lists.linux-foundation.org
Cc: joro@8bytes.org, jroedel@suse.de, alex.williamson@redhat.com,
	Suravee Suthikulpanit &lt;suravee.suthikulpanit@amd.com&gt;
Subject: [RFC PATCH v2 1/2] vfio/type1: Adopt fast IOTLB flush interface
	when unmap IOVAs
Date: Wed, 27 Dec 2017 04:20:34 -0500
Message-Id: &lt;1514366435-12723-2-git-send-email-suravee.suthikulpanit@amd.com&gt;
X-Mailer: git-send-email 1.8.3.1
In-Reply-To: &lt;1514366435-12723-1-git-send-email-suravee.suthikulpanit@amd.com&gt;
References: &lt;1514366435-12723-1-git-send-email-suravee.suthikulpanit@amd.com&gt;
MIME-Version: 1.0
Content-Type: text/plain
X-Originating-IP: [114.109.128.54]
X-ClientProxiedBy: HK2PR02CA0155.apcprd02.prod.outlook.com (10.171.30.15) To
	MWHPR12MB1742.namprd12.prod.outlook.com (10.175.55.13)
X-MS-PublicTrafficType: Email
X-MS-Office365-Filtering-Correlation-Id: 101733d4-6f51-48a5-e6f9-08d54d0b23c8
X-MS-Office365-Filtering-HT: Tenant
X-Microsoft-Antispam: UriScan:; BCL:0; PCL:0;
	RULEID:(4534020)(4602075)(4627115)(201703031133081)(201702281549075)(5600026)(4604075)(48565401081)(2017052603307)(7153060);
	SRVR:MWHPR12MB1742; 
X-Microsoft-Exchange-Diagnostics: 1; MWHPR12MB1742;
	3:2ah0eIz/QnLCQPcR9HwVp9t5zFT9bsBncXreJhGCfiHeS+GJYTLmPZK7sxnDPjrkmC/TjA7U7GFEsEOqKE/5YpOpEQwiBF2f0HpifoIHkgG7XJii6Rgi9iCH/0PLQqVRX6w1GYXphLm5cOmonMy/kYZ9Kia9G0Z/PpfYQim0er1eqL7OBnDvvuKhnFC/V4gVuymvCsr5CWVC3Zpv2QyzPINln+ARxwZ1asO4YwOfMdNa8Rvsi8BwTlKgb4pHOrhc;
	25:WBppp39LDdGs+hv2Ffgs6RutPHNShaymPTolsSKZFfkjjoXxh4c2LAn+s4esA1RupCMqOkp8pU6Dl6l3P0QVI2a0JJJFrdLFqyYip4TyN9hY5VVOjJngLuC3bnhR8umfnVvjw4vuLJ2MXlmQHWhyIhp18qVuQghZX6sEvfa1wTCC5wCQsfLi0wUjFJhozyccbT2scTgfMgFXESdGUgRm32yzvoDOvGfDnlH+mUcUcnGnli4/bQlUfYuStKZdqp7yb70O0qdWBHOUctFCu4+epfATc37J6uR1DVDdKxQgHwCe24+P4xjUKYHtBRM8oqW4yBT4WKxEPZ7YyG+/p9kihw==;
	31:MjyBv20zsIHymbUnBEW5FIemV/Hdo6Of07dtzT5/yEgsPQppsmPW1xHOQBOMOfjYEdq2Ng/xxsdr2G7Ydi3Rm9yC1C8b5jHDI6itbme00V+aLmo8u58Ibcb4aBJjKdo+bPPCai06LSL0pPzjotgJtfH/48a1F9UrR1xOIx7sgRvalDfzyvoYmyto8wxIgT3pML0V2Pv0GdZOmN08Op8w4FBBTzzwfoKamUQjSrpG9a8=
X-MS-TrafficTypeDiagnostic: MWHPR12MB1742:
X-Microsoft-Exchange-Diagnostics: 1; MWHPR12MB1742;
	20:qbu9xrz4z1mnj1ROTgDdAL6Lom6aHKr88bgOpTsGZuYQUk7RjyA9So0mPIiQur1wm0AfhMElmu+Ju8OSa6rhBndzgnbY8EeQeLKEn1wIR5i19XzQ4xRe9NVdPHahzZbrjmAISnVKhWWZIgEFQFRygQ733O5B6FVnF9/42JdUkD2+OQxleGIFIGtbzCVvIjGmBEhmKyR0ekBYn2CVu4BHUlb5X7JmfSN6mD/0hzxb8cO4gpuMnms1OPLoGdXsq8X8NMM3PLbpQLgsn2BkBAAI2Z3WPHSfi9TZE1QaZ1dst4ND4dVSO/b0tIzdSlc0K5/vCErNXid8bI0ahKW7Qf2GCe9Z9mq0sOzSniid9SMgIEhibon0B2lHnkYxXzZPqbp4b9NW+k5cJqkT47s9lCZfgV3Z+ACMR9cDeZgMQtcbWdTH2qJ3mgClIhDIjt5GbIsmnid74+XLvH5hT/BNFyPFmwRvWcdROIhzIQ7vHdr2ljzdHz75UrheVaudc5s5XFd2;
	4:YZClyASfBe18VepvbeStMAplrjy7WId6rFYJWaLzxmUKm39e8C0WUwiruCh6ExuUgZbmkp7Y3tRkrQ9UWgnwRcAHcNhVBcMXUngAPjUAwla5R353/u9B5Ch7qV62uJDeekW/Ou2pe/GvrsXcMJrZu6GbKrhrlhgA3YidRg6fxg/pGHEBHChf0Zvyi7Xa8fJyHTsPEGAyU3mpyssXye+L1V9jA4j2oXo70on8V66Ls9qjgDIRPrVLNs9nA77Eg9jP5h8QakXVR+ys0q3Lw3GwyRfG9Qs5slKvOMkU4ywAjSkkgugRV7yMImIOojs6twc7
X-Microsoft-Antispam-PRVS: &lt;MWHPR12MB174239C34747D9F9F2626E22F3070@MWHPR12MB1742.namprd12.prod.outlook.com&gt;
X-Exchange-Antispam-Report-Test: UriScan:(767451399110);
X-Exchange-Antispam-Report-CFA-Test: BCL:0; PCL:0;
	RULEID:(6040470)(2401047)(5005006)(8121501046)(3231023)(944501075)(93006095)(93001095)(10201501046)(3002001)(6055026)(6041268)(20161123564045)(20161123562045)(201703131423095)(201702281528075)(20161123555045)(201703061421075)(201703061406153)(20161123560045)(20161123558120)(6072148)(201708071742011);
	SRVR:MWHPR12MB1742; BCL:0; PCL:0;
	RULEID:(100000803101)(100110400095); SRVR:MWHPR12MB1742; 
X-Forefront-PRVS: 0534947130
X-Forefront-Antispam-Report: SFV:NSPM;
	SFS:(10009020)(396003)(376002)(39380400002)(366004)(346002)(39860400002)(199004)(189003)(68736007)(25786009)(106356001)(105586002)(6486002)(8936002)(97736004)(66066001)(86362001)(47776003)(3846002)(6116002)(4720700003)(5660300001)(7736002)(305945005)(6666003)(2950100002)(8676002)(81166006)(81156014)(50226002)(72206003)(478600001)(316002)(16526018)(16586007)(2906002)(4326008)(59450400001)(53936002)(51416003)(50466002)(48376002)(6512007)(36756003)(52116002)(386003)(6506007)(76176011)(217873001);
	DIR:OUT; SFP:1101; SCL:1; SRVR:MWHPR12MB1742;
	H:ssuthiku-rhel74.localdomain; FPR:; SPF:None;
	PTR:InfoNoRecords; A:1; MX:1; LANG:en; 
Received-SPF: None (protection.outlook.com: amd.com does not designate
	permitted sender hosts)
X-Microsoft-Exchange-Diagnostics: =?us-ascii?Q?1; MWHPR12MB1742;
	23:nJxOVdGFwGNGMUKhIrFFjjQvTAq5drRT0MHCdBLwO?=
	=?us-ascii?Q?gyFgsPeKVzqzM5s0DFUZZGxrNXhNK7TuJ14N12wjrS6+i6WF+FnhhZs0UlHa?=
	=?us-ascii?Q?mTUoyHC4w5AYJNOWfduarpSIHa2nmipzwqmle9IJWI/5vC1V5D/lOsC/yA6q?=
	=?us-ascii?Q?Fxho/G6GOo33xPBJvvGoTMUB7AMEdaz/Mz0556F0GCR57afCbQChAFnyGlCG?=
	=?us-ascii?Q?u656GjtOEAOV1Hrk/4PJwKt4788J9QIHGTK6mNxRoL45GdDEFJmbFF9OtG98?=
	=?us-ascii?Q?cMAogu0SE7OqxpcrTrzJ8VtBio4tSRwfm1s0RQPQ0Mtnl6GApr/5v2KmJ5Ra?=
	=?us-ascii?Q?I3asDxnZSN0oq68pBflH9ndAwxPU3SRmU91srRKPvsaUZgVO+rBZ0p4GIUyM?=
	=?us-ascii?Q?gRw3nw+YTW7Bx9W5HAu0iueGwTZThceLSzTKCaholrw+BQr/VgnV3WQU2pqi?=
	=?us-ascii?Q?fhr23jDa1tcEZEy3krLPC2ZoWmjPmlWMu58xDLfMb1jQOjmlMpDZf/lKJhqH?=
	=?us-ascii?Q?24r9fSNXd2DRTIi4VEmMRsU1+sLpS24ZINbO1eHp9obGQCW2rg0J4VAkyKQU?=
	=?us-ascii?Q?KlFPObUV2MJQkiuI/rDp7V8AKMHINUKFyhCupLpsmAHhSmDjuvorz7J5vPcv?=
	=?us-ascii?Q?n/E2oDKkM5vkG90HusmUhi5Gn9MnoPj5+h29ya0gC2fU1S/m7zGOJCwtNgDb?=
	=?us-ascii?Q?khYz+pZnDYlTLOrJnj9dNYGNOZA5H4qfNqH7/FI3kVdfBk+0ns4CQLzCn+gq?=
	=?us-ascii?Q?bh2Qz8S1/buAAYz3kF3gpFNzgmPDv80kwVzzqe1f7jxVcHWWAu2je6P22RK2?=
	=?us-ascii?Q?+D22/dgR7H0ogCAd6O2iLKKKJnNdzudWWpSHC1as0s28lDr6D9b8DysVZcdx?=
	=?us-ascii?Q?KpCuyk4jYxltp52zXdMwmC63q+z13ett1nA2AgWUhuHUX5ake7AiSprGOUWx?=
	=?us-ascii?Q?+deSzgcWHKq3Wh7ywOmSTA3X7Q9Gf3NgtD9XRZagaFnuwBVq4hJ8KVcJ9UbI?=
	=?us-ascii?Q?pB5tvFOTHPEmsaEyCCsFxkoZPDH2e1opiKthQRC3tTJ0K9+UxI0BBAzQ6F2P?=
	=?us-ascii?Q?mEqUbEEgfgU9TiuUhthzuW0Tk4K5KAXGyfzK8TwncFcNf2SzAgtSQzvYKWuU?=
	=?us-ascii?Q?XyNdz2P17MJnKIs+ethYkYEVz/irZpH?=
X-Microsoft-Exchange-Diagnostics: 1; MWHPR12MB1742;
	6:bNW6Dpdc0VeX3R3PrTlUfNRsAMGYDKENFuYvqiWmUH6Hhq1+96LXMM7wqXXWFj6VtHLChOi3ODqaQ8vKQR2FLXCY4b2DgIZjOjcUMxE7YXYEptuYJmy/0UUkooE2BAqHoVZXjAHjC0ckpYm6e6LrTRZSfBKRHzx19PYTXKsmr/7QQz8QME4/ru7lhuVM8WR21er+2ZJZ3j+Gal1Jst/I/eeSUct4ZnTx+fOWBpJFhbw8FkThm8/h2vJJJn2jEfO+OFUtll664zbeu31myrFFRPdCFRT9sOV8a24wTinsi4cuucBuw/FB3bCZXmEQFswNYt+Hc+5dQr3QeTQ4MUutSL19WOxj/DvTasW3Lntx+Sw=;
	5:SIY/mWvIdzRFkFokedhzBo2nkq/pY5pApqa5RzMvSzY46AhztFWaMTeIkdhqG4+hAm4xRO/WjG4VRLMOFCf09BSHBnDH2lXpJqoCrm3bpCrnpxpUO1gW2HA12yP8WkRl4/dsg7K/a0Wx3ZDGkGPeUJpUaCwe3jOOQhGDUNE8iUQ=;
	24:yhGJPa1avkzz34/KPe5tHPJIo94UTjflTLGLOc7/eIJEO44Sj/ct5vBI8SZ1btaiQD2p9FBj4pXOq3xekRzSzfbiDSS3kJKHhQ0ReM7T0xA=;
	7:6/f4OJNKw0itr/elEIwesA+aR4/6GMVibCgHmkVE2oXygvfA7IK7wFIwSC/MmdzpmR1Csj4o6Fn8kKiNDJweTA0XWURmXheED042iPQd3jviAPNerxBNSpiuMb+ATVX8CG7kEudQTgtDTQAINFHLzVBrhhwYgaz+ts74HIdx9SBshfpEapMWWdC21yC/B6GzyIAsyfGcYqp8LI6t4hM8Dn4vnIwwT5+iRfgHTI5fTSI+iNq5YdLB4ZLw55vtlbA5
SpamDiagnosticOutput: 1:99
SpamDiagnosticMetadata: NSPM
X-Microsoft-Exchange-Diagnostics: 1; MWHPR12MB1742;
	20:xUNrV2V+1mhA/bhzTjTmuxOs6RQe2DfjazFQfAaUrahGWtyeX5Jziam+Ac/gEeFbyVo47NdYdCfnCdNVy7CsQ4wWoudc819ps3/GiJQUDwhY+WRoCnBm33tap31pw4mwF0aEYTTRIQHfkL5XPjYEVMLtUKOoCv6oViI3RJgKfQl/psFZIHacNhV/r04vbLfd60+9IfUU8Enlvwxhivnto6jv+t955ouiV1uJbZuaNqz/aIXdO73xWPr8UfFLiAn8
X-OriginatorOrg: amd.com
X-MS-Exchange-CrossTenant-OriginalArrivalTime: 27 Dec 2017 09:20:56.2984
	(UTC)
X-MS-Exchange-CrossTenant-Network-Message-Id: 101733d4-6f51-48a5-e6f9-08d54d0b23c8
X-MS-Exchange-CrossTenant-FromEntityHeader: Hosted
X-MS-Exchange-CrossTenant-Id: 3dd8961f-e488-4e60-8e11-a82d994e183d
X-MS-Exchange-Transport-CrossTenantHeadersStamped: MWHPR12MB1742
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2908">Suthikulpanit, Suravee</a> - Dec. 27, 2017, 9:20 a.m.</div>
<pre class="content">
VFIO IOMMU type1 currently upmaps IOVA pages synchronously, which requires
IOTLB flushing for every unmapping. This results in large IOTLB flushing
overhead when handling pass-through devices has a large number of mapped
IOVAs.

This can be avoided by using the new IOTLB flushing interface.

Cc: Alex Williamson &lt;alex.williamson@redhat.com&gt;
Cc: Joerg Roedel &lt;jroedel@suse.de&gt;
<span class="signed-off-by">Signed-off-by: Suravee Suthikulpanit &lt;suravee.suthikulpanit@amd.com&gt;</span>
---
 drivers/vfio/vfio_iommu_type1.c | 89 +++++++++++++++++++++++++++++++++++------
 1 file changed, 77 insertions(+), 12 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=7781">Alex Williamson</a> - Jan. 8, 2018, 8:53 p.m.</div>
<pre class="content">
On Wed, 27 Dec 2017 04:20:34 -0500
Suravee Suthikulpanit &lt;suravee.suthikulpanit@amd.com&gt; wrote:
<span class="quote">
&gt; VFIO IOMMU type1 currently upmaps IOVA pages synchronously, which requires</span>
<span class="quote">&gt; IOTLB flushing for every unmapping. This results in large IOTLB flushing</span>
<span class="quote">&gt; overhead when handling pass-through devices has a large number of mapped</span>
<span class="quote">&gt; IOVAs.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This can be avoided by using the new IOTLB flushing interface.</span>

Hi Suravee,

I&#39;ve been playing with other ways we might do this, but I can&#39;t come up
with anything better.  A few comments below...
<span class="quote">
&gt; </span>
<span class="quote">&gt; Cc: Alex Williamson &lt;alex.williamson@redhat.com&gt;</span>
<span class="quote">&gt; Cc: Joerg Roedel &lt;jroedel@suse.de&gt;</span>
<span class="quote">&gt; Signed-off-by: Suravee Suthikulpanit &lt;suravee.suthikulpanit@amd.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  drivers/vfio/vfio_iommu_type1.c | 89 +++++++++++++++++++++++++++++++++++------</span>
<span class="quote">&gt;  1 file changed, 77 insertions(+), 12 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/drivers/vfio/vfio_iommu_type1.c b/drivers/vfio/vfio_iommu_type1.c</span>
<span class="quote">&gt; index e30e29a..f000844 100644</span>
<span class="quote">&gt; --- a/drivers/vfio/vfio_iommu_type1.c</span>
<span class="quote">&gt; +++ b/drivers/vfio/vfio_iommu_type1.c</span>
<span class="quote">&gt; @@ -102,6 +102,13 @@ struct vfio_pfn {</span>
<span class="quote">&gt;  	atomic_t		ref_count;</span>
<span class="quote">&gt;  };</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +struct vfio_regions{</span>
<span class="quote">&gt; +	struct list_head list;</span>
<span class="quote">&gt; +	dma_addr_t iova;</span>
<span class="quote">&gt; +	phys_addr_t phys;</span>
<span class="quote">&gt; +	size_t len;</span>
<span class="quote">&gt; +};</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  #define IS_IOMMU_CAP_DOMAIN_IN_CONTAINER(iommu)	\</span>
<span class="quote">&gt;  					(!list_empty(&amp;iommu-&gt;domain_list))</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -479,6 +486,40 @@ static long vfio_unpin_pages_remote(struct vfio_dma *dma, dma_addr_t iova,</span>
<span class="quote">&gt;  	return unlocked;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * Generally, VFIO needs to unpin remote pages after each IOTLB flush.</span>
<span class="quote">&gt; + * Therefore, when using IOTLB flush sync interface, VFIO need to keep track</span>
<span class="quote">&gt; + * of these regions (currently using a list).</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * This value specifies maximum number of regions for each IOTLB flush sync.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +#define VFIO_IOMMU_TLB_SYNC_MAX		512</span>

Is this an arbitrary value or are there non-obvious considerations for
this value should we want to further tune it in the future?
<span class="quote">
&gt; +</span>
<span class="quote">&gt; +static long vfio_sync_and_unpin(struct vfio_dma *dma, struct vfio_domain *domain,</span>
<span class="quote">&gt; +				struct list_head *regions, bool do_accounting)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	long unlocked = 0;</span>
<span class="quote">&gt; +	struct vfio_regions *entry, *next;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	iommu_tlb_sync(domain-&gt;domain);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	list_for_each_entry_safe(entry, next, regions, list) {</span>
<span class="quote">&gt; +		unlocked += vfio_unpin_pages_remote(dma,</span>
<span class="quote">&gt; +						    entry-&gt;iova,</span>
<span class="quote">&gt; +						    entry-&gt;phys &gt;&gt; PAGE_SHIFT,</span>
<span class="quote">&gt; +						    entry-&gt;len &gt;&gt; PAGE_SHIFT,</span>
<span class="quote">&gt; +						    false);</span>
<span class="quote">&gt; +		list_del(&amp;entry-&gt;list);</span>
<span class="quote">&gt; +		kfree(entry);</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (do_accounting) {</span>
<span class="quote">&gt; +		vfio_lock_acct(dma-&gt;task, -unlocked, NULL);</span>
<span class="quote">&gt; +		return 0;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +	return unlocked;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  static int vfio_pin_page_external(struct vfio_dma *dma, unsigned long vaddr,</span>
<span class="quote">&gt;  				  unsigned long *pfn_base, bool do_accounting)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; @@ -653,7 +694,10 @@ static long vfio_unmap_unpin(struct vfio_iommu *iommu, struct vfio_dma *dma,</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	dma_addr_t iova = dma-&gt;iova, end = dma-&gt;iova + dma-&gt;size;</span>
<span class="quote">&gt;  	struct vfio_domain *domain, *d;</span>
<span class="quote">&gt; +	struct list_head unmapped_regions;</span>
<span class="quote">&gt; +	struct vfio_regions *entry;</span>
<span class="quote">&gt;  	long unlocked = 0;</span>
<span class="quote">&gt; +	int cnt = 0;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (!dma-&gt;size)</span>
<span class="quote">&gt;  		return 0;</span>
<span class="quote">&gt; @@ -661,6 +705,8 @@ static long vfio_unmap_unpin(struct vfio_iommu *iommu, struct vfio_dma *dma,</span>
<span class="quote">&gt;  	if (!IS_IOMMU_CAP_DOMAIN_IN_CONTAINER(iommu))</span>
<span class="quote">&gt;  		return 0;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +	INIT_LIST_HEAD(&amp;unmapped_regions);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt;  	 * We use the IOMMU to track the physical addresses, otherwise we&#39;d</span>
<span class="quote">&gt;  	 * need a much more complicated tracking system.  Unfortunately that</span>
<span class="quote">&gt; @@ -698,24 +744,36 @@ static long vfio_unmap_unpin(struct vfio_iommu *iommu, struct vfio_dma *dma,</span>
<span class="quote">&gt;  				break;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -		unmapped = iommu_unmap(domain-&gt;domain, iova, len);</span>
<span class="quote">&gt; -		if (WARN_ON(!unmapped))</span>
<span class="quote">&gt; +		entry = kzalloc(sizeof(*entry), GFP_KERNEL);</span>
<span class="quote">&gt; +		if (!entry)</span>
<span class="quote">&gt;  			break;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -		unlocked += vfio_unpin_pages_remote(dma, iova,</span>
<span class="quote">&gt; -						    phys &gt;&gt; PAGE_SHIFT,</span>
<span class="quote">&gt; -						    unmapped &gt;&gt; PAGE_SHIFT,</span>
<span class="quote">&gt; -						    false);</span>
<span class="quote">&gt; +		unmapped = iommu_unmap_fast(domain-&gt;domain, iova, len);</span>
<span class="quote">&gt; +		if (WARN_ON(!unmapped)) {</span>
<span class="quote">&gt; +			kfree(entry);</span>
<span class="quote">&gt; +			break;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		iommu_tlb_range_add(domain-&gt;domain, iova, unmapped);</span>
<span class="quote">&gt; +		entry-&gt;iova = iova;</span>
<span class="quote">&gt; +		entry-&gt;phys = phys;</span>
<span class="quote">&gt; +		entry-&gt;len  = unmapped;</span>
<span class="quote">&gt; +		list_add_tail(&amp;entry-&gt;list, &amp;unmapped_regions);</span>
<span class="quote">&gt; +		cnt++;</span>
<span class="quote">&gt;  		iova += unmapped;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +		if (cnt &gt;= VFIO_IOMMU_TLB_SYNC_MAX) {</span>
<span class="quote">&gt; +			unlocked += vfio_sync_and_unpin(dma, domain, &amp;unmapped_regions,</span>
<span class="quote">&gt; +							do_accounting);</span>

Exceeds 80 columns here.
<span class="quote">
&gt; +			cnt = 0;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt;  		cond_resched();</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +	if (cnt)</span>
<span class="quote">&gt; +		unlocked += vfio_sync_and_unpin(dma, domain, &amp;unmapped_regions,</span>
<span class="quote">&gt; +						do_accounting);</span>
<span class="quote">&gt;  	dma-&gt;iommu_mapped = false;</span>
<span class="quote">&gt; -	if (do_accounting) {</span>
<span class="quote">&gt; -		vfio_lock_acct(dma-&gt;task, -unlocked, NULL);</span>
<span class="quote">&gt; -		return 0;</span>
<span class="quote">&gt; -	}</span>
<span class="quote">&gt;  	return unlocked;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -878,6 +936,7 @@ static int map_try_harder(struct vfio_domain *domain, dma_addr_t iova,</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	long i;</span>
<span class="quote">&gt;  	int ret = 0;</span>
<span class="quote">&gt; +	size_t unmapped = 0;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	for (i = 0; i &lt; npage; i++, pfn++, iova += PAGE_SIZE) {</span>
<span class="quote">&gt;  		ret = iommu_map(domain-&gt;domain, iova,</span>
<span class="quote">&gt; @@ -887,8 +946,14 @@ static int map_try_harder(struct vfio_domain *domain, dma_addr_t iova,</span>
<span class="quote">&gt;  			break;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	for (; i &lt; npage &amp;&amp; i &gt; 0; i--, iova -= PAGE_SIZE)</span>
<span class="quote">&gt; -		iommu_unmap(domain-&gt;domain, iova, PAGE_SIZE);</span>
<span class="quote">&gt; +	for (; i &lt; npage &amp;&amp; i &gt; 0; i--, iova -= PAGE_SIZE) {</span>
<span class="quote">&gt; +		unmapped = iommu_unmap_fast(domain-&gt;domain, iova, PAGE_SIZE);</span>
<span class="quote">&gt; +		if (WARN_ON(!unmapped))</span>
<span class="quote">&gt; +			break;</span>
<span class="quote">&gt; +		iommu_tlb_range_add(domain-&gt;domain, iova, unmapped);</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +	if (unmapped)</span>
<span class="quote">&gt; +		iommu_tlb_sync(domain-&gt;domain);</span>

Using unmapped here seems a little sketchy, for instance if we got back
zero on the last call to iommu_unmap_fast() but had other ranges queued
for flush.  Do we even need a WARN_ON and break here, are we just
trying to skip adding a zero range?  The intent is that we either leave
this function with everything mapped or nothing mapped, so perhaps we
should warn and continue.  Assuming a spurious sync is ok, we could
check (i &lt; npage) for the sync condition, the only risk being we had no
mappings at all and therefore no unmaps.

TBH, I wonder if this function is even needed anymore or if the mapping
problem in amd_iommu has since ben fixed.

Also, I&#39;m not sure why you&#39;re gating adding fast flushing to amd_iommu
on vfio making use of it.  These can be done independently.  Thanks,

Alex
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=7781">Alex Williamson</a> - Jan. 8, 2018, 9:07 p.m.</div>
<pre class="content">
On Mon, 8 Jan 2018 13:53:29 -0700
Alex Williamson &lt;alex.williamson@redhat.com&gt; wrote:
<span class="quote">
&gt; On Wed, 27 Dec 2017 04:20:34 -0500</span>
<span class="quote">&gt; Suravee Suthikulpanit &lt;suravee.suthikulpanit@amd.com&gt; wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; VFIO IOMMU type1 currently upmaps IOVA pages synchronously, which requires</span>
<span class="quote">&gt; &gt; IOTLB flushing for every unmapping. This results in large IOTLB flushing</span>
<span class="quote">&gt; &gt; overhead when handling pass-through devices has a large number of mapped</span>
<span class="quote">&gt; &gt; IOVAs.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; This can be avoided by using the new IOTLB flushing interface.  </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Hi Suravee,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I&#39;ve been playing with other ways we might do this, but I can&#39;t come up</span>
<span class="quote">&gt; with anything better.  A few comments below...</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Cc: Alex Williamson &lt;alex.williamson@redhat.com&gt;</span>
<span class="quote">&gt; &gt; Cc: Joerg Roedel &lt;jroedel@suse.de&gt;</span>
<span class="quote">&gt; &gt; Signed-off-by: Suravee Suthikulpanit &lt;suravee.suthikulpanit@amd.com&gt;</span>
<span class="quote">&gt; &gt; ---</span>
<span class="quote">&gt; &gt;  drivers/vfio/vfio_iommu_type1.c | 89 +++++++++++++++++++++++++++++++++++------</span>
<span class="quote">&gt; &gt;  1 file changed, 77 insertions(+), 12 deletions(-)</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; diff --git a/drivers/vfio/vfio_iommu_type1.c b/drivers/vfio/vfio_iommu_type1.c</span>
<span class="quote">&gt; &gt; index e30e29a..f000844 100644</span>
<span class="quote">&gt; &gt; --- a/drivers/vfio/vfio_iommu_type1.c</span>
<span class="quote">&gt; &gt; +++ b/drivers/vfio/vfio_iommu_type1.c</span>
<span class="quote">&gt; &gt; @@ -102,6 +102,13 @@ struct vfio_pfn {</span>
<span class="quote">&gt; &gt;  	atomic_t		ref_count;</span>
<span class="quote">&gt; &gt;  };</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; +struct vfio_regions{</span>
<span class="quote">&gt; &gt; +	struct list_head list;</span>
<span class="quote">&gt; &gt; +	dma_addr_t iova;</span>
<span class="quote">&gt; &gt; +	phys_addr_t phys;</span>
<span class="quote">&gt; &gt; +	size_t len;</span>
<span class="quote">&gt; &gt; +};</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt;  #define IS_IOMMU_CAP_DOMAIN_IN_CONTAINER(iommu)	\</span>
<span class="quote">&gt; &gt;  					(!list_empty(&amp;iommu-&gt;domain_list))</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; @@ -479,6 +486,40 @@ static long vfio_unpin_pages_remote(struct vfio_dma *dma, dma_addr_t iova,</span>
<span class="quote">&gt; &gt;  	return unlocked;</span>
<span class="quote">&gt; &gt;  }</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; +/*</span>
<span class="quote">&gt; &gt; + * Generally, VFIO needs to unpin remote pages after each IOTLB flush.</span>
<span class="quote">&gt; &gt; + * Therefore, when using IOTLB flush sync interface, VFIO need to keep track</span>
<span class="quote">&gt; &gt; + * of these regions (currently using a list).</span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; &gt; + * This value specifies maximum number of regions for each IOTLB flush sync.</span>
<span class="quote">&gt; &gt; + */</span>
<span class="quote">&gt; &gt; +#define VFIO_IOMMU_TLB_SYNC_MAX		512  </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Is this an arbitrary value or are there non-obvious considerations for</span>
<span class="quote">&gt; this value should we want to further tune it in the future?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static long vfio_sync_and_unpin(struct vfio_dma *dma, struct vfio_domain *domain,</span>
<span class="quote">&gt; &gt; +				struct list_head *regions, bool do_accounting)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	long unlocked = 0;</span>
<span class="quote">&gt; &gt; +	struct vfio_regions *entry, *next;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	iommu_tlb_sync(domain-&gt;domain);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	list_for_each_entry_safe(entry, next, regions, list) {</span>
<span class="quote">&gt; &gt; +		unlocked += vfio_unpin_pages_remote(dma,</span>
<span class="quote">&gt; &gt; +						    entry-&gt;iova,</span>
<span class="quote">&gt; &gt; +						    entry-&gt;phys &gt;&gt; PAGE_SHIFT,</span>
<span class="quote">&gt; &gt; +						    entry-&gt;len &gt;&gt; PAGE_SHIFT,</span>
<span class="quote">&gt; &gt; +						    false);</span>
<span class="quote">&gt; &gt; +		list_del(&amp;entry-&gt;list);</span>
<span class="quote">&gt; &gt; +		kfree(entry);</span>
<span class="quote">&gt; &gt; +	}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	if (do_accounting) {</span>
<span class="quote">&gt; &gt; +		vfio_lock_acct(dma-&gt;task, -unlocked, NULL);</span>
<span class="quote">&gt; &gt; +		return 0;</span>
<span class="quote">&gt; &gt; +	}</span>
<span class="quote">&gt; &gt; +	return unlocked;</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt;  static int vfio_pin_page_external(struct vfio_dma *dma, unsigned long vaddr,</span>
<span class="quote">&gt; &gt;  				  unsigned long *pfn_base, bool do_accounting)</span>
<span class="quote">&gt; &gt;  {</span>
<span class="quote">&gt; &gt; @@ -653,7 +694,10 @@ static long vfio_unmap_unpin(struct vfio_iommu *iommu, struct vfio_dma *dma,</span>
<span class="quote">&gt; &gt;  {</span>
<span class="quote">&gt; &gt;  	dma_addr_t iova = dma-&gt;iova, end = dma-&gt;iova + dma-&gt;size;</span>
<span class="quote">&gt; &gt;  	struct vfio_domain *domain, *d;</span>
<span class="quote">&gt; &gt; +	struct list_head unmapped_regions;</span>
<span class="quote">&gt; &gt; +	struct vfio_regions *entry;</span>
<span class="quote">&gt; &gt;  	long unlocked = 0;</span>
<span class="quote">&gt; &gt; +	int cnt = 0;</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  	if (!dma-&gt;size)</span>
<span class="quote">&gt; &gt;  		return 0;</span>
<span class="quote">&gt; &gt; @@ -661,6 +705,8 @@ static long vfio_unmap_unpin(struct vfio_iommu *iommu, struct vfio_dma *dma,</span>
<span class="quote">&gt; &gt;  	if (!IS_IOMMU_CAP_DOMAIN_IN_CONTAINER(iommu))</span>
<span class="quote">&gt; &gt;  		return 0;</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; +	INIT_LIST_HEAD(&amp;unmapped_regions);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt;  	/*</span>
<span class="quote">&gt; &gt;  	 * We use the IOMMU to track the physical addresses, otherwise we&#39;d</span>
<span class="quote">&gt; &gt;  	 * need a much more complicated tracking system.  Unfortunately that</span>
<span class="quote">&gt; &gt; @@ -698,24 +744,36 @@ static long vfio_unmap_unpin(struct vfio_iommu *iommu, struct vfio_dma *dma,</span>
<span class="quote">&gt; &gt;  				break;</span>
<span class="quote">&gt; &gt;  		}</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; -		unmapped = iommu_unmap(domain-&gt;domain, iova, len);</span>
<span class="quote">&gt; &gt; -		if (WARN_ON(!unmapped))</span>
<span class="quote">&gt; &gt; +		entry = kzalloc(sizeof(*entry), GFP_KERNEL);</span>
<span class="quote">&gt; &gt; +		if (!entry)</span>
<span class="quote">&gt; &gt;  			break;</span>


Turns out this nagged at me a bit too, this function only gets called
once to dump the vfio_dma, so bailing out here leaves pages pinned and
IOMMU mappings in place, for a performance optimization that we could
just skip.  We could sync&amp;unpin anything collected up to this point and
continue this step with a synchronous unmap/unpin.  Thanks,

Alex
<span class="quote">
&gt; &gt;  </span>
<span class="quote">&gt; &gt; -		unlocked += vfio_unpin_pages_remote(dma, iova,</span>
<span class="quote">&gt; &gt; -						    phys &gt;&gt; PAGE_SHIFT,</span>
<span class="quote">&gt; &gt; -						    unmapped &gt;&gt; PAGE_SHIFT,</span>
<span class="quote">&gt; &gt; -						    false);</span>
<span class="quote">&gt; &gt; +		unmapped = iommu_unmap_fast(domain-&gt;domain, iova, len);</span>
<span class="quote">&gt; &gt; +		if (WARN_ON(!unmapped)) {</span>
<span class="quote">&gt; &gt; +			kfree(entry);</span>
<span class="quote">&gt; &gt; +			break;</span>
<span class="quote">&gt; &gt; +		}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		iommu_tlb_range_add(domain-&gt;domain, iova, unmapped);</span>
<span class="quote">&gt; &gt; +		entry-&gt;iova = iova;</span>
<span class="quote">&gt; &gt; +		entry-&gt;phys = phys;</span>
<span class="quote">&gt; &gt; +		entry-&gt;len  = unmapped;</span>
<span class="quote">&gt; &gt; +		list_add_tail(&amp;entry-&gt;list, &amp;unmapped_regions);</span>
<span class="quote">&gt; &gt; +		cnt++;</span>
<span class="quote">&gt; &gt;  		iova += unmapped;</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; +		if (cnt &gt;= VFIO_IOMMU_TLB_SYNC_MAX) {</span>
<span class="quote">&gt; &gt; +			unlocked += vfio_sync_and_unpin(dma, domain, &amp;unmapped_regions,</span>
<span class="quote">&gt; &gt; +							do_accounting);  </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Exceeds 80 columns here.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; +			cnt = 0;</span>
<span class="quote">&gt; &gt; +		}</span>
<span class="quote">&gt; &gt;  		cond_resched();</span>
<span class="quote">&gt; &gt;  	}</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; +	if (cnt)</span>
<span class="quote">&gt; &gt; +		unlocked += vfio_sync_and_unpin(dma, domain, &amp;unmapped_regions,</span>
<span class="quote">&gt; &gt; +						do_accounting);</span>
<span class="quote">&gt; &gt;  	dma-&gt;iommu_mapped = false;</span>
<span class="quote">&gt; &gt; -	if (do_accounting) {</span>
<span class="quote">&gt; &gt; -		vfio_lock_acct(dma-&gt;task, -unlocked, NULL);</span>
<span class="quote">&gt; &gt; -		return 0;</span>
<span class="quote">&gt; &gt; -	}</span>
<span class="quote">&gt; &gt;  	return unlocked;</span>
<span class="quote">&gt; &gt;  }</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; @@ -878,6 +936,7 @@ static int map_try_harder(struct vfio_domain *domain, dma_addr_t iova,</span>
<span class="quote">&gt; &gt;  {</span>
<span class="quote">&gt; &gt;  	long i;</span>
<span class="quote">&gt; &gt;  	int ret = 0;</span>
<span class="quote">&gt; &gt; +	size_t unmapped = 0;</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  	for (i = 0; i &lt; npage; i++, pfn++, iova += PAGE_SIZE) {</span>
<span class="quote">&gt; &gt;  		ret = iommu_map(domain-&gt;domain, iova,</span>
<span class="quote">&gt; &gt; @@ -887,8 +946,14 @@ static int map_try_harder(struct vfio_domain *domain, dma_addr_t iova,</span>
<span class="quote">&gt; &gt;  			break;</span>
<span class="quote">&gt; &gt;  	}</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; -	for (; i &lt; npage &amp;&amp; i &gt; 0; i--, iova -= PAGE_SIZE)</span>
<span class="quote">&gt; &gt; -		iommu_unmap(domain-&gt;domain, iova, PAGE_SIZE);</span>
<span class="quote">&gt; &gt; +	for (; i &lt; npage &amp;&amp; i &gt; 0; i--, iova -= PAGE_SIZE) {</span>
<span class="quote">&gt; &gt; +		unmapped = iommu_unmap_fast(domain-&gt;domain, iova, PAGE_SIZE);</span>
<span class="quote">&gt; &gt; +		if (WARN_ON(!unmapped))</span>
<span class="quote">&gt; &gt; +			break;</span>
<span class="quote">&gt; &gt; +		iommu_tlb_range_add(domain-&gt;domain, iova, unmapped);</span>
<span class="quote">&gt; &gt; +	}</span>
<span class="quote">&gt; &gt; +	if (unmapped)</span>
<span class="quote">&gt; &gt; +		iommu_tlb_sync(domain-&gt;domain);  </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Using unmapped here seems a little sketchy, for instance if we got back</span>
<span class="quote">&gt; zero on the last call to iommu_unmap_fast() but had other ranges queued</span>
<span class="quote">&gt; for flush.  Do we even need a WARN_ON and break here, are we just</span>
<span class="quote">&gt; trying to skip adding a zero range?  The intent is that we either leave</span>
<span class="quote">&gt; this function with everything mapped or nothing mapped, so perhaps we</span>
<span class="quote">&gt; should warn and continue.  Assuming a spurious sync is ok, we could</span>
<span class="quote">&gt; check (i &lt; npage) for the sync condition, the only risk being we had no</span>
<span class="quote">&gt; mappings at all and therefore no unmaps.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; TBH, I wonder if this function is even needed anymore or if the mapping</span>
<span class="quote">&gt; problem in amd_iommu has since ben fixed.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Also, I&#39;m not sure why you&#39;re gating adding fast flushing to amd_iommu</span>
<span class="quote">&gt; on vfio making use of it.  These can be done independently.  Thanks,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Alex</span>
<span class="quote">&gt; _______________________________________________</span>
<span class="quote">&gt; iommu mailing list</span>
<span class="quote">&gt; iommu@lists.linux-foundation.org</span>
<span class="quote">&gt; https://lists.linuxfoundation.org/mailman/listinfo/iommu</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2908">Suthikulpanit, Suravee</a> - Jan. 17, 2018, 4:54 p.m.</div>
<pre class="content">
Hi Alex,

On 1/9/18 4:07 AM, Alex Williamson wrote:
<span class="quote">&gt;&gt;&gt; @@ -661,6 +705,8 @@ static long vfio_unmap_unpin(struct vfio_iommu *iommu, struct vfio_dma *dma,</span>
<span class="quote">&gt;&gt;&gt;   	if (!IS_IOMMU_CAP_DOMAIN_IN_CONTAINER(iommu))</span>
<span class="quote">&gt;&gt;&gt;   		return 0;</span>
<span class="quote">&gt;&gt;&gt;   </span>
<span class="quote">&gt;&gt;&gt; +	INIT_LIST_HEAD(&amp;unmapped_regions);</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;   	/*</span>
<span class="quote">&gt;&gt;&gt;   	 * We use the IOMMU to track the physical addresses, otherwise we&#39;d</span>
<span class="quote">&gt;&gt;&gt;   	 * need a much more complicated tracking system.  Unfortunately that</span>
<span class="quote">&gt;&gt;&gt; @@ -698,24 +744,36 @@ static long vfio_unmap_unpin(struct vfio_iommu *iommu, struct vfio_dma *dma,</span>
<span class="quote">&gt;&gt;&gt;   				break;</span>
<span class="quote">&gt;&gt;&gt;   		}</span>
<span class="quote">&gt;&gt;&gt;   </span>
<span class="quote">&gt;&gt;&gt; -		unmapped = iommu_unmap(domain-&gt;domain, iova, len);</span>
<span class="quote">&gt;&gt;&gt; -		if (WARN_ON(!unmapped))</span>
<span class="quote">&gt;&gt;&gt; +		entry = kzalloc(sizeof(*entry), GFP_KERNEL);</span>
<span class="quote">&gt;&gt;&gt; +		if (!entry)</span>
<span class="quote">&gt;&gt;&gt;   			break;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Turns out this nagged at me a bit too, this function only gets called</span>
<span class="quote">&gt; once to dump the vfio_dma, so bailing out here leaves pages pinned and</span>
<span class="quote">&gt; IOMMU mappings in place, for a performance optimization that we could</span>
<span class="quote">&gt; just skip.  We could sync&amp;unpin anything collected up to this point and</span>
<span class="quote">&gt; continue this step with a synchronous unmap/unpin.  Thanks,</span>

Ah, that&#39;s an over look in my part also. Thanks for catching this. I&#39;ll implement
the fallback mechanism per your suggestion in v3.

Thanks,
Suravee
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2908">Suthikulpanit, Suravee</a> - Jan. 18, 2018, 3:25 a.m.</div>
<pre class="content">
Hi Alex,

On 1/9/18 3:53 AM, Alex Williamson wrote:
<span class="quote">&gt; On Wed, 27 Dec 2017 04:20:34 -0500</span>
<span class="quote">&gt; Suravee Suthikulpanit &lt;suravee.suthikulpanit@amd.com&gt; wrote:</span>
<span class="quote">&gt;&gt; diff --git a/drivers/vfio/vfio_iommu_type1.c b/drivers/vfio/vfio_iommu_type1.c</span>
<span class="quote">&gt;&gt; index e30e29a..f000844 100644</span>
<span class="quote">&gt;&gt; --- a/drivers/vfio/vfio_iommu_type1.c</span>
<span class="quote">&gt;&gt; +++ b/drivers/vfio/vfio_iommu_type1.c</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; ... &gt;&gt;</span>
<span class="quote">&gt;&gt; @@ -479,6 +486,40 @@ static long vfio_unpin_pages_remote(struct vfio_dma *dma, dma_addr_t iova,</span>
<span class="quote">&gt;&gt;   	return unlocked;</span>
<span class="quote">&gt;&gt;   }</span>
<span class="quote">&gt;&gt;   </span>
<span class="quote">&gt;&gt; +/*</span>
<span class="quote">&gt;&gt; + * Generally, VFIO needs to unpin remote pages after each IOTLB flush.</span>
<span class="quote">&gt;&gt; + * Therefore, when using IOTLB flush sync interface, VFIO need to keep track</span>
<span class="quote">&gt;&gt; + * of these regions (currently using a list).</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + * This value specifies maximum number of regions for each IOTLB flush sync.</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;&gt; +#define VFIO_IOMMU_TLB_SYNC_MAX		512</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Is this an arbitrary value or are there non-obvious considerations for</span>
<span class="quote">&gt; this value should we want to further tune it in the future?</span>

This is just an arbitrary value for now, which we could try further tuning.
On some dGPUs that I have been using, I have seen max of ~1500 regions within an unmap call.
In most case, I see less than 100 regions in an unmap call. The structure is currently 40 bytes.
So, I figured capping at 512 entry in the list is 20KB is reasonable. Let me know what you think.
<span class="quote">
&gt;&gt; ....</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; @@ -887,8 +946,14 @@ static int map_try_harder(struct vfio_domain *domain, dma_addr_t iova,</span>
<span class="quote">&gt;&gt;   			break;</span>
<span class="quote">&gt;&gt;   	}</span>
<span class="quote">&gt;&gt;   </span>
<span class="quote">&gt;&gt; -	for (; i &lt; npage &amp;&amp; i &gt; 0; i--, iova -= PAGE_SIZE)</span>
<span class="quote">&gt;&gt; -		iommu_unmap(domain-&gt;domain, iova, PAGE_SIZE);</span>
<span class="quote">&gt;&gt; +	for (; i &lt; npage &amp;&amp; i &gt; 0; i--, iova -= PAGE_SIZE) {</span>
<span class="quote">&gt;&gt; +		unmapped = iommu_unmap_fast(domain-&gt;domain, iova, PAGE_SIZE);</span>
<span class="quote">&gt;&gt; +		if (WARN_ON(!unmapped))</span>
<span class="quote">&gt;&gt; +			break;</span>
<span class="quote">&gt;&gt; +		iommu_tlb_range_add(domain-&gt;domain, iova, unmapped);</span>
<span class="quote">&gt;&gt; +	}</span>
<span class="quote">&gt;&gt; +	if (unmapped)</span>
<span class="quote">&gt;&gt; +		iommu_tlb_sync(domain-&gt;domain);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Using unmapped here seems a little sketchy, for instance if we got back</span>
<span class="quote">&gt; zero on the last call to iommu_unmap_fast() but had other ranges queued</span>
<span class="quote">&gt; for flush.  Do we even need a WARN_ON and break here, are we just</span>
<span class="quote">&gt; trying to skip adding a zero range?  The intent is that we either leave</span>
<span class="quote">&gt; this function with everything mapped or nothing mapped, so perhaps we</span>
<span class="quote">&gt; should warn and continue.  Assuming a spurious sync is ok, we could</span>
<span class="quote">&gt; check (i &lt; npage) for the sync condition, the only risk being we had no</span>
<span class="quote">&gt; mappings at all and therefore no unmaps.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; TBH, I wonder if this function is even needed anymore or if the mapping</span>
<span class="quote">&gt; problem in amd_iommu has since ben fixed.</span>

Actually, I never hit this execution path in my test runs. I could just left this
unchanged and use the slow unmap path to simplify the logic. I&#39;m not aware of
the history of why this logic is needed for AMD IOMMU. Is this a bug in the driver or
the hardware?
<span class="quote">
&gt; Also, I&#39;m not sure why you&#39;re gating adding fast flushing to amd_iommu</span>
<span class="quote">&gt; on vfio making use of it.  These can be done independently.  Thanks,</span>

Currently, the fast unmap interface is mainly called by VFIO. So, I thought I would
submit the patches together for review. If you would prefer, I can submit the IOMMU part
separately.

Thanks,
Suravee
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=7781">Alex Williamson</a> - Jan. 18, 2018, 5:34 p.m.</div>
<pre class="content">
On Thu, 18 Jan 2018 10:25:12 +0700
Suravee Suthikulpanit &lt;suravee.suthikulpanit@amd.com&gt; wrote:
<span class="quote">
&gt; Hi Alex,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; On 1/9/18 3:53 AM, Alex Williamson wrote:</span>
<span class="quote">&gt; &gt; On Wed, 27 Dec 2017 04:20:34 -0500</span>
<span class="quote">&gt; &gt; Suravee Suthikulpanit &lt;suravee.suthikulpanit@amd.com&gt; wrote:  </span>
<span class="quote">&gt; &gt;&gt; diff --git a/drivers/vfio/vfio_iommu_type1.c b/drivers/vfio/vfio_iommu_type1.c</span>
<span class="quote">&gt; &gt;&gt; index e30e29a..f000844 100644</span>
<span class="quote">&gt; &gt;&gt; --- a/drivers/vfio/vfio_iommu_type1.c</span>
<span class="quote">&gt; &gt;&gt; +++ b/drivers/vfio/vfio_iommu_type1.c</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; ... &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; @@ -479,6 +486,40 @@ static long vfio_unpin_pages_remote(struct vfio_dma *dma, dma_addr_t iova,</span>
<span class="quote">&gt; &gt;&gt;   	return unlocked;</span>
<span class="quote">&gt; &gt;&gt;   }</span>
<span class="quote">&gt; &gt;&gt;   </span>
<span class="quote">&gt; &gt;&gt; +/*</span>
<span class="quote">&gt; &gt;&gt; + * Generally, VFIO needs to unpin remote pages after each IOTLB flush.</span>
<span class="quote">&gt; &gt;&gt; + * Therefore, when using IOTLB flush sync interface, VFIO need to keep track</span>
<span class="quote">&gt; &gt;&gt; + * of these regions (currently using a list).</span>
<span class="quote">&gt; &gt;&gt; + *</span>
<span class="quote">&gt; &gt;&gt; + * This value specifies maximum number of regions for each IOTLB flush sync.</span>
<span class="quote">&gt; &gt;&gt; + */</span>
<span class="quote">&gt; &gt;&gt; +#define VFIO_IOMMU_TLB_SYNC_MAX		512  </span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Is this an arbitrary value or are there non-obvious considerations for</span>
<span class="quote">&gt; &gt; this value should we want to further tune it in the future?  </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This is just an arbitrary value for now, which we could try further tuning.</span>
<span class="quote">&gt; On some dGPUs that I have been using, I have seen max of ~1500 regions within an unmap call.</span>
<span class="quote">&gt; In most case, I see less than 100 regions in an unmap call. The structure is currently 40 bytes.</span>
<span class="quote">&gt; So, I figured capping at 512 entry in the list is 20KB is reasonable. Let me know what you think.</span>

Seems like a reasonable starting point.  For a VM user, the maximum
size of an unmap will largely depend on the size of the VM,
predominantly the size of memory above the 4G boundary in the VM.  That
could be 100s of GB.  In the best case, iommu_unmap could return 1GB
ranges, in the worst case, 4K.  So I&#39;m not sure there&#39;s really any
upper bound, thus the cond_resched(), but we will opportunistically
coalesce pages unless disabled via module option.
<span class="quote">
&gt; &gt;&gt; ....</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; @@ -887,8 +946,14 @@ static int map_try_harder(struct vfio_domain *domain, dma_addr_t iova,</span>
<span class="quote">&gt; &gt;&gt;   			break;</span>
<span class="quote">&gt; &gt;&gt;   	}</span>
<span class="quote">&gt; &gt;&gt;   </span>
<span class="quote">&gt; &gt;&gt; -	for (; i &lt; npage &amp;&amp; i &gt; 0; i--, iova -= PAGE_SIZE)</span>
<span class="quote">&gt; &gt;&gt; -		iommu_unmap(domain-&gt;domain, iova, PAGE_SIZE);</span>
<span class="quote">&gt; &gt;&gt; +	for (; i &lt; npage &amp;&amp; i &gt; 0; i--, iova -= PAGE_SIZE) {</span>
<span class="quote">&gt; &gt;&gt; +		unmapped = iommu_unmap_fast(domain-&gt;domain, iova, PAGE_SIZE);</span>
<span class="quote">&gt; &gt;&gt; +		if (WARN_ON(!unmapped))</span>
<span class="quote">&gt; &gt;&gt; +			break;</span>
<span class="quote">&gt; &gt;&gt; +		iommu_tlb_range_add(domain-&gt;domain, iova, unmapped);</span>
<span class="quote">&gt; &gt;&gt; +	}</span>
<span class="quote">&gt; &gt;&gt; +	if (unmapped)</span>
<span class="quote">&gt; &gt;&gt; +		iommu_tlb_sync(domain-&gt;domain);  </span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Using unmapped here seems a little sketchy, for instance if we got back</span>
<span class="quote">&gt; &gt; zero on the last call to iommu_unmap_fast() but had other ranges queued</span>
<span class="quote">&gt; &gt; for flush.  Do we even need a WARN_ON and break here, are we just</span>
<span class="quote">&gt; &gt; trying to skip adding a zero range?  The intent is that we either leave</span>
<span class="quote">&gt; &gt; this function with everything mapped or nothing mapped, so perhaps we</span>
<span class="quote">&gt; &gt; should warn and continue.  Assuming a spurious sync is ok, we could</span>
<span class="quote">&gt; &gt; check (i &lt; npage) for the sync condition, the only risk being we had no</span>
<span class="quote">&gt; &gt; mappings at all and therefore no unmaps.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; TBH, I wonder if this function is even needed anymore or if the mapping</span>
<span class="quote">&gt; &gt; problem in amd_iommu has since ben fixed.  </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Actually, I never hit this execution path in my test runs. I could just left this</span>
<span class="quote">&gt; unchanged and use the slow unmap path to simplify the logic. I&#39;m not aware of</span>
<span class="quote">&gt; the history of why this logic is needed for AMD IOMMU. Is this a bug in the driver or</span>
<span class="quote">&gt; the hardware?</span>

It was a bug in amd_iommu code and unfortunately I&#39;ve lost details
beyond what we see in the comments there, iommu ptes previously mapped
as 4k can&#39;t be unmapped and remapped using large page sizes,
presumably pmds still in place or something along those lines.  If
you&#39;re testing on AMD and not triggering this, let&#39;s not try to
optimize this path, it&#39;s just a crusty old safety net.
<span class="quote"> 
&gt; &gt; Also, I&#39;m not sure why you&#39;re gating adding fast flushing to amd_iommu</span>
<span class="quote">&gt; &gt; on vfio making use of it.  These can be done independently.  Thanks,  </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Currently, the fast unmap interface is mainly called by VFIO. So, I thought I would</span>
<span class="quote">&gt; submit the patches together for review. If you would prefer, I can submit the IOMMU part</span>
<span class="quote">&gt; separately.</span>

I think it makes more sense and is most efficient to uncouple them.
Joerg probably isn&#39;t paying attention to the amd_iommu changes because
I&#39;m commenting on the vfio parts and I&#39;m not looking at the amd_iommu
changes.  The code doesn&#39;t depend on them going in together, so we can
do them in parallel if they&#39;re split.  Thanks,

Alex
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/drivers/vfio/vfio_iommu_type1.c b/drivers/vfio/vfio_iommu_type1.c</span>
<span class="p_header">index e30e29a..f000844 100644</span>
<span class="p_header">--- a/drivers/vfio/vfio_iommu_type1.c</span>
<span class="p_header">+++ b/drivers/vfio/vfio_iommu_type1.c</span>
<span class="p_chunk">@@ -102,6 +102,13 @@</span> <span class="p_context"> struct vfio_pfn {</span>
 	atomic_t		ref_count;
 };
 
<span class="p_add">+struct vfio_regions{</span>
<span class="p_add">+	struct list_head list;</span>
<span class="p_add">+	dma_addr_t iova;</span>
<span class="p_add">+	phys_addr_t phys;</span>
<span class="p_add">+	size_t len;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
 #define IS_IOMMU_CAP_DOMAIN_IN_CONTAINER(iommu)	\
 					(!list_empty(&amp;iommu-&gt;domain_list))
 
<span class="p_chunk">@@ -479,6 +486,40 @@</span> <span class="p_context"> static long vfio_unpin_pages_remote(struct vfio_dma *dma, dma_addr_t iova,</span>
 	return unlocked;
 }
 
<span class="p_add">+/*</span>
<span class="p_add">+ * Generally, VFIO needs to unpin remote pages after each IOTLB flush.</span>
<span class="p_add">+ * Therefore, when using IOTLB flush sync interface, VFIO need to keep track</span>
<span class="p_add">+ * of these regions (currently using a list).</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This value specifies maximum number of regions for each IOTLB flush sync.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define VFIO_IOMMU_TLB_SYNC_MAX		512</span>
<span class="p_add">+</span>
<span class="p_add">+static long vfio_sync_and_unpin(struct vfio_dma *dma, struct vfio_domain *domain,</span>
<span class="p_add">+				struct list_head *regions, bool do_accounting)</span>
<span class="p_add">+{</span>
<span class="p_add">+	long unlocked = 0;</span>
<span class="p_add">+	struct vfio_regions *entry, *next;</span>
<span class="p_add">+</span>
<span class="p_add">+	iommu_tlb_sync(domain-&gt;domain);</span>
<span class="p_add">+</span>
<span class="p_add">+	list_for_each_entry_safe(entry, next, regions, list) {</span>
<span class="p_add">+		unlocked += vfio_unpin_pages_remote(dma,</span>
<span class="p_add">+						    entry-&gt;iova,</span>
<span class="p_add">+						    entry-&gt;phys &gt;&gt; PAGE_SHIFT,</span>
<span class="p_add">+						    entry-&gt;len &gt;&gt; PAGE_SHIFT,</span>
<span class="p_add">+						    false);</span>
<span class="p_add">+		list_del(&amp;entry-&gt;list);</span>
<span class="p_add">+		kfree(entry);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (do_accounting) {</span>
<span class="p_add">+		vfio_lock_acct(dma-&gt;task, -unlocked, NULL);</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return unlocked;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static int vfio_pin_page_external(struct vfio_dma *dma, unsigned long vaddr,
 				  unsigned long *pfn_base, bool do_accounting)
 {
<span class="p_chunk">@@ -653,7 +694,10 @@</span> <span class="p_context"> static long vfio_unmap_unpin(struct vfio_iommu *iommu, struct vfio_dma *dma,</span>
 {
 	dma_addr_t iova = dma-&gt;iova, end = dma-&gt;iova + dma-&gt;size;
 	struct vfio_domain *domain, *d;
<span class="p_add">+	struct list_head unmapped_regions;</span>
<span class="p_add">+	struct vfio_regions *entry;</span>
 	long unlocked = 0;
<span class="p_add">+	int cnt = 0;</span>
 
 	if (!dma-&gt;size)
 		return 0;
<span class="p_chunk">@@ -661,6 +705,8 @@</span> <span class="p_context"> static long vfio_unmap_unpin(struct vfio_iommu *iommu, struct vfio_dma *dma,</span>
 	if (!IS_IOMMU_CAP_DOMAIN_IN_CONTAINER(iommu))
 		return 0;
 
<span class="p_add">+	INIT_LIST_HEAD(&amp;unmapped_regions);</span>
<span class="p_add">+</span>
 	/*
 	 * We use the IOMMU to track the physical addresses, otherwise we&#39;d
 	 * need a much more complicated tracking system.  Unfortunately that
<span class="p_chunk">@@ -698,24 +744,36 @@</span> <span class="p_context"> static long vfio_unmap_unpin(struct vfio_iommu *iommu, struct vfio_dma *dma,</span>
 				break;
 		}
 
<span class="p_del">-		unmapped = iommu_unmap(domain-&gt;domain, iova, len);</span>
<span class="p_del">-		if (WARN_ON(!unmapped))</span>
<span class="p_add">+		entry = kzalloc(sizeof(*entry), GFP_KERNEL);</span>
<span class="p_add">+		if (!entry)</span>
 			break;
 
<span class="p_del">-		unlocked += vfio_unpin_pages_remote(dma, iova,</span>
<span class="p_del">-						    phys &gt;&gt; PAGE_SHIFT,</span>
<span class="p_del">-						    unmapped &gt;&gt; PAGE_SHIFT,</span>
<span class="p_del">-						    false);</span>
<span class="p_add">+		unmapped = iommu_unmap_fast(domain-&gt;domain, iova, len);</span>
<span class="p_add">+		if (WARN_ON(!unmapped)) {</span>
<span class="p_add">+			kfree(entry);</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		iommu_tlb_range_add(domain-&gt;domain, iova, unmapped);</span>
<span class="p_add">+		entry-&gt;iova = iova;</span>
<span class="p_add">+		entry-&gt;phys = phys;</span>
<span class="p_add">+		entry-&gt;len  = unmapped;</span>
<span class="p_add">+		list_add_tail(&amp;entry-&gt;list, &amp;unmapped_regions);</span>
<span class="p_add">+		cnt++;</span>
 		iova += unmapped;
 
<span class="p_add">+		if (cnt &gt;= VFIO_IOMMU_TLB_SYNC_MAX) {</span>
<span class="p_add">+			unlocked += vfio_sync_and_unpin(dma, domain, &amp;unmapped_regions,</span>
<span class="p_add">+							do_accounting);</span>
<span class="p_add">+			cnt = 0;</span>
<span class="p_add">+		}</span>
 		cond_resched();
 	}
 
<span class="p_add">+	if (cnt)</span>
<span class="p_add">+		unlocked += vfio_sync_and_unpin(dma, domain, &amp;unmapped_regions,</span>
<span class="p_add">+						do_accounting);</span>
 	dma-&gt;iommu_mapped = false;
<span class="p_del">-	if (do_accounting) {</span>
<span class="p_del">-		vfio_lock_acct(dma-&gt;task, -unlocked, NULL);</span>
<span class="p_del">-		return 0;</span>
<span class="p_del">-	}</span>
 	return unlocked;
 }
 
<span class="p_chunk">@@ -878,6 +936,7 @@</span> <span class="p_context"> static int map_try_harder(struct vfio_domain *domain, dma_addr_t iova,</span>
 {
 	long i;
 	int ret = 0;
<span class="p_add">+	size_t unmapped = 0;</span>
 
 	for (i = 0; i &lt; npage; i++, pfn++, iova += PAGE_SIZE) {
 		ret = iommu_map(domain-&gt;domain, iova,
<span class="p_chunk">@@ -887,8 +946,14 @@</span> <span class="p_context"> static int map_try_harder(struct vfio_domain *domain, dma_addr_t iova,</span>
 			break;
 	}
 
<span class="p_del">-	for (; i &lt; npage &amp;&amp; i &gt; 0; i--, iova -= PAGE_SIZE)</span>
<span class="p_del">-		iommu_unmap(domain-&gt;domain, iova, PAGE_SIZE);</span>
<span class="p_add">+	for (; i &lt; npage &amp;&amp; i &gt; 0; i--, iova -= PAGE_SIZE) {</span>
<span class="p_add">+		unmapped = iommu_unmap_fast(domain-&gt;domain, iova, PAGE_SIZE);</span>
<span class="p_add">+		if (WARN_ON(!unmapped))</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		iommu_tlb_range_add(domain-&gt;domain, iova, unmapped);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	if (unmapped)</span>
<span class="p_add">+		iommu_tlb_sync(domain-&gt;domain);</span>
 
 	return ret;
 }

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



