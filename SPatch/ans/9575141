
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>userfaultfd: hugetlbfs: add UFFDIO_COPY support for shared mappings - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    userfaultfd: hugetlbfs: add UFFDIO_COPY support for shared mappings</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=124511">Mike Kravetz</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Feb. 15, 2017, 9:46 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1487195210-12839-1-git-send-email-mike.kravetz@oracle.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9575141/mbox/"
   >mbox</a>
|
   <a href="/patch/9575141/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9575141/">/patch/9575141/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	25B7D6045F for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 15 Feb 2017 21:47:59 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 189EA28544
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 15 Feb 2017 21:47:59 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 0D2282855D; Wed, 15 Feb 2017 21:47:59 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	UNPARSEABLE_RELAY autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 7AE1928544
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 15 Feb 2017 21:47:56 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752705AbdBOVry (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Wed, 15 Feb 2017 16:47:54 -0500
Received: from aserp1040.oracle.com ([141.146.126.69]:26087 &quot;EHLO
	aserp1040.oracle.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1752598AbdBOVrx (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Wed, 15 Feb 2017 16:47:53 -0500
Received: from userv0022.oracle.com (userv0022.oracle.com [156.151.31.74])
	by aserp1040.oracle.com (Sentrion-MTA-4.3.2/Sentrion-MTA-4.3.2) with
	ESMTP id v1FLl3AO007303
	(version=TLSv1.2 cipher=ECDHE-RSA-AES256-GCM-SHA384 bits=256
	verify=OK); Wed, 15 Feb 2017 21:47:04 GMT
Received: from aserv0122.oracle.com (aserv0122.oracle.com [141.146.126.236])
	by userv0022.oracle.com (8.14.4/8.14.4) with ESMTP id
	v1FLl1hG026096
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-GCM-SHA384 bits=256
	verify=OK); Wed, 15 Feb 2017 21:47:01 GMT
Received: from abhmp0005.oracle.com (abhmp0005.oracle.com [141.146.116.11])
	by aserv0122.oracle.com (8.14.4/8.14.4) with ESMTP id
	v1FLl0bv018979; Wed, 15 Feb 2017 21:47:00 GMT
Received: from monkey.oracle.com (/50.188.161.229)
	by default (Oracle Beehive Gateway v4.0)
	with ESMTP ; Wed, 15 Feb 2017 13:46:59 -0800
From: Mike Kravetz &lt;mike.kravetz@oracle.com&gt;
To: linux-mm@kvack.org, linux-kernel@vger.kernel.org
Cc: Mike Kravetz &lt;mike.kravetz@oracle.com&gt;,
	Andrea Arcangeli &lt;aarcange@redhat.com&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	Mike Rapoport &lt;rppt@linux.vnet.ibm.com&gt;,
	&quot;Dr. David Alan Gilbert&quot; &lt;dgilbert@redhat.com&gt;,
	Hillf Danton &lt;hillf.zj@alibaba-inc.com&gt;,
	Pavel Emelyanov &lt;xemul@parallels.com&gt;
Subject: [PATCH] userfaultfd: hugetlbfs: add UFFDIO_COPY support for shared
	mappings
Date: Wed, 15 Feb 2017 13:46:50 -0800
Message-Id: &lt;1487195210-12839-1-git-send-email-mike.kravetz@oracle.com&gt;
X-Mailer: git-send-email 2.7.4
X-Source-IP: userv0022.oracle.com [156.151.31.74]
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=124511">Mike Kravetz</a> - Feb. 15, 2017, 9:46 p.m.</div>
<pre class="content">
When userfaultfd hugetlbfs support was originally added, it followed
the pattern of anon mappings and did not support any vmas marked
VM_SHARED.  As such, support was only added for private mappings.

Remove this limitation and support shared mappings.  The primary
functional change required is adding pages to the page cache.  More
subtle changes are required for huge page reservation handling in
error paths.  A lengthy comment in the code describes the reservation
handling.
<span class="signed-off-by">
Signed-off-by: Mike Kravetz &lt;mike.kravetz@oracle.com&gt;</span>
Cc: Andrea Arcangeli &lt;aarcange@redhat.com&gt;
Cc: Andrew Morton &lt;akpm@linux-foundation.org&gt;
Cc: Mike Rapoport &lt;rppt@linux.vnet.ibm.com&gt;
Cc: &quot;Dr. David Alan Gilbert&quot; &lt;dgilbert@redhat.com&gt;
Cc: Hillf Danton &lt;hillf.zj@alibaba-inc.com&gt;
Cc: Mike Kravetz &lt;mike.kravetz@oracle.com&gt;
Cc: Pavel Emelyanov &lt;xemul@parallels.com&gt;
---
 mm/hugetlb.c     | 25 +++++++++++++++++++++--
 mm/userfaultfd.c | 60 +++++++++++++++++++++++++++++++++++++++++---------------
 2 files changed, 67 insertions(+), 18 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=965">Andrea Arcangeli</a> - Feb. 16, 2017, 6:41 p.m.</div>
<pre class="content">
On Wed, Feb 15, 2017 at 01:46:50PM -0800, Mike Kravetz wrote:
<span class="quote">&gt; diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="quote">&gt; index d0d1d08..41f6c51 100644</span>
<span class="quote">&gt; --- a/mm/hugetlb.c</span>
<span class="quote">&gt; +++ b/mm/hugetlb.c</span>
<span class="quote">&gt; @@ -4029,6 +4029,18 @@ int hugetlb_mcopy_atomic_pte(struct mm_struct *dst_mm,</span>
<span class="quote">&gt;  	__SetPageUptodate(page);</span>
<span class="quote">&gt;  	set_page_huge_active(page);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * If shared, add to page cache</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	if (dst_vma-&gt;vm_flags &amp; VM_SHARED) {</span>

Minor nitpick, this could be a:

      int vm_shared = dst_vma-&gt;vm_flags &amp; VM_SHARED;

(int faster than bool here as VM_SHARED won&#39;t have to be converted into 0|1)
<span class="quote">
&gt; @@ -386,7 +413,8 @@ static __always_inline ssize_t __mcopy_atomic(struct mm_struct *dst_mm,</span>
<span class="quote">&gt;  		goto out_unlock;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	err = -EINVAL;</span>
<span class="quote">&gt; -	if (!vma_is_shmem(dst_vma) &amp;&amp; dst_vma-&gt;vm_flags &amp; VM_SHARED)</span>
<span class="quote">&gt; +	if (!vma_is_shmem(dst_vma) &amp;&amp; !is_vm_hugetlb_page(dst_vma) &amp;&amp;</span>
<span class="quote">&gt; +	    dst_vma-&gt;vm_flags &amp; VM_SHARED)</span>
<span class="quote">&gt;  		goto out_unlock;</span>

Other minor nitpick, this could have been faster as:

     if (vma_is_anonymous(dst_vma) &amp;&amp; dst_vma-&gt;vm_flags &amp; VM_SHARED)

Thinking twice, the only case we need to rule out is shmem_zero_setup
(it&#39;s not anon vmas can be really VM_SHARED or they wouldn&#39;t be anon
vmas in the first place) so even the above is superfluous because
shmem_zero_setup does:

	vma-&gt;vm_ops = &amp;shmem_vm_ops;

So I would turn it into:


     /*
      * shmem_zero_setup is invoked in mmap for MAP_ANONYMOUS|MAP_SHARED but
      * it will overwrite vm_ops, so vma_is_anonymous must return false.
      */
     if (WARN_ON_ONCE(vma_is_anonymous(dst_vma) &amp;&amp; dst_vma-&gt;vm_flags &amp; VM_SHARED))
 		goto out_unlock;
<span class="reviewed-by">

Reviewed-by: Andrea Arcangeli &lt;aarcange@redhat.com&gt;</span>
---


Changing topic: thinking about the last part, I was wondering about
vma_is_anonymous because it&#39;s well known issue, it&#39;s only guaranteed
fully correct during page faults because the weird cases that can
return a false positive cannot generate page faults and they run
remap_pfn_range before releasing the mmap_sem for writing within
mmap(2) itself.

More than a year ago I discussed with Kirill (CC&#39;ed) the
vma_is_anonymous() false positives: some VM_IO vma leaves vma-&gt;vm_ops
NULL, which is generally non concerning because putting an anon page
in there shouldn&#39;t have any adverse side effects for the rest of the
system.

It was critical for khugepaged, because khugepaged will run out of the
app own control, so khugepaged must be fully accurate of it&#39;ll just
activate on VM_IO special mappings, but that&#39;s definitely not the case
for userfaultfd.

Still I was wondering if we could be more strict by adding a
vma-&gt;vm_flags &amp; VM_NO_KHUGEPAGED check so that VM_IO regions will not
pass registration (only in fs/userfaultfd.c:vma_can_userfault;
mm/userfaultfd.c doesn&#39;t need any of that as it requires registration
to be passed first and vm_userfaultfd_ctx already armed).

A fully accurate vma_is_anonymous could be implemented like this:

   vma_is_anonymous &amp;&amp; !(vm_flags &amp; VM_NO_KHUGEPAGED)

Which is what khugepaged internally uses.

This will also work for MAP_PRIVATE on /dev/zero (required to work by
the non cooperative case).

Side note: MAP_PRIVATE /dev/zero is very special and it&#39;s the only
case in the kernel a &quot;true&quot; anon vma has vm_file set. I think it&#39;d be
cleaner to &quot;decouple&quot; the vma from /dev/zero the same way
MAP_SHARED|MAP_ANON &quot;couples&quot; the vma to a pseudo /dev/zero, so anon
vmas would always have vm_file NULL (not done because it&#39;d risk to
break stuff by changing the /proc/pid/maps output), but that&#39;s besides
the point and the above solution deployed already by khugepaged
already works with the current /dev/zero MAP_PRIVATE code.

Kirill what&#39;s your take on making the registration checks stricter?
If we would add a vma_is_anonymous_not_in_fault implemented like above
vma_can_userfault would just need a
s/vma_is_anonymous/vma_is_anonymous_not_in_fault/ and it would be more
strict. khugepaged could be then converted to use it too instead of
hardcoding this vm_flags check. Unless I&#39;m mistaken I would consider
such a change to the registration code, purely a cleanup to add a more
strict check.

Alternatively we could just leave things as is and depend on apps
using VM_IO with remap_pfn_range with vm_file set and vm_ops NULL, not
to screw themselves up by filling their regions with anon pages. I
don&#39;t see how it could break anything (except themselves) if they do,
but I&#39;d appreciate a second thought on that. And at least for the
remap_pfn_range users it won&#39;t even get that far, because a graceful
-EEXIST would be returned instead. The change would effectively
convert a -EEXIST returned later into a stricter -EINVAL returned
early at registration time, for a case that is erratic by design.

Thanks,
Andrea
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=874">Kirill A. Shutemov</a> - Feb. 21, 2017, 1:25 p.m.</div>
<pre class="content">
On Thu, Feb 16, 2017 at 07:41:00PM +0100, Andrea Arcangeli wrote:
<span class="quote">&gt; Kirill what&#39;s your take on making the registration checks stricter?</span>
<span class="quote">&gt; If we would add a vma_is_anonymous_not_in_fault implemented like above</span>
<span class="quote">&gt; vma_can_userfault would just need a</span>
<span class="quote">&gt; s/vma_is_anonymous/vma_is_anonymous_not_in_fault/ and it would be more</span>
<span class="quote">&gt; strict. khugepaged could be then converted to use it too instead of</span>
<span class="quote">&gt; hardcoding this vm_flags check. Unless I&#39;m mistaken I would consider</span>
<span class="quote">&gt; such a change to the registration code, purely a cleanup to add a more</span>
<span class="quote">&gt; strict check.</span>

[sorry for later response]

I think more strict vma_is_anonymous() is a good thing.

But I don&#39;t see a point introducing one more helper. Let&#39;s just make the
existing helper work better.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=965">Andrea Arcangeli</a> - Feb. 22, 2017, 3:15 p.m.</div>
<pre class="content">
On Tue, Feb 21, 2017 at 04:25:45PM +0300, Kirill A. Shutemov wrote:
<span class="quote">&gt; I think more strict vma_is_anonymous() is a good thing.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; But I don&#39;t see a point introducing one more helper. Let&#39;s just make the</span>
<span class="quote">&gt; existing helper work better.</span>

That would be simpler agreed. The point of having an &quot;unsafe&quot; faster
version was only for code running in page fault context where the
additional check is unnecessary.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=874">Kirill A. Shutemov</a> - Feb. 23, 2017, 2:56 p.m.</div>
<pre class="content">
On Wed, Feb 22, 2017 at 04:15:07PM +0100, Andrea Arcangeli wrote:
<span class="quote">&gt; On Tue, Feb 21, 2017 at 04:25:45PM +0300, Kirill A. Shutemov wrote:</span>
<span class="quote">&gt; &gt; I think more strict vma_is_anonymous() is a good thing.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; But I don&#39;t see a point introducing one more helper. Let&#39;s just make the</span>
<span class="quote">&gt; &gt; existing helper work better.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; That would be simpler agreed. The point of having an &quot;unsafe&quot; faster</span>
<span class="quote">&gt; version was only for code running in page fault context where the</span>
<span class="quote">&gt; additional check is unnecessary.</span>

Well, I don&#39;t think that the cost of additional check is significant here.
And we can bring -&gt;vm_ops a bit closer to -&gt;vm_flags to avoid potential
cache miss.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="p_header">index d0d1d08..41f6c51 100644</span>
<span class="p_header">--- a/mm/hugetlb.c</span>
<span class="p_header">+++ b/mm/hugetlb.c</span>
<span class="p_chunk">@@ -4029,6 +4029,18 @@</span> <span class="p_context"> int hugetlb_mcopy_atomic_pte(struct mm_struct *dst_mm,</span>
 	__SetPageUptodate(page);
 	set_page_huge_active(page);
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If shared, add to page cache</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (dst_vma-&gt;vm_flags &amp; VM_SHARED) {</span>
<span class="p_add">+		struct address_space *mapping = dst_vma-&gt;vm_file-&gt;f_mapping;</span>
<span class="p_add">+		pgoff_t idx = vma_hugecache_offset(h, dst_vma, dst_addr);</span>
<span class="p_add">+</span>
<span class="p_add">+		ret = huge_add_to_page_cache(page, mapping, idx);</span>
<span class="p_add">+		if (ret)</span>
<span class="p_add">+			goto out_release_nounlock;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	ptl = huge_pte_lockptr(h, dst_mm, dst_pte);
 	spin_lock(ptl);
 
<span class="p_chunk">@@ -4036,8 +4048,12 @@</span> <span class="p_context"> int hugetlb_mcopy_atomic_pte(struct mm_struct *dst_mm,</span>
 	if (!huge_pte_none(huge_ptep_get(dst_pte)))
 		goto out_release_unlock;
 
<span class="p_del">-	ClearPagePrivate(page);</span>
<span class="p_del">-	hugepage_add_new_anon_rmap(page, dst_vma, dst_addr);</span>
<span class="p_add">+	if (dst_vma-&gt;vm_flags &amp; VM_SHARED) {</span>
<span class="p_add">+		page_dup_rmap(page, true);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		ClearPagePrivate(page);</span>
<span class="p_add">+		hugepage_add_new_anon_rmap(page, dst_vma, dst_addr);</span>
<span class="p_add">+	}</span>
 
 	_dst_pte = make_huge_pte(dst_vma, page, dst_vma-&gt;vm_flags &amp; VM_WRITE);
 	if (dst_vma-&gt;vm_flags &amp; VM_WRITE)
<span class="p_chunk">@@ -4054,11 +4070,16 @@</span> <span class="p_context"> int hugetlb_mcopy_atomic_pte(struct mm_struct *dst_mm,</span>
 	update_mmu_cache(dst_vma, dst_addr, dst_pte);
 
 	spin_unlock(ptl);
<span class="p_add">+	if (dst_vma-&gt;vm_flags &amp; VM_SHARED)</span>
<span class="p_add">+		unlock_page(page);</span>
 	ret = 0;
 out:
 	return ret;
 out_release_unlock:
 	spin_unlock(ptl);
<span class="p_add">+out_release_nounlock:</span>
<span class="p_add">+	if (dst_vma-&gt;vm_flags &amp; VM_SHARED)</span>
<span class="p_add">+		unlock_page(page);</span>
 	put_page(page);
 	goto out;
 }
<span class="p_header">diff --git a/mm/userfaultfd.c b/mm/userfaultfd.c</span>
<span class="p_header">index b861cf9..6703308 100644</span>
<span class="p_header">--- a/mm/userfaultfd.c</span>
<span class="p_header">+++ b/mm/userfaultfd.c</span>
<span class="p_chunk">@@ -211,11 +211,9 @@</span> <span class="p_context"> static __always_inline ssize_t __mcopy_atomic_hugetlb(struct mm_struct *dst_mm,</span>
 			goto out_unlock;
 
 		/*
<span class="p_del">-		 * Make sure the vma is not shared, that the remaining dst</span>
<span class="p_del">-		 * range is both valid and fully within a single existing vma.</span>
<span class="p_add">+		 * Make sure the remaining dst range is both valid and</span>
<span class="p_add">+		 * fully within a single existing vma.</span>
 		 */
<span class="p_del">-		if (dst_vma-&gt;vm_flags &amp; VM_SHARED)</span>
<span class="p_del">-			goto out_unlock;</span>
 		if (dst_start &lt; dst_vma-&gt;vm_start ||
 		    dst_start + len &gt; dst_vma-&gt;vm_end)
 			goto out_unlock;
<span class="p_chunk">@@ -226,11 +224,13 @@</span> <span class="p_context"> static __always_inline ssize_t __mcopy_atomic_hugetlb(struct mm_struct *dst_mm,</span>
 		goto out_unlock;
 
 	/*
<span class="p_del">-	 * Ensure the dst_vma has a anon_vma.</span>
<span class="p_add">+	 * If not shared, ensure the dst_vma has a anon_vma.</span>
 	 */
 	err = -ENOMEM;
<span class="p_del">-	if (unlikely(anon_vma_prepare(dst_vma)))</span>
<span class="p_del">-		goto out_unlock;</span>
<span class="p_add">+	if (!(dst_vma-&gt;vm_flags &amp; VM_SHARED)) {</span>
<span class="p_add">+		if (unlikely(anon_vma_prepare(dst_vma)))</span>
<span class="p_add">+			goto out_unlock;</span>
<span class="p_add">+	}</span>
 
 	h = hstate_vma(dst_vma);
 
<span class="p_chunk">@@ -306,18 +306,45 @@</span> <span class="p_context"> static __always_inline ssize_t __mcopy_atomic_hugetlb(struct mm_struct *dst_mm,</span>
 	if (page) {
 		/*
 		 * We encountered an error and are about to free a newly
<span class="p_del">-		 * allocated huge page.  It is possible that there was a</span>
<span class="p_del">-		 * reservation associated with the page that has been</span>
<span class="p_del">-		 * consumed.  See the routine restore_reserve_on_error</span>
<span class="p_del">-		 * for details.  Unfortunately, we can not call</span>
<span class="p_del">-		 * restore_reserve_on_error now as it would require holding</span>
<span class="p_del">-		 * mmap_sem.  Clear the PagePrivate flag so that the global</span>
<span class="p_add">+		 * allocated huge page.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * Reservation handling is very subtle, and is different for</span>
<span class="p_add">+		 * private and shared mappings.  See the routine</span>
<span class="p_add">+		 * restore_reserve_on_error for details.  Unfortunately, we</span>
<span class="p_add">+		 * can not call restore_reserve_on_error now as it would</span>
<span class="p_add">+		 * require holding mmap_sem.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * If a reservation for the page existed in the reservation</span>
<span class="p_add">+		 * map of a private mapping, the map was modified to indicate</span>
<span class="p_add">+		 * the reservation was consumed when the page was allocated.</span>
<span class="p_add">+		 * We clear the PagePrivate flag now so that the global</span>
 		 * reserve count will not be incremented in free_huge_page.
 		 * The reservation map will still indicate the reservation
 		 * was consumed and possibly prevent later page allocation.
<span class="p_del">-		 * This is better than leaking a global reservation.</span>
<span class="p_add">+		 * This is better than leaking a global reservation.  If no</span>
<span class="p_add">+		 * reservation existed, it is still safe to clear PagePrivate</span>
<span class="p_add">+		 * as no adjustments to reservation counts were made during</span>
<span class="p_add">+		 * allocation.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * The reservation map for shared mappings indicates which</span>
<span class="p_add">+		 * pages have reservations.  When a huge page is allocated</span>
<span class="p_add">+		 * for an address with a reservation, no change is made to</span>
<span class="p_add">+		 * the reserve map.  In this case PagePrivate will be set</span>
<span class="p_add">+		 * to indicate that the global reservation count should be</span>
<span class="p_add">+		 * incremented when the page is freed.  This is the desired</span>
<span class="p_add">+		 * behavior.  However, when a huge page is allocated for an</span>
<span class="p_add">+		 * address without a reservation a reservation entry is added</span>
<span class="p_add">+		 * to the reservation map, and PagePrivate will not be set.</span>
<span class="p_add">+		 * When the page is freed, the global reserve count will NOT</span>
<span class="p_add">+		 * be incremented and it will appear as though we have leaked</span>
<span class="p_add">+		 * reserved page.  In this case, set PagePrivate so that the</span>
<span class="p_add">+		 * global reserve count will be incremented to match the</span>
<span class="p_add">+		 * reservation map entry which was created.</span>
 		 */
<span class="p_del">-		ClearPagePrivate(page);</span>
<span class="p_add">+		if (dst_vma-&gt;vm_flags &amp; VM_SHARED)</span>
<span class="p_add">+			SetPagePrivate(page);</span>
<span class="p_add">+		else</span>
<span class="p_add">+			ClearPagePrivate(page);</span>
 		put_page(page);
 	}
 	BUG_ON(copied &lt; 0);
<span class="p_chunk">@@ -386,7 +413,8 @@</span> <span class="p_context"> static __always_inline ssize_t __mcopy_atomic(struct mm_struct *dst_mm,</span>
 		goto out_unlock;
 
 	err = -EINVAL;
<span class="p_del">-	if (!vma_is_shmem(dst_vma) &amp;&amp; dst_vma-&gt;vm_flags &amp; VM_SHARED)</span>
<span class="p_add">+	if (!vma_is_shmem(dst_vma) &amp;&amp; !is_vm_hugetlb_page(dst_vma) &amp;&amp;</span>
<span class="p_add">+	    dst_vma-&gt;vm_flags &amp; VM_SHARED)</span>
 		goto out_unlock;
 	if (dst_start &lt; dst_vma-&gt;vm_start ||
 	    dst_start + len &gt; dst_vma-&gt;vm_end)

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



