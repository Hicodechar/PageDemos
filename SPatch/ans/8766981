
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[PATCHv6,03/30] mm: introduce fault_env - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [PATCHv6,03/30] mm: introduce fault_env</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=40781">Kirill A. Shutemov</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>April 6, 2016, 10:50 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1459983080-106718-4-git-send-email-kirill.shutemov@linux.intel.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/8766981/mbox/"
   >mbox</a>
|
   <a href="/patch/8766981/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/8766981/">/patch/8766981/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork1.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork1.web.kernel.org (Postfix) with ESMTP id 8A2709F36E
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed,  6 Apr 2016 22:59:56 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 724E02017D
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed,  6 Apr 2016 22:59:52 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id 262EB20160
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed,  6 Apr 2016 22:59:48 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1754922AbcDFW7m (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Wed, 6 Apr 2016 18:59:42 -0400
Received: from mga11.intel.com ([192.55.52.93]:42977 &quot;EHLO mga11.intel.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1754321AbcDFWva (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Wed, 6 Apr 2016 18:51:30 -0400
Received: from orsmga002.jf.intel.com ([10.7.209.21])
	by fmsmga102.fm.intel.com with ESMTP; 06 Apr 2016 15:51:28 -0700
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i=&quot;5.24,448,1455004800&quot;; d=&quot;scan&#39;208&quot;;a=&quot;949634578&quot;
Received: from black.fi.intel.com ([10.237.72.93])
	by orsmga002.jf.intel.com with ESMTP; 06 Apr 2016 15:51:22 -0700
Received: by black.fi.intel.com (Postfix, from userid 1000)
	id C268E269; Thu,  7 Apr 2016 01:51:21 +0300 (EEST)
From: &quot;Kirill A. Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt;
To: Hugh Dickins &lt;hughd@google.com&gt;, Andrea Arcangeli &lt;aarcange@redhat.com&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;
Cc: Dave Hansen &lt;dave.hansen@intel.com&gt;, Vlastimil Babka &lt;vbabka@suse.cz&gt;,
	Christoph Lameter &lt;cl@gentwo.org&gt;,
	Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;,
	Jerome Marchand &lt;jmarchan@redhat.com&gt;, Yang Shi &lt;yang.shi@linaro.org&gt;,
	Sasha Levin &lt;sasha.levin@oracle.com&gt;,
	Andres Lagar-Cavilla &lt;andreslc@google.com&gt;,
	Ning Qu &lt;quning@gmail.com&gt;, linux-kernel@vger.kernel.org,
	linux-mm@kvack.org, linux-fsdevel@vger.kernel.org,
	&quot;Kirill A. Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt;
Subject: [PATCHv6 03/30] mm: introduce fault_env
Date: Thu,  7 Apr 2016 01:50:53 +0300
Message-Id: &lt;1459983080-106718-4-git-send-email-kirill.shutemov@linux.intel.com&gt;
X-Mailer: git-send-email 2.8.0.rc3
In-Reply-To: &lt;1459983080-106718-1-git-send-email-kirill.shutemov@linux.intel.com&gt;
References: &lt;1459983080-106718-1-git-send-email-kirill.shutemov@linux.intel.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-7.9 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=40781">Kirill A. Shutemov</a> - April 6, 2016, 10:50 p.m.</div>
<pre class="content">
The idea borrowed from Peter&#39;s patch from patchset on speculative page
faults[1]:

Instead of passing around the endless list of function arguments,
replace the lot with a single structure so we can change context
without endless function signature changes.

The changes are mostly mechanical with exception of faultaround code:
filemap_map_pages() got reworked a bit.

This patch is preparation for the next one.

[1] http://lkml.kernel.org/r/20141020222841.302891540@infradead.org
<span class="signed-off-by">
Signed-off-by: Kirill A. Shutemov &lt;kirill.shutemov@linux.intel.com&gt;</span>
<span class="acked-by">Acked-by: Peter Zijlstra (Intel) &lt;peterz@infradead.org&gt;</span>
---
 Documentation/filesystems/Locking |  10 +-
 fs/userfaultfd.c                  |  22 +-
 include/linux/huge_mm.h           |  20 +-
 include/linux/mm.h                |  34 ++-
 include/linux/userfaultfd_k.h     |   8 +-
 mm/filemap.c                      |  28 +-
 mm/huge_memory.c                  | 280 +++++++++---------
 mm/internal.h                     |   4 +-
 mm/memory.c                       | 579 ++++++++++++++++++--------------------
 mm/nommu.c                        |   3 +-
 10 files changed, 473 insertions(+), 515 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/Documentation/filesystems/Locking b/Documentation/filesystems/Locking</span>
<span class="p_header">index 619af9bfdcb3..7f77837a7cf1 100644</span>
<span class="p_header">--- a/Documentation/filesystems/Locking</span>
<span class="p_header">+++ b/Documentation/filesystems/Locking</span>
<span class="p_chunk">@@ -544,13 +544,13 @@</span> <span class="p_context"> subsequent truncate), and then return with VM_FAULT_LOCKED, and the page</span>
 locked. The VM will unlock the page.
 
 	-&gt;map_pages() is called when VM asks to map easy accessible pages.
<span class="p_del">-Filesystem should find and map pages associated with offsets from &quot;pgoff&quot;</span>
<span class="p_del">-till &quot;max_pgoff&quot;. -&gt;map_pages() is called with page table locked and must</span>
<span class="p_add">+Filesystem should find and map pages associated with offsets from &quot;start_pgoff&quot;</span>
<span class="p_add">+till &quot;end_pgoff&quot;. -&gt;map_pages() is called with page table locked and must</span>
 not block.  If it&#39;s not possible to reach a page without blocking,
 filesystem should skip it. Filesystem should use do_set_pte() to setup
<span class="p_del">-page table entry. Pointer to entry associated with offset &quot;pgoff&quot; is</span>
<span class="p_del">-passed in &quot;pte&quot; field in vm_fault structure. Pointers to entries for other</span>
<span class="p_del">-offsets should be calculated relative to &quot;pte&quot;.</span>
<span class="p_add">+page table entry. Pointer to entry associated with the page is passed in</span>
<span class="p_add">+&quot;pte&quot; field in fault_env structure. Pointers to entries for other offsets</span>
<span class="p_add">+should be calculated relative to &quot;pte&quot;.</span>
 
 	-&gt;page_mkwrite() is called when a previously read-only pte is
 about to become writeable. The filesystem again must ensure that there are
<span class="p_header">diff --git a/fs/userfaultfd.c b/fs/userfaultfd.c</span>
<span class="p_header">index 66cdb44616d5..588e9da27ab4 100644</span>
<span class="p_header">--- a/fs/userfaultfd.c</span>
<span class="p_header">+++ b/fs/userfaultfd.c</span>
<span class="p_chunk">@@ -257,10 +257,9 @@</span> <span class="p_context"> out:</span>
  * fatal_signal_pending()s, and the mmap_sem must be released before
  * returning it.
  */
<span class="p_del">-int handle_userfault(struct vm_area_struct *vma, unsigned long address,</span>
<span class="p_del">-		     unsigned int flags, unsigned long reason)</span>
<span class="p_add">+int handle_userfault(struct fault_env *fe, unsigned long reason)</span>
 {
<span class="p_del">-	struct mm_struct *mm = vma-&gt;vm_mm;</span>
<span class="p_add">+	struct mm_struct *mm = fe-&gt;vma-&gt;vm_mm;</span>
 	struct userfaultfd_ctx *ctx;
 	struct userfaultfd_wait_queue uwq;
 	int ret;
<span class="p_chunk">@@ -269,7 +268,7 @@</span> <span class="p_context"> int handle_userfault(struct vm_area_struct *vma, unsigned long address,</span>
 	BUG_ON(!rwsem_is_locked(&amp;mm-&gt;mmap_sem));
 
 	ret = VM_FAULT_SIGBUS;
<span class="p_del">-	ctx = vma-&gt;vm_userfaultfd_ctx.ctx;</span>
<span class="p_add">+	ctx = fe-&gt;vma-&gt;vm_userfaultfd_ctx.ctx;</span>
 	if (!ctx)
 		goto out;
 
<span class="p_chunk">@@ -302,17 +301,17 @@</span> <span class="p_context"> int handle_userfault(struct vm_area_struct *vma, unsigned long address,</span>
 	 * without first stopping userland access to the memory. For
 	 * VM_UFFD_MISSING userfaults this is enough for now.
 	 */
<span class="p_del">-	if (unlikely(!(flags &amp; FAULT_FLAG_ALLOW_RETRY))) {</span>
<span class="p_add">+	if (unlikely(!(fe-&gt;flags &amp; FAULT_FLAG_ALLOW_RETRY))) {</span>
 		/*
 		 * Validate the invariant that nowait must allow retry
 		 * to be sure not to return SIGBUS erroneously on
 		 * nowait invocations.
 		 */
<span class="p_del">-		BUG_ON(flags &amp; FAULT_FLAG_RETRY_NOWAIT);</span>
<span class="p_add">+		BUG_ON(fe-&gt;flags &amp; FAULT_FLAG_RETRY_NOWAIT);</span>
 #ifdef CONFIG_DEBUG_VM
 		if (printk_ratelimit()) {
 			printk(KERN_WARNING
<span class="p_del">-			       &quot;FAULT_FLAG_ALLOW_RETRY missing %x\n&quot;, flags);</span>
<span class="p_add">+			       &quot;FAULT_FLAG_ALLOW_RETRY missing %x\n&quot;, fe-&gt;flags);</span>
 			dump_stack();
 		}
 #endif
<span class="p_chunk">@@ -324,7 +323,7 @@</span> <span class="p_context"> int handle_userfault(struct vm_area_struct *vma, unsigned long address,</span>
 	 * and wait.
 	 */
 	ret = VM_FAULT_RETRY;
<span class="p_del">-	if (flags &amp; FAULT_FLAG_RETRY_NOWAIT)</span>
<span class="p_add">+	if (fe-&gt;flags &amp; FAULT_FLAG_RETRY_NOWAIT)</span>
 		goto out;
 
 	/* take the reference before dropping the mmap_sem */
<span class="p_chunk">@@ -332,10 +331,11 @@</span> <span class="p_context"> int handle_userfault(struct vm_area_struct *vma, unsigned long address,</span>
 
 	init_waitqueue_func_entry(&amp;uwq.wq, userfaultfd_wake_function);
 	uwq.wq.private = current;
<span class="p_del">-	uwq.msg = userfault_msg(address, flags, reason);</span>
<span class="p_add">+	uwq.msg = userfault_msg(fe-&gt;address, fe-&gt;flags, reason);</span>
 	uwq.ctx = ctx;
 
<span class="p_del">-	return_to_userland = (flags &amp; (FAULT_FLAG_USER|FAULT_FLAG_KILLABLE)) ==</span>
<span class="p_add">+	return_to_userland =</span>
<span class="p_add">+		(fe-&gt;flags &amp; (FAULT_FLAG_USER|FAULT_FLAG_KILLABLE)) ==</span>
 		(FAULT_FLAG_USER|FAULT_FLAG_KILLABLE);
 
 	spin_lock(&amp;ctx-&gt;fault_pending_wqh.lock);
<span class="p_chunk">@@ -353,7 +353,7 @@</span> <span class="p_context"> int handle_userfault(struct vm_area_struct *vma, unsigned long address,</span>
 			  TASK_KILLABLE);
 	spin_unlock(&amp;ctx-&gt;fault_pending_wqh.lock);
 
<span class="p_del">-	must_wait = userfaultfd_must_wait(ctx, address, flags, reason);</span>
<span class="p_add">+	must_wait = userfaultfd_must_wait(ctx, fe-&gt;address, fe-&gt;flags, reason);</span>
 	up_read(&amp;mm-&gt;mmap_sem);
 
 	if (likely(must_wait &amp;&amp; !ACCESS_ONCE(ctx-&gt;released) &amp;&amp;
<span class="p_header">diff --git a/include/linux/huge_mm.h b/include/linux/huge_mm.h</span>
<span class="p_header">index 7008623e24b1..cad4ca270fda 100644</span>
<span class="p_header">--- a/include/linux/huge_mm.h</span>
<span class="p_header">+++ b/include/linux/huge_mm.h</span>
<span class="p_chunk">@@ -1,20 +1,12 @@</span> <span class="p_context"></span>
 #ifndef _LINUX_HUGE_MM_H
 #define _LINUX_HUGE_MM_H
 
<span class="p_del">-extern int do_huge_pmd_anonymous_page(struct mm_struct *mm,</span>
<span class="p_del">-				      struct vm_area_struct *vma,</span>
<span class="p_del">-				      unsigned long address, pmd_t *pmd,</span>
<span class="p_del">-				      unsigned int flags);</span>
<span class="p_add">+extern int do_huge_pmd_anonymous_page(struct fault_env *fe);</span>
 extern int copy_huge_pmd(struct mm_struct *dst_mm, struct mm_struct *src_mm,
 			 pmd_t *dst_pmd, pmd_t *src_pmd, unsigned long addr,
 			 struct vm_area_struct *vma);
<span class="p_del">-extern void huge_pmd_set_accessed(struct mm_struct *mm,</span>
<span class="p_del">-				  struct vm_area_struct *vma,</span>
<span class="p_del">-				  unsigned long address, pmd_t *pmd,</span>
<span class="p_del">-				  pmd_t orig_pmd, int dirty);</span>
<span class="p_del">-extern int do_huge_pmd_wp_page(struct mm_struct *mm, struct vm_area_struct *vma,</span>
<span class="p_del">-			       unsigned long address, pmd_t *pmd,</span>
<span class="p_del">-			       pmd_t orig_pmd);</span>
<span class="p_add">+extern void huge_pmd_set_accessed(struct fault_env *fe, pmd_t orig_pmd);</span>
<span class="p_add">+extern int do_huge_pmd_wp_page(struct fault_env *fe, pmd_t orig_pmd);</span>
 extern struct page *follow_trans_huge_pmd(struct vm_area_struct *vma,
 					  unsigned long addr,
 					  pmd_t *pmd,
<span class="p_chunk">@@ -136,8 +128,7 @@</span> <span class="p_context"> static inline int hpage_nr_pages(struct page *page)</span>
 	return 1;
 }
 
<span class="p_del">-extern int do_huge_pmd_numa_page(struct mm_struct *mm, struct vm_area_struct *vma,</span>
<span class="p_del">-				unsigned long addr, pmd_t pmd, pmd_t *pmdp);</span>
<span class="p_add">+extern int do_huge_pmd_numa_page(struct fault_env *fe, pmd_t orig_pmd);</span>
 
 extern struct page *huge_zero_page;
 
<span class="p_chunk">@@ -197,8 +188,7 @@</span> <span class="p_context"> static inline spinlock_t *pmd_trans_huge_lock(pmd_t *pmd,</span>
 	return NULL;
 }
 
<span class="p_del">-static inline int do_huge_pmd_numa_page(struct mm_struct *mm, struct vm_area_struct *vma,</span>
<span class="p_del">-					unsigned long addr, pmd_t pmd, pmd_t *pmdp)</span>
<span class="p_add">+static inline int do_huge_pmd_numa_page(struct fault_env *fe, pmd_t orig_pmd)</span>
 {
 	return 0;
 }
<span class="p_header">diff --git a/include/linux/mm.h b/include/linux/mm.h</span>
<span class="p_header">index 8fc68ed3c862..696ec39b616b 100644</span>
<span class="p_header">--- a/include/linux/mm.h</span>
<span class="p_header">+++ b/include/linux/mm.h</span>
<span class="p_chunk">@@ -299,10 +299,27 @@</span> <span class="p_context"> struct vm_fault {</span>
 					 * is set (which is also implied by
 					 * VM_FAULT_ERROR).
 					 */
<span class="p_del">-	/* for -&gt;map_pages() only */</span>
<span class="p_del">-	pgoff_t max_pgoff;		/* map pages for offset from pgoff till</span>
<span class="p_del">-					 * max_pgoff inclusive */</span>
<span class="p_del">-	pte_t *pte;			/* pte entry associated with -&gt;pgoff */</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Page fault context: passes though page fault handler instead of endless list</span>
<span class="p_add">+ * of function arguments.</span>
<span class="p_add">+ */</span>
<span class="p_add">+struct fault_env {</span>
<span class="p_add">+	struct vm_area_struct *vma;	/* Target VMA */</span>
<span class="p_add">+	unsigned long address;		/* Faulting virtual address */</span>
<span class="p_add">+	unsigned int flags;		/* FAULT_FLAG_xxx flags */</span>
<span class="p_add">+	pmd_t *pmd;			/* Pointer to pmd entry matching</span>
<span class="p_add">+					 * the &#39;address&#39;</span>
<span class="p_add">+					 */</span>
<span class="p_add">+	pte_t *pte;			/* Pointer to pte entry matching</span>
<span class="p_add">+					 * the &#39;address&#39;. NULL if the page</span>
<span class="p_add">+					 * table hasn&#39;t been allocated.</span>
<span class="p_add">+					 */</span>
<span class="p_add">+	spinlock_t *ptl;		/* Page table lock.</span>
<span class="p_add">+					 * Protects pte page table if &#39;pte&#39;</span>
<span class="p_add">+					 * is not NULL, otherwise pmd.</span>
<span class="p_add">+					 */</span>
 };
 
 /*
<span class="p_chunk">@@ -317,7 +334,8 @@</span> <span class="p_context"> struct vm_operations_struct {</span>
 	int (*fault)(struct vm_area_struct *vma, struct vm_fault *vmf);
 	int (*pmd_fault)(struct vm_area_struct *, unsigned long address,
 						pmd_t *, unsigned int flags);
<span class="p_del">-	void (*map_pages)(struct vm_area_struct *vma, struct vm_fault *vmf);</span>
<span class="p_add">+	void (*map_pages)(struct fault_env *fe,</span>
<span class="p_add">+			pgoff_t start_pgoff, pgoff_t end_pgoff);</span>
 
 	/* notification that a previously read-only page is about to become
 	 * writable, if an error is returned it will cause a SIGBUS */
<span class="p_chunk">@@ -583,8 +601,7 @@</span> <span class="p_context"> static inline pte_t maybe_mkwrite(pte_t pte, struct vm_area_struct *vma)</span>
 	return pte;
 }
 
<span class="p_del">-void do_set_pte(struct vm_area_struct *vma, unsigned long address,</span>
<span class="p_del">-		struct page *page, pte_t *pte, bool write, bool anon);</span>
<span class="p_add">+void do_set_pte(struct fault_env *fe, struct page *page);</span>
 #endif
 
 /*
<span class="p_chunk">@@ -2119,7 +2136,8 @@</span> <span class="p_context"> extern void truncate_inode_pages_final(struct address_space *);</span>
 
 /* generic vm_area_ops exported for stackable file systems */
 extern int filemap_fault(struct vm_area_struct *, struct vm_fault *);
<span class="p_del">-extern void filemap_map_pages(struct vm_area_struct *vma, struct vm_fault *vmf);</span>
<span class="p_add">+extern void filemap_map_pages(struct fault_env *fe,</span>
<span class="p_add">+		pgoff_t start_pgoff, pgoff_t end_pgoff);</span>
 extern int filemap_page_mkwrite(struct vm_area_struct *vma, struct vm_fault *vmf);
 
 /* mm/page-writeback.c */
<span class="p_header">diff --git a/include/linux/userfaultfd_k.h b/include/linux/userfaultfd_k.h</span>
<span class="p_header">index 587480ad41b7..dd66a952e8cd 100644</span>
<span class="p_header">--- a/include/linux/userfaultfd_k.h</span>
<span class="p_header">+++ b/include/linux/userfaultfd_k.h</span>
<span class="p_chunk">@@ -27,8 +27,7 @@</span> <span class="p_context"></span>
 #define UFFD_SHARED_FCNTL_FLAGS (O_CLOEXEC | O_NONBLOCK)
 #define UFFD_FLAGS_SET (EFD_SHARED_FCNTL_FLAGS)
 
<span class="p_del">-extern int handle_userfault(struct vm_area_struct *vma, unsigned long address,</span>
<span class="p_del">-			    unsigned int flags, unsigned long reason);</span>
<span class="p_add">+extern int handle_userfault(struct fault_env *fe, unsigned long reason);</span>
 
 extern ssize_t mcopy_atomic(struct mm_struct *dst_mm, unsigned long dst_start,
 			    unsigned long src_start, unsigned long len);
<span class="p_chunk">@@ -56,10 +55,7 @@</span> <span class="p_context"> static inline bool userfaultfd_armed(struct vm_area_struct *vma)</span>
 #else /* CONFIG_USERFAULTFD */
 
 /* mm helpers */
<span class="p_del">-static inline int handle_userfault(struct vm_area_struct *vma,</span>
<span class="p_del">-				   unsigned long address,</span>
<span class="p_del">-				   unsigned int flags,</span>
<span class="p_del">-				   unsigned long reason)</span>
<span class="p_add">+static inline int handle_userfault(struct fault_env *fe, unsigned long reason)</span>
 {
 	return VM_FAULT_SIGBUS;
 }
<span class="p_header">diff --git a/mm/filemap.c b/mm/filemap.c</span>
<span class="p_header">index f2479af09da9..e32e5d70fc0c 100644</span>
<span class="p_header">--- a/mm/filemap.c</span>
<span class="p_header">+++ b/mm/filemap.c</span>
<span class="p_chunk">@@ -2131,22 +2131,27 @@</span> <span class="p_context"> page_not_uptodate:</span>
 }
 EXPORT_SYMBOL(filemap_fault);
 
<span class="p_del">-void filemap_map_pages(struct vm_area_struct *vma, struct vm_fault *vmf)</span>
<span class="p_add">+void filemap_map_pages(struct fault_env *fe,</span>
<span class="p_add">+		pgoff_t start_pgoff, pgoff_t end_pgoff)</span>
 {
 	struct radix_tree_iter iter;
 	void **slot;
<span class="p_del">-	struct file *file = vma-&gt;vm_file;</span>
<span class="p_add">+	struct file *file = fe-&gt;vma-&gt;vm_file;</span>
 	struct address_space *mapping = file-&gt;f_mapping;
<span class="p_add">+	pgoff_t last_pgoff = start_pgoff;</span>
 	loff_t size;
 	struct page *page;
<span class="p_del">-	unsigned long address = (unsigned long) vmf-&gt;virtual_address;</span>
<span class="p_del">-	unsigned long addr;</span>
<span class="p_del">-	pte_t *pte;</span>
 
 	rcu_read_lock();
<span class="p_del">-	radix_tree_for_each_slot(slot, &amp;mapping-&gt;page_tree, &amp;iter, vmf-&gt;pgoff) {</span>
<span class="p_del">-		if (iter.index &gt; vmf-&gt;max_pgoff)</span>
<span class="p_add">+	radix_tree_for_each_slot(slot, &amp;mapping-&gt;page_tree, &amp;iter,</span>
<span class="p_add">+			start_pgoff) {</span>
<span class="p_add">+		if (iter.index &gt; end_pgoff)</span>
 			break;
<span class="p_add">+		fe-&gt;pte += iter.index - last_pgoff;</span>
<span class="p_add">+		fe-&gt;address += (iter.index - last_pgoff) &lt;&lt; PAGE_SHIFT;</span>
<span class="p_add">+		last_pgoff = iter.index;</span>
<span class="p_add">+		if (!pte_none(*fe-&gt;pte))</span>
<span class="p_add">+			goto next;</span>
 repeat:
 		page = radix_tree_deref_slot(slot);
 		if (unlikely(!page))
<span class="p_chunk">@@ -2182,14 +2187,9 @@</span> <span class="p_context"> repeat:</span>
 		if (page-&gt;index &gt;= size &gt;&gt; PAGE_SHIFT)
 			goto unlock;
 
<span class="p_del">-		pte = vmf-&gt;pte + page-&gt;index - vmf-&gt;pgoff;</span>
<span class="p_del">-		if (!pte_none(*pte))</span>
<span class="p_del">-			goto unlock;</span>
<span class="p_del">-</span>
 		if (file-&gt;f_ra.mmap_miss &gt; 0)
 			file-&gt;f_ra.mmap_miss--;
<span class="p_del">-		addr = address + (page-&gt;index - vmf-&gt;pgoff) * PAGE_SIZE;</span>
<span class="p_del">-		do_set_pte(vma, addr, page, pte, false, false);</span>
<span class="p_add">+		do_set_pte(fe, page);</span>
 		unlock_page(page);
 		goto next;
 unlock:
<span class="p_chunk">@@ -2197,7 +2197,7 @@</span> <span class="p_context"> unlock:</span>
 skip:
 		put_page(page);
 next:
<span class="p_del">-		if (iter.index == vmf-&gt;max_pgoff)</span>
<span class="p_add">+		if (iter.index == end_pgoff)</span>
 			break;
 	}
 	rcu_read_unlock();
<span class="p_header">diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="p_header">index 8237a40a7fab..455e7a785c89 100644</span>
<span class="p_header">--- a/mm/huge_memory.c</span>
<span class="p_header">+++ b/mm/huge_memory.c</span>
<span class="p_chunk">@@ -821,26 +821,23 @@</span> <span class="p_context"> void prep_transhuge_page(struct page *page)</span>
 	set_compound_page_dtor(page, TRANSHUGE_PAGE_DTOR);
 }
 
<span class="p_del">-static int __do_huge_pmd_anonymous_page(struct mm_struct *mm,</span>
<span class="p_del">-					struct vm_area_struct *vma,</span>
<span class="p_del">-					unsigned long address, pmd_t *pmd,</span>
<span class="p_del">-					struct page *page, gfp_t gfp,</span>
<span class="p_del">-					unsigned int flags)</span>
<span class="p_add">+static int __do_huge_pmd_anonymous_page(struct fault_env *fe, struct page *page,</span>
<span class="p_add">+		gfp_t gfp)</span>
 {
<span class="p_add">+	struct vm_area_struct *vma = fe-&gt;vma;</span>
 	struct mem_cgroup *memcg;
 	pgtable_t pgtable;
<span class="p_del">-	spinlock_t *ptl;</span>
<span class="p_del">-	unsigned long haddr = address &amp; HPAGE_PMD_MASK;</span>
<span class="p_add">+	unsigned long haddr = fe-&gt;address &amp; HPAGE_PMD_MASK;</span>
 
 	VM_BUG_ON_PAGE(!PageCompound(page), page);
 
<span class="p_del">-	if (mem_cgroup_try_charge(page, mm, gfp, &amp;memcg, true)) {</span>
<span class="p_add">+	if (mem_cgroup_try_charge(page, vma-&gt;vm_mm, gfp, &amp;memcg, true)) {</span>
 		put_page(page);
 		count_vm_event(THP_FAULT_FALLBACK);
 		return VM_FAULT_FALLBACK;
 	}
 
<span class="p_del">-	pgtable = pte_alloc_one(mm, haddr);</span>
<span class="p_add">+	pgtable = pte_alloc_one(vma-&gt;vm_mm, haddr);</span>
 	if (unlikely(!pgtable)) {
 		mem_cgroup_cancel_charge(page, memcg, true);
 		put_page(page);
<span class="p_chunk">@@ -855,12 +852,12 @@</span> <span class="p_context"> static int __do_huge_pmd_anonymous_page(struct mm_struct *mm,</span>
 	 */
 	__SetPageUptodate(page);
 
<span class="p_del">-	ptl = pmd_lock(mm, pmd);</span>
<span class="p_del">-	if (unlikely(!pmd_none(*pmd))) {</span>
<span class="p_del">-		spin_unlock(ptl);</span>
<span class="p_add">+	fe-&gt;ptl = pmd_lock(vma-&gt;vm_mm, fe-&gt;pmd);</span>
<span class="p_add">+	if (unlikely(!pmd_none(*fe-&gt;pmd))) {</span>
<span class="p_add">+		spin_unlock(fe-&gt;ptl);</span>
 		mem_cgroup_cancel_charge(page, memcg, true);
 		put_page(page);
<span class="p_del">-		pte_free(mm, pgtable);</span>
<span class="p_add">+		pte_free(vma-&gt;vm_mm, pgtable);</span>
 	} else {
 		pmd_t entry;
 
<span class="p_chunk">@@ -868,12 +865,11 @@</span> <span class="p_context"> static int __do_huge_pmd_anonymous_page(struct mm_struct *mm,</span>
 		if (userfaultfd_missing(vma)) {
 			int ret;
 
<span class="p_del">-			spin_unlock(ptl);</span>
<span class="p_add">+			spin_unlock(fe-&gt;ptl);</span>
 			mem_cgroup_cancel_charge(page, memcg, true);
 			put_page(page);
<span class="p_del">-			pte_free(mm, pgtable);</span>
<span class="p_del">-			ret = handle_userfault(vma, address, flags,</span>
<span class="p_del">-					       VM_UFFD_MISSING);</span>
<span class="p_add">+			pte_free(vma-&gt;vm_mm, pgtable);</span>
<span class="p_add">+			ret = handle_userfault(fe, VM_UFFD_MISSING);</span>
 			VM_BUG_ON(ret &amp; VM_FAULT_FALLBACK);
 			return ret;
 		}
<span class="p_chunk">@@ -883,11 +879,11 @@</span> <span class="p_context"> static int __do_huge_pmd_anonymous_page(struct mm_struct *mm,</span>
 		page_add_new_anon_rmap(page, vma, haddr, true);
 		mem_cgroup_commit_charge(page, memcg, false, true);
 		lru_cache_add_active_or_unevictable(page, vma);
<span class="p_del">-		pgtable_trans_huge_deposit(mm, pmd, pgtable);</span>
<span class="p_del">-		set_pmd_at(mm, haddr, pmd, entry);</span>
<span class="p_del">-		add_mm_counter(mm, MM_ANONPAGES, HPAGE_PMD_NR);</span>
<span class="p_del">-		atomic_long_inc(&amp;mm-&gt;nr_ptes);</span>
<span class="p_del">-		spin_unlock(ptl);</span>
<span class="p_add">+		pgtable_trans_huge_deposit(vma-&gt;vm_mm, fe-&gt;pmd, pgtable);</span>
<span class="p_add">+		set_pmd_at(vma-&gt;vm_mm, haddr, fe-&gt;pmd, entry);</span>
<span class="p_add">+		add_mm_counter(vma-&gt;vm_mm, MM_ANONPAGES, HPAGE_PMD_NR);</span>
<span class="p_add">+		atomic_long_inc(&amp;vma-&gt;vm_mm-&gt;nr_ptes);</span>
<span class="p_add">+		spin_unlock(fe-&gt;ptl);</span>
 		count_vm_event(THP_FAULT_ALLOC);
 	}
 
<span class="p_chunk">@@ -937,13 +933,12 @@</span> <span class="p_context"> static bool set_huge_zero_page(pgtable_t pgtable, struct mm_struct *mm,</span>
 	return true;
 }
 
<span class="p_del">-int do_huge_pmd_anonymous_page(struct mm_struct *mm, struct vm_area_struct *vma,</span>
<span class="p_del">-			       unsigned long address, pmd_t *pmd,</span>
<span class="p_del">-			       unsigned int flags)</span>
<span class="p_add">+int do_huge_pmd_anonymous_page(struct fault_env *fe)</span>
 {
<span class="p_add">+	struct vm_area_struct *vma = fe-&gt;vma;</span>
 	gfp_t gfp;
 	struct page *page;
<span class="p_del">-	unsigned long haddr = address &amp; HPAGE_PMD_MASK;</span>
<span class="p_add">+	unsigned long haddr = fe-&gt;address &amp; HPAGE_PMD_MASK;</span>
 
 	if (haddr &lt; vma-&gt;vm_start || haddr + HPAGE_PMD_SIZE &gt; vma-&gt;vm_end)
 		return VM_FAULT_FALLBACK;
<span class="p_chunk">@@ -951,42 +946,40 @@</span> <span class="p_context"> int do_huge_pmd_anonymous_page(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 		return VM_FAULT_OOM;
 	if (unlikely(khugepaged_enter(vma, vma-&gt;vm_flags)))
 		return VM_FAULT_OOM;
<span class="p_del">-	if (!(flags &amp; FAULT_FLAG_WRITE) &amp;&amp; !mm_forbids_zeropage(mm) &amp;&amp;</span>
<span class="p_add">+	if (!(fe-&gt;flags &amp; FAULT_FLAG_WRITE) &amp;&amp;</span>
<span class="p_add">+			!mm_forbids_zeropage(vma-&gt;vm_mm) &amp;&amp;</span>
 			transparent_hugepage_use_zero_page()) {
<span class="p_del">-		spinlock_t *ptl;</span>
 		pgtable_t pgtable;
 		struct page *zero_page;
 		bool set;
 		int ret;
<span class="p_del">-		pgtable = pte_alloc_one(mm, haddr);</span>
<span class="p_add">+		pgtable = pte_alloc_one(vma-&gt;vm_mm, haddr);</span>
 		if (unlikely(!pgtable))
 			return VM_FAULT_OOM;
 		zero_page = get_huge_zero_page();
 		if (unlikely(!zero_page)) {
<span class="p_del">-			pte_free(mm, pgtable);</span>
<span class="p_add">+			pte_free(vma-&gt;vm_mm, pgtable);</span>
 			count_vm_event(THP_FAULT_FALLBACK);
 			return VM_FAULT_FALLBACK;
 		}
<span class="p_del">-		ptl = pmd_lock(mm, pmd);</span>
<span class="p_add">+		fe-&gt;ptl = pmd_lock(vma-&gt;vm_mm, fe-&gt;pmd);</span>
 		ret = 0;
 		set = false;
<span class="p_del">-		if (pmd_none(*pmd)) {</span>
<span class="p_add">+		if (pmd_none(*fe-&gt;pmd)) {</span>
 			if (userfaultfd_missing(vma)) {
<span class="p_del">-				spin_unlock(ptl);</span>
<span class="p_del">-				ret = handle_userfault(vma, address, flags,</span>
<span class="p_del">-						       VM_UFFD_MISSING);</span>
<span class="p_add">+				spin_unlock(fe-&gt;ptl);</span>
<span class="p_add">+				ret = handle_userfault(fe, VM_UFFD_MISSING);</span>
 				VM_BUG_ON(ret &amp; VM_FAULT_FALLBACK);
 			} else {
<span class="p_del">-				set_huge_zero_page(pgtable, mm, vma,</span>
<span class="p_del">-						   haddr, pmd,</span>
<span class="p_del">-						   zero_page);</span>
<span class="p_del">-				spin_unlock(ptl);</span>
<span class="p_add">+				set_huge_zero_page(pgtable, vma-&gt;vm_mm, vma,</span>
<span class="p_add">+						   haddr, fe-&gt;pmd, zero_page);</span>
<span class="p_add">+				spin_unlock(fe-&gt;ptl);</span>
 				set = true;
 			}
 		} else
<span class="p_del">-			spin_unlock(ptl);</span>
<span class="p_add">+			spin_unlock(fe-&gt;ptl);</span>
 		if (!set) {
<span class="p_del">-			pte_free(mm, pgtable);</span>
<span class="p_add">+			pte_free(vma-&gt;vm_mm, pgtable);</span>
 			put_huge_zero_page();
 		}
 		return ret;
<span class="p_chunk">@@ -998,8 +991,7 @@</span> <span class="p_context"> int do_huge_pmd_anonymous_page(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 		return VM_FAULT_FALLBACK;
 	}
 	prep_transhuge_page(page);
<span class="p_del">-	return __do_huge_pmd_anonymous_page(mm, vma, address, pmd, page, gfp,</span>
<span class="p_del">-					    flags);</span>
<span class="p_add">+	return __do_huge_pmd_anonymous_page(fe, page, gfp);</span>
 }
 
 static void insert_pfn_pmd(struct vm_area_struct *vma, unsigned long addr,
<span class="p_chunk">@@ -1171,38 +1163,31 @@</span> <span class="p_context"> out:</span>
 	return ret;
 }
 
<span class="p_del">-void huge_pmd_set_accessed(struct mm_struct *mm,</span>
<span class="p_del">-			   struct vm_area_struct *vma,</span>
<span class="p_del">-			   unsigned long address,</span>
<span class="p_del">-			   pmd_t *pmd, pmd_t orig_pmd,</span>
<span class="p_del">-			   int dirty)</span>
<span class="p_add">+void huge_pmd_set_accessed(struct fault_env *fe, pmd_t orig_pmd)</span>
 {
<span class="p_del">-	spinlock_t *ptl;</span>
 	pmd_t entry;
 	unsigned long haddr;
 
<span class="p_del">-	ptl = pmd_lock(mm, pmd);</span>
<span class="p_del">-	if (unlikely(!pmd_same(*pmd, orig_pmd)))</span>
<span class="p_add">+	fe-&gt;ptl = pmd_lock(fe-&gt;vma-&gt;vm_mm, fe-&gt;pmd);</span>
<span class="p_add">+	if (unlikely(!pmd_same(*fe-&gt;pmd, orig_pmd)))</span>
 		goto unlock;
 
 	entry = pmd_mkyoung(orig_pmd);
<span class="p_del">-	haddr = address &amp; HPAGE_PMD_MASK;</span>
<span class="p_del">-	if (pmdp_set_access_flags(vma, haddr, pmd, entry, dirty))</span>
<span class="p_del">-		update_mmu_cache_pmd(vma, address, pmd);</span>
<span class="p_add">+	haddr = fe-&gt;address &amp; HPAGE_PMD_MASK;</span>
<span class="p_add">+	if (pmdp_set_access_flags(fe-&gt;vma, haddr, fe-&gt;pmd, entry,</span>
<span class="p_add">+				fe-&gt;flags &amp; FAULT_FLAG_WRITE))</span>
<span class="p_add">+		update_mmu_cache_pmd(fe-&gt;vma, fe-&gt;address, fe-&gt;pmd);</span>
 
 unlock:
<span class="p_del">-	spin_unlock(ptl);</span>
<span class="p_add">+	spin_unlock(fe-&gt;ptl);</span>
 }
 
<span class="p_del">-static int do_huge_pmd_wp_page_fallback(struct mm_struct *mm,</span>
<span class="p_del">-					struct vm_area_struct *vma,</span>
<span class="p_del">-					unsigned long address,</span>
<span class="p_del">-					pmd_t *pmd, pmd_t orig_pmd,</span>
<span class="p_del">-					struct page *page,</span>
<span class="p_del">-					unsigned long haddr)</span>
<span class="p_add">+static int do_huge_pmd_wp_page_fallback(struct fault_env *fe, pmd_t orig_pmd,</span>
<span class="p_add">+		struct page *page)</span>
 {
<span class="p_add">+	struct vm_area_struct *vma = fe-&gt;vma;</span>
<span class="p_add">+	unsigned long haddr = fe-&gt;address &amp; HPAGE_PMD_MASK;</span>
 	struct mem_cgroup *memcg;
<span class="p_del">-	spinlock_t *ptl;</span>
 	pgtable_t pgtable;
 	pmd_t _pmd;
 	int ret = 0, i;
<span class="p_chunk">@@ -1219,11 +1204,11 @@</span> <span class="p_context"> static int do_huge_pmd_wp_page_fallback(struct mm_struct *mm,</span>
 
 	for (i = 0; i &lt; HPAGE_PMD_NR; i++) {
 		pages[i] = alloc_page_vma_node(GFP_HIGHUSER_MOVABLE |
<span class="p_del">-					       __GFP_OTHER_NODE,</span>
<span class="p_del">-					       vma, address, page_to_nid(page));</span>
<span class="p_add">+					       __GFP_OTHER_NODE, vma,</span>
<span class="p_add">+					       fe-&gt;address, page_to_nid(page));</span>
 		if (unlikely(!pages[i] ||
<span class="p_del">-			     mem_cgroup_try_charge(pages[i], mm, GFP_KERNEL,</span>
<span class="p_del">-						   &amp;memcg, false))) {</span>
<span class="p_add">+			     mem_cgroup_try_charge(pages[i], vma-&gt;vm_mm,</span>
<span class="p_add">+				     GFP_KERNEL, &amp;memcg, false))) {</span>
 			if (pages[i])
 				put_page(pages[i]);
 			while (--i &gt;= 0) {
<span class="p_chunk">@@ -1249,41 +1234,41 @@</span> <span class="p_context"> static int do_huge_pmd_wp_page_fallback(struct mm_struct *mm,</span>
 
 	mmun_start = haddr;
 	mmun_end   = haddr + HPAGE_PMD_SIZE;
<span class="p_del">-	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_start(vma-&gt;vm_mm, mmun_start, mmun_end);</span>
 
<span class="p_del">-	ptl = pmd_lock(mm, pmd);</span>
<span class="p_del">-	if (unlikely(!pmd_same(*pmd, orig_pmd)))</span>
<span class="p_add">+	fe-&gt;ptl = pmd_lock(vma-&gt;vm_mm, fe-&gt;pmd);</span>
<span class="p_add">+	if (unlikely(!pmd_same(*fe-&gt;pmd, orig_pmd)))</span>
 		goto out_free_pages;
 	VM_BUG_ON_PAGE(!PageHead(page), page);
 
<span class="p_del">-	pmdp_huge_clear_flush_notify(vma, haddr, pmd);</span>
<span class="p_add">+	pmdp_huge_clear_flush_notify(vma, haddr, fe-&gt;pmd);</span>
 	/* leave pmd empty until pte is filled */
 
<span class="p_del">-	pgtable = pgtable_trans_huge_withdraw(mm, pmd);</span>
<span class="p_del">-	pmd_populate(mm, &amp;_pmd, pgtable);</span>
<span class="p_add">+	pgtable = pgtable_trans_huge_withdraw(vma-&gt;vm_mm, fe-&gt;pmd);</span>
<span class="p_add">+	pmd_populate(vma-&gt;vm_mm, &amp;_pmd, pgtable);</span>
 
 	for (i = 0; i &lt; HPAGE_PMD_NR; i++, haddr += PAGE_SIZE) {
<span class="p_del">-		pte_t *pte, entry;</span>
<span class="p_add">+		pte_t entry;</span>
 		entry = mk_pte(pages[i], vma-&gt;vm_page_prot);
 		entry = maybe_mkwrite(pte_mkdirty(entry), vma);
 		memcg = (void *)page_private(pages[i]);
 		set_page_private(pages[i], 0);
<span class="p_del">-		page_add_new_anon_rmap(pages[i], vma, haddr, false);</span>
<span class="p_add">+		page_add_new_anon_rmap(pages[i], fe-&gt;vma, haddr, false);</span>
 		mem_cgroup_commit_charge(pages[i], memcg, false, false);
 		lru_cache_add_active_or_unevictable(pages[i], vma);
<span class="p_del">-		pte = pte_offset_map(&amp;_pmd, haddr);</span>
<span class="p_del">-		VM_BUG_ON(!pte_none(*pte));</span>
<span class="p_del">-		set_pte_at(mm, haddr, pte, entry);</span>
<span class="p_del">-		pte_unmap(pte);</span>
<span class="p_add">+		fe-&gt;pte = pte_offset_map(&amp;_pmd, haddr);</span>
<span class="p_add">+		VM_BUG_ON(!pte_none(*fe-&gt;pte));</span>
<span class="p_add">+		set_pte_at(vma-&gt;vm_mm, haddr, fe-&gt;pte, entry);</span>
<span class="p_add">+		pte_unmap(fe-&gt;pte);</span>
 	}
 	kfree(pages);
 
 	smp_wmb(); /* make pte visible before pmd */
<span class="p_del">-	pmd_populate(mm, pmd, pgtable);</span>
<span class="p_add">+	pmd_populate(vma-&gt;vm_mm, fe-&gt;pmd, pgtable);</span>
 	page_remove_rmap(page, true);
<span class="p_del">-	spin_unlock(ptl);</span>
<span class="p_add">+	spin_unlock(fe-&gt;ptl);</span>
 
<span class="p_del">-	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_end(vma-&gt;vm_mm, mmun_start, mmun_end);</span>
 
 	ret |= VM_FAULT_WRITE;
 	put_page(page);
<span class="p_chunk">@@ -1292,8 +1277,8 @@</span> <span class="p_context"> out:</span>
 	return ret;
 
 out_free_pages:
<span class="p_del">-	spin_unlock(ptl);</span>
<span class="p_del">-	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="p_add">+	spin_unlock(fe-&gt;ptl);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_end(vma-&gt;vm_mm, mmun_start, mmun_end);</span>
 	for (i = 0; i &lt; HPAGE_PMD_NR; i++) {
 		memcg = (void *)page_private(pages[i]);
 		set_page_private(pages[i], 0);
<span class="p_chunk">@@ -1304,25 +1289,23 @@</span> <span class="p_context"> out_free_pages:</span>
 	goto out;
 }
 
<span class="p_del">-int do_huge_pmd_wp_page(struct mm_struct *mm, struct vm_area_struct *vma,</span>
<span class="p_del">-			unsigned long address, pmd_t *pmd, pmd_t orig_pmd)</span>
<span class="p_add">+int do_huge_pmd_wp_page(struct fault_env *fe, pmd_t orig_pmd)</span>
 {
<span class="p_del">-	spinlock_t *ptl;</span>
<span class="p_del">-	int ret = 0;</span>
<span class="p_add">+	struct vm_area_struct *vma = fe-&gt;vma;</span>
 	struct page *page = NULL, *new_page;
 	struct mem_cgroup *memcg;
<span class="p_del">-	unsigned long haddr;</span>
<span class="p_add">+	unsigned long haddr = fe-&gt;address &amp; HPAGE_PMD_MASK;</span>
 	unsigned long mmun_start;	/* For mmu_notifiers */
 	unsigned long mmun_end;		/* For mmu_notifiers */
 	gfp_t huge_gfp;			/* for allocation and charge */
<span class="p_add">+	int ret = 0;</span>
 
<span class="p_del">-	ptl = pmd_lockptr(mm, pmd);</span>
<span class="p_add">+	fe-&gt;ptl = pmd_lockptr(vma-&gt;vm_mm, fe-&gt;pmd);</span>
 	VM_BUG_ON_VMA(!vma-&gt;anon_vma, vma);
<span class="p_del">-	haddr = address &amp; HPAGE_PMD_MASK;</span>
 	if (is_huge_zero_pmd(orig_pmd))
 		goto alloc;
<span class="p_del">-	spin_lock(ptl);</span>
<span class="p_del">-	if (unlikely(!pmd_same(*pmd, orig_pmd)))</span>
<span class="p_add">+	spin_lock(fe-&gt;ptl);</span>
<span class="p_add">+	if (unlikely(!pmd_same(*fe-&gt;pmd, orig_pmd)))</span>
 		goto out_unlock;
 
 	page = pmd_page(orig_pmd);
<span class="p_chunk">@@ -1341,13 +1324,13 @@</span> <span class="p_context"> int do_huge_pmd_wp_page(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 		pmd_t entry;
 		entry = pmd_mkyoung(orig_pmd);
 		entry = maybe_pmd_mkwrite(pmd_mkdirty(entry), vma);
<span class="p_del">-		if (pmdp_set_access_flags(vma, haddr, pmd, entry,  1))</span>
<span class="p_del">-			update_mmu_cache_pmd(vma, address, pmd);</span>
<span class="p_add">+		if (pmdp_set_access_flags(vma, haddr, fe-&gt;pmd, entry,  1))</span>
<span class="p_add">+			update_mmu_cache_pmd(vma, fe-&gt;address, fe-&gt;pmd);</span>
 		ret |= VM_FAULT_WRITE;
 		goto out_unlock;
 	}
 	get_page(page);
<span class="p_del">-	spin_unlock(ptl);</span>
<span class="p_add">+	spin_unlock(fe-&gt;ptl);</span>
 alloc:
 	if (transparent_hugepage_enabled(vma) &amp;&amp;
 	    !transparent_hugepage_debug_cow()) {
<span class="p_chunk">@@ -1360,13 +1343,12 @@</span> <span class="p_context"> alloc:</span>
 		prep_transhuge_page(new_page);
 	} else {
 		if (!page) {
<span class="p_del">-			split_huge_pmd(vma, pmd, address);</span>
<span class="p_add">+			split_huge_pmd(vma, fe-&gt;pmd, fe-&gt;address);</span>
 			ret |= VM_FAULT_FALLBACK;
 		} else {
<span class="p_del">-			ret = do_huge_pmd_wp_page_fallback(mm, vma, address,</span>
<span class="p_del">-					pmd, orig_pmd, page, haddr);</span>
<span class="p_add">+			ret = do_huge_pmd_wp_page_fallback(fe, orig_pmd, page);</span>
 			if (ret &amp; VM_FAULT_OOM) {
<span class="p_del">-				split_huge_pmd(vma, pmd, address);</span>
<span class="p_add">+				split_huge_pmd(vma, fe-&gt;pmd, fe-&gt;address);</span>
 				ret |= VM_FAULT_FALLBACK;
 			}
 			put_page(page);
<span class="p_chunk">@@ -1375,14 +1357,12 @@</span> <span class="p_context"> alloc:</span>
 		goto out;
 	}
 
<span class="p_del">-	if (unlikely(mem_cgroup_try_charge(new_page, mm, huge_gfp, &amp;memcg,</span>
<span class="p_del">-					   true))) {</span>
<span class="p_add">+	if (unlikely(mem_cgroup_try_charge(new_page, vma-&gt;vm_mm,</span>
<span class="p_add">+					huge_gfp, &amp;memcg, true))) {</span>
 		put_page(new_page);
<span class="p_del">-		if (page) {</span>
<span class="p_del">-			split_huge_pmd(vma, pmd, address);</span>
<span class="p_add">+		split_huge_pmd(vma, fe-&gt;pmd, fe-&gt;address);</span>
<span class="p_add">+		if (page)</span>
 			put_page(page);
<span class="p_del">-		} else</span>
<span class="p_del">-			split_huge_pmd(vma, pmd, address);</span>
 		ret |= VM_FAULT_FALLBACK;
 		count_vm_event(THP_FAULT_FALLBACK);
 		goto out;
<span class="p_chunk">@@ -1398,13 +1378,13 @@</span> <span class="p_context"> alloc:</span>
 
 	mmun_start = haddr;
 	mmun_end   = haddr + HPAGE_PMD_SIZE;
<span class="p_del">-	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_start(vma-&gt;vm_mm, mmun_start, mmun_end);</span>
 
<span class="p_del">-	spin_lock(ptl);</span>
<span class="p_add">+	spin_lock(fe-&gt;ptl);</span>
 	if (page)
 		put_page(page);
<span class="p_del">-	if (unlikely(!pmd_same(*pmd, orig_pmd))) {</span>
<span class="p_del">-		spin_unlock(ptl);</span>
<span class="p_add">+	if (unlikely(!pmd_same(*fe-&gt;pmd, orig_pmd))) {</span>
<span class="p_add">+		spin_unlock(fe-&gt;ptl);</span>
 		mem_cgroup_cancel_charge(new_page, memcg, true);
 		put_page(new_page);
 		goto out_mn;
<span class="p_chunk">@@ -1412,14 +1392,14 @@</span> <span class="p_context"> alloc:</span>
 		pmd_t entry;
 		entry = mk_huge_pmd(new_page, vma-&gt;vm_page_prot);
 		entry = maybe_pmd_mkwrite(pmd_mkdirty(entry), vma);
<span class="p_del">-		pmdp_huge_clear_flush_notify(vma, haddr, pmd);</span>
<span class="p_add">+		pmdp_huge_clear_flush_notify(vma, haddr, fe-&gt;pmd);</span>
 		page_add_new_anon_rmap(new_page, vma, haddr, true);
 		mem_cgroup_commit_charge(new_page, memcg, false, true);
 		lru_cache_add_active_or_unevictable(new_page, vma);
<span class="p_del">-		set_pmd_at(mm, haddr, pmd, entry);</span>
<span class="p_del">-		update_mmu_cache_pmd(vma, address, pmd);</span>
<span class="p_add">+		set_pmd_at(vma-&gt;vm_mm, haddr, fe-&gt;pmd, entry);</span>
<span class="p_add">+		update_mmu_cache_pmd(vma, fe-&gt;address, fe-&gt;pmd);</span>
 		if (!page) {
<span class="p_del">-			add_mm_counter(mm, MM_ANONPAGES, HPAGE_PMD_NR);</span>
<span class="p_add">+			add_mm_counter(vma-&gt;vm_mm, MM_ANONPAGES, HPAGE_PMD_NR);</span>
 			put_huge_zero_page();
 		} else {
 			VM_BUG_ON_PAGE(!PageHead(page), page);
<span class="p_chunk">@@ -1428,13 +1408,13 @@</span> <span class="p_context"> alloc:</span>
 		}
 		ret |= VM_FAULT_WRITE;
 	}
<span class="p_del">-	spin_unlock(ptl);</span>
<span class="p_add">+	spin_unlock(fe-&gt;ptl);</span>
 out_mn:
<span class="p_del">-	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_end(vma-&gt;vm_mm, mmun_start, mmun_end);</span>
 out:
 	return ret;
 out_unlock:
<span class="p_del">-	spin_unlock(ptl);</span>
<span class="p_add">+	spin_unlock(fe-&gt;ptl);</span>
 	return ret;
 }
 
<span class="p_chunk">@@ -1494,13 +1474,12 @@</span> <span class="p_context"> out:</span>
 }
 
 /* NUMA hinting page fault entry point for trans huge pmds */
<span class="p_del">-int do_huge_pmd_numa_page(struct mm_struct *mm, struct vm_area_struct *vma,</span>
<span class="p_del">-				unsigned long addr, pmd_t pmd, pmd_t *pmdp)</span>
<span class="p_add">+int do_huge_pmd_numa_page(struct fault_env *fe, pmd_t pmd)</span>
 {
<span class="p_del">-	spinlock_t *ptl;</span>
<span class="p_add">+	struct vm_area_struct *vma = fe-&gt;vma;</span>
 	struct anon_vma *anon_vma = NULL;
 	struct page *page;
<span class="p_del">-	unsigned long haddr = addr &amp; HPAGE_PMD_MASK;</span>
<span class="p_add">+	unsigned long haddr = fe-&gt;address &amp; HPAGE_PMD_MASK;</span>
 	int page_nid = -1, this_nid = numa_node_id();
 	int target_nid, last_cpupid = -1;
 	bool page_locked;
<span class="p_chunk">@@ -1511,8 +1490,8 @@</span> <span class="p_context"> int do_huge_pmd_numa_page(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 	/* A PROT_NONE fault should not end up here */
 	BUG_ON(!(vma-&gt;vm_flags &amp; (VM_READ | VM_EXEC | VM_WRITE)));
 
<span class="p_del">-	ptl = pmd_lock(mm, pmdp);</span>
<span class="p_del">-	if (unlikely(!pmd_same(pmd, *pmdp)))</span>
<span class="p_add">+	fe-&gt;ptl = pmd_lock(vma-&gt;vm_mm, fe-&gt;pmd);</span>
<span class="p_add">+	if (unlikely(!pmd_same(pmd, *fe-&gt;pmd)))</span>
 		goto out_unlock;
 
 	/*
<span class="p_chunk">@@ -1520,9 +1499,9 @@</span> <span class="p_context"> int do_huge_pmd_numa_page(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 	 * without disrupting NUMA hinting information. Do not relock and
 	 * check_same as the page may no longer be mapped.
 	 */
<span class="p_del">-	if (unlikely(pmd_trans_migrating(*pmdp))) {</span>
<span class="p_del">-		page = pmd_page(*pmdp);</span>
<span class="p_del">-		spin_unlock(ptl);</span>
<span class="p_add">+	if (unlikely(pmd_trans_migrating(*fe-&gt;pmd))) {</span>
<span class="p_add">+		page = pmd_page(*fe-&gt;pmd);</span>
<span class="p_add">+		spin_unlock(fe-&gt;ptl);</span>
 		wait_on_page_locked(page);
 		goto out;
 	}
<span class="p_chunk">@@ -1555,7 +1534,7 @@</span> <span class="p_context"> int do_huge_pmd_numa_page(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 
 	/* Migration could have started since the pmd_trans_migrating check */
 	if (!page_locked) {
<span class="p_del">-		spin_unlock(ptl);</span>
<span class="p_add">+		spin_unlock(fe-&gt;ptl);</span>
 		wait_on_page_locked(page);
 		page_nid = -1;
 		goto out;
<span class="p_chunk">@@ -1566,12 +1545,12 @@</span> <span class="p_context"> int do_huge_pmd_numa_page(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 	 * to serialises splits
 	 */
 	get_page(page);
<span class="p_del">-	spin_unlock(ptl);</span>
<span class="p_add">+	spin_unlock(fe-&gt;ptl);</span>
 	anon_vma = page_lock_anon_vma_read(page);
 
 	/* Confirm the PMD did not change while page_table_lock was released */
<span class="p_del">-	spin_lock(ptl);</span>
<span class="p_del">-	if (unlikely(!pmd_same(pmd, *pmdp))) {</span>
<span class="p_add">+	spin_lock(fe-&gt;ptl);</span>
<span class="p_add">+	if (unlikely(!pmd_same(pmd, *fe-&gt;pmd))) {</span>
 		unlock_page(page);
 		put_page(page);
 		page_nid = -1;
<span class="p_chunk">@@ -1589,9 +1568,9 @@</span> <span class="p_context"> int do_huge_pmd_numa_page(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 	 * Migrate the THP to the requested node, returns with page unlocked
 	 * and access rights restored.
 	 */
<span class="p_del">-	spin_unlock(ptl);</span>
<span class="p_del">-	migrated = migrate_misplaced_transhuge_page(mm, vma,</span>
<span class="p_del">-				pmdp, pmd, addr, page, target_nid);</span>
<span class="p_add">+	spin_unlock(fe-&gt;ptl);</span>
<span class="p_add">+	migrated = migrate_misplaced_transhuge_page(vma-&gt;vm_mm, vma,</span>
<span class="p_add">+				fe-&gt;pmd, pmd, fe-&gt;address, page, target_nid);</span>
 	if (migrated) {
 		flags |= TNF_MIGRATED;
 		page_nid = target_nid;
<span class="p_chunk">@@ -1606,18 +1585,18 @@</span> <span class="p_context"> clear_pmdnuma:</span>
 	pmd = pmd_mkyoung(pmd);
 	if (was_writable)
 		pmd = pmd_mkwrite(pmd);
<span class="p_del">-	set_pmd_at(mm, haddr, pmdp, pmd);</span>
<span class="p_del">-	update_mmu_cache_pmd(vma, addr, pmdp);</span>
<span class="p_add">+	set_pmd_at(vma-&gt;vm_mm, haddr, fe-&gt;pmd, pmd);</span>
<span class="p_add">+	update_mmu_cache_pmd(vma, fe-&gt;address, fe-&gt;pmd);</span>
 	unlock_page(page);
 out_unlock:
<span class="p_del">-	spin_unlock(ptl);</span>
<span class="p_add">+	spin_unlock(fe-&gt;ptl);</span>
 
 out:
 	if (anon_vma)
 		page_unlock_anon_vma_read(anon_vma);
 
 	if (page_nid != -1)
<span class="p_del">-		task_numa_fault(last_cpupid, page_nid, HPAGE_PMD_NR, flags);</span>
<span class="p_add">+		task_numa_fault(last_cpupid, page_nid, HPAGE_PMD_NR, fe-&gt;flags);</span>
 
 	return 0;
 }
<span class="p_chunk">@@ -2399,29 +2378,32 @@</span> <span class="p_context"> static void __collapse_huge_page_swapin(struct mm_struct *mm,</span>
 					struct vm_area_struct *vma,
 					unsigned long address, pmd_t *pmd)
 {
<span class="p_del">-	unsigned long _address;</span>
<span class="p_del">-	pte_t *pte, pteval;</span>
<span class="p_add">+	pte_t pteval;</span>
 	int swapped_in = 0, ret = 0;
<span class="p_del">-</span>
<span class="p_del">-	pte = pte_offset_map(pmd, address);</span>
<span class="p_del">-	for (_address = address; _address &lt; address + HPAGE_PMD_NR*PAGE_SIZE;</span>
<span class="p_del">-	     pte++, _address += PAGE_SIZE) {</span>
<span class="p_del">-		pteval = *pte;</span>
<span class="p_add">+	struct fault_env fe = {</span>
<span class="p_add">+		.vma = vma,</span>
<span class="p_add">+		.address = address,</span>
<span class="p_add">+		.flags = FAULT_FLAG_ALLOW_RETRY|FAULT_FLAG_RETRY_NOWAIT,</span>
<span class="p_add">+		.pmd = pmd,</span>
<span class="p_add">+	};</span>
<span class="p_add">+</span>
<span class="p_add">+	fe.pte = pte_offset_map(pmd, address);</span>
<span class="p_add">+	for (; fe.address &lt; address + HPAGE_PMD_NR*PAGE_SIZE;</span>
<span class="p_add">+			fe.pte++, fe.address += PAGE_SIZE) {</span>
<span class="p_add">+		pteval = *fe.pte;</span>
 		if (!is_swap_pte(pteval))
 			continue;
 		swapped_in++;
<span class="p_del">-		ret = do_swap_page(mm, vma, _address, pte, pmd,</span>
<span class="p_del">-				   FAULT_FLAG_ALLOW_RETRY|FAULT_FLAG_RETRY_NOWAIT,</span>
<span class="p_del">-				   pteval);</span>
<span class="p_add">+		ret = do_swap_page(&amp;fe, pteval);</span>
 		if (ret &amp; VM_FAULT_ERROR) {
 			trace_mm_collapse_huge_page_swapin(mm, swapped_in, 0);
 			return;
 		}
 		/* pte is unmapped now, we need to map it */
<span class="p_del">-		pte = pte_offset_map(pmd, _address);</span>
<span class="p_add">+		fe.pte = pte_offset_map(pmd, fe.address);</span>
 	}
<span class="p_del">-	pte--;</span>
<span class="p_del">-	pte_unmap(pte);</span>
<span class="p_add">+	fe.pte--;</span>
<span class="p_add">+	pte_unmap(fe.pte);</span>
 	trace_mm_collapse_huge_page_swapin(mm, swapped_in, 1);
 }
 
<span class="p_header">diff --git a/mm/internal.h b/mm/internal.h</span>
<span class="p_header">index 6e9222a41c4a..1458bd904daf 100644</span>
<span class="p_header">--- a/mm/internal.h</span>
<span class="p_header">+++ b/mm/internal.h</span>
<span class="p_chunk">@@ -35,9 +35,7 @@</span> <span class="p_context"></span>
 /* Do not use these with a slab allocator */
 #define GFP_SLAB_BUG_MASK (__GFP_DMA32|__GFP_HIGHMEM|~__GFP_BITS_MASK)
 
<span class="p_del">-extern int do_swap_page(struct mm_struct *mm, struct vm_area_struct *vma,</span>
<span class="p_del">-			unsigned long address, pte_t *page_table, pmd_t *pmd,</span>
<span class="p_del">-			unsigned int flags, pte_t orig_pte);</span>
<span class="p_add">+int do_swap_page(struct fault_env *fe, pte_t orig_pte);</span>
 
 void free_pgtables(struct mmu_gather *tlb, struct vm_area_struct *start_vma,
 		unsigned long floor, unsigned long ceiling);
<span class="p_header">diff --git a/mm/memory.c b/mm/memory.c</span>
<span class="p_header">index 195262f40e32..4fe2bdd65cce 100644</span>
<span class="p_header">--- a/mm/memory.c</span>
<span class="p_header">+++ b/mm/memory.c</span>
<span class="p_chunk">@@ -2020,13 +2020,11 @@</span> <span class="p_context"> static int do_page_mkwrite(struct vm_area_struct *vma, struct page *page,</span>
  * case, all we need to do here is to mark the page as writable and update
  * any related book-keeping.
  */
<span class="p_del">-static inline int wp_page_reuse(struct mm_struct *mm,</span>
<span class="p_del">-			struct vm_area_struct *vma, unsigned long address,</span>
<span class="p_del">-			pte_t *page_table, spinlock_t *ptl, pte_t orig_pte,</span>
<span class="p_del">-			struct page *page, int page_mkwrite,</span>
<span class="p_del">-			int dirty_shared)</span>
<span class="p_del">-	__releases(ptl)</span>
<span class="p_add">+static inline int wp_page_reuse(struct fault_env *fe, pte_t orig_pte,</span>
<span class="p_add">+			struct page *page, int page_mkwrite, int dirty_shared)</span>
<span class="p_add">+	__releases(fe-&gt;ptl)</span>
 {
<span class="p_add">+	struct vm_area_struct *vma = fe-&gt;vma;</span>
 	pte_t entry;
 	/*
 	 * Clear the pages cpupid information as the existing
<span class="p_chunk">@@ -2036,12 +2034,12 @@</span> <span class="p_context"> static inline int wp_page_reuse(struct mm_struct *mm,</span>
 	if (page)
 		page_cpupid_xchg_last(page, (1 &lt;&lt; LAST_CPUPID_SHIFT) - 1);
 
<span class="p_del">-	flush_cache_page(vma, address, pte_pfn(orig_pte));</span>
<span class="p_add">+	flush_cache_page(vma, fe-&gt;address, pte_pfn(orig_pte));</span>
 	entry = pte_mkyoung(orig_pte);
 	entry = maybe_mkwrite(pte_mkdirty(entry), vma);
<span class="p_del">-	if (ptep_set_access_flags(vma, address, page_table, entry, 1))</span>
<span class="p_del">-		update_mmu_cache(vma, address, page_table);</span>
<span class="p_del">-	pte_unmap_unlock(page_table, ptl);</span>
<span class="p_add">+	if (ptep_set_access_flags(vma, fe-&gt;address, fe-&gt;pte, entry, 1))</span>
<span class="p_add">+		update_mmu_cache(vma, fe-&gt;address, fe-&gt;pte);</span>
<span class="p_add">+	pte_unmap_unlock(fe-&gt;pte, fe-&gt;ptl);</span>
 
 	if (dirty_shared) {
 		struct address_space *mapping;
<span class="p_chunk">@@ -2087,30 +2085,31 @@</span> <span class="p_context"> static inline int wp_page_reuse(struct mm_struct *mm,</span>
  *   held to the old page, as well as updating the rmap.
  * - In any case, unlock the PTL and drop the reference we took to the old page.
  */
<span class="p_del">-static int wp_page_copy(struct mm_struct *mm, struct vm_area_struct *vma,</span>
<span class="p_del">-			unsigned long address, pte_t *page_table, pmd_t *pmd,</span>
<span class="p_del">-			pte_t orig_pte, struct page *old_page)</span>
<span class="p_add">+static int wp_page_copy(struct fault_env *fe, pte_t orig_pte,</span>
<span class="p_add">+		struct page *old_page)</span>
 {
<span class="p_add">+	struct vm_area_struct *vma = fe-&gt;vma;</span>
<span class="p_add">+	struct mm_struct *mm = vma-&gt;vm_mm;</span>
 	struct page *new_page = NULL;
<span class="p_del">-	spinlock_t *ptl = NULL;</span>
 	pte_t entry;
 	int page_copied = 0;
<span class="p_del">-	const unsigned long mmun_start = address &amp; PAGE_MASK;	/* For mmu_notifiers */</span>
<span class="p_del">-	const unsigned long mmun_end = mmun_start + PAGE_SIZE;	/* For mmu_notifiers */</span>
<span class="p_add">+	const unsigned long mmun_start = fe-&gt;address &amp; PAGE_MASK;</span>
<span class="p_add">+	const unsigned long mmun_end = mmun_start + PAGE_SIZE;</span>
 	struct mem_cgroup *memcg;
 
 	if (unlikely(anon_vma_prepare(vma)))
 		goto oom;
 
 	if (is_zero_pfn(pte_pfn(orig_pte))) {
<span class="p_del">-		new_page = alloc_zeroed_user_highpage_movable(vma, address);</span>
<span class="p_add">+		new_page = alloc_zeroed_user_highpage_movable(vma, fe-&gt;address);</span>
 		if (!new_page)
 			goto oom;
 	} else {
<span class="p_del">-		new_page = alloc_page_vma(GFP_HIGHUSER_MOVABLE, vma, address);</span>
<span class="p_add">+		new_page = alloc_page_vma(GFP_HIGHUSER_MOVABLE, vma,</span>
<span class="p_add">+				fe-&gt;address);</span>
 		if (!new_page)
 			goto oom;
<span class="p_del">-		cow_user_page(new_page, old_page, address, vma);</span>
<span class="p_add">+		cow_user_page(new_page, old_page, fe-&gt;address, vma);</span>
 	}
 
 	if (mem_cgroup_try_charge(new_page, mm, GFP_KERNEL, &amp;memcg, false))
<span class="p_chunk">@@ -2123,8 +2122,8 @@</span> <span class="p_context"> static int wp_page_copy(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 	/*
 	 * Re-check the pte - we dropped the lock
 	 */
<span class="p_del">-	page_table = pte_offset_map_lock(mm, pmd, address, &amp;ptl);</span>
<span class="p_del">-	if (likely(pte_same(*page_table, orig_pte))) {</span>
<span class="p_add">+	fe-&gt;pte = pte_offset_map_lock(mm, fe-&gt;pmd, fe-&gt;address, &amp;fe-&gt;ptl);</span>
<span class="p_add">+	if (likely(pte_same(*fe-&gt;pte, orig_pte))) {</span>
 		if (old_page) {
 			if (!PageAnon(old_page)) {
 				dec_mm_counter_fast(mm,
<span class="p_chunk">@@ -2134,7 +2133,7 @@</span> <span class="p_context"> static int wp_page_copy(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 		} else {
 			inc_mm_counter_fast(mm, MM_ANONPAGES);
 		}
<span class="p_del">-		flush_cache_page(vma, address, pte_pfn(orig_pte));</span>
<span class="p_add">+		flush_cache_page(vma, fe-&gt;address, pte_pfn(orig_pte));</span>
 		entry = mk_pte(new_page, vma-&gt;vm_page_prot);
 		entry = maybe_mkwrite(pte_mkdirty(entry), vma);
 		/*
<span class="p_chunk">@@ -2143,8 +2142,8 @@</span> <span class="p_context"> static int wp_page_copy(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 		 * seen in the presence of one thread doing SMC and another
 		 * thread doing COW.
 		 */
<span class="p_del">-		ptep_clear_flush_notify(vma, address, page_table);</span>
<span class="p_del">-		page_add_new_anon_rmap(new_page, vma, address, false);</span>
<span class="p_add">+		ptep_clear_flush_notify(vma, fe-&gt;address, fe-&gt;pte);</span>
<span class="p_add">+		page_add_new_anon_rmap(new_page, vma, fe-&gt;address, false);</span>
 		mem_cgroup_commit_charge(new_page, memcg, false, false);
 		lru_cache_add_active_or_unevictable(new_page, vma);
 		/*
<span class="p_chunk">@@ -2152,8 +2151,8 @@</span> <span class="p_context"> static int wp_page_copy(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 		 * mmu page tables (such as kvm shadow page tables), we want the
 		 * new page to be mapped directly into the secondary page table.
 		 */
<span class="p_del">-		set_pte_at_notify(mm, address, page_table, entry);</span>
<span class="p_del">-		update_mmu_cache(vma, address, page_table);</span>
<span class="p_add">+		set_pte_at_notify(mm, fe-&gt;address, fe-&gt;pte, entry);</span>
<span class="p_add">+		update_mmu_cache(vma, fe-&gt;address, fe-&gt;pte);</span>
 		if (old_page) {
 			/*
 			 * Only after switching the pte to the new page may
<span class="p_chunk">@@ -2190,7 +2189,7 @@</span> <span class="p_context"> static int wp_page_copy(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 	if (new_page)
 		put_page(new_page);
 
<span class="p_del">-	pte_unmap_unlock(page_table, ptl);</span>
<span class="p_add">+	pte_unmap_unlock(fe-&gt;pte, fe-&gt;ptl);</span>
 	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);
 	if (old_page) {
 		/*
<span class="p_chunk">@@ -2218,44 +2217,43 @@</span> <span class="p_context"> oom:</span>
  * Handle write page faults for VM_MIXEDMAP or VM_PFNMAP for a VM_SHARED
  * mapping
  */
<span class="p_del">-static int wp_pfn_shared(struct mm_struct *mm,</span>
<span class="p_del">-			struct vm_area_struct *vma, unsigned long address,</span>
<span class="p_del">-			pte_t *page_table, spinlock_t *ptl, pte_t orig_pte,</span>
<span class="p_del">-			pmd_t *pmd)</span>
<span class="p_add">+static int wp_pfn_shared(struct fault_env *fe,  pte_t orig_pte)</span>
 {
<span class="p_add">+	struct vm_area_struct *vma = fe-&gt;vma;</span>
<span class="p_add">+</span>
 	if (vma-&gt;vm_ops &amp;&amp; vma-&gt;vm_ops-&gt;pfn_mkwrite) {
 		struct vm_fault vmf = {
 			.page = NULL,
<span class="p_del">-			.pgoff = linear_page_index(vma, address),</span>
<span class="p_del">-			.virtual_address = (void __user *)(address &amp; PAGE_MASK),</span>
<span class="p_add">+			.pgoff = linear_page_index(vma, fe-&gt;address),</span>
<span class="p_add">+			.virtual_address =</span>
<span class="p_add">+				(void __user *)(fe-&gt;address &amp; PAGE_MASK),</span>
 			.flags = FAULT_FLAG_WRITE | FAULT_FLAG_MKWRITE,
 		};
 		int ret;
 
<span class="p_del">-		pte_unmap_unlock(page_table, ptl);</span>
<span class="p_add">+		pte_unmap_unlock(fe-&gt;pte, fe-&gt;ptl);</span>
 		ret = vma-&gt;vm_ops-&gt;pfn_mkwrite(vma, &amp;vmf);
 		if (ret &amp; VM_FAULT_ERROR)
 			return ret;
<span class="p_del">-		page_table = pte_offset_map_lock(mm, pmd, address, &amp;ptl);</span>
<span class="p_add">+		fe-&gt;pte = pte_offset_map_lock(vma-&gt;vm_mm, fe-&gt;pmd, fe-&gt;address,</span>
<span class="p_add">+				&amp;fe-&gt;ptl);</span>
 		/*
 		 * We might have raced with another page fault while we
 		 * released the pte_offset_map_lock.
 		 */
<span class="p_del">-		if (!pte_same(*page_table, orig_pte)) {</span>
<span class="p_del">-			pte_unmap_unlock(page_table, ptl);</span>
<span class="p_add">+		if (!pte_same(*fe-&gt;pte, orig_pte)) {</span>
<span class="p_add">+			pte_unmap_unlock(fe-&gt;pte, fe-&gt;ptl);</span>
 			return 0;
 		}
 	}
<span class="p_del">-	return wp_page_reuse(mm, vma, address, page_table, ptl, orig_pte,</span>
<span class="p_del">-			     NULL, 0, 0);</span>
<span class="p_add">+	return wp_page_reuse(fe, orig_pte, NULL, 0, 0);</span>
 }
 
<span class="p_del">-static int wp_page_shared(struct mm_struct *mm, struct vm_area_struct *vma,</span>
<span class="p_del">-			  unsigned long address, pte_t *page_table,</span>
<span class="p_del">-			  pmd_t *pmd, spinlock_t *ptl, pte_t orig_pte,</span>
<span class="p_del">-			  struct page *old_page)</span>
<span class="p_del">-	__releases(ptl)</span>
<span class="p_add">+static int wp_page_shared(struct fault_env *fe, pte_t orig_pte,</span>
<span class="p_add">+		struct page *old_page)</span>
<span class="p_add">+	__releases(fe-&gt;ptl)</span>
 {
<span class="p_add">+	struct vm_area_struct *vma = fe-&gt;vma;</span>
 	int page_mkwrite = 0;
 
 	get_page(old_page);
<span class="p_chunk">@@ -2263,8 +2261,8 @@</span> <span class="p_context"> static int wp_page_shared(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 	if (vma-&gt;vm_ops &amp;&amp; vma-&gt;vm_ops-&gt;page_mkwrite) {
 		int tmp;
 
<span class="p_del">-		pte_unmap_unlock(page_table, ptl);</span>
<span class="p_del">-		tmp = do_page_mkwrite(vma, old_page, address);</span>
<span class="p_add">+		pte_unmap_unlock(fe-&gt;pte, fe-&gt;ptl);</span>
<span class="p_add">+		tmp = do_page_mkwrite(vma, old_page, fe-&gt;address);</span>
 		if (unlikely(!tmp || (tmp &amp;
 				      (VM_FAULT_ERROR | VM_FAULT_NOPAGE)))) {
 			put_page(old_page);
<span class="p_chunk">@@ -2276,19 +2274,18 @@</span> <span class="p_context"> static int wp_page_shared(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 		 * they did, we just return, as we can count on the
 		 * MMU to tell us if they didn&#39;t also make it writable.
 		 */
<span class="p_del">-		page_table = pte_offset_map_lock(mm, pmd, address,</span>
<span class="p_del">-						 &amp;ptl);</span>
<span class="p_del">-		if (!pte_same(*page_table, orig_pte)) {</span>
<span class="p_add">+		fe-&gt;pte = pte_offset_map_lock(vma-&gt;vm_mm, fe-&gt;pmd, fe-&gt;address,</span>
<span class="p_add">+						 &amp;fe-&gt;ptl);</span>
<span class="p_add">+		if (!pte_same(*fe-&gt;pte, orig_pte)) {</span>
 			unlock_page(old_page);
<span class="p_del">-			pte_unmap_unlock(page_table, ptl);</span>
<span class="p_add">+			pte_unmap_unlock(fe-&gt;pte, fe-&gt;ptl);</span>
 			put_page(old_page);
 			return 0;
 		}
 		page_mkwrite = 1;
 	}
 
<span class="p_del">-	return wp_page_reuse(mm, vma, address, page_table, ptl,</span>
<span class="p_del">-			     orig_pte, old_page, page_mkwrite, 1);</span>
<span class="p_add">+	return wp_page_reuse(fe, orig_pte, old_page, page_mkwrite, 1);</span>
 }
 
 /*
<span class="p_chunk">@@ -2309,14 +2306,13 @@</span> <span class="p_context"> static int wp_page_shared(struct mm_struct *mm, struct vm_area_struct *vma,</span>
  * but allow concurrent faults), with pte both mapped and locked.
  * We return with mmap_sem still held, but pte unmapped and unlocked.
  */
<span class="p_del">-static int do_wp_page(struct mm_struct *mm, struct vm_area_struct *vma,</span>
<span class="p_del">-		unsigned long address, pte_t *page_table, pmd_t *pmd,</span>
<span class="p_del">-		spinlock_t *ptl, pte_t orig_pte)</span>
<span class="p_del">-	__releases(ptl)</span>
<span class="p_add">+static int do_wp_page(struct fault_env *fe, pte_t orig_pte)</span>
<span class="p_add">+	__releases(fe-&gt;ptl)</span>
 {
<span class="p_add">+	struct vm_area_struct *vma = fe-&gt;vma;</span>
 	struct page *old_page;
 
<span class="p_del">-	old_page = vm_normal_page(vma, address, orig_pte);</span>
<span class="p_add">+	old_page = vm_normal_page(vma, fe-&gt;address, orig_pte);</span>
 	if (!old_page) {
 		/*
 		 * VM_MIXEDMAP !pfn_valid() case, or VM_SOFTDIRTY clear on a
<span class="p_chunk">@@ -2327,12 +2323,10 @@</span> <span class="p_context"> static int do_wp_page(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 		 */
 		if ((vma-&gt;vm_flags &amp; (VM_WRITE|VM_SHARED)) ==
 				     (VM_WRITE|VM_SHARED))
<span class="p_del">-			return wp_pfn_shared(mm, vma, address, page_table, ptl,</span>
<span class="p_del">-					     orig_pte, pmd);</span>
<span class="p_add">+			return wp_pfn_shared(fe, orig_pte);</span>
 
<span class="p_del">-		pte_unmap_unlock(page_table, ptl);</span>
<span class="p_del">-		return wp_page_copy(mm, vma, address, page_table, pmd,</span>
<span class="p_del">-				    orig_pte, old_page);</span>
<span class="p_add">+		pte_unmap_unlock(fe-&gt;pte, fe-&gt;ptl);</span>
<span class="p_add">+		return wp_page_copy(fe, orig_pte, old_page);</span>
 	}
 
 	/*
<span class="p_chunk">@@ -2342,13 +2336,13 @@</span> <span class="p_context"> static int do_wp_page(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 	if (PageAnon(old_page) &amp;&amp; !PageKsm(old_page)) {
 		if (!trylock_page(old_page)) {
 			get_page(old_page);
<span class="p_del">-			pte_unmap_unlock(page_table, ptl);</span>
<span class="p_add">+			pte_unmap_unlock(fe-&gt;pte, fe-&gt;ptl);</span>
 			lock_page(old_page);
<span class="p_del">-			page_table = pte_offset_map_lock(mm, pmd, address,</span>
<span class="p_del">-							 &amp;ptl);</span>
<span class="p_del">-			if (!pte_same(*page_table, orig_pte)) {</span>
<span class="p_add">+			fe-&gt;pte = pte_offset_map_lock(vma-&gt;vm_mm, fe-&gt;pmd,</span>
<span class="p_add">+					fe-&gt;address, &amp;fe-&gt;ptl);</span>
<span class="p_add">+			if (!pte_same(*fe-&gt;pte, orig_pte)) {</span>
 				unlock_page(old_page);
<span class="p_del">-				pte_unmap_unlock(page_table, ptl);</span>
<span class="p_add">+				pte_unmap_unlock(fe-&gt;pte, fe-&gt;ptl);</span>
 				put_page(old_page);
 				return 0;
 			}
<span class="p_chunk">@@ -2360,16 +2354,14 @@</span> <span class="p_context"> static int do_wp_page(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 			 * the rmap code will not search our parent or siblings.
 			 * Protected against the rmap code by the page lock.
 			 */
<span class="p_del">-			page_move_anon_rmap(old_page, vma, address);</span>
<span class="p_add">+			page_move_anon_rmap(old_page, vma, fe-&gt;address);</span>
 			unlock_page(old_page);
<span class="p_del">-			return wp_page_reuse(mm, vma, address, page_table, ptl,</span>
<span class="p_del">-					     orig_pte, old_page, 0, 0);</span>
<span class="p_add">+			return wp_page_reuse(fe, orig_pte, old_page, 0, 0);</span>
 		}
 		unlock_page(old_page);
 	} else if (unlikely((vma-&gt;vm_flags &amp; (VM_WRITE|VM_SHARED)) ==
 					(VM_WRITE|VM_SHARED))) {
<span class="p_del">-		return wp_page_shared(mm, vma, address, page_table, pmd,</span>
<span class="p_del">-				      ptl, orig_pte, old_page);</span>
<span class="p_add">+		return wp_page_shared(fe, orig_pte, old_page);</span>
 	}
 
 	/*
<span class="p_chunk">@@ -2377,9 +2369,8 @@</span> <span class="p_context"> static int do_wp_page(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 	 */
 	get_page(old_page);
 
<span class="p_del">-	pte_unmap_unlock(page_table, ptl);</span>
<span class="p_del">-	return wp_page_copy(mm, vma, address, page_table, pmd,</span>
<span class="p_del">-			    orig_pte, old_page);</span>
<span class="p_add">+	pte_unmap_unlock(fe-&gt;pte, fe-&gt;ptl);</span>
<span class="p_add">+	return wp_page_copy(fe, orig_pte, old_page);</span>
 }
 
 static void unmap_mapping_range_vma(struct vm_area_struct *vma,
<span class="p_chunk">@@ -2469,11 +2460,9 @@</span> <span class="p_context"> EXPORT_SYMBOL(unmap_mapping_range);</span>
  * We return with the mmap_sem locked or unlocked in the same cases
  * as does filemap_fault().
  */
<span class="p_del">-int do_swap_page(struct mm_struct *mm, struct vm_area_struct *vma,</span>
<span class="p_del">-		unsigned long address, pte_t *page_table, pmd_t *pmd,</span>
<span class="p_del">-		unsigned int flags, pte_t orig_pte)</span>
<span class="p_add">+int do_swap_page(struct fault_env *fe, pte_t orig_pte)</span>
 {
<span class="p_del">-	spinlock_t *ptl;</span>
<span class="p_add">+	struct vm_area_struct *vma = fe-&gt;vma;</span>
 	struct page *page, *swapcache;
 	struct mem_cgroup *memcg;
 	swp_entry_t entry;
<span class="p_chunk">@@ -2482,17 +2471,17 @@</span> <span class="p_context"> int do_swap_page(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 	int exclusive = 0;
 	int ret = 0;
 
<span class="p_del">-	if (!pte_unmap_same(mm, pmd, page_table, orig_pte))</span>
<span class="p_add">+	if (!pte_unmap_same(vma-&gt;vm_mm, fe-&gt;pmd, fe-&gt;pte, orig_pte))</span>
 		goto out;
 
 	entry = pte_to_swp_entry(orig_pte);
 	if (unlikely(non_swap_entry(entry))) {
 		if (is_migration_entry(entry)) {
<span class="p_del">-			migration_entry_wait(mm, pmd, address);</span>
<span class="p_add">+			migration_entry_wait(vma-&gt;vm_mm, fe-&gt;pmd, fe-&gt;address);</span>
 		} else if (is_hwpoison_entry(entry)) {
 			ret = VM_FAULT_HWPOISON;
 		} else {
<span class="p_del">-			print_bad_pte(vma, address, orig_pte, NULL);</span>
<span class="p_add">+			print_bad_pte(vma, fe-&gt;address, orig_pte, NULL);</span>
 			ret = VM_FAULT_SIGBUS;
 		}
 		goto out;
<span class="p_chunk">@@ -2501,14 +2490,15 @@</span> <span class="p_context"> int do_swap_page(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 	page = lookup_swap_cache(entry);
 	if (!page) {
 		page = swapin_readahead(entry,
<span class="p_del">-					GFP_HIGHUSER_MOVABLE, vma, address);</span>
<span class="p_add">+					GFP_HIGHUSER_MOVABLE, vma, fe-&gt;address);</span>
 		if (!page) {
 			/*
 			 * Back out if somebody else faulted in this pte
 			 * while we released the pte lock.
 			 */
<span class="p_del">-			page_table = pte_offset_map_lock(mm, pmd, address, &amp;ptl);</span>
<span class="p_del">-			if (likely(pte_same(*page_table, orig_pte)))</span>
<span class="p_add">+			fe-&gt;pte = pte_offset_map_lock(vma-&gt;vm_mm, fe-&gt;pmd,</span>
<span class="p_add">+					fe-&gt;address, &amp;fe-&gt;ptl);</span>
<span class="p_add">+			if (likely(pte_same(*fe-&gt;pte, orig_pte)))</span>
 				ret = VM_FAULT_OOM;
 			delayacct_clear_flag(DELAYACCT_PF_SWAPIN);
 			goto unlock;
<span class="p_chunk">@@ -2517,7 +2507,7 @@</span> <span class="p_context"> int do_swap_page(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 		/* Had to read the page from swap area: Major fault */
 		ret = VM_FAULT_MAJOR;
 		count_vm_event(PGMAJFAULT);
<span class="p_del">-		mem_cgroup_count_vm_event(mm, PGMAJFAULT);</span>
<span class="p_add">+		mem_cgroup_count_vm_event(vma-&gt;vm_mm, PGMAJFAULT);</span>
 	} else if (PageHWPoison(page)) {
 		/*
 		 * hwpoisoned dirty swapcache pages are kept for killing
<span class="p_chunk">@@ -2530,7 +2520,7 @@</span> <span class="p_context"> int do_swap_page(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 	}
 
 	swapcache = page;
<span class="p_del">-	locked = lock_page_or_retry(page, mm, flags);</span>
<span class="p_add">+	locked = lock_page_or_retry(page, vma-&gt;vm_mm, fe-&gt;flags);</span>
 
 	delayacct_clear_flag(DELAYACCT_PF_SWAPIN);
 	if (!locked) {
<span class="p_chunk">@@ -2547,14 +2537,15 @@</span> <span class="p_context"> int do_swap_page(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 	if (unlikely(!PageSwapCache(page) || page_private(page) != entry.val))
 		goto out_page;
 
<span class="p_del">-	page = ksm_might_need_to_copy(page, vma, address);</span>
<span class="p_add">+	page = ksm_might_need_to_copy(page, vma, fe-&gt;address);</span>
 	if (unlikely(!page)) {
 		ret = VM_FAULT_OOM;
 		page = swapcache;
 		goto out_page;
 	}
 
<span class="p_del">-	if (mem_cgroup_try_charge(page, mm, GFP_KERNEL, &amp;memcg, false)) {</span>
<span class="p_add">+	if (mem_cgroup_try_charge(page, vma-&gt;vm_mm, GFP_KERNEL,</span>
<span class="p_add">+				&amp;memcg, false)) {</span>
 		ret = VM_FAULT_OOM;
 		goto out_page;
 	}
<span class="p_chunk">@@ -2562,8 +2553,9 @@</span> <span class="p_context"> int do_swap_page(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 	/*
 	 * Back out if somebody else already faulted in this pte.
 	 */
<span class="p_del">-	page_table = pte_offset_map_lock(mm, pmd, address, &amp;ptl);</span>
<span class="p_del">-	if (unlikely(!pte_same(*page_table, orig_pte)))</span>
<span class="p_add">+	fe-&gt;pte = pte_offset_map_lock(vma-&gt;vm_mm, fe-&gt;pmd, fe-&gt;address,</span>
<span class="p_add">+			&amp;fe-&gt;ptl);</span>
<span class="p_add">+	if (unlikely(!pte_same(*fe-&gt;pte, orig_pte)))</span>
 		goto out_nomap;
 
 	if (unlikely(!PageUptodate(page))) {
<span class="p_chunk">@@ -2581,24 +2573,24 @@</span> <span class="p_context"> int do_swap_page(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 	 * must be called after the swap_free(), or it will never succeed.
 	 */
 
<span class="p_del">-	inc_mm_counter_fast(mm, MM_ANONPAGES);</span>
<span class="p_del">-	dec_mm_counter_fast(mm, MM_SWAPENTS);</span>
<span class="p_add">+	inc_mm_counter_fast(vma-&gt;vm_mm, MM_ANONPAGES);</span>
<span class="p_add">+	dec_mm_counter_fast(vma-&gt;vm_mm, MM_SWAPENTS);</span>
 	pte = mk_pte(page, vma-&gt;vm_page_prot);
<span class="p_del">-	if ((flags &amp; FAULT_FLAG_WRITE) &amp;&amp; reuse_swap_page(page)) {</span>
<span class="p_add">+	if ((fe-&gt;flags &amp; FAULT_FLAG_WRITE) &amp;&amp; reuse_swap_page(page)) {</span>
 		pte = maybe_mkwrite(pte_mkdirty(pte), vma);
<span class="p_del">-		flags &amp;= ~FAULT_FLAG_WRITE;</span>
<span class="p_add">+		fe-&gt;flags &amp;= ~FAULT_FLAG_WRITE;</span>
 		ret |= VM_FAULT_WRITE;
 		exclusive = RMAP_EXCLUSIVE;
 	}
 	flush_icache_page(vma, page);
 	if (pte_swp_soft_dirty(orig_pte))
 		pte = pte_mksoft_dirty(pte);
<span class="p_del">-	set_pte_at(mm, address, page_table, pte);</span>
<span class="p_add">+	set_pte_at(vma-&gt;vm_mm, fe-&gt;address, fe-&gt;pte, pte);</span>
 	if (page == swapcache) {
<span class="p_del">-		do_page_add_anon_rmap(page, vma, address, exclusive);</span>
<span class="p_add">+		do_page_add_anon_rmap(page, vma, fe-&gt;address, exclusive);</span>
 		mem_cgroup_commit_charge(page, memcg, true, false);
 	} else { /* ksm created a completely new copy */
<span class="p_del">-		page_add_new_anon_rmap(page, vma, address, false);</span>
<span class="p_add">+		page_add_new_anon_rmap(page, vma, fe-&gt;address, false);</span>
 		mem_cgroup_commit_charge(page, memcg, false, false);
 		lru_cache_add_active_or_unevictable(page, vma);
 	}
<span class="p_chunk">@@ -2621,22 +2613,22 @@</span> <span class="p_context"> int do_swap_page(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 		put_page(swapcache);
 	}
 
<span class="p_del">-	if (flags &amp; FAULT_FLAG_WRITE) {</span>
<span class="p_del">-		ret |= do_wp_page(mm, vma, address, page_table, pmd, ptl, pte);</span>
<span class="p_add">+	if (fe-&gt;flags &amp; FAULT_FLAG_WRITE) {</span>
<span class="p_add">+		ret |= do_wp_page(fe, pte);</span>
 		if (ret &amp; VM_FAULT_ERROR)
 			ret &amp;= VM_FAULT_ERROR;
 		goto out;
 	}
 
 	/* No need to invalidate - it was non-present before */
<span class="p_del">-	update_mmu_cache(vma, address, page_table);</span>
<span class="p_add">+	update_mmu_cache(vma, fe-&gt;address, fe-&gt;pte);</span>
 unlock:
<span class="p_del">-	pte_unmap_unlock(page_table, ptl);</span>
<span class="p_add">+	pte_unmap_unlock(fe-&gt;pte, fe-&gt;ptl);</span>
 out:
 	return ret;
 out_nomap:
 	mem_cgroup_cancel_charge(page, memcg, false);
<span class="p_del">-	pte_unmap_unlock(page_table, ptl);</span>
<span class="p_add">+	pte_unmap_unlock(fe-&gt;pte, fe-&gt;ptl);</span>
 out_page:
 	unlock_page(page);
 out_release:
<span class="p_chunk">@@ -2687,37 +2679,36 @@</span> <span class="p_context"> static inline int check_stack_guard_page(struct vm_area_struct *vma, unsigned lo</span>
  * but allow concurrent faults), and pte mapped but not yet locked.
  * We return with mmap_sem still held, but pte unmapped and unlocked.
  */
<span class="p_del">-static int do_anonymous_page(struct mm_struct *mm, struct vm_area_struct *vma,</span>
<span class="p_del">-		unsigned long address, pte_t *page_table, pmd_t *pmd,</span>
<span class="p_del">-		unsigned int flags)</span>
<span class="p_add">+static int do_anonymous_page(struct fault_env *fe)</span>
 {
<span class="p_add">+	struct vm_area_struct *vma = fe-&gt;vma;</span>
 	struct mem_cgroup *memcg;
 	struct page *page;
<span class="p_del">-	spinlock_t *ptl;</span>
 	pte_t entry;
 
<span class="p_del">-	pte_unmap(page_table);</span>
<span class="p_add">+	pte_unmap(fe-&gt;pte);</span>
 
 	/* File mapping without -&gt;vm_ops ? */
 	if (vma-&gt;vm_flags &amp; VM_SHARED)
 		return VM_FAULT_SIGBUS;
 
 	/* Check if we need to add a guard page to the stack */
<span class="p_del">-	if (check_stack_guard_page(vma, address) &lt; 0)</span>
<span class="p_add">+	if (check_stack_guard_page(vma, fe-&gt;address) &lt; 0)</span>
 		return VM_FAULT_SIGSEGV;
 
 	/* Use the zero-page for reads */
<span class="p_del">-	if (!(flags &amp; FAULT_FLAG_WRITE) &amp;&amp; !mm_forbids_zeropage(mm)) {</span>
<span class="p_del">-		entry = pte_mkspecial(pfn_pte(my_zero_pfn(address),</span>
<span class="p_add">+	if (!(fe-&gt;flags &amp; FAULT_FLAG_WRITE) &amp;&amp;</span>
<span class="p_add">+			!mm_forbids_zeropage(vma-&gt;vm_mm)) {</span>
<span class="p_add">+		entry = pte_mkspecial(pfn_pte(my_zero_pfn(fe-&gt;address),</span>
 						vma-&gt;vm_page_prot));
<span class="p_del">-		page_table = pte_offset_map_lock(mm, pmd, address, &amp;ptl);</span>
<span class="p_del">-		if (!pte_none(*page_table))</span>
<span class="p_add">+		fe-&gt;pte = pte_offset_map_lock(vma-&gt;vm_mm, fe-&gt;pmd, fe-&gt;address,</span>
<span class="p_add">+				&amp;fe-&gt;ptl);</span>
<span class="p_add">+		if (!pte_none(*fe-&gt;pte))</span>
 			goto unlock;
 		/* Deliver the page fault to userland, check inside PT lock */
 		if (userfaultfd_missing(vma)) {
<span class="p_del">-			pte_unmap_unlock(page_table, ptl);</span>
<span class="p_del">-			return handle_userfault(vma, address, flags,</span>
<span class="p_del">-						VM_UFFD_MISSING);</span>
<span class="p_add">+			pte_unmap_unlock(fe-&gt;pte, fe-&gt;ptl);</span>
<span class="p_add">+			return handle_userfault(fe, VM_UFFD_MISSING);</span>
 		}
 		goto setpte;
 	}
<span class="p_chunk">@@ -2725,11 +2716,11 @@</span> <span class="p_context"> static int do_anonymous_page(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 	/* Allocate our own private page. */
 	if (unlikely(anon_vma_prepare(vma)))
 		goto oom;
<span class="p_del">-	page = alloc_zeroed_user_highpage_movable(vma, address);</span>
<span class="p_add">+	page = alloc_zeroed_user_highpage_movable(vma, fe-&gt;address);</span>
 	if (!page)
 		goto oom;
 
<span class="p_del">-	if (mem_cgroup_try_charge(page, mm, GFP_KERNEL, &amp;memcg, false))</span>
<span class="p_add">+	if (mem_cgroup_try_charge(page, vma-&gt;vm_mm, GFP_KERNEL, &amp;memcg, false))</span>
 		goto oom_free_page;
 
 	/*
<span class="p_chunk">@@ -2743,30 +2734,30 @@</span> <span class="p_context"> static int do_anonymous_page(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 	if (vma-&gt;vm_flags &amp; VM_WRITE)
 		entry = pte_mkwrite(pte_mkdirty(entry));
 
<span class="p_del">-	page_table = pte_offset_map_lock(mm, pmd, address, &amp;ptl);</span>
<span class="p_del">-	if (!pte_none(*page_table))</span>
<span class="p_add">+	fe-&gt;pte = pte_offset_map_lock(vma-&gt;vm_mm, fe-&gt;pmd, fe-&gt;address,</span>
<span class="p_add">+			&amp;fe-&gt;ptl);</span>
<span class="p_add">+	if (!pte_none(*fe-&gt;pte))</span>
 		goto release;
 
 	/* Deliver the page fault to userland, check inside PT lock */
 	if (userfaultfd_missing(vma)) {
<span class="p_del">-		pte_unmap_unlock(page_table, ptl);</span>
<span class="p_add">+		pte_unmap_unlock(fe-&gt;pte, fe-&gt;ptl);</span>
 		mem_cgroup_cancel_charge(page, memcg, false);
 		put_page(page);
<span class="p_del">-		return handle_userfault(vma, address, flags,</span>
<span class="p_del">-					VM_UFFD_MISSING);</span>
<span class="p_add">+		return handle_userfault(fe, VM_UFFD_MISSING);</span>
 	}
 
<span class="p_del">-	inc_mm_counter_fast(mm, MM_ANONPAGES);</span>
<span class="p_del">-	page_add_new_anon_rmap(page, vma, address, false);</span>
<span class="p_add">+	inc_mm_counter_fast(vma-&gt;vm_mm, MM_ANONPAGES);</span>
<span class="p_add">+	page_add_new_anon_rmap(page, vma, fe-&gt;address, false);</span>
 	mem_cgroup_commit_charge(page, memcg, false, false);
 	lru_cache_add_active_or_unevictable(page, vma);
 setpte:
<span class="p_del">-	set_pte_at(mm, address, page_table, entry);</span>
<span class="p_add">+	set_pte_at(vma-&gt;vm_mm, fe-&gt;address, fe-&gt;pte, entry);</span>
 
 	/* No need to invalidate - it was non-present before */
<span class="p_del">-	update_mmu_cache(vma, address, page_table);</span>
<span class="p_add">+	update_mmu_cache(vma, fe-&gt;address, fe-&gt;pte);</span>
 unlock:
<span class="p_del">-	pte_unmap_unlock(page_table, ptl);</span>
<span class="p_add">+	pte_unmap_unlock(fe-&gt;pte, fe-&gt;ptl);</span>
 	return 0;
 release:
 	mem_cgroup_cancel_charge(page, memcg, false);
<span class="p_chunk">@@ -2783,16 +2774,16 @@</span> <span class="p_context"> oom:</span>
  * released depending on flags and vma-&gt;vm_ops-&gt;fault() return value.
  * See filemap_fault() and __lock_page_retry().
  */
<span class="p_del">-static int __do_fault(struct vm_area_struct *vma, unsigned long address,</span>
<span class="p_del">-			pgoff_t pgoff, unsigned int flags,</span>
<span class="p_del">-			struct page *cow_page, struct page **page)</span>
<span class="p_add">+static int __do_fault(struct fault_env *fe, pgoff_t pgoff,</span>
<span class="p_add">+		struct page *cow_page, struct page **page)</span>
 {
<span class="p_add">+	struct vm_area_struct *vma = fe-&gt;vma;</span>
 	struct vm_fault vmf;
 	int ret;
 
<span class="p_del">-	vmf.virtual_address = (void __user *)(address &amp; PAGE_MASK);</span>
<span class="p_add">+	vmf.virtual_address = (void __user *)(fe-&gt;address &amp; PAGE_MASK);</span>
 	vmf.pgoff = pgoff;
<span class="p_del">-	vmf.flags = flags;</span>
<span class="p_add">+	vmf.flags = fe-&gt;flags;</span>
 	vmf.page = NULL;
 	vmf.gfp_mask = __get_fault_gfp_mask(vma);
 	vmf.cow_page = cow_page;
<span class="p_chunk">@@ -2823,38 +2814,36 @@</span> <span class="p_context"> static int __do_fault(struct vm_area_struct *vma, unsigned long address,</span>
 /**
  * do_set_pte - setup new PTE entry for given page and add reverse page mapping.
  *
<span class="p_del">- * @vma: virtual memory area</span>
<span class="p_del">- * @address: user virtual address</span>
<span class="p_add">+ * @fe: fault environment</span>
  * @page: page to map
<span class="p_del">- * @pte: pointer to target page table entry</span>
<span class="p_del">- * @write: true, if new entry is writable</span>
<span class="p_del">- * @anon: true, if it&#39;s anonymous page</span>
  *
<span class="p_del">- * Caller must hold page table lock relevant for @pte.</span>
<span class="p_add">+ * Caller must hold page table lock relevant for @fe-&gt;pte.</span>
  *
  * Target users are page handler itself and implementations of
  * vm_ops-&gt;map_pages.
  */
<span class="p_del">-void do_set_pte(struct vm_area_struct *vma, unsigned long address,</span>
<span class="p_del">-		struct page *page, pte_t *pte, bool write, bool anon)</span>
<span class="p_add">+void do_set_pte(struct fault_env *fe, struct page *page)</span>
 {
<span class="p_add">+	struct vm_area_struct *vma = fe-&gt;vma;</span>
<span class="p_add">+	bool write = fe-&gt;flags &amp; FAULT_FLAG_WRITE;</span>
 	pte_t entry;
 
 	flush_icache_page(vma, page);
 	entry = mk_pte(page, vma-&gt;vm_page_prot);
 	if (write)
 		entry = maybe_mkwrite(pte_mkdirty(entry), vma);
<span class="p_del">-	if (anon) {</span>
<span class="p_add">+	/* copy-on-write page */</span>
<span class="p_add">+	if (write &amp;&amp; !(vma-&gt;vm_flags &amp; VM_SHARED)) {</span>
 		inc_mm_counter_fast(vma-&gt;vm_mm, MM_ANONPAGES);
<span class="p_del">-		page_add_new_anon_rmap(page, vma, address, false);</span>
<span class="p_add">+		page_add_new_anon_rmap(page, vma, fe-&gt;address, false);</span>
 	} else {
 		inc_mm_counter_fast(vma-&gt;vm_mm, mm_counter_file(page));
 		page_add_file_rmap(page);
 	}
<span class="p_del">-	set_pte_at(vma-&gt;vm_mm, address, pte, entry);</span>
<span class="p_add">+	set_pte_at(vma-&gt;vm_mm, fe-&gt;address, fe-&gt;pte, entry);</span>
 
 	/* no need to invalidate: a not-present page won&#39;t be cached */
<span class="p_del">-	update_mmu_cache(vma, address, pte);</span>
<span class="p_add">+	update_mmu_cache(vma, fe-&gt;address, fe-&gt;pte);</span>
 }
 
 static unsigned long fault_around_bytes __read_mostly =
<span class="p_chunk">@@ -2921,57 +2910,53 @@</span> <span class="p_context"> late_initcall(fault_around_debugfs);</span>
  * fault_around_pages() value (and therefore to page order).  This way it&#39;s
  * easier to guarantee that we don&#39;t cross page table boundaries.
  */
<span class="p_del">-static void do_fault_around(struct vm_area_struct *vma, unsigned long address,</span>
<span class="p_del">-		pte_t *pte, pgoff_t pgoff, unsigned int flags)</span>
<span class="p_add">+static void do_fault_around(struct fault_env *fe, pgoff_t start_pgoff)</span>
 {
<span class="p_del">-	unsigned long start_addr, nr_pages, mask;</span>
<span class="p_del">-	pgoff_t max_pgoff;</span>
<span class="p_del">-	struct vm_fault vmf;</span>
<span class="p_add">+	unsigned long address = fe-&gt;address, start_addr, nr_pages, mask;</span>
<span class="p_add">+	pte_t *pte = fe-&gt;pte;</span>
<span class="p_add">+	pgoff_t end_pgoff;</span>
 	int off;
 
 	nr_pages = READ_ONCE(fault_around_bytes) &gt;&gt; PAGE_SHIFT;
 	mask = ~(nr_pages * PAGE_SIZE - 1) &amp; PAGE_MASK;
 
<span class="p_del">-	start_addr = max(address &amp; mask, vma-&gt;vm_start);</span>
<span class="p_del">-	off = ((address - start_addr) &gt;&gt; PAGE_SHIFT) &amp; (PTRS_PER_PTE - 1);</span>
<span class="p_del">-	pte -= off;</span>
<span class="p_del">-	pgoff -= off;</span>
<span class="p_add">+	start_addr = max(fe-&gt;address &amp; mask, fe-&gt;vma-&gt;vm_start);</span>
<span class="p_add">+	off = ((fe-&gt;address - start_addr) &gt;&gt; PAGE_SHIFT) &amp; (PTRS_PER_PTE - 1);</span>
<span class="p_add">+	fe-&gt;pte -= off;</span>
<span class="p_add">+	start_pgoff -= off;</span>
 
 	/*
<span class="p_del">-	 *  max_pgoff is either end of page table or end of vma</span>
<span class="p_del">-	 *  or fault_around_pages() from pgoff, depending what is nearest.</span>
<span class="p_add">+	 *  end_pgoff is either end of page table or end of vma</span>
<span class="p_add">+	 *  or fault_around_pages() from start_pgoff, depending what is nearest.</span>
 	 */
<span class="p_del">-	max_pgoff = pgoff - ((start_addr &gt;&gt; PAGE_SHIFT) &amp; (PTRS_PER_PTE - 1)) +</span>
<span class="p_add">+	end_pgoff = start_pgoff -</span>
<span class="p_add">+		((start_addr &gt;&gt; PAGE_SHIFT) &amp; (PTRS_PER_PTE - 1)) +</span>
 		PTRS_PER_PTE - 1;
<span class="p_del">-	max_pgoff = min3(max_pgoff, vma_pages(vma) + vma-&gt;vm_pgoff - 1,</span>
<span class="p_del">-			pgoff + nr_pages - 1);</span>
<span class="p_add">+	end_pgoff = min3(end_pgoff, vma_pages(fe-&gt;vma) + fe-&gt;vma-&gt;vm_pgoff - 1,</span>
<span class="p_add">+			start_pgoff + nr_pages - 1);</span>
 
 	/* Check if it makes any sense to call -&gt;map_pages */
<span class="p_del">-	while (!pte_none(*pte)) {</span>
<span class="p_del">-		if (++pgoff &gt; max_pgoff)</span>
<span class="p_del">-			return;</span>
<span class="p_del">-		start_addr += PAGE_SIZE;</span>
<span class="p_del">-		if (start_addr &gt;= vma-&gt;vm_end)</span>
<span class="p_del">-			return;</span>
<span class="p_del">-		pte++;</span>
<span class="p_add">+	fe-&gt;address = start_addr;</span>
<span class="p_add">+	while (!pte_none(*fe-&gt;pte)) {</span>
<span class="p_add">+		if (++start_pgoff &gt; end_pgoff)</span>
<span class="p_add">+			goto out;</span>
<span class="p_add">+		fe-&gt;address += PAGE_SIZE;</span>
<span class="p_add">+		if (fe-&gt;address &gt;= fe-&gt;vma-&gt;vm_end)</span>
<span class="p_add">+			goto out;</span>
<span class="p_add">+		fe-&gt;pte++;</span>
 	}
 
<span class="p_del">-	vmf.virtual_address = (void __user *) start_addr;</span>
<span class="p_del">-	vmf.pte = pte;</span>
<span class="p_del">-	vmf.pgoff = pgoff;</span>
<span class="p_del">-	vmf.max_pgoff = max_pgoff;</span>
<span class="p_del">-	vmf.flags = flags;</span>
<span class="p_del">-	vmf.gfp_mask = __get_fault_gfp_mask(vma);</span>
<span class="p_del">-	vma-&gt;vm_ops-&gt;map_pages(vma, &amp;vmf);</span>
<span class="p_add">+	fe-&gt;vma-&gt;vm_ops-&gt;map_pages(fe, start_pgoff, end_pgoff);</span>
<span class="p_add">+out:</span>
<span class="p_add">+	/* restore fault_env */</span>
<span class="p_add">+	fe-&gt;pte = pte;</span>
<span class="p_add">+	fe-&gt;address = address;</span>
 }
 
<span class="p_del">-static int do_read_fault(struct mm_struct *mm, struct vm_area_struct *vma,</span>
<span class="p_del">-		unsigned long address, pmd_t *pmd,</span>
<span class="p_del">-		pgoff_t pgoff, unsigned int flags, pte_t orig_pte)</span>
<span class="p_add">+static int do_read_fault(struct fault_env *fe, pgoff_t pgoff, pte_t orig_pte)</span>
 {
<span class="p_add">+	struct vm_area_struct *vma = fe-&gt;vma;</span>
 	struct page *fault_page;
<span class="p_del">-	spinlock_t *ptl;</span>
<span class="p_del">-	pte_t *pte;</span>
 	int ret = 0;
 
 	/*
<span class="p_chunk">@@ -2980,64 +2965,64 @@</span> <span class="p_context"> static int do_read_fault(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 	 * something).
 	 */
 	if (vma-&gt;vm_ops-&gt;map_pages &amp;&amp; fault_around_bytes &gt;&gt; PAGE_SHIFT &gt; 1) {
<span class="p_del">-		pte = pte_offset_map_lock(mm, pmd, address, &amp;ptl);</span>
<span class="p_del">-		do_fault_around(vma, address, pte, pgoff, flags);</span>
<span class="p_del">-		if (!pte_same(*pte, orig_pte))</span>
<span class="p_add">+		fe-&gt;pte = pte_offset_map_lock(vma-&gt;vm_mm, fe-&gt;pmd, fe-&gt;address,</span>
<span class="p_add">+				&amp;fe-&gt;ptl);</span>
<span class="p_add">+		do_fault_around(fe, pgoff);</span>
<span class="p_add">+		if (!pte_same(*fe-&gt;pte, orig_pte))</span>
 			goto unlock_out;
<span class="p_del">-		pte_unmap_unlock(pte, ptl);</span>
<span class="p_add">+		pte_unmap_unlock(fe-&gt;pte, fe-&gt;ptl);</span>
 	}
 
<span class="p_del">-	ret = __do_fault(vma, address, pgoff, flags, NULL, &amp;fault_page);</span>
<span class="p_add">+	ret = __do_fault(fe, pgoff, NULL, &amp;fault_page);</span>
 	if (unlikely(ret &amp; (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY)))
 		return ret;
 
<span class="p_del">-	pte = pte_offset_map_lock(mm, pmd, address, &amp;ptl);</span>
<span class="p_del">-	if (unlikely(!pte_same(*pte, orig_pte))) {</span>
<span class="p_del">-		pte_unmap_unlock(pte, ptl);</span>
<span class="p_add">+	fe-&gt;pte = pte_offset_map_lock(vma-&gt;vm_mm, fe-&gt;pmd, fe-&gt;address, &amp;fe-&gt;ptl);</span>
<span class="p_add">+	if (unlikely(!pte_same(*fe-&gt;pte, orig_pte))) {</span>
<span class="p_add">+		pte_unmap_unlock(fe-&gt;pte, fe-&gt;ptl);</span>
 		unlock_page(fault_page);
 		put_page(fault_page);
 		return ret;
 	}
<span class="p_del">-	do_set_pte(vma, address, fault_page, pte, false, false);</span>
<span class="p_add">+	do_set_pte(fe, fault_page);</span>
 	unlock_page(fault_page);
 unlock_out:
<span class="p_del">-	pte_unmap_unlock(pte, ptl);</span>
<span class="p_add">+	pte_unmap_unlock(fe-&gt;pte, fe-&gt;ptl);</span>
 	return ret;
 }
 
<span class="p_del">-static int do_cow_fault(struct mm_struct *mm, struct vm_area_struct *vma,</span>
<span class="p_del">-		unsigned long address, pmd_t *pmd,</span>
<span class="p_del">-		pgoff_t pgoff, unsigned int flags, pte_t orig_pte)</span>
<span class="p_add">+static int do_cow_fault(struct fault_env *fe, pgoff_t pgoff, pte_t orig_pte)</span>
 {
<span class="p_add">+	struct vm_area_struct *vma = fe-&gt;vma;</span>
 	struct page *fault_page, *new_page;
 	struct mem_cgroup *memcg;
<span class="p_del">-	spinlock_t *ptl;</span>
<span class="p_del">-	pte_t *pte;</span>
 	int ret;
 
 	if (unlikely(anon_vma_prepare(vma)))
 		return VM_FAULT_OOM;
 
<span class="p_del">-	new_page = alloc_page_vma(GFP_HIGHUSER_MOVABLE, vma, address);</span>
<span class="p_add">+	new_page = alloc_page_vma(GFP_HIGHUSER_MOVABLE, vma, fe-&gt;address);</span>
 	if (!new_page)
 		return VM_FAULT_OOM;
 
<span class="p_del">-	if (mem_cgroup_try_charge(new_page, mm, GFP_KERNEL, &amp;memcg, false)) {</span>
<span class="p_add">+	if (mem_cgroup_try_charge(new_page, vma-&gt;vm_mm, GFP_KERNEL,</span>
<span class="p_add">+				&amp;memcg, false)) {</span>
 		put_page(new_page);
 		return VM_FAULT_OOM;
 	}
 
<span class="p_del">-	ret = __do_fault(vma, address, pgoff, flags, new_page, &amp;fault_page);</span>
<span class="p_add">+	ret = __do_fault(fe, pgoff, new_page, &amp;fault_page);</span>
 	if (unlikely(ret &amp; (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY)))
 		goto uncharge_out;
 
 	if (fault_page)
<span class="p_del">-		copy_user_highpage(new_page, fault_page, address, vma);</span>
<span class="p_add">+		copy_user_highpage(new_page, fault_page, fe-&gt;address, vma);</span>
 	__SetPageUptodate(new_page);
 
<span class="p_del">-	pte = pte_offset_map_lock(mm, pmd, address, &amp;ptl);</span>
<span class="p_del">-	if (unlikely(!pte_same(*pte, orig_pte))) {</span>
<span class="p_del">-		pte_unmap_unlock(pte, ptl);</span>
<span class="p_add">+	fe-&gt;pte = pte_offset_map_lock(vma-&gt;vm_mm, fe-&gt;pmd, fe-&gt;address,</span>
<span class="p_add">+			&amp;fe-&gt;ptl);</span>
<span class="p_add">+	if (unlikely(!pte_same(*fe-&gt;pte, orig_pte))) {</span>
<span class="p_add">+		pte_unmap_unlock(fe-&gt;pte, fe-&gt;ptl);</span>
 		if (fault_page) {
 			unlock_page(fault_page);
 			put_page(fault_page);
<span class="p_chunk">@@ -3050,10 +3035,10 @@</span> <span class="p_context"> static int do_cow_fault(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 		}
 		goto uncharge_out;
 	}
<span class="p_del">-	do_set_pte(vma, address, new_page, pte, true, true);</span>
<span class="p_add">+	do_set_pte(fe, new_page);</span>
 	mem_cgroup_commit_charge(new_page, memcg, false, false);
 	lru_cache_add_active_or_unevictable(new_page, vma);
<span class="p_del">-	pte_unmap_unlock(pte, ptl);</span>
<span class="p_add">+	pte_unmap_unlock(fe-&gt;pte, fe-&gt;ptl);</span>
 	if (fault_page) {
 		unlock_page(fault_page);
 		put_page(fault_page);
<span class="p_chunk">@@ -3071,18 +3056,15 @@</span> <span class="p_context"> uncharge_out:</span>
 	return ret;
 }
 
<span class="p_del">-static int do_shared_fault(struct mm_struct *mm, struct vm_area_struct *vma,</span>
<span class="p_del">-		unsigned long address, pmd_t *pmd,</span>
<span class="p_del">-		pgoff_t pgoff, unsigned int flags, pte_t orig_pte)</span>
<span class="p_add">+static int do_shared_fault(struct fault_env *fe, pgoff_t pgoff, pte_t orig_pte)</span>
 {
<span class="p_add">+	struct vm_area_struct *vma = fe-&gt;vma;</span>
 	struct page *fault_page;
 	struct address_space *mapping;
<span class="p_del">-	spinlock_t *ptl;</span>
<span class="p_del">-	pte_t *pte;</span>
 	int dirtied = 0;
 	int ret, tmp;
 
<span class="p_del">-	ret = __do_fault(vma, address, pgoff, flags, NULL, &amp;fault_page);</span>
<span class="p_add">+	ret = __do_fault(fe, pgoff, NULL, &amp;fault_page);</span>
 	if (unlikely(ret &amp; (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY)))
 		return ret;
 
<span class="p_chunk">@@ -3092,7 +3074,7 @@</span> <span class="p_context"> static int do_shared_fault(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 	 */
 	if (vma-&gt;vm_ops-&gt;page_mkwrite) {
 		unlock_page(fault_page);
<span class="p_del">-		tmp = do_page_mkwrite(vma, fault_page, address);</span>
<span class="p_add">+		tmp = do_page_mkwrite(vma, fault_page, fe-&gt;address);</span>
 		if (unlikely(!tmp ||
 				(tmp &amp; (VM_FAULT_ERROR | VM_FAULT_NOPAGE)))) {
 			put_page(fault_page);
<span class="p_chunk">@@ -3100,15 +3082,16 @@</span> <span class="p_context"> static int do_shared_fault(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 		}
 	}
 
<span class="p_del">-	pte = pte_offset_map_lock(mm, pmd, address, &amp;ptl);</span>
<span class="p_del">-	if (unlikely(!pte_same(*pte, orig_pte))) {</span>
<span class="p_del">-		pte_unmap_unlock(pte, ptl);</span>
<span class="p_add">+	fe-&gt;pte = pte_offset_map_lock(vma-&gt;vm_mm, fe-&gt;pmd, fe-&gt;address,</span>
<span class="p_add">+			&amp;fe-&gt;ptl);</span>
<span class="p_add">+	if (unlikely(!pte_same(*fe-&gt;pte, orig_pte))) {</span>
<span class="p_add">+		pte_unmap_unlock(fe-&gt;pte, fe-&gt;ptl);</span>
 		unlock_page(fault_page);
 		put_page(fault_page);
 		return ret;
 	}
<span class="p_del">-	do_set_pte(vma, address, fault_page, pte, true, false);</span>
<span class="p_del">-	pte_unmap_unlock(pte, ptl);</span>
<span class="p_add">+	do_set_pte(fe, fault_page);</span>
<span class="p_add">+	pte_unmap_unlock(fe-&gt;pte, fe-&gt;ptl);</span>
 
 	if (set_page_dirty(fault_page))
 		dirtied = 1;
<span class="p_chunk">@@ -3140,23 +3123,20 @@</span> <span class="p_context"> static int do_shared_fault(struct mm_struct *mm, struct vm_area_struct *vma,</span>
  * The mmap_sem may have been released depending on flags and our
  * return value.  See filemap_fault() and __lock_page_or_retry().
  */
<span class="p_del">-static int do_fault(struct mm_struct *mm, struct vm_area_struct *vma,</span>
<span class="p_del">-		unsigned long address, pte_t *page_table, pmd_t *pmd,</span>
<span class="p_del">-		unsigned int flags, pte_t orig_pte)</span>
<span class="p_add">+static int do_fault(struct fault_env *fe, pte_t orig_pte)</span>
 {
<span class="p_del">-	pgoff_t pgoff = linear_page_index(vma, address);</span>
<span class="p_add">+	struct vm_area_struct *vma = fe-&gt;vma;</span>
<span class="p_add">+	pgoff_t pgoff = linear_page_index(vma, fe-&gt;address);</span>
 
<span class="p_del">-	pte_unmap(page_table);</span>
<span class="p_add">+	pte_unmap(fe-&gt;pte);</span>
 	/* The VMA was not fully populated on mmap() or missing VM_DONTEXPAND */
 	if (!vma-&gt;vm_ops-&gt;fault)
 		return VM_FAULT_SIGBUS;
<span class="p_del">-	if (!(flags &amp; FAULT_FLAG_WRITE))</span>
<span class="p_del">-		return do_read_fault(mm, vma, address, pmd, pgoff, flags,</span>
<span class="p_del">-				orig_pte);</span>
<span class="p_add">+	if (!(fe-&gt;flags &amp; FAULT_FLAG_WRITE))</span>
<span class="p_add">+		return do_read_fault(fe, pgoff,	orig_pte);</span>
 	if (!(vma-&gt;vm_flags &amp; VM_SHARED))
<span class="p_del">-		return do_cow_fault(mm, vma, address, pmd, pgoff, flags,</span>
<span class="p_del">-				orig_pte);</span>
<span class="p_del">-	return do_shared_fault(mm, vma, address, pmd, pgoff, flags, orig_pte);</span>
<span class="p_add">+		return do_cow_fault(fe, pgoff, orig_pte);</span>
<span class="p_add">+	return do_shared_fault(fe, pgoff, orig_pte);</span>
 }
 
 static int numa_migrate_prep(struct page *page, struct vm_area_struct *vma,
<span class="p_chunk">@@ -3174,11 +3154,10 @@</span> <span class="p_context"> static int numa_migrate_prep(struct page *page, struct vm_area_struct *vma,</span>
 	return mpol_misplaced(page, vma, addr);
 }
 
<span class="p_del">-static int do_numa_page(struct mm_struct *mm, struct vm_area_struct *vma,</span>
<span class="p_del">-		   unsigned long addr, pte_t pte, pte_t *ptep, pmd_t *pmd)</span>
<span class="p_add">+static int do_numa_page(struct fault_env *fe, pte_t pte)</span>
 {
<span class="p_add">+	struct vm_area_struct *vma = fe-&gt;vma;</span>
 	struct page *page = NULL;
<span class="p_del">-	spinlock_t *ptl;</span>
 	int page_nid = -1;
 	int last_cpupid;
 	int target_nid;
<span class="p_chunk">@@ -3198,10 +3177,10 @@</span> <span class="p_context"> static int do_numa_page(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 	* page table entry is not accessible, so there would be no
 	* concurrent hardware modifications to the PTE.
 	*/
<span class="p_del">-	ptl = pte_lockptr(mm, pmd);</span>
<span class="p_del">-	spin_lock(ptl);</span>
<span class="p_del">-	if (unlikely(!pte_same(*ptep, pte))) {</span>
<span class="p_del">-		pte_unmap_unlock(ptep, ptl);</span>
<span class="p_add">+	fe-&gt;ptl = pte_lockptr(vma-&gt;vm_mm, fe-&gt;pmd);</span>
<span class="p_add">+	spin_lock(fe-&gt;ptl);</span>
<span class="p_add">+	if (unlikely(!pte_same(*fe-&gt;pte, pte))) {</span>
<span class="p_add">+		pte_unmap_unlock(fe-&gt;pte, fe-&gt;ptl);</span>
 		goto out;
 	}
 
<span class="p_chunk">@@ -3210,18 +3189,18 @@</span> <span class="p_context"> static int do_numa_page(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 	pte = pte_mkyoung(pte);
 	if (was_writable)
 		pte = pte_mkwrite(pte);
<span class="p_del">-	set_pte_at(mm, addr, ptep, pte);</span>
<span class="p_del">-	update_mmu_cache(vma, addr, ptep);</span>
<span class="p_add">+	set_pte_at(vma-&gt;vm_mm, fe-&gt;address, fe-&gt;pte, pte);</span>
<span class="p_add">+	update_mmu_cache(vma, fe-&gt;address, fe-&gt;pte);</span>
 
<span class="p_del">-	page = vm_normal_page(vma, addr, pte);</span>
<span class="p_add">+	page = vm_normal_page(vma, fe-&gt;address, pte);</span>
 	if (!page) {
<span class="p_del">-		pte_unmap_unlock(ptep, ptl);</span>
<span class="p_add">+		pte_unmap_unlock(fe-&gt;pte, fe-&gt;ptl);</span>
 		return 0;
 	}
 
 	/* TODO: handle PTE-mapped THP */
 	if (PageCompound(page)) {
<span class="p_del">-		pte_unmap_unlock(ptep, ptl);</span>
<span class="p_add">+		pte_unmap_unlock(fe-&gt;pte, fe-&gt;ptl);</span>
 		return 0;
 	}
 
<span class="p_chunk">@@ -3245,8 +3224,9 @@</span> <span class="p_context"> static int do_numa_page(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 
 	last_cpupid = page_cpupid_last(page);
 	page_nid = page_to_nid(page);
<span class="p_del">-	target_nid = numa_migrate_prep(page, vma, addr, page_nid, &amp;flags);</span>
<span class="p_del">-	pte_unmap_unlock(ptep, ptl);</span>
<span class="p_add">+	target_nid = numa_migrate_prep(page, vma, fe-&gt;address, page_nid,</span>
<span class="p_add">+			&amp;flags);</span>
<span class="p_add">+	pte_unmap_unlock(fe-&gt;pte, fe-&gt;ptl);</span>
 	if (target_nid == -1) {
 		put_page(page);
 		goto out;
<span class="p_chunk">@@ -3266,24 +3246,24 @@</span> <span class="p_context"> out:</span>
 	return 0;
 }
 
<span class="p_del">-static int create_huge_pmd(struct mm_struct *mm, struct vm_area_struct *vma,</span>
<span class="p_del">-			unsigned long address, pmd_t *pmd, unsigned int flags)</span>
<span class="p_add">+static int create_huge_pmd(struct fault_env *fe)</span>
 {
<span class="p_add">+	struct vm_area_struct *vma = fe-&gt;vma;</span>
 	if (vma_is_anonymous(vma))
<span class="p_del">-		return do_huge_pmd_anonymous_page(mm, vma, address, pmd, flags);</span>
<span class="p_add">+		return do_huge_pmd_anonymous_page(fe);</span>
 	if (vma-&gt;vm_ops-&gt;pmd_fault)
<span class="p_del">-		return vma-&gt;vm_ops-&gt;pmd_fault(vma, address, pmd, flags);</span>
<span class="p_add">+		return vma-&gt;vm_ops-&gt;pmd_fault(vma, fe-&gt;address, fe-&gt;pmd,</span>
<span class="p_add">+				fe-&gt;flags);</span>
 	return VM_FAULT_FALLBACK;
 }
 
<span class="p_del">-static int wp_huge_pmd(struct mm_struct *mm, struct vm_area_struct *vma,</span>
<span class="p_del">-			unsigned long address, pmd_t *pmd, pmd_t orig_pmd,</span>
<span class="p_del">-			unsigned int flags)</span>
<span class="p_add">+static int wp_huge_pmd(struct fault_env *fe, pmd_t orig_pmd)</span>
 {
<span class="p_del">-	if (vma_is_anonymous(vma))</span>
<span class="p_del">-		return do_huge_pmd_wp_page(mm, vma, address, pmd, orig_pmd);</span>
<span class="p_del">-	if (vma-&gt;vm_ops-&gt;pmd_fault)</span>
<span class="p_del">-		return vma-&gt;vm_ops-&gt;pmd_fault(vma, address, pmd, flags);</span>
<span class="p_add">+	if (vma_is_anonymous(fe-&gt;vma))</span>
<span class="p_add">+		return do_huge_pmd_wp_page(fe, orig_pmd);</span>
<span class="p_add">+	if (fe-&gt;vma-&gt;vm_ops-&gt;pmd_fault)</span>
<span class="p_add">+		return fe-&gt;vma-&gt;vm_ops-&gt;pmd_fault(fe-&gt;vma, fe-&gt;address, fe-&gt;pmd,</span>
<span class="p_add">+				fe-&gt;flags);</span>
 	return VM_FAULT_FALLBACK;
 }
 
<span class="p_chunk">@@ -3303,12 +3283,9 @@</span> <span class="p_context"> static int wp_huge_pmd(struct mm_struct *mm, struct vm_area_struct *vma,</span>
  * The mmap_sem may have been released depending on flags and our
  * return value.  See filemap_fault() and __lock_page_or_retry().
  */
<span class="p_del">-static int handle_pte_fault(struct mm_struct *mm,</span>
<span class="p_del">-		     struct vm_area_struct *vma, unsigned long address,</span>
<span class="p_del">-		     pte_t *pte, pmd_t *pmd, unsigned int flags)</span>
<span class="p_add">+static int handle_pte_fault(struct fault_env *fe)</span>
 {
 	pte_t entry;
<span class="p_del">-	spinlock_t *ptl;</span>
 
 	/*
 	 * some architectures can have larger ptes than wordsize,
<span class="p_chunk">@@ -3318,37 +3295,34 @@</span> <span class="p_context"> static int handle_pte_fault(struct mm_struct *mm,</span>
 	 * we later double check anyway with the ptl lock held. So here
 	 * a barrier will do.
 	 */
<span class="p_del">-	entry = *pte;</span>
<span class="p_add">+	entry = *fe-&gt;pte;</span>
 	barrier();
 	if (!pte_present(entry)) {
 		if (pte_none(entry)) {
<span class="p_del">-			if (vma_is_anonymous(vma))</span>
<span class="p_del">-				return do_anonymous_page(mm, vma, address,</span>
<span class="p_del">-							 pte, pmd, flags);</span>
<span class="p_add">+			if (vma_is_anonymous(fe-&gt;vma))</span>
<span class="p_add">+				return do_anonymous_page(fe);</span>
 			else
<span class="p_del">-				return do_fault(mm, vma, address, pte, pmd,</span>
<span class="p_del">-						flags, entry);</span>
<span class="p_add">+				return do_fault(fe, entry);</span>
 		}
<span class="p_del">-		return do_swap_page(mm, vma, address,</span>
<span class="p_del">-					pte, pmd, flags, entry);</span>
<span class="p_add">+		return do_swap_page(fe, entry);</span>
 	}
 
 	if (pte_protnone(entry))
<span class="p_del">-		return do_numa_page(mm, vma, address, entry, pte, pmd);</span>
<span class="p_add">+		return do_numa_page(fe, entry);</span>
 
<span class="p_del">-	ptl = pte_lockptr(mm, pmd);</span>
<span class="p_del">-	spin_lock(ptl);</span>
<span class="p_del">-	if (unlikely(!pte_same(*pte, entry)))</span>
<span class="p_add">+	fe-&gt;ptl = pte_lockptr(fe-&gt;vma-&gt;vm_mm, fe-&gt;pmd);</span>
<span class="p_add">+	spin_lock(fe-&gt;ptl);</span>
<span class="p_add">+	if (unlikely(!pte_same(*fe-&gt;pte, entry)))</span>
 		goto unlock;
<span class="p_del">-	if (flags &amp; FAULT_FLAG_WRITE) {</span>
<span class="p_add">+	if (fe-&gt;flags &amp; FAULT_FLAG_WRITE) {</span>
 		if (!pte_write(entry))
<span class="p_del">-			return do_wp_page(mm, vma, address,</span>
<span class="p_del">-					pte, pmd, ptl, entry);</span>
<span class="p_add">+			return do_wp_page(fe, entry);</span>
 		entry = pte_mkdirty(entry);
 	}
 	entry = pte_mkyoung(entry);
<span class="p_del">-	if (ptep_set_access_flags(vma, address, pte, entry, flags &amp; FAULT_FLAG_WRITE)) {</span>
<span class="p_del">-		update_mmu_cache(vma, address, pte);</span>
<span class="p_add">+	if (ptep_set_access_flags(fe-&gt;vma, fe-&gt;address, fe-&gt;pte, entry,</span>
<span class="p_add">+				fe-&gt;flags &amp; FAULT_FLAG_WRITE)) {</span>
<span class="p_add">+		update_mmu_cache(fe-&gt;vma, fe-&gt;address, fe-&gt;pte);</span>
 	} else {
 		/*
 		 * This is needed only for protection faults but the arch code
<span class="p_chunk">@@ -3356,11 +3330,11 @@</span> <span class="p_context"> static int handle_pte_fault(struct mm_struct *mm,</span>
 		 * This still avoids useless tlb flushes for .text page faults
 		 * with threads.
 		 */
<span class="p_del">-		if (flags &amp; FAULT_FLAG_WRITE)</span>
<span class="p_del">-			flush_tlb_fix_spurious_fault(vma, address);</span>
<span class="p_add">+		if (fe-&gt;flags &amp; FAULT_FLAG_WRITE)</span>
<span class="p_add">+			flush_tlb_fix_spurious_fault(fe-&gt;vma, fe-&gt;address);</span>
 	}
 unlock:
<span class="p_del">-	pte_unmap_unlock(pte, ptl);</span>
<span class="p_add">+	pte_unmap_unlock(fe-&gt;pte, fe-&gt;ptl);</span>
 	return 0;
 }
 
<span class="p_chunk">@@ -3373,51 +3347,42 @@</span> <span class="p_context"> unlock:</span>
 static int __handle_mm_fault(struct vm_area_struct *vma, unsigned long address,
 		unsigned int flags)
 {
<span class="p_add">+	struct fault_env fe = {</span>
<span class="p_add">+		.vma = vma,</span>
<span class="p_add">+		.address = address,</span>
<span class="p_add">+		.flags = flags,</span>
<span class="p_add">+	};</span>
 	struct mm_struct *mm = vma-&gt;vm_mm;
 	pgd_t *pgd;
 	pud_t *pud;
<span class="p_del">-	pmd_t *pmd;</span>
<span class="p_del">-	pte_t *pte;</span>
<span class="p_del">-</span>
<span class="p_del">-	if (!arch_vma_access_permitted(vma, flags &amp; FAULT_FLAG_WRITE,</span>
<span class="p_del">-					    flags &amp; FAULT_FLAG_INSTRUCTION,</span>
<span class="p_del">-					    flags &amp; FAULT_FLAG_REMOTE))</span>
<span class="p_del">-		return VM_FAULT_SIGSEGV;</span>
<span class="p_del">-</span>
<span class="p_del">-	if (unlikely(is_vm_hugetlb_page(vma)))</span>
<span class="p_del">-		return hugetlb_fault(mm, vma, address, flags);</span>
 
 	pgd = pgd_offset(mm, address);
 	pud = pud_alloc(mm, pgd, address);
 	if (!pud)
 		return VM_FAULT_OOM;
<span class="p_del">-	pmd = pmd_alloc(mm, pud, address);</span>
<span class="p_del">-	if (!pmd)</span>
<span class="p_add">+	fe.pmd = pmd_alloc(mm, pud, address);</span>
<span class="p_add">+	if (!fe.pmd)</span>
 		return VM_FAULT_OOM;
<span class="p_del">-	if (pmd_none(*pmd) &amp;&amp; transparent_hugepage_enabled(vma)) {</span>
<span class="p_del">-		int ret = create_huge_pmd(mm, vma, address, pmd, flags);</span>
<span class="p_add">+	if (pmd_none(*fe.pmd) &amp;&amp; transparent_hugepage_enabled(vma)) {</span>
<span class="p_add">+		int ret = create_huge_pmd(&amp;fe);</span>
 		if (!(ret &amp; VM_FAULT_FALLBACK))
 			return ret;
 	} else {
<span class="p_del">-		pmd_t orig_pmd = *pmd;</span>
<span class="p_add">+		pmd_t orig_pmd = *fe.pmd;</span>
 		int ret;
 
 		barrier();
 		if (pmd_trans_huge(orig_pmd) || pmd_devmap(orig_pmd)) {
<span class="p_del">-			unsigned int dirty = flags &amp; FAULT_FLAG_WRITE;</span>
<span class="p_del">-</span>
 			if (pmd_protnone(orig_pmd))
<span class="p_del">-				return do_huge_pmd_numa_page(mm, vma, address,</span>
<span class="p_del">-							     orig_pmd, pmd);</span>
<span class="p_add">+				return do_huge_pmd_numa_page(&amp;fe, orig_pmd);</span>
 
<span class="p_del">-			if (dirty &amp;&amp; !pmd_write(orig_pmd)) {</span>
<span class="p_del">-				ret = wp_huge_pmd(mm, vma, address, pmd,</span>
<span class="p_del">-							orig_pmd, flags);</span>
<span class="p_add">+			if ((fe.flags &amp; FAULT_FLAG_WRITE) &amp;&amp;</span>
<span class="p_add">+					!pmd_write(orig_pmd)) {</span>
<span class="p_add">+				ret = wp_huge_pmd(&amp;fe, orig_pmd);</span>
 				if (!(ret &amp; VM_FAULT_FALLBACK))
 					return ret;
 			} else {
<span class="p_del">-				huge_pmd_set_accessed(mm, vma, address, pmd,</span>
<span class="p_del">-						      orig_pmd, dirty);</span>
<span class="p_add">+				huge_pmd_set_accessed(&amp;fe, orig_pmd);</span>
 				return 0;
 			}
 		}
<span class="p_chunk">@@ -3428,7 +3393,7 @@</span> <span class="p_context"> static int __handle_mm_fault(struct vm_area_struct *vma, unsigned long address,</span>
 	 * run pte_offset_map on the pmd, if an huge pmd could
 	 * materialize from under us from a different thread.
 	 */
<span class="p_del">-	if (unlikely(pte_alloc(mm, pmd, address)))</span>
<span class="p_add">+	if (unlikely(pte_alloc(fe.vma-&gt;vm_mm, fe.pmd, fe.address)))</span>
 		return VM_FAULT_OOM;
 	/*
 	 * If a huge pmd materialized under us just retry later.  Use
<span class="p_chunk">@@ -3441,7 +3406,7 @@</span> <span class="p_context"> static int __handle_mm_fault(struct vm_area_struct *vma, unsigned long address,</span>
 	 * through an atomic read in C, which is what pmd_trans_unstable()
 	 * provides.
 	 */
<span class="p_del">-	if (unlikely(pmd_trans_unstable(pmd) || pmd_devmap(*pmd)))</span>
<span class="p_add">+	if (unlikely(pmd_trans_unstable(fe.pmd) || pmd_devmap(*fe.pmd)))</span>
 		return 0;
 	/*
 	 * A regular pmd is established and it can&#39;t morph into a huge pmd
<span class="p_chunk">@@ -3449,9 +3414,9 @@</span> <span class="p_context"> static int __handle_mm_fault(struct vm_area_struct *vma, unsigned long address,</span>
 	 * read mode and khugepaged takes it in write mode. So now it&#39;s
 	 * safe to run pte_offset_map().
 	 */
<span class="p_del">-	pte = pte_offset_map(pmd, address);</span>
<span class="p_add">+	fe.pte = pte_offset_map(fe.pmd, fe.address);</span>
 
<span class="p_del">-	return handle_pte_fault(mm, vma, address, pte, pmd, flags);</span>
<span class="p_add">+	return handle_pte_fault(&amp;fe);</span>
 }
 
 /*
<span class="p_chunk">@@ -3480,7 +3445,15 @@</span> <span class="p_context"> int handle_mm_fault(struct vm_area_struct *vma, unsigned long address,</span>
 	if (flags &amp; FAULT_FLAG_USER)
 		mem_cgroup_oom_enable();
 
<span class="p_del">-	ret = __handle_mm_fault(vma, address, flags);</span>
<span class="p_add">+	if (!arch_vma_access_permitted(vma, flags &amp; FAULT_FLAG_WRITE,</span>
<span class="p_add">+					    flags &amp; FAULT_FLAG_INSTRUCTION,</span>
<span class="p_add">+					    flags &amp; FAULT_FLAG_REMOTE))</span>
<span class="p_add">+		return VM_FAULT_SIGSEGV;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (unlikely(is_vm_hugetlb_page(vma)))</span>
<span class="p_add">+		ret = hugetlb_fault(vma-&gt;vm_mm, vma, address, flags);</span>
<span class="p_add">+	else</span>
<span class="p_add">+		ret = __handle_mm_fault(vma, address, flags);</span>
 
 	if (flags &amp; FAULT_FLAG_USER) {
 		mem_cgroup_oom_disable();
<span class="p_header">diff --git a/mm/nommu.c b/mm/nommu.c</span>
<span class="p_header">index 102e257cc6c3..146c2139f506 100644</span>
<span class="p_header">--- a/mm/nommu.c</span>
<span class="p_header">+++ b/mm/nommu.c</span>
<span class="p_chunk">@@ -1811,7 +1811,8 @@</span> <span class="p_context"> int filemap_fault(struct vm_area_struct *vma, struct vm_fault *vmf)</span>
 }
 EXPORT_SYMBOL(filemap_fault);
 
<span class="p_del">-void filemap_map_pages(struct vm_area_struct *vma, struct vm_fault *vmf)</span>
<span class="p_add">+void filemap_map_pages(struct fault_env *fe,</span>
<span class="p_add">+		pgoff_t start_pgoff, pgoff_t end_pgoff)</span>
 {
 	BUG();
 }

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



