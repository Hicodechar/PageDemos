
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>mm: Flush the TLB for a single address in a huge page - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    mm: Flush the TLB for a single address in a huge page</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=352">Catalin Marinas</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>July 22, 2015, 5:13 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1437585214-22481-1-git-send-email-catalin.marinas@arm.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/6845441/mbox/"
   >mbox</a>
|
   <a href="/patch/6845441/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/6845441/">/patch/6845441/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork1.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork1.web.kernel.org (Postfix) with ESMTP id 492369F1D4
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 22 Jul 2015 17:13:48 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 5D64F206C7
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 22 Jul 2015 17:13:47 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id 305A9206CC
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 22 Jul 2015 17:13:46 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S934873AbbGVRNn (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Wed, 22 Jul 2015 13:13:43 -0400
Received: from foss.arm.com ([217.140.101.70]:37068 &quot;EHLO foss.arm.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S934598AbbGVRNm (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Wed, 22 Jul 2015 13:13:42 -0400
Received: from usa-sjc-imap-foss1.foss.arm.com (unknown [10.72.51.249])
	by usa-sjc-mx-foss1.foss.arm.com (Postfix) with ESMTP id 26A5C75;
	Wed, 22 Jul 2015 10:13:56 -0700 (PDT)
Received: from e104818-lin.cambridge.arm.com
	(usa-sjc-imap-foss1.foss.arm.com [10.72.51.249])
	by usa-sjc-imap-foss1.foss.arm.com (Postfix) with ESMTPA id
	159083F23A; Wed, 22 Jul 2015 10:13:40 -0700 (PDT)
From: Catalin Marinas &lt;catalin.marinas@arm.com&gt;
To: linux-mm@kvack.org
Cc: linux-kernel@vger.kernel.org, Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	Andrea Arcangeli &lt;aarcange@redhat.com&gt;
Subject: [PATCH] mm: Flush the TLB for a single address in a huge page
Date: Wed, 22 Jul 2015 18:13:34 +0100
Message-Id: &lt;1437585214-22481-1-git-send-email-catalin.marinas@arm.com&gt;
X-Mailer: git-send-email 2.1.4
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-8.1 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=352">Catalin Marinas</a> - July 22, 2015, 5:13 p.m.</div>
<pre class="content">
When the page table entry is a huge page (and not a table), there is no
need to flush the TLB by range. This patch changes flush_tlb_range() to
flush_tlb_page() in functions where we know the pmd entry is a huge
page.
<span class="signed-off-by">
Signed-off-by: Catalin Marinas &lt;catalin.marinas@arm.com&gt;</span>
Cc: Andrew Morton &lt;akpm@linux-foundation.org&gt;
Cc: Andrea Arcangeli &lt;aarcange@redhat.com&gt;
---

Hi,

That&#39;s just a minor improvement but it saves iterating over each small
page in a huge page when a single TLB entry is used (we already have a
similar assumption in __tlb_adjust_range).

Thanks.

 mm/pgtable-generic.c | 10 +++++-----
 1 file changed, 5 insertions(+), 5 deletions(-)

--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=579">David Rientjes</a> - July 22, 2015, 9:39 p.m.</div>
<pre class="content">
On Wed, 22 Jul 2015, Catalin Marinas wrote:
<span class="quote">
&gt; When the page table entry is a huge page (and not a table), there is no</span>
<span class="quote">&gt; need to flush the TLB by range. This patch changes flush_tlb_range() to</span>
<span class="quote">&gt; flush_tlb_page() in functions where we know the pmd entry is a huge</span>
<span class="quote">&gt; page.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Catalin Marinas &lt;catalin.marinas@arm.com&gt;</span>
<span class="quote">&gt; Cc: Andrew Morton &lt;akpm@linux-foundation.org&gt;</span>
<span class="quote">&gt; Cc: Andrea Arcangeli &lt;aarcange@redhat.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Hi,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; That&#39;s just a minor improvement but it saves iterating over each small</span>
<span class="quote">&gt; page in a huge page when a single TLB entry is used (we already have a</span>
<span class="quote">&gt; similar assumption in __tlb_adjust_range).</span>
<span class="quote">&gt; </span>

For x86 smp, this seems to mean the difference between unconditional 
flush_tlb_page() and local_flush_tlb() due to 
tlb_single_page_flush_ceiling, so I don&#39;t think this just removes the 
iteration.
<span class="quote">
&gt; Thanks.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  mm/pgtable-generic.c | 10 +++++-----</span>
<span class="quote">&gt;  1 file changed, 5 insertions(+), 5 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/mm/pgtable-generic.c b/mm/pgtable-generic.c</span>
<span class="quote">&gt; index 6b674e00153c..ff17eca26211 100644</span>
<span class="quote">&gt; --- a/mm/pgtable-generic.c</span>
<span class="quote">&gt; +++ b/mm/pgtable-generic.c</span>
<span class="quote">&gt; @@ -67,7 +67,7 @@ int pmdp_set_access_flags(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  	VM_BUG_ON(address &amp; ~HPAGE_PMD_MASK);</span>
<span class="quote">&gt;  	if (changed) {</span>
<span class="quote">&gt;  		set_pmd_at(vma-&gt;vm_mm, address, pmdp, entry);</span>
<span class="quote">&gt; -		flush_tlb_range(vma, address, address + HPAGE_PMD_SIZE);</span>
<span class="quote">&gt; +		flush_tlb_page(vma, address);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  	return changed;</span>
<span class="quote">&gt;  #else /* CONFIG_TRANSPARENT_HUGEPAGE */</span>
<span class="quote">&gt; @@ -101,7 +101,7 @@ int pmdp_clear_flush_young(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  #endif /* CONFIG_TRANSPARENT_HUGEPAGE */</span>
<span class="quote">&gt;  	young = pmdp_test_and_clear_young(vma, address, pmdp);</span>
<span class="quote">&gt;  	if (young)</span>
<span class="quote">&gt; -		flush_tlb_range(vma, address, address + HPAGE_PMD_SIZE);</span>
<span class="quote">&gt; +		flush_tlb_page(vma, address);</span>
<span class="quote">&gt;  	return young;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt; @@ -128,7 +128,7 @@ pmd_t pmdp_huge_clear_flush(struct vm_area_struct *vma, unsigned long address,</span>
<span class="quote">&gt;  	VM_BUG_ON(address &amp; ~HPAGE_PMD_MASK);</span>
<span class="quote">&gt;  	VM_BUG_ON(!pmd_trans_huge(*pmdp));</span>
<span class="quote">&gt;  	pmd = pmdp_huge_get_and_clear(vma-&gt;vm_mm, address, pmdp);</span>
<span class="quote">&gt; -	flush_tlb_range(vma, address, address + HPAGE_PMD_SIZE);</span>
<span class="quote">&gt; +	flush_tlb_page(vma, address);</span>
<span class="quote">&gt;  	return pmd;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  #endif /* CONFIG_TRANSPARENT_HUGEPAGE */</span>
<span class="quote">&gt; @@ -143,7 +143,7 @@ void pmdp_splitting_flush(struct vm_area_struct *vma, unsigned long address,</span>
<span class="quote">&gt;  	VM_BUG_ON(address &amp; ~HPAGE_PMD_MASK);</span>
<span class="quote">&gt;  	set_pmd_at(vma-&gt;vm_mm, address, pmdp, pmd);</span>
<span class="quote">&gt;  	/* tlb flush only to serialize against gup-fast */</span>
<span class="quote">&gt; -	flush_tlb_range(vma, address, address + HPAGE_PMD_SIZE);</span>
<span class="quote">&gt; +	flush_tlb_page(vma, address);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  #endif /* CONFIG_TRANSPARENT_HUGEPAGE */</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt; @@ -195,7 +195,7 @@ void pmdp_invalidate(struct vm_area_struct *vma, unsigned long address,</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	pmd_t entry = *pmdp;</span>
<span class="quote">&gt;  	set_pmd_at(vma-&gt;vm_mm, address, pmdp, pmd_mknotpresent(entry));</span>
<span class="quote">&gt; -	flush_tlb_range(vma, address, address + HPAGE_PMD_SIZE);</span>
<span class="quote">&gt; +	flush_tlb_page(vma, address);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  #endif /* CONFIG_TRANSPARENT_HUGEPAGE */</span>
<span class="quote">&gt;  #endif</span>
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=352">Catalin Marinas</a> - July 22, 2015, 10:48 p.m.</div>
<pre class="content">
On 22 July 2015 at 22:39, David Rientjes &lt;rientjes@google.com&gt; wrote:
<span class="quote">&gt; On Wed, 22 Jul 2015, Catalin Marinas wrote:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; When the page table entry is a huge page (and not a table), there is no</span>
<span class="quote">&gt;&gt; need to flush the TLB by range. This patch changes flush_tlb_range() to</span>
<span class="quote">&gt;&gt; flush_tlb_page() in functions where we know the pmd entry is a huge</span>
<span class="quote">&gt;&gt; page.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Signed-off-by: Catalin Marinas &lt;catalin.marinas@arm.com&gt;</span>
<span class="quote">&gt;&gt; Cc: Andrew Morton &lt;akpm@linux-foundation.org&gt;</span>
<span class="quote">&gt;&gt; Cc: Andrea Arcangeli &lt;aarcange@redhat.com&gt;</span>
<span class="quote">&gt;&gt; ---</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Hi,</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; That&#39;s just a minor improvement but it saves iterating over each small</span>
<span class="quote">&gt;&gt; page in a huge page when a single TLB entry is used (we already have a</span>
<span class="quote">&gt;&gt; similar assumption in __tlb_adjust_range).</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; For x86 smp, this seems to mean the difference between unconditional</span>
<span class="quote">&gt; flush_tlb_page() and local_flush_tlb() due to</span>
<span class="quote">&gt; tlb_single_page_flush_ceiling, so I don&#39;t think this just removes the</span>
<span class="quote">&gt; iteration.</span>

You are right, on x86 the tlb_single_page_flush_ceiling seems to be
33, so for an HPAGE_SIZE range the code does a local_flush_tlb()
always. I would say a single page TLB flush is more efficient than a
whole TLB flush but I&#39;m not familiar enough with x86.

Alternatively, I could introduce a flush_tlb_pmd_huge_page (suggested
by Andrea separately) and let the architectures deal with this as they
see fit. The default definition would do a flush_tlb_range(vma,
address, address + HPAGE_SIZE). For arm64, I&#39;ll define it as
flush_tlb_page(vma, address).
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=65121">Dave Hansen</a> - July 22, 2015, 11:05 p.m.</div>
<pre class="content">
On 07/22/2015 03:48 PM, Catalin Marinas wrote:
<span class="quote">&gt; You are right, on x86 the tlb_single_page_flush_ceiling seems to be</span>
<span class="quote">&gt; 33, so for an HPAGE_SIZE range the code does a local_flush_tlb()</span>
<span class="quote">&gt; always. I would say a single page TLB flush is more efficient than a</span>
<span class="quote">&gt; whole TLB flush but I&#39;m not familiar enough with x86.</span>

The last time I looked, the instruction to invalidate a single page is
more expensive than the instruction to flush the entire TLB.  We also
don&#39;t bother doing ranged flushes _ever_ for hugetlbfs TLB
invalidations, but that was just because the work done around commit
e7b52ffd4 didn&#39;t see any benefit.

That said, I can&#39;t imagine this will hurt anything.  We also have TLBs
that can mix 2M and 4k pages and I don&#39;t think we did back when we put
that code in originally.

--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=352">Catalin Marinas</a> - July 23, 2015, 10:49 a.m.</div>
<pre class="content">
On Thu, Jul 23, 2015 at 12:05:21AM +0100, Dave Hansen wrote:
<span class="quote">&gt; On 07/22/2015 03:48 PM, Catalin Marinas wrote:</span>
<span class="quote">&gt; &gt; You are right, on x86 the tlb_single_page_flush_ceiling seems to be</span>
<span class="quote">&gt; &gt; 33, so for an HPAGE_SIZE range the code does a local_flush_tlb()</span>
<span class="quote">&gt; &gt; always. I would say a single page TLB flush is more efficient than a</span>
<span class="quote">&gt; &gt; whole TLB flush but I&#39;m not familiar enough with x86.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The last time I looked, the instruction to invalidate a single page is</span>
<span class="quote">&gt; more expensive than the instruction to flush the entire TLB. </span>

I was thinking of the overall cost of re-populating the TLB after being
nuked rather than the instruction itself.
<span class="quote">
&gt; We also don&#39;t bother doing ranged flushes _ever_ for hugetlbfs TLB</span>
<span class="quote">&gt; invalidations, but that was just because the work done around commit</span>
<span class="quote">&gt; e7b52ffd4 didn&#39;t see any benefit.</span>

For huge pages, there are indeed fewer page table levels to fetch, so I
guess the impact is not significant. With virtualisation/nested pages,
at least on ARM, refilling the TLB for guest would take longer (though
it&#39;s highly dependent on the microarchitecture implementation, whether
it caches the guest PA to host PA separately).
<span class="quote">
&gt; That said, I can&#39;t imagine this will hurt anything.  We also have TLBs</span>
<span class="quote">&gt; that can mix 2M and 4k pages and I don&#39;t think we did back when we put</span>
<span class="quote">&gt; that code in originally.</span>

Another question is whether flushing a single address is enough for a
huge page. I assumed it is since tlb_remove_pmd_tlb_entry() only adjusts
the mmu_gather range by PAGE_SIZE (rather than HPAGE_SIZE) and no-one
complained so far. AFAICT, there are only 3 architectures that don&#39;t use
asm-generic/tlb.h but they all seem to handle this case:

arch/arm: it implements tlb_remove_pmd_tlb_entry() in a similar way to
the generic one

arch/s390: tlb_remove_pmd_tlb_entry() is a no-op

arch/ia64: does not support THP
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=965">Andrea Arcangeli</a> - July 23, 2015, 2:13 p.m.</div>
<pre class="content">
On Thu, Jul 23, 2015 at 11:49:38AM +0100, Catalin Marinas wrote:
<span class="quote">&gt; On Thu, Jul 23, 2015 at 12:05:21AM +0100, Dave Hansen wrote:</span>
<span class="quote">&gt; &gt; On 07/22/2015 03:48 PM, Catalin Marinas wrote:</span>
<span class="quote">&gt; &gt; &gt; You are right, on x86 the tlb_single_page_flush_ceiling seems to be</span>
<span class="quote">&gt; &gt; &gt; 33, so for an HPAGE_SIZE range the code does a local_flush_tlb()</span>
<span class="quote">&gt; &gt; &gt; always. I would say a single page TLB flush is more efficient than a</span>
<span class="quote">&gt; &gt; &gt; whole TLB flush but I&#39;m not familiar enough with x86.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; The last time I looked, the instruction to invalidate a single page is</span>
<span class="quote">&gt; &gt; more expensive than the instruction to flush the entire TLB. </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I was thinking of the overall cost of re-populating the TLB after being</span>
<span class="quote">&gt; nuked rather than the instruction itself.</span>

Unless I&#39;m not aware about timing differences in flushing 2MB TLB
entries vs flushing 4kb TLB entries with invlpg, the benchmarks that
have been run to tune the optimal tlb_single_page_flush_ceiling value,
should already guarantee us that this is a valid optimization (as we
just got one entry, we&#39;re not even close to the 33 ceiling that makes
it more a grey area).
<span class="quote">
&gt; &gt; That said, I can&#39;t imagine this will hurt anything.  We also have TLBs</span>
<span class="quote">&gt; &gt; that can mix 2M and 4k pages and I don&#39;t think we did back when we put</span>
<span class="quote">&gt; &gt; that code in originally.</span>

Dave, I&#39;m confused about this. We should still stick to an invariant
that we can&#39;t ever mix 2M and 4k TLB entries if their mappings end up
overlapping on the same physical memory (if this isn&#39;t enforced in
common code, some x86 implementation errata triggers, and it really
oopses with machine checks so it&#39;s not just theoretical). Perhaps I
misunderstood what you meant with mix 2M and 4k pages though.
<span class="quote">
&gt; Another question is whether flushing a single address is enough for a</span>
<span class="quote">&gt; huge page. I assumed it is since tlb_remove_pmd_tlb_entry() only adjusts</span>

That&#39;s the primary reason why the range flush was used currently (and
it must be still used in pmdp_collapse_flush as that deals with 4k TLB
entries, but your patch correctly isn&#39;t touching that one).

I recall having used flush_tlb_page initially for the 2MB invalidates,
but then I switched to the range version purely to be safer. If we can
optimize this now I&#39;d certainly be happy about that. Back then there
was not yet tlb_remove_pmd_tlb_entry which already started to optimize
things for this.
<span class="quote">
&gt; the mmu_gather range by PAGE_SIZE (rather than HPAGE_SIZE) and</span>
<span class="quote">&gt; no-one complained so far. AFAICT, there are only 3 architectures</span>
<span class="quote">&gt; that don&#39;t use asm-generic/tlb.h but they all seem to handle this</span>
<span class="quote">&gt; case:</span>

Agreed that archs using the generic tlb.h that sets the tlb-&gt;end to
address+PAGE_SIZE should be fine with the flush_tlb_page.
<span class="quote">
&gt; arch/arm: it implements tlb_remove_pmd_tlb_entry() in a similar way to</span>
<span class="quote">&gt; the generic one</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; arch/s390: tlb_remove_pmd_tlb_entry() is a no-op</span>

I guess s390 is fine too but I&#39;m not convinced that the fact it won&#39;t
adjust the tlb-&gt;start/end is a guarantees that flush_tlb_page is
enough when a single 2MB TLB has to be invalidated (not during range
zapping).

For the range zapping, could the arch decide to unconditionally flush
the whole TLB without doing the tlb-&gt;start/end tracking by overriding
tlb_gather_mmu in a way that won&#39;t call __tlb_reset_range? There seems
to be quite some flexibility in the per-arch tlb_gather_mmu setup in
order to unconditionally set tlb-&gt;start/end to the total range zapped,
without actually narrowing it down during the pagetable walk.

This is why I was thinking a flush_tlb_pmd_huge_page might have been
safer. However if hugetlbfs is basically assuming flush_tlb_page works
like Dave said, and if s390 is fine as well, I think we can just apply
this patch which follows the generic tlb_remove_pmd_tlb_entry
optimization.

Thanks,
Andrea
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=65121">Dave Hansen</a> - July 23, 2015, 2:41 p.m.</div>
<pre class="content">
On 07/23/2015 07:13 AM, Andrea Arcangeli wrote:
<span class="quote">&gt; On Thu, Jul 23, 2015 at 11:49:38AM +0100, Catalin Marinas wrote:</span>
<span class="quote">&gt;&gt; On Thu, Jul 23, 2015 at 12:05:21AM +0100, Dave Hansen wrote:</span>
<span class="quote">&gt;&gt;&gt; On 07/22/2015 03:48 PM, Catalin Marinas wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; You are right, on x86 the tlb_single_page_flush_ceiling seems to be</span>
<span class="quote">&gt;&gt;&gt;&gt; 33, so for an HPAGE_SIZE range the code does a local_flush_tlb()</span>
<span class="quote">&gt;&gt;&gt;&gt; always. I would say a single page TLB flush is more efficient than a</span>
<span class="quote">&gt;&gt;&gt;&gt; whole TLB flush but I&#39;m not familiar enough with x86.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; The last time I looked, the instruction to invalidate a single page is</span>
<span class="quote">&gt;&gt;&gt; more expensive than the instruction to flush the entire TLB. </span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; I was thinking of the overall cost of re-populating the TLB after being</span>
<span class="quote">&gt;&gt; nuked rather than the instruction itself.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Unless I&#39;m not aware about timing differences in flushing 2MB TLB</span>
<span class="quote">&gt; entries vs flushing 4kb TLB entries with invlpg, the benchmarks that</span>
<span class="quote">&gt; have been run to tune the optimal tlb_single_page_flush_ceiling value,</span>
<span class="quote">&gt; should already guarantee us that this is a valid optimization (as we</span>
<span class="quote">&gt; just got one entry, we&#39;re not even close to the 33 ceiling that makes</span>
<span class="quote">&gt; it more a grey area).</span>

We had a discussion about this a few weeks ago:

	https://lkml.org/lkml/2015/6/25/666

The argument is that the CPU is so good at refilling the TLB that it
rarely waits on it, so the &quot;cost&quot; can be very very low.
<span class="quote">
&gt;&gt;&gt; That said, I can&#39;t imagine this will hurt anything.  We also have TLBs</span>
<span class="quote">&gt;&gt;&gt; that can mix 2M and 4k pages and I don&#39;t think we did back when we put</span>
<span class="quote">&gt;&gt;&gt; that code in originally.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Dave, I&#39;m confused about this. We should still stick to an invariant</span>
<span class="quote">&gt; that we can&#39;t ever mix 2M and 4k TLB entries if their mappings end up</span>
<span class="quote">&gt; overlapping on the same physical memory (if this isn&#39;t enforced in</span>
<span class="quote">&gt; common code, some x86 implementation errata triggers, and it really</span>
<span class="quote">&gt; oopses with machine checks so it&#39;s not just theoretical). Perhaps I</span>
<span class="quote">&gt; misunderstood what you meant with mix 2M and 4k pages though.</span>

On older CPUs we had dedicated 2M TLB slots.  Now, we have an STLB that
can hold 2M and 4k entries at the same time.  That will surely change
the performance profile enough that whatever testing we did in the past
is fairly stale now.

I didn&#39;t mean mixing 4k and 2M mappings for the same virtual address.

--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=965">Andrea Arcangeli</a> - July 23, 2015, 3:58 p.m.</div>
<pre class="content">
On Thu, Jul 23, 2015 at 07:41:24AM -0700, Dave Hansen wrote:
<span class="quote">&gt; We had a discussion about this a few weeks ago:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	https://lkml.org/lkml/2015/6/25/666</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The argument is that the CPU is so good at refilling the TLB that it</span>
<span class="quote">&gt; rarely waits on it, so the &quot;cost&quot; can be very very low.</span>

That was about a new optimization adding more infrastructure to extend
these kind of optimizations, but the infrastructure has a cost so I&#39;m
not sure this is relevant for this discussion, as there is not
infrastructure or overhead here.

If we were to do close to 33 invlpg it would be a gray area, but it&#39;s
just one.

I also refer to code that is already in the kernel:

   if (base_pages_to_flush &gt; tlb_single_page_flush_ceiling) {

You wrote the patch that uses the tlb_single_page_flush_ceiling, so if
the above discussion would be relevant with regard to flush_tlb_page,
are you implying that the above optimization in the kernel, should
also be removed?

When these flush_tlb_range optimizations were introduced, it was
measured with benchmark that they helped IIRC. If it&#39;s not true
anymore with latest CPU I don&#39;t know but there should be at least a
subset of those CPUs where this helps. So I doubt it should be removed
for all CPUs out there.

Also if flush_tlb_page is slower on x86 (I doubt), then x86 should be
implement it without invlpg but for the common code it would still
make sense to use flush_tlb_page.
<span class="quote">
&gt; On older CPUs we had dedicated 2M TLB slots.  Now, we have an STLB that</span>
<span class="quote">&gt; can hold 2M and 4k entries at the same time.  That will surely change</span>
<span class="quote">&gt; the performance profile enough that whatever testing we did in the past</span>
<span class="quote">&gt; is fairly stale now.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I didn&#39;t mean mixing 4k and 2M mappings for the same virtual address.</span>

Thanks for the clarification, got what you meant now.

I still can&#39;t see why flush_tlb_page isn&#39;t an obviously valid
optimization though.

The tlb_single_page_flush_ceiling optimization has nothing to do with
2MB pages. But if that is still valid (or if it has ever been valid
for older CPUs), why is flush_tlb_page not a valid optimization at
least for those older CPUS? Why is it worth doing single invalidates
on 4k pages and not on 2MB pages?

It surely was helpful to do invlpg invalidated on 4k pages, up to 33
in a row, with x86 CPUs as you wrote the code quoted above to do
that, and it is still in the current kernel. So why are 2MB pages
different?

I don&#39;t see a relation with this and the fact 2MB and 4KB TLB entries
aren&#39;t separated anymore. If something the fact the TLB can use the
same TLB entry for 2MB and 4KB pages, means doing invlpg on 2MB is
even more helpful with newer CPUs as there can be more 2MB TLB entries
to preserve than before. So I&#39;m slightly confused now.

Thanks,
Andrea
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=352">Catalin Marinas</a> - July 23, 2015, 4:16 p.m.</div>
<pre class="content">
On Thu, Jul 23, 2015 at 03:41:24PM +0100, Dave Hansen wrote:
<span class="quote">&gt; On 07/23/2015 07:13 AM, Andrea Arcangeli wrote:</span>
<span class="quote">&gt; &gt; On Thu, Jul 23, 2015 at 11:49:38AM +0100, Catalin Marinas wrote:</span>
<span class="quote">&gt; &gt;&gt; On Thu, Jul 23, 2015 at 12:05:21AM +0100, Dave Hansen wrote:</span>
<span class="quote">&gt; &gt;&gt;&gt; On 07/22/2015 03:48 PM, Catalin Marinas wrote:</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; You are right, on x86 the tlb_single_page_flush_ceiling seems to be</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; 33, so for an HPAGE_SIZE range the code does a local_flush_tlb()</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; always. I would say a single page TLB flush is more efficient than a</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; whole TLB flush but I&#39;m not familiar enough with x86.</span>
<span class="quote">&gt; &gt;&gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt; The last time I looked, the instruction to invalidate a single page is</span>
<span class="quote">&gt; &gt;&gt;&gt; more expensive than the instruction to flush the entire TLB. </span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; I was thinking of the overall cost of re-populating the TLB after being</span>
<span class="quote">&gt; &gt;&gt; nuked rather than the instruction itself.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Unless I&#39;m not aware about timing differences in flushing 2MB TLB</span>
<span class="quote">&gt; &gt; entries vs flushing 4kb TLB entries with invlpg, the benchmarks that</span>
<span class="quote">&gt; &gt; have been run to tune the optimal tlb_single_page_flush_ceiling value,</span>
<span class="quote">&gt; &gt; should already guarantee us that this is a valid optimization (as we</span>
<span class="quote">&gt; &gt; just got one entry, we&#39;re not even close to the 33 ceiling that makes</span>
<span class="quote">&gt; &gt; it more a grey area).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; We had a discussion about this a few weeks ago:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	https://lkml.org/lkml/2015/6/25/666</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The argument is that the CPU is so good at refilling the TLB that it</span>
<span class="quote">&gt; rarely waits on it, so the &quot;cost&quot; can be very very low.</span>

Interesting thread. I can see from Ingo&#39;s benchmarks that invlpg is much
more expensive than the cr3 write but I can&#39;t really comment on the
refill cost (it may be small with page table caching in L1/L2). The
problem with small/targeted benchmarks is that you don&#39;t see the overall
impact.

On ARM, most recent CPUs can cache intermediate page table levels in the
TLB (usually as VA-&gt;pte translation). ARM64 introduces a new TLB
flushing instruction that only touches the last level (pte, huge pmd).
In theory this should be cheaper overall since the CPU doesn&#39;t need to
refill intermediate levels. In practice, it&#39;s probably lost in the
noise.

Anyway, if you want to keep the option of a full TLB flush for x86 on
huge pages, I&#39;m happy to repost a v2 with a separate
flush_tlb_pmd_huge_page that arch code can define as it sees fit.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=352">Catalin Marinas</a> - July 23, 2015, 4:49 p.m.</div>
<pre class="content">
On Thu, Jul 23, 2015 at 03:13:03PM +0100, Andrea Arcangeli wrote:
<span class="quote">&gt; On Thu, Jul 23, 2015 at 11:49:38AM +0100, Catalin Marinas wrote:</span>
<span class="quote">&gt; &gt; On Thu, Jul 23, 2015 at 12:05:21AM +0100, Dave Hansen wrote:</span>
<span class="quote">&gt; &gt; &gt; On 07/22/2015 03:48 PM, Catalin Marinas wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; You are right, on x86 the tlb_single_page_flush_ceiling seems to be</span>
<span class="quote">&gt; &gt; &gt; &gt; 33, so for an HPAGE_SIZE range the code does a local_flush_tlb()</span>
<span class="quote">&gt; &gt; &gt; &gt; always. I would say a single page TLB flush is more efficient than a</span>
<span class="quote">&gt; &gt; &gt; &gt; whole TLB flush but I&#39;m not familiar enough with x86.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; The last time I looked, the instruction to invalidate a single page is</span>
<span class="quote">&gt; &gt; &gt; more expensive than the instruction to flush the entire TLB. </span>
[...]
<span class="quote">&gt; &gt; Another question is whether flushing a single address is enough for a</span>
<span class="quote">&gt; &gt; huge page. I assumed it is since tlb_remove_pmd_tlb_entry() only adjusts</span>
[...]
<span class="quote">&gt; &gt; the mmu_gather range by PAGE_SIZE (rather than HPAGE_SIZE) and</span>
<span class="quote">&gt; &gt; no-one complained so far. AFAICT, there are only 3 architectures</span>
<span class="quote">&gt; &gt; that don&#39;t use asm-generic/tlb.h but they all seem to handle this</span>
<span class="quote">&gt; &gt; case:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Agreed that archs using the generic tlb.h that sets the tlb-&gt;end to</span>
<span class="quote">&gt; address+PAGE_SIZE should be fine with the flush_tlb_page.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; arch/arm: it implements tlb_remove_pmd_tlb_entry() in a similar way to</span>
<span class="quote">&gt; &gt; the generic one</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; arch/s390: tlb_remove_pmd_tlb_entry() is a no-op</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I guess s390 is fine too but I&#39;m not convinced that the fact it won&#39;t</span>
<span class="quote">&gt; adjust the tlb-&gt;start/end is a guarantees that flush_tlb_page is</span>
<span class="quote">&gt; enough when a single 2MB TLB has to be invalidated (not during range</span>
<span class="quote">&gt; zapping).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; For the range zapping, could the arch decide to unconditionally flush</span>
<span class="quote">&gt; the whole TLB without doing the tlb-&gt;start/end tracking by overriding</span>
<span class="quote">&gt; tlb_gather_mmu in a way that won&#39;t call __tlb_reset_range? There seems</span>
<span class="quote">&gt; to be quite some flexibility in the per-arch tlb_gather_mmu setup in</span>
<span class="quote">&gt; order to unconditionally set tlb-&gt;start/end to the total range zapped,</span>
<span class="quote">&gt; without actually narrowing it down during the pagetable walk.</span>

You are right, looking at the s390 code, tlb_finish_mmu() flushes the
whole TLB, so the ranges don&#39;t seem to matter. I&#39;m cc&#39;ing the s390
maintainers to confirm whether this patch affects them in any way:

https://lkml.org/lkml/2015/7/22/521

IIUC, all the functions touched by this patch are implemented by s390 in
its specific way, so I don&#39;t think it makes any difference:

pmdp_set_access_flags
pmdp_clear_flush_young
pmdp_huge_clear_flush
pmdp_splitting_flush
pmdp_invalidate
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=65121">Dave Hansen</a> - July 23, 2015, 4:52 p.m.</div>
<pre class="content">
On 07/23/2015 08:58 AM, Andrea Arcangeli wrote:
<span class="quote">&gt; You wrote the patch that uses the tlb_single_page_flush_ceiling, so if</span>
<span class="quote">&gt; the above discussion would be relevant with regard to flush_tlb_page,</span>
<span class="quote">&gt; are you implying that the above optimization in the kernel, should</span>
<span class="quote">&gt; also be removed?</span>

When I put that in, my goal was to bring consistency to how we handled
things without regressing anything.  I was never able to measure any
nice macro-level benefits to a particular flush behavior.

We can also now just easily disable the ranged flushes if we want to, or
leave them in place for small flushes only.
<span class="quote">
&gt; When these flush_tlb_range optimizations were introduced, it was</span>
<span class="quote">&gt; measured with benchmark that they helped IIRC. If it&#39;s not true</span>
<span class="quote">&gt; anymore with latest CPU I don&#39;t know but there should be at least a</span>
<span class="quote">&gt; subset of those CPUs where this helps. So I doubt it should be removed</span>
<span class="quote">&gt; for all CPUs out there.</span>

I tried to reproduce the results and had a difficult time doing so.
<span class="quote">
&gt; The tlb_single_page_flush_ceiling optimization has nothing to do with</span>
<span class="quote">&gt; 2MB pages. But if that is still valid (or if it has ever been valid</span>
<span class="quote">&gt; for older CPUs), why is flush_tlb_page not a valid optimization at</span>
<span class="quote">&gt; least for those older CPUS? Why is it worth doing single invalidates</span>
<span class="quote">&gt; on 4k pages and not on 2MB pages?</span>

I haven&#39;t seen any solid evidence that we should do it for one and not
the other.
<span class="quote">
&gt; It surely was helpful to do invlpg invalidated on 4k pages, up to 33</span>
<span class="quote">&gt; in a row, with x86 CPUs as you wrote the code quoted above to do</span>
<span class="quote">&gt; that, and it is still in the current kernel. So why are 2MB pages</span>
<span class="quote">&gt; different?</span>

They were originally different because the work that introduced &#39;invlpg&#39;
didn&#39;t see a benefit from using &#39;invlpg&#39; on 2M pages.  I didn&#39;t
reevaluate it when I hacked on the code and just left it as it was.

It would be great if someone would go and collect some recent data on
using &#39;invlpg&#39; on 2M pages!
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=65121">Dave Hansen</a> - July 23, 2015, 4:55 p.m.</div>
<pre class="content">
On 07/23/2015 09:16 AM, Catalin Marinas wrote:
<span class="quote">&gt; Anyway, if you want to keep the option of a full TLB flush for x86 on</span>
<span class="quote">&gt; huge pages, I&#39;m happy to repost a v2 with a separate</span>
<span class="quote">&gt; flush_tlb_pmd_huge_page that arch code can define as it sees fit.</span>

I think your patch is fine on x86.  We need to keep an eye out for any
regressions, but I think it&#39;s OK.
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=965">Andrea Arcangeli</a> - July 23, 2015, 5:13 p.m.</div>
<pre class="content">
On Thu, Jul 23, 2015 at 09:55:33AM -0700, Dave Hansen wrote:
<span class="quote">&gt; On 07/23/2015 09:16 AM, Catalin Marinas wrote:</span>
<span class="quote">&gt; &gt; Anyway, if you want to keep the option of a full TLB flush for x86 on</span>
<span class="quote">&gt; &gt; huge pages, I&#39;m happy to repost a v2 with a separate</span>
<span class="quote">&gt; &gt; flush_tlb_pmd_huge_page that arch code can define as it sees fit.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I think your patch is fine on x86.  We need to keep an eye out for any</span>
<span class="quote">&gt; regressions, but I think it&#39;s OK.</span>

That&#39;s my view as well.

I&#39;ve read more of the other thread and I quote Ingo:

&quot; It barely makes sense for a 2 pages and gets exponentially
worse. It&#39;s probably done in microcode and its performance is
horrible. &quot;

So in our case it&#39;s just 1 page (not 2, not 33), and considering it
prevents to invalidate all other TLB entries, it&#39;s most certainly a
win: it requires zero additional infrastructure and best of all it can
also avoid to flush the entire TLB for remote CPUs too again without
infrastructure or pfn arrays or multiple invlpg.

As further confirmation that for 1 entry invlpg is worth it, even
flush_tlb_page-&gt;flush_tlb_func invokes __flush_tlb_single in the IPI
handler instead of local_flush_tlb().

So the discussion there was about the additional infrastructure and a
flood of invlpg, perhaps more than 33, I agree a local_flush_tlb()
sounds better for that.

The question left for x86 is if invlpg is even slower for 2MB pages
than it is for 4k pages, but I&#39;d be surprised if it is, especially on
newer CPUs where the TLB can use different page size for each TLB
entry. Why we didn&#39;t do flush_tlb_page before wasn&#39;t related to such a
concern at least.
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=93">Martin Schwidefsky</a> - July 24, 2015, 7:17 a.m.</div>
<pre class="content">
On Thu, 23 Jul 2015 17:49:21 +0100
Catalin Marinas &lt;catalin.marinas@arm.com&gt; wrote:
<span class="quote">
&gt; On Thu, Jul 23, 2015 at 03:13:03PM +0100, Andrea Arcangeli wrote:</span>
<span class="quote">&gt; &gt; On Thu, Jul 23, 2015 at 11:49:38AM +0100, Catalin Marinas wrote:</span>
<span class="quote">&gt; &gt; &gt; On Thu, Jul 23, 2015 at 12:05:21AM +0100, Dave Hansen wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; On 07/22/2015 03:48 PM, Catalin Marinas wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; You are right, on x86 the tlb_single_page_flush_ceiling seems to be</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; 33, so for an HPAGE_SIZE range the code does a local_flush_tlb()</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; always. I would say a single page TLB flush is more efficient than a</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; whole TLB flush but I&#39;m not familiar enough with x86.</span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; The last time I looked, the instruction to invalidate a single page is</span>
<span class="quote">&gt; &gt; &gt; &gt; more expensive than the instruction to flush the entire TLB. </span>
<span class="quote">&gt; [...]</span>
<span class="quote">&gt; &gt; &gt; Another question is whether flushing a single address is enough for a</span>
<span class="quote">&gt; &gt; &gt; huge page. I assumed it is since tlb_remove_pmd_tlb_entry() only adjusts</span>
<span class="quote">&gt; [...]</span>
<span class="quote">&gt; &gt; &gt; the mmu_gather range by PAGE_SIZE (rather than HPAGE_SIZE) and</span>
<span class="quote">&gt; &gt; &gt; no-one complained so far. AFAICT, there are only 3 architectures</span>
<span class="quote">&gt; &gt; &gt; that don&#39;t use asm-generic/tlb.h but they all seem to handle this</span>
<span class="quote">&gt; &gt; &gt; case:</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Agreed that archs using the generic tlb.h that sets the tlb-&gt;end to</span>
<span class="quote">&gt; &gt; address+PAGE_SIZE should be fine with the flush_tlb_page.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; arch/arm: it implements tlb_remove_pmd_tlb_entry() in a similar way to</span>
<span class="quote">&gt; &gt; &gt; the generic one</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; arch/s390: tlb_remove_pmd_tlb_entry() is a no-op</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I guess s390 is fine too but I&#39;m not convinced that the fact it won&#39;t</span>
<span class="quote">&gt; &gt; adjust the tlb-&gt;start/end is a guarantees that flush_tlb_page is</span>
<span class="quote">&gt; &gt; enough when a single 2MB TLB has to be invalidated (not during range</span>
<span class="quote">&gt; &gt; zapping).</span>

tlb_remove_pmd_tlb_entry() is a no-op because pmdp_get_and_clear_full()
already did the job. s390 is special in regard to TLB flushing, the
machines have the requirement that a pte/pmd needs to be invalidated
with specific instruction if there is a process that might use the
translation path. In this case the IDTE instruction needs to be used
which sets the invalid bit in the pmd *and* flushes the TLB at the
same time. The code still tries to be lazy and do batched flushes to
improve performance. All in all quite complicated..
<span class="quote">
&gt; &gt; For the range zapping, could the arch decide to unconditionally flush</span>
<span class="quote">&gt; &gt; the whole TLB without doing the tlb-&gt;start/end tracking by overriding</span>
<span class="quote">&gt; &gt; tlb_gather_mmu in a way that won&#39;t call __tlb_reset_range? There seems</span>
<span class="quote">&gt; &gt; to be quite some flexibility in the per-arch tlb_gather_mmu setup in</span>
<span class="quote">&gt; &gt; order to unconditionally set tlb-&gt;start/end to the total range zapped,</span>
<span class="quote">&gt; &gt; without actually narrowing it down during the pagetable walk.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; You are right, looking at the s390 code, tlb_finish_mmu() flushes the</span>
<span class="quote">&gt; whole TLB, so the ranges don&#39;t seem to matter. I&#39;m cc&#39;ing the s390</span>
<span class="quote">&gt; maintainers to confirm whether this patch affects them in any way:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; https://lkml.org/lkml/2015/7/22/521</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; IIUC, all the functions touched by this patch are implemented by s390 in</span>
<span class="quote">&gt; its specific way, so I don&#39;t think it makes any difference:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; pmdp_set_access_flags</span>
<span class="quote">&gt; pmdp_clear_flush_young</span>
<span class="quote">&gt; pmdp_huge_clear_flush</span>
<span class="quote">&gt; pmdp_splitting_flush</span>
<span class="quote">&gt; pmdp_invalidate</span>

tlb_finish_mmu may flush all entries for a specific address space, not
the whole TLB. And it does so only for batched operations. If all changes
to the page tables have been done with IPTE/IDTE then flush_mm will not
be set and no full address space flush is done.

But to answer the question: s390 is fine with the change outlined in
https://lkml.org/lkml/2015/7/22/521
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/mm/pgtable-generic.c b/mm/pgtable-generic.c</span>
<span class="p_header">index 6b674e00153c..ff17eca26211 100644</span>
<span class="p_header">--- a/mm/pgtable-generic.c</span>
<span class="p_header">+++ b/mm/pgtable-generic.c</span>
<span class="p_chunk">@@ -67,7 +67,7 @@</span> <span class="p_context"> int pmdp_set_access_flags(struct vm_area_struct *vma,</span>
 	VM_BUG_ON(address &amp; ~HPAGE_PMD_MASK);
 	if (changed) {
 		set_pmd_at(vma-&gt;vm_mm, address, pmdp, entry);
<span class="p_del">-		flush_tlb_range(vma, address, address + HPAGE_PMD_SIZE);</span>
<span class="p_add">+		flush_tlb_page(vma, address);</span>
 	}
 	return changed;
 #else /* CONFIG_TRANSPARENT_HUGEPAGE */
<span class="p_chunk">@@ -101,7 +101,7 @@</span> <span class="p_context"> int pmdp_clear_flush_young(struct vm_area_struct *vma,</span>
 #endif /* CONFIG_TRANSPARENT_HUGEPAGE */
 	young = pmdp_test_and_clear_young(vma, address, pmdp);
 	if (young)
<span class="p_del">-		flush_tlb_range(vma, address, address + HPAGE_PMD_SIZE);</span>
<span class="p_add">+		flush_tlb_page(vma, address);</span>
 	return young;
 }
 #endif
<span class="p_chunk">@@ -128,7 +128,7 @@</span> <span class="p_context"> pmd_t pmdp_huge_clear_flush(struct vm_area_struct *vma, unsigned long address,</span>
 	VM_BUG_ON(address &amp; ~HPAGE_PMD_MASK);
 	VM_BUG_ON(!pmd_trans_huge(*pmdp));
 	pmd = pmdp_huge_get_and_clear(vma-&gt;vm_mm, address, pmdp);
<span class="p_del">-	flush_tlb_range(vma, address, address + HPAGE_PMD_SIZE);</span>
<span class="p_add">+	flush_tlb_page(vma, address);</span>
 	return pmd;
 }
 #endif /* CONFIG_TRANSPARENT_HUGEPAGE */
<span class="p_chunk">@@ -143,7 +143,7 @@</span> <span class="p_context"> void pmdp_splitting_flush(struct vm_area_struct *vma, unsigned long address,</span>
 	VM_BUG_ON(address &amp; ~HPAGE_PMD_MASK);
 	set_pmd_at(vma-&gt;vm_mm, address, pmdp, pmd);
 	/* tlb flush only to serialize against gup-fast */
<span class="p_del">-	flush_tlb_range(vma, address, address + HPAGE_PMD_SIZE);</span>
<span class="p_add">+	flush_tlb_page(vma, address);</span>
 }
 #endif /* CONFIG_TRANSPARENT_HUGEPAGE */
 #endif
<span class="p_chunk">@@ -195,7 +195,7 @@</span> <span class="p_context"> void pmdp_invalidate(struct vm_area_struct *vma, unsigned long address,</span>
 {
 	pmd_t entry = *pmdp;
 	set_pmd_at(vma-&gt;vm_mm, address, pmdp, pmd_mknotpresent(entry));
<span class="p_del">-	flush_tlb_range(vma, address, address + HPAGE_PMD_SIZE);</span>
<span class="p_add">+	flush_tlb_page(vma, address);</span>
 }
 #endif /* CONFIG_TRANSPARENT_HUGEPAGE */
 #endif

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



