
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v5,16/39] nds32: DMA mapping API - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v5,16/39] nds32: DMA mapping API</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=171217">Greentime Hu</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Jan. 2, 2018, 8:24 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;ecd1742c718c5b45983486cf9dfb44adb6e06923.1514874857.git.green.hu@gmail.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10140095/mbox/"
   >mbox</a>
|
   <a href="/patch/10140095/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10140095/">/patch/10140095/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	6D12B601A1 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  2 Jan 2018 08:36:46 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 573E028712
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  2 Jan 2018 08:36:46 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 499E828A78; Tue,  2 Jan 2018 08:36:46 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-7.0 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	DKIM_VALID, DKIM_VALID_AU, FREEMAIL_FROM,
	RCVD_IN_DNSWL_HI autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 1A7C628712
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  2 Jan 2018 08:36:45 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752879AbeABIgl (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 2 Jan 2018 03:36:41 -0500
Received: from mail-pl0-f68.google.com ([209.85.160.68]:40066 &quot;EHLO
	mail-pl0-f68.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1752288AbeABI1M (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 2 Jan 2018 03:27:12 -0500
Received: by mail-pl0-f68.google.com with SMTP id 62so26840603pld.7;
	Tue, 02 Jan 2018 00:27:12 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=gmail.com; s=20161025;
	h=from:to:cc:subject:date:message-id:in-reply-to:references
	:in-reply-to:references;
	bh=UsNv4ziPPHmjMoJuaa2XevflsQ+7d75mIruQ4H8p+3w=;
	b=X+Me4m8TfmBH2i96k4OmX+cm1wkpaGDqB65DKW5ELN6yq3FM2msCVURlIURQDwICpb
	rKuU585y6jtmitGgvxcFswt57rp2z75GQzpUyf2UmyWXbKnql8Rqkv+Hs045HBLNuDVs
	7AUun7XvqZouug44mjPr/eAtBF//WgO/ZpLInj5gb/IYNPEow0E4R0gTZ2Tt99mZa7zb
	B3f/5ew/Wn22qiydQ46gSbmpiQKTJ3km+KuQjbPidpfuh95gtlJ7L5KG1HsIQpwcAJjn
	q7IXW03Wb8HKIJEPJUBRD0E4LReLU3IrnnGWScaemIDU0rU4lS8HNA0cdnDVrGZsVNnA
	MvEQ==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20161025;
	h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
	:references:in-reply-to:references;
	bh=UsNv4ziPPHmjMoJuaa2XevflsQ+7d75mIruQ4H8p+3w=;
	b=Ip9N0XZzwO650rBGAaTgKRzRKUVtDESpG4O1etVYMxYqrXrq1zpJjPptJij2N+2lWC
	0g1n0D/FvyFkz+31tLu1Xv4cgshQY3MI3/6yD9/tun5pFiiMZNaJ4aOZ/KX5ee8zZbI8
	f3rgFp/pThXpbgDWgFLbd0LlmtdMiAHzy4h8M+nOAoJE7Q3402e0G48+UAUPbobsSGPr
	B/KBAyiIMyJlTV/cmEZfF6GkyklBDO7S00oBTeQnMpv9Nl3B8SYJEPw4kQzaJZaTsEv/
	JIVSQ7FQinNY7YjTcb4MeLDbWjloA6FVclrIllqAtvafC8B34lFqOfC0RO9H7SiRbCPr
	hU1w==
X-Gm-Message-State: AKGB3mJqWcudjAglgi6HSzxeZ18D1D2J0OKQz8QR1b6DUnsr48Hh+iHm
	5n5hY9eyRw+eAI+cwvfmVOs=
X-Google-Smtp-Source: ACJfBotI3pAmqKeC4aL1ZYVKoZB8IErPvRdG9h5picnW6KN3Wfd3SxLj2hTvyFkLUV+nq2y62USWgQ==
X-Received: by 10.124.22.132 with SMTP id v4mr44578669ply.158.1514881631756; 
	Tue, 02 Jan 2018 00:27:11 -0800 (PST)
Received: from app09.andestech.com ([118.163.51.199])
	by smtp.gmail.com with ESMTPSA id
	76sm19681729pge.89.2018.01.02.00.27.07
	(version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
	Tue, 02 Jan 2018 00:27:11 -0800 (PST)
From: Greentime Hu &lt;green.hu@gmail.com&gt;
To: greentime@andestech.com, linux-kernel@vger.kernel.org,
	arnd@arndb.de, linux-arch@vger.kernel.org, tglx@linutronix.de,
	jason@lakedaemon.net, marc.zyngier@arm.com, robh+dt@kernel.org,
	netdev@vger.kernel.org, deanbo422@gmail.com,
	devicetree@vger.kernel.org, viro@zeniv.linux.org.uk,
	dhowells@redhat.com, will.deacon@arm.com,
	daniel.lezcano@linaro.org, linux-serial@vger.kernel.org,
	geert.uytterhoeven@gmail.com, linus.walleij@linaro.org,
	mark.rutland@arm.com, greg@kroah.com, ren_guo@c-sky.com,
	rdunlap@infradead.org, davem@davemloft.net, jonas@southpole.se,
	stefan.kristiansson@saunalahti.fi, shorne@gmail.com
Cc: green.hu@gmail.com, Vincent Chen &lt;vincentc@andestech.com&gt;
Subject: [PATCH v5 16/39] nds32: DMA mapping API
Date: Tue,  2 Jan 2018 16:24:48 +0800
Message-Id: &lt;ecd1742c718c5b45983486cf9dfb44adb6e06923.1514874857.git.green.hu@gmail.com&gt;
X-Mailer: git-send-email 1.7.9.5
In-Reply-To: &lt;cover.1514874857.git.green.hu@gmail.com&gt;
References: &lt;cover.1514874857.git.green.hu@gmail.com&gt;
In-Reply-To: &lt;cover.1514874857.git.green.hu@gmail.com&gt;
References: &lt;cover.1514874857.git.green.hu@gmail.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=171217">Greentime Hu</a> - Jan. 2, 2018, 8:24 a.m.</div>
<pre class="content">
<span class="from">From: Greentime Hu &lt;greentime@andestech.com&gt;</span>

This patch adds support for the DMA mapping API. It uses dma_map_ops for
flexibility.
<span class="signed-off-by">
Signed-off-by: Vincent Chen &lt;vincentc@andestech.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Greentime Hu &lt;greentime@andestech.com&gt;</span>
---
 arch/nds32/include/asm/dma-mapping.h |   14 ++
 arch/nds32/kernel/dma.c              |  459 ++++++++++++++++++++++++++++++++++
 2 files changed, 473 insertions(+)
 create mode 100644 arch/nds32/include/asm/dma-mapping.h
 create mode 100644 arch/nds32/kernel/dma.c
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/nds32/include/asm/dma-mapping.h b/arch/nds32/include/asm/dma-mapping.h</span>
new file mode 100644
<span class="p_header">index 0000000..2dd47d24</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/nds32/include/asm/dma-mapping.h</span>
<span class="p_chunk">@@ -0,0 +1,14 @@</span> <span class="p_context"></span>
<span class="p_add">+// SPDX-License-Identifier: GPL-2.0</span>
<span class="p_add">+// Copyright (C) 2005-2017 Andes Technology Corporation</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef ASMNDS32_DMA_MAPPING_H</span>
<span class="p_add">+#define ASMNDS32_DMA_MAPPING_H</span>
<span class="p_add">+</span>
<span class="p_add">+extern struct dma_map_ops nds32_dma_ops;</span>
<span class="p_add">+</span>
<span class="p_add">+static inline struct dma_map_ops *get_arch_dma_ops(struct bus_type *bus)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return &amp;nds32_dma_ops;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#endif</span>
<span class="p_header">diff --git a/arch/nds32/kernel/dma.c b/arch/nds32/kernel/dma.c</span>
new file mode 100644
<span class="p_header">index 0000000..9bd1dc7</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/nds32/kernel/dma.c</span>
<span class="p_chunk">@@ -0,0 +1,459 @@</span> <span class="p_context"></span>
<span class="p_add">+// SPDX-License-Identifier: GPL-2.0</span>
<span class="p_add">+// Copyright (C) 2005-2017 Andes Technology Corporation</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/types.h&gt;</span>
<span class="p_add">+#include &lt;linux/mm.h&gt;</span>
<span class="p_add">+#include &lt;linux/export.h&gt;</span>
<span class="p_add">+#include &lt;linux/string.h&gt;</span>
<span class="p_add">+#include &lt;linux/scatterlist.h&gt;</span>
<span class="p_add">+#include &lt;linux/dma-mapping.h&gt;</span>
<span class="p_add">+#include &lt;linux/io.h&gt;</span>
<span class="p_add">+#include &lt;linux/cache.h&gt;</span>
<span class="p_add">+#include &lt;linux/highmem.h&gt;</span>
<span class="p_add">+#include &lt;linux/slab.h&gt;</span>
<span class="p_add">+#include &lt;asm/cacheflush.h&gt;</span>
<span class="p_add">+#include &lt;asm/tlbflush.h&gt;</span>
<span class="p_add">+#include &lt;asm/dma-mapping.h&gt;</span>
<span class="p_add">+#include &lt;asm/proc-fns.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * This is the page table (2MB) covering uncached, DMA consistent allocations</span>
<span class="p_add">+ */</span>
<span class="p_add">+static pte_t *consistent_pte;</span>
<span class="p_add">+static DEFINE_RAW_SPINLOCK(consistent_lock);</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * VM region handling support.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This should become something generic, handling VM region allocations for</span>
<span class="p_add">+ * vmalloc and similar (ioremap, module space, etc).</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * I envisage vmalloc()&#39;s supporting vm_struct becoming:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *  struct vm_struct {</span>
<span class="p_add">+ *    struct vm_region	region;</span>
<span class="p_add">+ *    unsigned long	flags;</span>
<span class="p_add">+ *    struct page	**pages;</span>
<span class="p_add">+ *    unsigned int	nr_pages;</span>
<span class="p_add">+ *    unsigned long	phys_addr;</span>
<span class="p_add">+ *  };</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * get_vm_area() would then call vm_region_alloc with an appropriate</span>
<span class="p_add">+ * struct vm_region head (eg):</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *  struct vm_region vmalloc_head = {</span>
<span class="p_add">+ *	.vm_list	= LIST_HEAD_INIT(vmalloc_head.vm_list),</span>
<span class="p_add">+ *	.vm_start	= VMALLOC_START,</span>
<span class="p_add">+ *	.vm_end		= VMALLOC_END,</span>
<span class="p_add">+ *  };</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * However, vmalloc_head.vm_start is variable (typically, it is dependent on</span>
<span class="p_add">+ * the amount of RAM found at boot time.)  I would imagine that get_vm_area()</span>
<span class="p_add">+ * would have to initialise this each time prior to calling vm_region_alloc().</span>
<span class="p_add">+ */</span>
<span class="p_add">+struct arch_vm_region {</span>
<span class="p_add">+	struct list_head vm_list;</span>
<span class="p_add">+	unsigned long vm_start;</span>
<span class="p_add">+	unsigned long vm_end;</span>
<span class="p_add">+	struct page *vm_pages;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+static struct arch_vm_region consistent_head = {</span>
<span class="p_add">+	.vm_list = LIST_HEAD_INIT(consistent_head.vm_list),</span>
<span class="p_add">+	.vm_start = CONSISTENT_BASE,</span>
<span class="p_add">+	.vm_end = CONSISTENT_END,</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+static struct arch_vm_region *vm_region_alloc(struct arch_vm_region *head,</span>
<span class="p_add">+					      size_t size, int gfp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long addr = head-&gt;vm_start, end = head-&gt;vm_end - size;</span>
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+	struct arch_vm_region *c, *new;</span>
<span class="p_add">+</span>
<span class="p_add">+	new = kmalloc(sizeof(struct arch_vm_region), gfp);</span>
<span class="p_add">+	if (!new)</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+</span>
<span class="p_add">+	raw_spin_lock_irqsave(&amp;consistent_lock, flags);</span>
<span class="p_add">+</span>
<span class="p_add">+	list_for_each_entry(c, &amp;head-&gt;vm_list, vm_list) {</span>
<span class="p_add">+		if ((addr + size) &lt; addr)</span>
<span class="p_add">+			goto nospc;</span>
<span class="p_add">+		if ((addr + size) &lt;= c-&gt;vm_start)</span>
<span class="p_add">+			goto found;</span>
<span class="p_add">+		addr = c-&gt;vm_end;</span>
<span class="p_add">+		if (addr &gt; end)</span>
<span class="p_add">+			goto nospc;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+found:</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Insert this entry _before_ the one we found.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	list_add_tail(&amp;new-&gt;vm_list, &amp;c-&gt;vm_list);</span>
<span class="p_add">+	new-&gt;vm_start = addr;</span>
<span class="p_add">+	new-&gt;vm_end = addr + size;</span>
<span class="p_add">+</span>
<span class="p_add">+	raw_spin_unlock_irqrestore(&amp;consistent_lock, flags);</span>
<span class="p_add">+	return new;</span>
<span class="p_add">+</span>
<span class="p_add">+nospc:</span>
<span class="p_add">+	raw_spin_unlock_irqrestore(&amp;consistent_lock, flags);</span>
<span class="p_add">+	kfree(new);</span>
<span class="p_add">+out:</span>
<span class="p_add">+	return NULL;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static struct arch_vm_region *vm_region_find(struct arch_vm_region *head,</span>
<span class="p_add">+					     unsigned long addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct arch_vm_region *c;</span>
<span class="p_add">+</span>
<span class="p_add">+	list_for_each_entry(c, &amp;head-&gt;vm_list, vm_list) {</span>
<span class="p_add">+		if (c-&gt;vm_start == addr)</span>
<span class="p_add">+			goto out;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	c = NULL;</span>
<span class="p_add">+out:</span>
<span class="p_add">+	return c;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* FIXME: attrs is not used. */</span>
<span class="p_add">+static void *nds32_dma_alloc_coherent(struct device *dev, size_t size,</span>
<span class="p_add">+				      dma_addr_t * handle, gfp_t gfp,</span>
<span class="p_add">+				      unsigned long attrs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct page *page;</span>
<span class="p_add">+	struct arch_vm_region *c;</span>
<span class="p_add">+	unsigned long order;</span>
<span class="p_add">+	u64 mask = ~0ULL, limit;</span>
<span class="p_add">+	pgprot_t prot = pgprot_noncached(PAGE_KERNEL);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!consistent_pte) {</span>
<span class="p_add">+		pr_err(&quot;%s: not initialized\n&quot;, __func__);</span>
<span class="p_add">+		dump_stack();</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (dev) {</span>
<span class="p_add">+		mask = dev-&gt;coherent_dma_mask;</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Sanity check the DMA mask - it must be non-zero, and</span>
<span class="p_add">+		 * must be able to be satisfied by a DMA allocation.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (mask == 0) {</span>
<span class="p_add">+			dev_warn(dev, &quot;coherent DMA mask is unset\n&quot;);</span>
<span class="p_add">+			goto no_page;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Sanity check the allocation size.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	size = PAGE_ALIGN(size);</span>
<span class="p_add">+	limit = (mask + 1) &amp; ~mask;</span>
<span class="p_add">+	if ((limit &amp;&amp; size &gt;= limit) ||</span>
<span class="p_add">+	    size &gt;= (CONSISTENT_END - CONSISTENT_BASE)) {</span>
<span class="p_add">+		pr_warn(&quot;coherent allocation too big &quot;</span>
<span class="p_add">+			&quot;(requested %#x mask %#llx)\n&quot;, size, mask);</span>
<span class="p_add">+		goto no_page;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	order = get_order(size);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (mask != 0xffffffff)</span>
<span class="p_add">+		gfp |= GFP_DMA;</span>
<span class="p_add">+</span>
<span class="p_add">+	page = alloc_pages(gfp, order);</span>
<span class="p_add">+	if (!page)</span>
<span class="p_add">+		goto no_page;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Invalidate any data that might be lurking in the</span>
<span class="p_add">+	 * kernel direct-mapped region for device DMA.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	{</span>
<span class="p_add">+		unsigned long kaddr = (unsigned long)page_address(page);</span>
<span class="p_add">+		memset(page_address(page), 0, size);</span>
<span class="p_add">+		cpu_dma_wbinval_range(kaddr, kaddr + size);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Allocate a virtual address in the consistent mapping region.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	c = vm_region_alloc(&amp;consistent_head, size,</span>
<span class="p_add">+			    gfp &amp; ~(__GFP_DMA | __GFP_HIGHMEM));</span>
<span class="p_add">+	if (c) {</span>
<span class="p_add">+		pte_t *pte = consistent_pte + CONSISTENT_OFFSET(c-&gt;vm_start);</span>
<span class="p_add">+		struct page *end = page + (1 &lt;&lt; order);</span>
<span class="p_add">+</span>
<span class="p_add">+		c-&gt;vm_pages = page;</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Set the &quot;dma handle&quot;</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		*handle = page_to_phys(page);</span>
<span class="p_add">+</span>
<span class="p_add">+		do {</span>
<span class="p_add">+			BUG_ON(!pte_none(*pte));</span>
<span class="p_add">+</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * x86 does not mark the pages reserved...</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			SetPageReserved(page);</span>
<span class="p_add">+			set_pte(pte, mk_pte(page, prot));</span>
<span class="p_add">+			page++;</span>
<span class="p_add">+			pte++;</span>
<span class="p_add">+		} while (size -= PAGE_SIZE);</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Free the otherwise unused pages.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		while (page &lt; end) {</span>
<span class="p_add">+			__free_page(page);</span>
<span class="p_add">+			page++;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		return (void *)c-&gt;vm_start;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (page)</span>
<span class="p_add">+		__free_pages(page, order);</span>
<span class="p_add">+no_page:</span>
<span class="p_add">+	*handle = ~0;</span>
<span class="p_add">+	return NULL;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void nds32_dma_free(struct device *dev, size_t size, void *cpu_addr,</span>
<span class="p_add">+			   dma_addr_t handle, unsigned long attrs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct arch_vm_region *c;</span>
<span class="p_add">+	unsigned long flags, addr;</span>
<span class="p_add">+	pte_t *ptep;</span>
<span class="p_add">+</span>
<span class="p_add">+	size = PAGE_ALIGN(size);</span>
<span class="p_add">+</span>
<span class="p_add">+	raw_spin_lock_irqsave(&amp;consistent_lock, flags);</span>
<span class="p_add">+</span>
<span class="p_add">+	c = vm_region_find(&amp;consistent_head, (unsigned long)cpu_addr);</span>
<span class="p_add">+	if (!c)</span>
<span class="p_add">+		goto no_area;</span>
<span class="p_add">+</span>
<span class="p_add">+	if ((c-&gt;vm_end - c-&gt;vm_start) != size) {</span>
<span class="p_add">+		pr_err(&quot;%s: freeing wrong coherent size (%ld != %d)\n&quot;,</span>
<span class="p_add">+		       __func__, c-&gt;vm_end - c-&gt;vm_start, size);</span>
<span class="p_add">+		dump_stack();</span>
<span class="p_add">+		size = c-&gt;vm_end - c-&gt;vm_start;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	ptep = consistent_pte + CONSISTENT_OFFSET(c-&gt;vm_start);</span>
<span class="p_add">+	addr = c-&gt;vm_start;</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		pte_t pte = ptep_get_and_clear(&amp;init_mm, addr, ptep);</span>
<span class="p_add">+		unsigned long pfn;</span>
<span class="p_add">+</span>
<span class="p_add">+		ptep++;</span>
<span class="p_add">+		addr += PAGE_SIZE;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!pte_none(pte) &amp;&amp; pte_present(pte)) {</span>
<span class="p_add">+			pfn = pte_pfn(pte);</span>
<span class="p_add">+</span>
<span class="p_add">+			if (pfn_valid(pfn)) {</span>
<span class="p_add">+				struct page *page = pfn_to_page(pfn);</span>
<span class="p_add">+</span>
<span class="p_add">+				/*</span>
<span class="p_add">+				 * x86 does not mark the pages reserved...</span>
<span class="p_add">+				 */</span>
<span class="p_add">+				ClearPageReserved(page);</span>
<span class="p_add">+</span>
<span class="p_add">+				__free_page(page);</span>
<span class="p_add">+				continue;</span>
<span class="p_add">+			}</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		pr_crit(&quot;%s: bad page in kernel page table\n&quot;, __func__);</span>
<span class="p_add">+	} while (size -= PAGE_SIZE);</span>
<span class="p_add">+</span>
<span class="p_add">+	flush_tlb_kernel_range(c-&gt;vm_start, c-&gt;vm_end);</span>
<span class="p_add">+</span>
<span class="p_add">+	list_del(&amp;c-&gt;vm_list);</span>
<span class="p_add">+</span>
<span class="p_add">+	raw_spin_unlock_irqrestore(&amp;consistent_lock, flags);</span>
<span class="p_add">+</span>
<span class="p_add">+	kfree(c);</span>
<span class="p_add">+	return;</span>
<span class="p_add">+</span>
<span class="p_add">+no_area:</span>
<span class="p_add">+	raw_spin_unlock_irqrestore(&amp;consistent_lock, flags);</span>
<span class="p_add">+	pr_err(&quot;%s: trying to free invalid coherent area: %p\n&quot;,</span>
<span class="p_add">+	       __func__, cpu_addr);</span>
<span class="p_add">+	dump_stack();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Initialise the consistent memory allocation.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static int __init consistent_init(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pgd_t *pgd;</span>
<span class="p_add">+	pmd_t *pmd;</span>
<span class="p_add">+	pte_t *pte;</span>
<span class="p_add">+	int ret = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		pgd = pgd_offset(&amp;init_mm, CONSISTENT_BASE);</span>
<span class="p_add">+		pmd = pmd_alloc(&amp;init_mm, pgd, CONSISTENT_BASE);</span>
<span class="p_add">+		if (!pmd) {</span>
<span class="p_add">+			pr_err(&quot;%s: no pmd tables\n&quot;, __func__);</span>
<span class="p_add">+			ret = -ENOMEM;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		/* The first level mapping may be created in somewhere.</span>
<span class="p_add">+		 * It&#39;s not necessary to warn here. */</span>
<span class="p_add">+		/* WARN_ON(!pmd_none(*pmd)); */</span>
<span class="p_add">+</span>
<span class="p_add">+		pte = pte_alloc_kernel(pmd, CONSISTENT_BASE);</span>
<span class="p_add">+		if (!pte) {</span>
<span class="p_add">+			ret = -ENOMEM;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		consistent_pte = pte;</span>
<span class="p_add">+	} while (0);</span>
<span class="p_add">+</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+core_initcall(consistent_init);</span>
<span class="p_add">+static void consistent_sync(void *vaddr, size_t size, int direction);</span>
<span class="p_add">+static dma_addr_t nds32_dma_map_page(struct device *dev, struct page *page,</span>
<span class="p_add">+				     unsigned long offset, size_t size,</span>
<span class="p_add">+				     enum dma_data_direction dir,</span>
<span class="p_add">+				     unsigned long attrs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	consistent_sync((void *)(page_address(page) + offset), size, dir);</span>
<span class="p_add">+	return page_to_phys(page) + offset;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void nds32_dma_unmap_page(struct device *dev, dma_addr_t handle,</span>
<span class="p_add">+				 size_t size, enum dma_data_direction dir,</span>
<span class="p_add">+				 unsigned long attrs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	consistent_sync(phys_to_virt(handle), size, dir);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Make an area consistent for devices.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static void consistent_sync(void *vaddr, size_t size, int direction)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long start = (unsigned long)vaddr;</span>
<span class="p_add">+	unsigned long end = start + size;</span>
<span class="p_add">+</span>
<span class="p_add">+	switch (direction) {</span>
<span class="p_add">+	case DMA_FROM_DEVICE:	/* invalidate only */</span>
<span class="p_add">+		cpu_dma_inval_range(start, end);</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case DMA_TO_DEVICE:	/* writeback only */</span>
<span class="p_add">+		cpu_dma_wb_range(start, end);</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case DMA_BIDIRECTIONAL:	/* writeback and invalidate */</span>
<span class="p_add">+		cpu_dma_wbinval_range(start, end);</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	default:</span>
<span class="p_add">+		BUG();</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int nds32_dma_map_sg(struct device *dev, struct scatterlist *sg,</span>
<span class="p_add">+			    int nents, enum dma_data_direction dir,</span>
<span class="p_add">+			    unsigned long attrs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i &lt; nents; i++, sg++) {</span>
<span class="p_add">+		void *virt;</span>
<span class="p_add">+		unsigned long pfn;</span>
<span class="p_add">+		struct page *page = sg_page(sg);</span>
<span class="p_add">+</span>
<span class="p_add">+		sg-&gt;dma_address = sg_phys(sg);</span>
<span class="p_add">+		pfn = page_to_pfn(page) + sg-&gt;offset / PAGE_SIZE;</span>
<span class="p_add">+		page = pfn_to_page(pfn);</span>
<span class="p_add">+		if (PageHighMem(page)) {</span>
<span class="p_add">+			virt = kmap_atomic(page);</span>
<span class="p_add">+			consistent_sync(virt, sg-&gt;length, dir);</span>
<span class="p_add">+			kunmap_atomic(virt);</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			if (sg-&gt;offset &gt; PAGE_SIZE)</span>
<span class="p_add">+				panic(&quot;sg-&gt;offset:%08x &gt; PAGE_SIZE\n&quot;,</span>
<span class="p_add">+				      sg-&gt;offset);</span>
<span class="p_add">+			virt = page_address(page) + sg-&gt;offset;</span>
<span class="p_add">+			consistent_sync(virt, sg-&gt;length, dir);</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return nents;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void nds32_dma_unmap_sg(struct device *dev, struct scatterlist *sg,</span>
<span class="p_add">+			       int nhwentries, enum dma_data_direction dir,</span>
<span class="p_add">+			       unsigned long attrs)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void</span>
<span class="p_add">+nds32_dma_sync_single_for_cpu(struct device *dev, dma_addr_t handle,</span>
<span class="p_add">+			      size_t size, enum dma_data_direction dir)</span>
<span class="p_add">+{</span>
<span class="p_add">+	consistent_sync((void *)phys_to_virt(handle), size, dir);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void</span>
<span class="p_add">+nds32_dma_sync_single_for_device(struct device *dev, dma_addr_t handle,</span>
<span class="p_add">+				 size_t size, enum dma_data_direction dir)</span>
<span class="p_add">+{</span>
<span class="p_add">+	consistent_sync((void *)phys_to_virt(handle), size, dir);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void</span>
<span class="p_add">+nds32_dma_sync_sg_for_cpu(struct device *dev, struct scatterlist *sg, int nents,</span>
<span class="p_add">+			  enum dma_data_direction dir)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i &lt; nents; i++, sg++) {</span>
<span class="p_add">+		char *virt =</span>
<span class="p_add">+		    page_address((struct page *)sg-&gt;page_link) + sg-&gt;offset;</span>
<span class="p_add">+		consistent_sync(virt, sg-&gt;length, dir);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void</span>
<span class="p_add">+nds32_dma_sync_sg_for_device(struct device *dev, struct scatterlist *sg,</span>
<span class="p_add">+			     int nents, enum dma_data_direction dir)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i &lt; nents; i++, sg++) {</span>
<span class="p_add">+		char *virt =</span>
<span class="p_add">+		    page_address((struct page *)sg-&gt;page_link) + sg-&gt;offset;</span>
<span class="p_add">+		consistent_sync(virt, sg-&gt;length, dir);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+struct dma_map_ops nds32_dma_ops = {</span>
<span class="p_add">+	.alloc = nds32_dma_alloc_coherent,</span>
<span class="p_add">+	.free = nds32_dma_free,</span>
<span class="p_add">+	.map_page = nds32_dma_map_page,</span>
<span class="p_add">+	.unmap_page = nds32_dma_unmap_page,</span>
<span class="p_add">+	.map_sg = nds32_dma_map_sg,</span>
<span class="p_add">+	.unmap_sg = nds32_dma_unmap_sg,</span>
<span class="p_add">+	.sync_single_for_device = nds32_dma_sync_single_for_device,</span>
<span class="p_add">+	.sync_single_for_cpu = nds32_dma_sync_single_for_cpu,</span>
<span class="p_add">+	.sync_sg_for_cpu = nds32_dma_sync_sg_for_cpu,</span>
<span class="p_add">+	.sync_sg_for_device = nds32_dma_sync_sg_for_device,</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+EXPORT_SYMBOL(nds32_dma_ops);</span>

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



