
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[PATCHv4,05/10] arm64: Use __pa_symbol for kernel symbols - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [PATCHv4,05/10] arm64: Use __pa_symbol for kernel symbols</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=130411">Laura Abbott</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Nov. 29, 2016, 6:55 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1480445729-27130-6-git-send-email-labbott@redhat.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9453011/mbox/"
   >mbox</a>
|
   <a href="/patch/9453011/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9453011/">/patch/9453011/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	CD0D860756 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 29 Nov 2016 18:58:12 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id B85CA283E6
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 29 Nov 2016 18:58:12 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id AD019283E8; Tue, 29 Nov 2016 18:58:12 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 7525D283F1
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 29 Nov 2016 18:58:11 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1755544AbcK2S5y (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 29 Nov 2016 13:57:54 -0500
Received: from mail-qk0-f178.google.com ([209.85.220.178]:35818 &quot;EHLO
	mail-qk0-f178.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1754722AbcK2Sz5 (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 29 Nov 2016 13:55:57 -0500
Received: by mail-qk0-f178.google.com with SMTP id n204so184708290qke.2
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Tue, 29 Nov 2016 10:55:57 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20130820;
	h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
	:references;
	bh=W+bk/xMf/8umWMA/BgY4IRtiNOpNutehAhu6IvtQ/n0=;
	b=Tzn+CiOVzgsQsLRHA1fplbqwx4LsOZePI3q3qoHulvSnUWicG9nJ9QPZq9ji+FGVyY
	u9fv53sgrYeewX3z9rAZp6Y8xmXD3mNiwqMVC+CTtqwhwD2gFLRjSwxaKyLDfRTm6Hg3
	MryfDbnetoQPiwC81NgI6YOkra9+yHisRTXLdtc7pogExD28Yrtg9cvSgBCrBlv0N2fi
	AHUG9WMrHap3nmCwldCdh5ysvjSA/FXmtBj9LEWwO08wQ/Zo627DJ+tJ+Pr/i9CX/tAT
	K5NS5pnDGDEDaBTCHvVeFuopxi3glBPdZ9RSXyv8AN3PUE0SCYXm7x3oyfbXnlqhxJes
	j50Q==
X-Gm-Message-State: AKaTC00FPWGoNOs8DxTgf+sLQw+mB4/s/P937OOc6GY1z2usVhUXL24yMjKWyjXQz5/iZ7P8
X-Received: by 10.55.155.146 with SMTP id
	d140mr24279882qke.127.1480445750849; 
	Tue, 29 Nov 2016 10:55:50 -0800 (PST)
Received: from labbott-redhat-machine.redhat.com ([2601:602:9800:177f::df9b])
	by smtp.gmail.com with ESMTPSA id
	p28sm31446489qtb.31.2016.11.29.10.55.47
	(version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
	Tue, 29 Nov 2016 10:55:49 -0800 (PST)
From: Laura Abbott &lt;labbott@redhat.com&gt;
To: Mark Rutland &lt;mark.rutland@arm.com&gt;,
	Ard Biesheuvel &lt;ard.biesheuvel@linaro.org&gt;,
	Will Deacon &lt;will.deacon@arm.com&gt;,
	Catalin Marinas &lt;catalin.marinas@arm.com&gt;,
	Christoffer Dall &lt;christoffer.dall@linaro.org&gt;,
	Marc Zyngier &lt;marc.zyngier@arm.com&gt;,
	Lorenzo Pieralisi &lt;lorenzo.pieralisi@arm.com&gt;
Cc: Laura Abbott &lt;labbott@redhat.com&gt;, Thomas Gleixner &lt;tglx@linutronix.de&gt;,
	Ingo Molnar &lt;mingo@redhat.com&gt;,
	&quot;H. Peter Anvin&quot; &lt;hpa@zytor.com&gt;, x86@kernel.org,
	linux-kernel@vger.kernel.org, linux-mm@kvack.org,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	Marek Szyprowski &lt;m.szyprowski@samsung.com&gt;,
	Joonsoo Kim &lt;iamjoonsoo.kim@lge.com&gt;,
	linux-arm-kernel@lists.infradead.org
Subject: [PATCHv4 05/10] arm64: Use __pa_symbol for kernel symbols
Date: Tue, 29 Nov 2016 10:55:24 -0800
Message-Id: &lt;1480445729-27130-6-git-send-email-labbott@redhat.com&gt;
X-Mailer: git-send-email 2.7.4
In-Reply-To: &lt;1480445729-27130-1-git-send-email-labbott@redhat.com&gt;
References: &lt;1480445729-27130-1-git-send-email-labbott@redhat.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=130411">Laura Abbott</a> - Nov. 29, 2016, 6:55 p.m.</div>
<pre class="content">
__pa_symbol is technically the marco that should be used for kernel
symbols. Switch to this as a pre-requisite for DEBUG_VIRTUAL which
will do bounds checking. As part of this, introduce lm_alias, a
macro which wraps the __va(__pa(...)) idiom used a few places to
get the alias.
<span class="signed-off-by">
Signed-off-by: Laura Abbott &lt;labbott@redhat.com&gt;</span>
---
v4: Stop calling __va early, conversion of a few more sites. I decided against
wrapping the __p*d_populate calls into new functions since the call sites
should be limited.
---
 arch/arm64/include/asm/kvm_mmu.h          |  4 ++--
 arch/arm64/include/asm/memory.h           |  2 ++
 arch/arm64/include/asm/mmu_context.h      |  6 +++---
 arch/arm64/include/asm/pgtable.h          |  2 +-
 arch/arm64/kernel/acpi_parking_protocol.c |  2 +-
 arch/arm64/kernel/cpu-reset.h             |  2 +-
 arch/arm64/kernel/cpufeature.c            |  2 +-
 arch/arm64/kernel/hibernate.c             | 13 +++++--------
 arch/arm64/kernel/insn.c                  |  2 +-
 arch/arm64/kernel/psci.c                  |  2 +-
 arch/arm64/kernel/setup.c                 |  8 ++++----
 arch/arm64/kernel/smp_spin_table.c        |  2 +-
 arch/arm64/kernel/vdso.c                  |  4 ++--
 arch/arm64/mm/init.c                      | 11 ++++++-----
 arch/arm64/mm/kasan_init.c                | 21 +++++++++++++-------
 arch/arm64/mm/mmu.c                       | 32 +++++++++++++++++++------------
 drivers/firmware/psci.c                   |  2 +-
 include/linux/mm.h                        |  4 ++++
 18 files changed, 70 insertions(+), 51 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=352">Catalin Marinas</a> - Nov. 30, 2016, 5:17 p.m.</div>
<pre class="content">
On Tue, Nov 29, 2016 at 10:55:24AM -0800, Laura Abbott wrote:
<span class="quote">&gt; --- a/arch/arm64/include/asm/memory.h</span>
<span class="quote">&gt; +++ b/arch/arm64/include/asm/memory.h</span>
<span class="quote">&gt; @@ -205,6 +205,8 @@ static inline void *phys_to_virt(phys_addr_t x)</span>
<span class="quote">&gt;  #define __va(x)			((void *)__phys_to_virt((phys_addr_t)(x)))</span>
<span class="quote">&gt;  #define pfn_to_kaddr(pfn)	__va((pfn) &lt;&lt; PAGE_SHIFT)</span>
<span class="quote">&gt;  #define virt_to_pfn(x)      __phys_to_pfn(__virt_to_phys((unsigned long)(x)))</span>
<span class="quote">&gt; +#define sym_to_pfn(x)	    __phys_to_pfn(__pa_symbol(x))</span>
<span class="quote">&gt; +#define lm_alias(x)		__va(__pa_symbol(x))</span>
[...]
<span class="quote">&gt; --- a/include/linux/mm.h</span>
<span class="quote">&gt; +++ b/include/linux/mm.h</span>
<span class="quote">&gt; @@ -76,6 +76,10 @@ extern int mmap_rnd_compat_bits __read_mostly;</span>
<span class="quote">&gt;  #define page_to_virt(x)	__va(PFN_PHYS(page_to_pfn(x)))</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +#ifndef lm_alias</span>
<span class="quote">&gt; +#define lm_alias(x)	__va(__pa_symbol(x))</span>
<span class="quote">&gt; +#endif</span>

You can drop the arm64-specific lm_alias macro as it&#39;s the same as the
generic one you introduced in the same patch.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=4640">Florian Fainelli</a> - Dec. 6, 2016, 12:50 a.m.</div>
<pre class="content">
On 11/29/2016 10:55 AM, Laura Abbott wrote:
<span class="quote">&gt; __pa_symbol is technically the marco that should be used for kernel</span>
<span class="quote">&gt; symbols. Switch to this as a pre-requisite for DEBUG_VIRTUAL which</span>
<span class="quote">&gt; will do bounds checking. As part of this, introduce lm_alias, a</span>
<span class="quote">&gt; macro which wraps the __va(__pa(...)) idiom used a few places to</span>
<span class="quote">&gt; get the alias.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Laura Abbott &lt;labbott@redhat.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt; v4: Stop calling __va early, conversion of a few more sites. I decided against</span>
<span class="quote">&gt; wrapping the __p*d_populate calls into new functions since the call sites</span>
<span class="quote">&gt; should be limited.</span>
<span class="quote">&gt; ---</span>
<span class="quote">

&gt; -	pud_populate(&amp;init_mm, pud, bm_pmd);</span>
<span class="quote">&gt; +	if (pud_none(*pud))</span>
<span class="quote">&gt; +		__pud_populate(pud, __pa_symbol(bm_pmd), PMD_TYPE_TABLE);</span>
<span class="quote">&gt;  	pmd = fixmap_pmd(addr);</span>
<span class="quote">&gt; -	pmd_populate_kernel(&amp;init_mm, pmd, bm_pte);</span>
<span class="quote">&gt; +	__pmd_populate(pmd, __pa_symbol(bm_pte), PMD_TYPE_TABLE);</span>

Is there a particular reason why pmd_populate_kernel() is not changed to
use __pa_symbol() instead of using __pa()? The other users in the arm64
kernel is arch/arm64/kernel/hibernate.c which seems to call this against
kernel symbols as well?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=352">Catalin Marinas</a> - Dec. 6, 2016, 11:46 a.m.</div>
<pre class="content">
On Mon, Dec 05, 2016 at 04:50:33PM -0800, Florian Fainelli wrote:
<span class="quote">&gt; On 11/29/2016 10:55 AM, Laura Abbott wrote:</span>
<span class="quote">&gt; &gt; __pa_symbol is technically the marco that should be used for kernel</span>
<span class="quote">&gt; &gt; symbols. Switch to this as a pre-requisite for DEBUG_VIRTUAL which</span>
<span class="quote">&gt; &gt; will do bounds checking. As part of this, introduce lm_alias, a</span>
<span class="quote">&gt; &gt; macro which wraps the __va(__pa(...)) idiom used a few places to</span>
<span class="quote">&gt; &gt; get the alias.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Signed-off-by: Laura Abbott &lt;labbott@redhat.com&gt;</span>
<span class="quote">&gt; &gt; ---</span>
<span class="quote">&gt; &gt; v4: Stop calling __va early, conversion of a few more sites. I decided against</span>
<span class="quote">&gt; &gt; wrapping the __p*d_populate calls into new functions since the call sites</span>
<span class="quote">&gt; &gt; should be limited.</span>
<span class="quote">&gt; &gt; ---</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; -	pud_populate(&amp;init_mm, pud, bm_pmd);</span>
<span class="quote">&gt; &gt; +	if (pud_none(*pud))</span>
<span class="quote">&gt; &gt; +		__pud_populate(pud, __pa_symbol(bm_pmd), PMD_TYPE_TABLE);</span>
<span class="quote">&gt; &gt;  	pmd = fixmap_pmd(addr);</span>
<span class="quote">&gt; &gt; -	pmd_populate_kernel(&amp;init_mm, pmd, bm_pte);</span>
<span class="quote">&gt; &gt; +	__pmd_populate(pmd, __pa_symbol(bm_pte), PMD_TYPE_TABLE);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Is there a particular reason why pmd_populate_kernel() is not changed to</span>
<span class="quote">&gt; use __pa_symbol() instead of using __pa()? The other users in the arm64</span>
<span class="quote">&gt; kernel is arch/arm64/kernel/hibernate.c which seems to call this against</span>
<span class="quote">&gt; kernel symbols as well?</span>

create_safe_exec_page() may allocate a pte from the linear map and
passes such pointer to pmd_populate_kernel(). The copy_pte() function
does something similar. In addition, we have the generic
__pte_alloc_kernel() in mm/memory.c using linear addresses.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=30282">Mark Rutland</a> - Dec. 6, 2016, 5:02 p.m.</div>
<pre class="content">
Hi,

As a heads-up, it looks like this got mangled somewhere. In the hunk at
arch/arm64/mm/kasan_init.c:68, &#39;do&#39; in the context became &#39;edo&#39;.
Deleting the &#39;e&#39; makes it apply.

I think this is almost there; other than James&#39;s hibernate bug I only
see one real issue, and everything else is a minor nit.

On Tue, Nov 29, 2016 at 10:55:24AM -0800, Laura Abbott wrote:
<span class="quote">&gt; __pa_symbol is technically the marco that should be used for kernel</span>
<span class="quote">&gt; symbols. Switch to this as a pre-requisite for DEBUG_VIRTUAL which</span>
<span class="quote">&gt; will do bounds checking. As part of this, introduce lm_alias, a</span>
<span class="quote">&gt; macro which wraps the __va(__pa(...)) idiom used a few places to</span>
<span class="quote">&gt; get the alias.</span>

I think the addition of the lm_alias() macro under include/mm should be
a separate preparatory patch. That way it&#39;s separate from the
arm64-specific parts, and more obvious to !arm64 people reviewing the
other parts.
<span class="quote">
&gt; Signed-off-by: Laura Abbott &lt;labbott@redhat.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt; v4: Stop calling __va early, conversion of a few more sites. I decided against</span>
<span class="quote">&gt; wrapping the __p*d_populate calls into new functions since the call sites</span>
<span class="quote">&gt; should be limited.</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  arch/arm64/include/asm/kvm_mmu.h          |  4 ++--</span>
<span class="quote">&gt;  arch/arm64/include/asm/memory.h           |  2 ++</span>
<span class="quote">&gt;  arch/arm64/include/asm/mmu_context.h      |  6 +++---</span>
<span class="quote">&gt;  arch/arm64/include/asm/pgtable.h          |  2 +-</span>
<span class="quote">&gt;  arch/arm64/kernel/acpi_parking_protocol.c |  2 +-</span>
<span class="quote">&gt;  arch/arm64/kernel/cpu-reset.h             |  2 +-</span>
<span class="quote">&gt;  arch/arm64/kernel/cpufeature.c            |  2 +-</span>
<span class="quote">&gt;  arch/arm64/kernel/hibernate.c             | 13 +++++--------</span>
<span class="quote">&gt;  arch/arm64/kernel/insn.c                  |  2 +-</span>
<span class="quote">&gt;  arch/arm64/kernel/psci.c                  |  2 +-</span>
<span class="quote">&gt;  arch/arm64/kernel/setup.c                 |  8 ++++----</span>
<span class="quote">&gt;  arch/arm64/kernel/smp_spin_table.c        |  2 +-</span>
<span class="quote">&gt;  arch/arm64/kernel/vdso.c                  |  4 ++--</span>
<span class="quote">&gt;  arch/arm64/mm/init.c                      | 11 ++++++-----</span>
<span class="quote">&gt;  arch/arm64/mm/kasan_init.c                | 21 +++++++++++++-------</span>
<span class="quote">&gt;  arch/arm64/mm/mmu.c                       | 32 +++++++++++++++++++------------</span>
<span class="quote">&gt;  drivers/firmware/psci.c                   |  2 +-</span>
<span class="quote">&gt;  include/linux/mm.h                        |  4 ++++</span>
<span class="quote">&gt;  18 files changed, 70 insertions(+), 51 deletions(-)</span>

It looks like we need to make sure these (directly) include &lt;linux/mm.h&gt;
for __pa_symbol() and lm_alias(), or there&#39;s some fragility, e.g.

[mark@leverpostej:~/src/linux]% uselinaro 15.08 make ARCH=arm64 CROSS_COMPILE=aarch64-linux-gnu- -j10 -s
arch/arm64/kernel/psci.c: In function &#39;cpu_psci_cpu_boot&#39;:
arch/arm64/kernel/psci.c:48:50: error: implicit declaration of function &#39;__pa_symbol&#39; [-Werror=implicit-function-declaration]
  int err = psci_ops.cpu_on(cpu_logical_map(cpu), __pa_symbol(secondary_entry));
                                                  ^
cc1: some warnings being treated as errors
make[1]: *** [arch/arm64/kernel/psci.o] Error 1
make: *** [arch/arm64/kernel] Error 2
make: *** Waiting for unfinished jobs....
<span class="quote">
&gt; --- a/arch/arm64/include/asm/memory.h</span>
<span class="quote">&gt; +++ b/arch/arm64/include/asm/memory.h</span>
<span class="quote">&gt; @@ -205,6 +205,8 @@ static inline void *phys_to_virt(phys_addr_t x)</span>
<span class="quote">&gt;  #define __va(x)			((void *)__phys_to_virt((phys_addr_t)(x)))</span>
<span class="quote">&gt;  #define pfn_to_kaddr(pfn)	__va((pfn) &lt;&lt; PAGE_SHIFT)</span>
<span class="quote">&gt;  #define virt_to_pfn(x)      __phys_to_pfn(__virt_to_phys((unsigned long)(x)))</span>
<span class="quote">&gt; +#define sym_to_pfn(x)	    __phys_to_pfn(__pa_symbol(x))</span>
<span class="quote">&gt; +#define lm_alias(x)		__va(__pa_symbol(x))</span>

As Catalin mentioned, we should be able to drop this copy of lm_alias(),
given we have the same in the core headers.
<span class="quote">
&gt; diff --git a/arch/arm64/kernel/vdso.c b/arch/arm64/kernel/vdso.c</span>
<span class="quote">&gt; index a2c2478..79cd86b 100644</span>
<span class="quote">&gt; --- a/arch/arm64/kernel/vdso.c</span>
<span class="quote">&gt; +++ b/arch/arm64/kernel/vdso.c</span>
<span class="quote">&gt; @@ -140,11 +140,11 @@ static int __init vdso_init(void)</span>
<span class="quote">&gt;  		return -ENOMEM;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/* Grab the vDSO data page. */</span>
<span class="quote">&gt; -	vdso_pagelist[0] = pfn_to_page(PHYS_PFN(__pa(vdso_data)));</span>
<span class="quote">&gt; +	vdso_pagelist[0] = phys_to_page(__pa_symbol(vdso_data));</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/* Grab the vDSO code pages. */</span>
<span class="quote">&gt;  	for (i = 0; i &lt; vdso_pages; i++)</span>
<span class="quote">&gt; -		vdso_pagelist[i + 1] = pfn_to_page(PHYS_PFN(__pa(&amp;vdso_start)) + i);</span>
<span class="quote">&gt; +		vdso_pagelist[i + 1] = pfn_to_page(PHYS_PFN(__pa_symbol(&amp;vdso_start)) + i);</span>

I see you added sym_to_pfn(), which we can use here to keep this short
and legible. It might also be worth using a temporary pfn_t, e.g.

	pfn = sym_to_pfn(&amp;vdso_start);

	for (i = 0; i &lt; vdso_pages; i++)
		vdso_pagelist[i + 1] = pfn_to_page(pfn + i);
<span class="quote">
&gt; diff --git a/drivers/firmware/psci.c b/drivers/firmware/psci.c</span>
<span class="quote">&gt; index 8263429..9defbe2 100644</span>
<span class="quote">&gt; --- a/drivers/firmware/psci.c</span>
<span class="quote">&gt; +++ b/drivers/firmware/psci.c</span>
<span class="quote">&gt; @@ -383,7 +383,7 @@ static int psci_suspend_finisher(unsigned long index)</span>
<span class="quote">&gt;  	u32 *state = __this_cpu_read(psci_power_state);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	return psci_ops.cpu_suspend(state[index - 1],</span>
<span class="quote">&gt; -				    virt_to_phys(cpu_resume));</span>
<span class="quote">&gt; +				    __pa_symbol(cpu_resume));</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  int psci_cpu_suspend_enter(unsigned long index)</span>

This should probably be its own patch since it&#39;s not under arch/arm64/.

I&#39;m happy for this to go via the arm64 tree with the rest regardless
(assuming Lorenzo has no objections).

Thanks,
Mark.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=130411">Laura Abbott</a> - Dec. 6, 2016, 7:12 p.m.</div>
<pre class="content">
On 12/06/2016 09:02 AM, Mark Rutland wrote:
<span class="quote">&gt; Hi,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; As a heads-up, it looks like this got mangled somewhere. In the hunk at</span>
<span class="quote">&gt; arch/arm64/mm/kasan_init.c:68, &#39;do&#39; in the context became &#39;edo&#39;.</span>
<span class="quote">&gt; Deleting the &#39;e&#39; makes it apply.</span>
<span class="quote">&gt; </span>

Argh, this must have come in while editing the .patch before e-mailing.
Sorry about that.
<span class="quote">
&gt; I think this is almost there; other than James&#39;s hibernate bug I only</span>
<span class="quote">&gt; see one real issue, and everything else is a minor nit.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; On Tue, Nov 29, 2016 at 10:55:24AM -0800, Laura Abbott wrote:</span>
<span class="quote">&gt;&gt; __pa_symbol is technically the marco that should be used for kernel</span>
<span class="quote">&gt;&gt; symbols. Switch to this as a pre-requisite for DEBUG_VIRTUAL which</span>
<span class="quote">&gt;&gt; will do bounds checking. As part of this, introduce lm_alias, a</span>
<span class="quote">&gt;&gt; macro which wraps the __va(__pa(...)) idiom used a few places to</span>
<span class="quote">&gt;&gt; get the alias.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I think the addition of the lm_alias() macro under include/mm should be</span>
<span class="quote">&gt; a separate preparatory patch. That way it&#39;s separate from the</span>
<span class="quote">&gt; arm64-specific parts, and more obvious to !arm64 people reviewing the</span>
<span class="quote">&gt; other parts.</span>
<span class="quote">&gt; </span>

I debated if it was more obvious to show how it was used in context
vs a stand alone patch. I think you&#39;re right that for non-arm64 reviewers
the separate patch would be easier to find.
<span class="quote">
&gt;&gt; Signed-off-by: Laura Abbott &lt;labbott@redhat.com&gt;</span>
<span class="quote">&gt;&gt; ---</span>
<span class="quote">&gt;&gt; v4: Stop calling __va early, conversion of a few more sites. I decided against</span>
<span class="quote">&gt;&gt; wrapping the __p*d_populate calls into new functions since the call sites</span>
<span class="quote">&gt;&gt; should be limited.</span>
<span class="quote">&gt;&gt; ---</span>
<span class="quote">&gt;&gt;  arch/arm64/include/asm/kvm_mmu.h          |  4 ++--</span>
<span class="quote">&gt;&gt;  arch/arm64/include/asm/memory.h           |  2 ++</span>
<span class="quote">&gt;&gt;  arch/arm64/include/asm/mmu_context.h      |  6 +++---</span>
<span class="quote">&gt;&gt;  arch/arm64/include/asm/pgtable.h          |  2 +-</span>
<span class="quote">&gt;&gt;  arch/arm64/kernel/acpi_parking_protocol.c |  2 +-</span>
<span class="quote">&gt;&gt;  arch/arm64/kernel/cpu-reset.h             |  2 +-</span>
<span class="quote">&gt;&gt;  arch/arm64/kernel/cpufeature.c            |  2 +-</span>
<span class="quote">&gt;&gt;  arch/arm64/kernel/hibernate.c             | 13 +++++--------</span>
<span class="quote">&gt;&gt;  arch/arm64/kernel/insn.c                  |  2 +-</span>
<span class="quote">&gt;&gt;  arch/arm64/kernel/psci.c                  |  2 +-</span>
<span class="quote">&gt;&gt;  arch/arm64/kernel/setup.c                 |  8 ++++----</span>
<span class="quote">&gt;&gt;  arch/arm64/kernel/smp_spin_table.c        |  2 +-</span>
<span class="quote">&gt;&gt;  arch/arm64/kernel/vdso.c                  |  4 ++--</span>
<span class="quote">&gt;&gt;  arch/arm64/mm/init.c                      | 11 ++++++-----</span>
<span class="quote">&gt;&gt;  arch/arm64/mm/kasan_init.c                | 21 +++++++++++++-------</span>
<span class="quote">&gt;&gt;  arch/arm64/mm/mmu.c                       | 32 +++++++++++++++++++------------</span>
<span class="quote">&gt;&gt;  drivers/firmware/psci.c                   |  2 +-</span>
<span class="quote">&gt;&gt;  include/linux/mm.h                        |  4 ++++</span>
<span class="quote">&gt;&gt;  18 files changed, 70 insertions(+), 51 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It looks like we need to make sure these (directly) include &lt;linux/mm.h&gt;</span>
<span class="quote">&gt; for __pa_symbol() and lm_alias(), or there&#39;s some fragility, e.g.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; [mark@leverpostej:~/src/linux]% uselinaro 15.08 make ARCH=arm64 CROSS_COMPILE=aarch64-linux-gnu- -j10 -s</span>
<span class="quote">&gt; arch/arm64/kernel/psci.c: In function &#39;cpu_psci_cpu_boot&#39;:</span>
<span class="quote">&gt; arch/arm64/kernel/psci.c:48:50: error: implicit declaration of function &#39;__pa_symbol&#39; [-Werror=implicit-function-declaration]</span>
<span class="quote">&gt;   int err = psci_ops.cpu_on(cpu_logical_map(cpu), __pa_symbol(secondary_entry));</span>
<span class="quote">&gt;                                                   ^</span>
<span class="quote">&gt; cc1: some warnings being treated as errors</span>
<span class="quote">&gt; make[1]: *** [arch/arm64/kernel/psci.o] Error 1</span>
<span class="quote">&gt; make: *** [arch/arm64/kernel] Error 2</span>
<span class="quote">&gt; make: *** Waiting for unfinished jobs....</span>
<span class="quote">&gt; </span>

Right, I&#39;ll double check. 
<span class="quote">
&gt;&gt; --- a/arch/arm64/include/asm/memory.h</span>
<span class="quote">&gt;&gt; +++ b/arch/arm64/include/asm/memory.h</span>
<span class="quote">&gt;&gt; @@ -205,6 +205,8 @@ static inline void *phys_to_virt(phys_addr_t x)</span>
<span class="quote">&gt;&gt;  #define __va(x)			((void *)__phys_to_virt((phys_addr_t)(x)))</span>
<span class="quote">&gt;&gt;  #define pfn_to_kaddr(pfn)	__va((pfn) &lt;&lt; PAGE_SHIFT)</span>
<span class="quote">&gt;&gt;  #define virt_to_pfn(x)      __phys_to_pfn(__virt_to_phys((unsigned long)(x)))</span>
<span class="quote">&gt;&gt; +#define sym_to_pfn(x)	    __phys_to_pfn(__pa_symbol(x))</span>
<span class="quote">&gt;&gt; +#define lm_alias(x)		__va(__pa_symbol(x))</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; As Catalin mentioned, we should be able to drop this copy of lm_alias(),</span>
<span class="quote">&gt; given we have the same in the core headers.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; diff --git a/arch/arm64/kernel/vdso.c b/arch/arm64/kernel/vdso.c</span>
<span class="quote">&gt;&gt; index a2c2478..79cd86b 100644</span>
<span class="quote">&gt;&gt; --- a/arch/arm64/kernel/vdso.c</span>
<span class="quote">&gt;&gt; +++ b/arch/arm64/kernel/vdso.c</span>
<span class="quote">&gt;&gt; @@ -140,11 +140,11 @@ static int __init vdso_init(void)</span>
<span class="quote">&gt;&gt;  		return -ENOMEM;</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  	/* Grab the vDSO data page. */</span>
<span class="quote">&gt;&gt; -	vdso_pagelist[0] = pfn_to_page(PHYS_PFN(__pa(vdso_data)));</span>
<span class="quote">&gt;&gt; +	vdso_pagelist[0] = phys_to_page(__pa_symbol(vdso_data));</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  	/* Grab the vDSO code pages. */</span>
<span class="quote">&gt;&gt;  	for (i = 0; i &lt; vdso_pages; i++)</span>
<span class="quote">&gt;&gt; -		vdso_pagelist[i + 1] = pfn_to_page(PHYS_PFN(__pa(&amp;vdso_start)) + i);</span>
<span class="quote">&gt;&gt; +		vdso_pagelist[i + 1] = pfn_to_page(PHYS_PFN(__pa_symbol(&amp;vdso_start)) + i);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I see you added sym_to_pfn(), which we can use here to keep this short</span>
<span class="quote">&gt; and legible. It might also be worth using a temporary pfn_t, e.g.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	pfn = sym_to_pfn(&amp;vdso_start);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	for (i = 0; i &lt; vdso_pages; i++)</span>
<span class="quote">&gt; 		vdso_pagelist[i + 1] = pfn_to_page(pfn + i);</span>
<span class="quote">&gt; </span>

Good idea.
<span class="quote">
&gt;&gt; diff --git a/drivers/firmware/psci.c b/drivers/firmware/psci.c</span>
<span class="quote">&gt;&gt; index 8263429..9defbe2 100644</span>
<span class="quote">&gt;&gt; --- a/drivers/firmware/psci.c</span>
<span class="quote">&gt;&gt; +++ b/drivers/firmware/psci.c</span>
<span class="quote">&gt;&gt; @@ -383,7 +383,7 @@ static int psci_suspend_finisher(unsigned long index)</span>
<span class="quote">&gt;&gt;  	u32 *state = __this_cpu_read(psci_power_state);</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  	return psci_ops.cpu_suspend(state[index - 1],</span>
<span class="quote">&gt;&gt; -				    virt_to_phys(cpu_resume));</span>
<span class="quote">&gt;&gt; +				    __pa_symbol(cpu_resume));</span>
<span class="quote">&gt;&gt;  }</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  int psci_cpu_suspend_enter(unsigned long index)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This should probably be its own patch since it&#39;s not under arch/arm64/.</span>
<span class="quote">&gt; </span>

Fine by me.
<span class="quote">
&gt; I&#39;m happy for this to go via the arm64 tree with the rest regardless</span>
<span class="quote">&gt; (assuming Lorenzo has no objections).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Thanks,</span>
<span class="quote">&gt; Mark.</span>
<span class="quote">&gt; </span>

Thanks,
Laura
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/arm64/include/asm/kvm_mmu.h b/arch/arm64/include/asm/kvm_mmu.h</span>
<span class="p_header">index 6f72fe8..55772c1 100644</span>
<span class="p_header">--- a/arch/arm64/include/asm/kvm_mmu.h</span>
<span class="p_header">+++ b/arch/arm64/include/asm/kvm_mmu.h</span>
<span class="p_chunk">@@ -47,7 +47,7 @@</span> <span class="p_context"></span>
  * If the page is in the bottom half, we have to use the top half. If
  * the page is in the top half, we have to use the bottom half:
  *
<span class="p_del">- * T = __virt_to_phys(__hyp_idmap_text_start)</span>
<span class="p_add">+ * T = __pa_symbol(__hyp_idmap_text_start)</span>
  * if (T &amp; BIT(VA_BITS - 1))
  *	HYP_VA_MIN = 0  //idmap in upper half
  * else
<span class="p_chunk">@@ -271,7 +271,7 @@</span> <span class="p_context"> static inline void __kvm_flush_dcache_pud(pud_t pud)</span>
 	kvm_flush_dcache_to_poc(page_address(page), PUD_SIZE);
 }
 
<span class="p_del">-#define kvm_virt_to_phys(x)		__virt_to_phys((unsigned long)(x))</span>
<span class="p_add">+#define kvm_virt_to_phys(x)		__pa_symbol(x)</span>
 
 void kvm_set_way_flush(struct kvm_vcpu *vcpu);
 void kvm_toggle_cache(struct kvm_vcpu *vcpu, bool was_enabled);
<span class="p_header">diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h</span>
<span class="p_header">index d773e2c..a219d3f 100644</span>
<span class="p_header">--- a/arch/arm64/include/asm/memory.h</span>
<span class="p_header">+++ b/arch/arm64/include/asm/memory.h</span>
<span class="p_chunk">@@ -205,6 +205,8 @@</span> <span class="p_context"> static inline void *phys_to_virt(phys_addr_t x)</span>
 #define __va(x)			((void *)__phys_to_virt((phys_addr_t)(x)))
 #define pfn_to_kaddr(pfn)	__va((pfn) &lt;&lt; PAGE_SHIFT)
 #define virt_to_pfn(x)      __phys_to_pfn(__virt_to_phys((unsigned long)(x)))
<span class="p_add">+#define sym_to_pfn(x)	    __phys_to_pfn(__pa_symbol(x))</span>
<span class="p_add">+#define lm_alias(x)		__va(__pa_symbol(x))</span>
 
 /*
  *  virt_to_page(k)	convert a _valid_ virtual address to struct page *
<span class="p_header">diff --git a/arch/arm64/include/asm/mmu_context.h b/arch/arm64/include/asm/mmu_context.h</span>
<span class="p_header">index a501853..ea0f969 100644</span>
<span class="p_header">--- a/arch/arm64/include/asm/mmu_context.h</span>
<span class="p_header">+++ b/arch/arm64/include/asm/mmu_context.h</span>
<span class="p_chunk">@@ -44,7 +44,7 @@</span> <span class="p_context"> static inline void contextidr_thread_switch(struct task_struct *next)</span>
  */
 static inline void cpu_set_reserved_ttbr0(void)
 {
<span class="p_del">-	unsigned long ttbr = virt_to_phys(empty_zero_page);</span>
<span class="p_add">+	unsigned long ttbr = __pa_symbol(empty_zero_page);</span>
 
 	write_sysreg(ttbr, ttbr0_el1);
 	isb();
<span class="p_chunk">@@ -113,7 +113,7 @@</span> <span class="p_context"> static inline void cpu_install_idmap(void)</span>
 	local_flush_tlb_all();
 	cpu_set_idmap_tcr_t0sz();
 
<span class="p_del">-	cpu_switch_mm(idmap_pg_dir, &amp;init_mm);</span>
<span class="p_add">+	cpu_switch_mm(lm_alias(idmap_pg_dir), &amp;init_mm);</span>
 }
 
 /*
<span class="p_chunk">@@ -128,7 +128,7 @@</span> <span class="p_context"> static inline void cpu_replace_ttbr1(pgd_t *pgd)</span>
 
 	phys_addr_t pgd_phys = virt_to_phys(pgd);
 
<span class="p_del">-	replace_phys = (void *)virt_to_phys(idmap_cpu_replace_ttbr1);</span>
<span class="p_add">+	replace_phys = (void *)__pa_symbol(idmap_cpu_replace_ttbr1);</span>
 
 	cpu_install_idmap();
 	replace_phys(pgd_phys);
<span class="p_header">diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h</span>
<span class="p_header">index ffbb9a5..090134c 100644</span>
<span class="p_header">--- a/arch/arm64/include/asm/pgtable.h</span>
<span class="p_header">+++ b/arch/arm64/include/asm/pgtable.h</span>
<span class="p_chunk">@@ -52,7 +52,7 @@</span> <span class="p_context"> extern void __pgd_error(const char *file, int line, unsigned long val);</span>
  * for zero-mapped memory areas etc..
  */
 extern unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)];
<span class="p_del">-#define ZERO_PAGE(vaddr)	pfn_to_page(PHYS_PFN(__pa(empty_zero_page)))</span>
<span class="p_add">+#define ZERO_PAGE(vaddr)	phys_to_page(__pa_symbol(empty_zero_page))</span>
 
 #define pte_ERROR(pte)		__pte_error(__FILE__, __LINE__, pte_val(pte))
 
<span class="p_header">diff --git a/arch/arm64/kernel/acpi_parking_protocol.c b/arch/arm64/kernel/acpi_parking_protocol.c</span>
<span class="p_header">index a32b401..df58310 100644</span>
<span class="p_header">--- a/arch/arm64/kernel/acpi_parking_protocol.c</span>
<span class="p_header">+++ b/arch/arm64/kernel/acpi_parking_protocol.c</span>
<span class="p_chunk">@@ -109,7 +109,7 @@</span> <span class="p_context"> static int acpi_parking_protocol_cpu_boot(unsigned int cpu)</span>
 	 * that read this address need to convert this address to the
 	 * Boot-Loader&#39;s endianness before jumping.
 	 */
<span class="p_del">-	writeq_relaxed(__pa(secondary_entry), &amp;mailbox-&gt;entry_point);</span>
<span class="p_add">+	writeq_relaxed(__pa_symbol(secondary_entry), &amp;mailbox-&gt;entry_point);</span>
 	writel_relaxed(cpu_entry-&gt;gic_cpu_id, &amp;mailbox-&gt;cpu_id);
 
 	arch_send_wakeup_ipi_mask(cpumask_of(cpu));
<span class="p_header">diff --git a/arch/arm64/kernel/cpu-reset.h b/arch/arm64/kernel/cpu-reset.h</span>
<span class="p_header">index d4e9ecb..6c2b1b4 100644</span>
<span class="p_header">--- a/arch/arm64/kernel/cpu-reset.h</span>
<span class="p_header">+++ b/arch/arm64/kernel/cpu-reset.h</span>
<span class="p_chunk">@@ -24,7 +24,7 @@</span> <span class="p_context"> static inline void __noreturn cpu_soft_restart(unsigned long el2_switch,</span>
 
 	el2_switch = el2_switch &amp;&amp; !is_kernel_in_hyp_mode() &amp;&amp;
 		is_hyp_mode_available();
<span class="p_del">-	restart = (void *)virt_to_phys(__cpu_soft_restart);</span>
<span class="p_add">+	restart = (void *)__pa_symbol(__cpu_soft_restart);</span>
 
 	cpu_install_idmap();
 	restart(el2_switch, entry, arg0, arg1, arg2);
<span class="p_header">diff --git a/arch/arm64/kernel/cpufeature.c b/arch/arm64/kernel/cpufeature.c</span>
<span class="p_header">index c02504e..6ccadf2 100644</span>
<span class="p_header">--- a/arch/arm64/kernel/cpufeature.c</span>
<span class="p_header">+++ b/arch/arm64/kernel/cpufeature.c</span>
<span class="p_chunk">@@ -736,7 +736,7 @@</span> <span class="p_context"> static bool runs_at_el2(const struct arm64_cpu_capabilities *entry, int __unused</span>
 static bool hyp_offset_low(const struct arm64_cpu_capabilities *entry,
 			   int __unused)
 {
<span class="p_del">-	phys_addr_t idmap_addr = virt_to_phys(__hyp_idmap_text_start);</span>
<span class="p_add">+	phys_addr_t idmap_addr = __pa_symbol(__hyp_idmap_text_start);</span>
 
 	/*
 	 * Activate the lower HYP offset only if:
<span class="p_header">diff --git a/arch/arm64/kernel/hibernate.c b/arch/arm64/kernel/hibernate.c</span>
<span class="p_header">index d55a7b0..4f0c77d 100644</span>
<span class="p_header">--- a/arch/arm64/kernel/hibernate.c</span>
<span class="p_header">+++ b/arch/arm64/kernel/hibernate.c</span>
<span class="p_chunk">@@ -50,9 +50,6 @@</span> <span class="p_context"></span>
  */
 extern int in_suspend;
 
<span class="p_del">-/* Find a symbols alias in the linear map */</span>
<span class="p_del">-#define LMADDR(x)	phys_to_virt(virt_to_phys(x))</span>
<span class="p_del">-</span>
 /* Do we need to reset el2? */
 #define el2_reset_needed() (is_hyp_mode_available() &amp;&amp; !is_kernel_in_hyp_mode())
 
<span class="p_chunk">@@ -102,8 +99,8 @@</span> <span class="p_context"> static inline void arch_hdr_invariants(struct arch_hibernate_hdr_invariants *i)</span>
 
 int pfn_is_nosave(unsigned long pfn)
 {
<span class="p_del">-	unsigned long nosave_begin_pfn = virt_to_pfn(&amp;__nosave_begin);</span>
<span class="p_del">-	unsigned long nosave_end_pfn = virt_to_pfn(&amp;__nosave_end - 1);</span>
<span class="p_add">+	unsigned long nosave_begin_pfn = sym_to_pfn(&amp;__nosave_begin);</span>
<span class="p_add">+	unsigned long nosave_end_pfn = sym_to_pfn(&amp;__nosave_end - 1);</span>
 
 	return (pfn &gt;= nosave_begin_pfn) &amp;&amp; (pfn &lt;= nosave_end_pfn);
 }
<span class="p_chunk">@@ -125,12 +122,12 @@</span> <span class="p_context"> int arch_hibernation_header_save(void *addr, unsigned int max_size)</span>
 		return -EOVERFLOW;
 
 	arch_hdr_invariants(&amp;hdr-&gt;invariants);
<span class="p_del">-	hdr-&gt;ttbr1_el1		= virt_to_phys(swapper_pg_dir);</span>
<span class="p_add">+	hdr-&gt;ttbr1_el1		= __pa_symbol(swapper_pg_dir);</span>
 	hdr-&gt;reenter_kernel	= _cpu_resume;
 
 	/* We can&#39;t use __hyp_get_vectors() because kvm may still be loaded */
 	if (el2_reset_needed())
<span class="p_del">-		hdr-&gt;__hyp_stub_vectors = virt_to_phys(__hyp_stub_vectors);</span>
<span class="p_add">+		hdr-&gt;__hyp_stub_vectors = __pa_symbol(__hyp_stub_vectors);</span>
 	else
 		hdr-&gt;__hyp_stub_vectors = 0;
 
<span class="p_chunk">@@ -484,7 +481,7 @@</span> <span class="p_context"> int swsusp_arch_resume(void)</span>
 	 * Since we only copied the linear map, we need to find restore_pblist&#39;s
 	 * linear map address.
 	 */
<span class="p_del">-	lm_restore_pblist = LMADDR(restore_pblist);</span>
<span class="p_add">+	lm_restore_pblist = lm_alias(restore_pblist);</span>
 
 	/*
 	 * We need a zero page that is zero before &amp; after resume in order to
<span class="p_header">diff --git a/arch/arm64/kernel/insn.c b/arch/arm64/kernel/insn.c</span>
<span class="p_header">index 6f2ac4f..f607b38 100644</span>
<span class="p_header">--- a/arch/arm64/kernel/insn.c</span>
<span class="p_header">+++ b/arch/arm64/kernel/insn.c</span>
<span class="p_chunk">@@ -97,7 +97,7 @@</span> <span class="p_context"> static void __kprobes *patch_map(void *addr, int fixmap)</span>
 	if (module &amp;&amp; IS_ENABLED(CONFIG_DEBUG_SET_MODULE_RONX))
 		page = vmalloc_to_page(addr);
 	else if (!module)
<span class="p_del">-		page = pfn_to_page(PHYS_PFN(__pa(addr)));</span>
<span class="p_add">+		page = phys_to_page(__pa_symbol(addr));</span>
 	else
 		return addr;
 
<span class="p_header">diff --git a/arch/arm64/kernel/psci.c b/arch/arm64/kernel/psci.c</span>
<span class="p_header">index 42816be..f0f2abb 100644</span>
<span class="p_header">--- a/arch/arm64/kernel/psci.c</span>
<span class="p_header">+++ b/arch/arm64/kernel/psci.c</span>
<span class="p_chunk">@@ -45,7 +45,7 @@</span> <span class="p_context"> static int __init cpu_psci_cpu_prepare(unsigned int cpu)</span>
 
 static int cpu_psci_cpu_boot(unsigned int cpu)
 {
<span class="p_del">-	int err = psci_ops.cpu_on(cpu_logical_map(cpu), __pa(secondary_entry));</span>
<span class="p_add">+	int err = psci_ops.cpu_on(cpu_logical_map(cpu), __pa_symbol(secondary_entry));</span>
 	if (err)
 		pr_err(&quot;failed to boot CPU%d (%d)\n&quot;, cpu, err);
 
<span class="p_header">diff --git a/arch/arm64/kernel/setup.c b/arch/arm64/kernel/setup.c</span>
<span class="p_header">index f534f49..e2dbc02 100644</span>
<span class="p_header">--- a/arch/arm64/kernel/setup.c</span>
<span class="p_header">+++ b/arch/arm64/kernel/setup.c</span>
<span class="p_chunk">@@ -199,10 +199,10 @@</span> <span class="p_context"> static void __init request_standard_resources(void)</span>
 	struct memblock_region *region;
 	struct resource *res;
 
<span class="p_del">-	kernel_code.start   = virt_to_phys(_text);</span>
<span class="p_del">-	kernel_code.end     = virt_to_phys(__init_begin - 1);</span>
<span class="p_del">-	kernel_data.start   = virt_to_phys(_sdata);</span>
<span class="p_del">-	kernel_data.end     = virt_to_phys(_end - 1);</span>
<span class="p_add">+	kernel_code.start   = __pa_symbol(_text);</span>
<span class="p_add">+	kernel_code.end     = __pa_symbol(__init_begin - 1);</span>
<span class="p_add">+	kernel_data.start   = __pa_symbol(_sdata);</span>
<span class="p_add">+	kernel_data.end     = __pa_symbol(_end - 1);</span>
 
 	for_each_memblock(memory, region) {
 		res = alloc_bootmem_low(sizeof(*res));
<span class="p_header">diff --git a/arch/arm64/kernel/smp_spin_table.c b/arch/arm64/kernel/smp_spin_table.c</span>
<span class="p_header">index 9a00eee..25fccca 100644</span>
<span class="p_header">--- a/arch/arm64/kernel/smp_spin_table.c</span>
<span class="p_header">+++ b/arch/arm64/kernel/smp_spin_table.c</span>
<span class="p_chunk">@@ -98,7 +98,7 @@</span> <span class="p_context"> static int smp_spin_table_cpu_prepare(unsigned int cpu)</span>
 	 * boot-loader&#39;s endianess before jumping. This is mandated by
 	 * the boot protocol.
 	 */
<span class="p_del">-	writeq_relaxed(__pa(secondary_holding_pen), release_addr);</span>
<span class="p_add">+	writeq_relaxed(__pa_symbol(secondary_holding_pen), release_addr);</span>
 	__flush_dcache_area((__force void *)release_addr,
 			    sizeof(*release_addr));
 
<span class="p_header">diff --git a/arch/arm64/kernel/vdso.c b/arch/arm64/kernel/vdso.c</span>
<span class="p_header">index a2c2478..79cd86b 100644</span>
<span class="p_header">--- a/arch/arm64/kernel/vdso.c</span>
<span class="p_header">+++ b/arch/arm64/kernel/vdso.c</span>
<span class="p_chunk">@@ -140,11 +140,11 @@</span> <span class="p_context"> static int __init vdso_init(void)</span>
 		return -ENOMEM;
 
 	/* Grab the vDSO data page. */
<span class="p_del">-	vdso_pagelist[0] = pfn_to_page(PHYS_PFN(__pa(vdso_data)));</span>
<span class="p_add">+	vdso_pagelist[0] = phys_to_page(__pa_symbol(vdso_data));</span>
 
 	/* Grab the vDSO code pages. */
 	for (i = 0; i &lt; vdso_pages; i++)
<span class="p_del">-		vdso_pagelist[i + 1] = pfn_to_page(PHYS_PFN(__pa(&amp;vdso_start)) + i);</span>
<span class="p_add">+		vdso_pagelist[i + 1] = pfn_to_page(PHYS_PFN(__pa_symbol(&amp;vdso_start)) + i);</span>
 
 	vdso_spec[0].pages = &amp;vdso_pagelist[0];
 	vdso_spec[1].pages = &amp;vdso_pagelist[1];
<span class="p_header">diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c</span>
<span class="p_header">index 212c4d1..95ef998 100644</span>
<span class="p_header">--- a/arch/arm64/mm/init.c</span>
<span class="p_header">+++ b/arch/arm64/mm/init.c</span>
<span class="p_chunk">@@ -209,8 +209,8 @@</span> <span class="p_context"> void __init arm64_memblock_init(void)</span>
 	 * linear mapping. Take care not to clip the kernel which may be
 	 * high in memory.
 	 */
<span class="p_del">-	memblock_remove(max_t(u64, memstart_addr + linear_region_size, __pa(_end)),</span>
<span class="p_del">-			ULLONG_MAX);</span>
<span class="p_add">+	memblock_remove(max_t(u64, memstart_addr + linear_region_size,</span>
<span class="p_add">+			__pa_symbol(_end)), ULLONG_MAX);</span>
 	if (memstart_addr + linear_region_size &lt; memblock_end_of_DRAM()) {
 		/* ensure that memstart_addr remains sufficiently aligned */
 		memstart_addr = round_up(memblock_end_of_DRAM() - linear_region_size,
<span class="p_chunk">@@ -225,7 +225,7 @@</span> <span class="p_context"> void __init arm64_memblock_init(void)</span>
 	 */
 	if (memory_limit != (phys_addr_t)ULLONG_MAX) {
 		memblock_mem_limit_remove_map(memory_limit);
<span class="p_del">-		memblock_add(__pa(_text), (u64)(_end - _text));</span>
<span class="p_add">+		memblock_add(__pa_symbol(_text), (u64)(_end - _text));</span>
 	}
 
 	if (IS_ENABLED(CONFIG_BLK_DEV_INITRD) &amp;&amp; initrd_start) {
<span class="p_chunk">@@ -278,7 +278,7 @@</span> <span class="p_context"> void __init arm64_memblock_init(void)</span>
 	 * Register the kernel text, kernel data, initrd, and initial
 	 * pagetables with memblock.
 	 */
<span class="p_del">-	memblock_reserve(__pa(_text), _end - _text);</span>
<span class="p_add">+	memblock_reserve(__pa_symbol(_text), _end - _text);</span>
 #ifdef CONFIG_BLK_DEV_INITRD
 	if (initrd_start) {
 		memblock_reserve(initrd_start, initrd_end - initrd_start);
<span class="p_chunk">@@ -483,7 +483,8 @@</span> <span class="p_context"> void __init mem_init(void)</span>
 
 void free_initmem(void)
 {
<span class="p_del">-	free_reserved_area(__va(__pa(__init_begin)), __va(__pa(__init_end)),</span>
<span class="p_add">+	free_reserved_area(lm_alias(__init_begin),</span>
<span class="p_add">+			   lm_alias(__init_end),</span>
 			   0, &quot;unused kernel&quot;);
 	/*
 	 * Unmap the __init region but leave the VM area in place. This
<span class="p_header">diff --git a/arch/arm64/mm/kasan_init.c b/arch/arm64/mm/kasan_init.c</span>
<span class="p_header">index 757009d..0fb8110 100644</span>
<span class="p_header">--- a/arch/arm64/mm/kasan_init.c</span>
<span class="p_header">+++ b/arch/arm64/mm/kasan_init.c</span>
<span class="p_chunk">@@ -26,6 +26,13 @@</span> <span class="p_context"></span>
 
 static pgd_t tmp_pg_dir[PTRS_PER_PGD] __initdata __aligned(PGD_SIZE);
 
<span class="p_add">+/*</span>
<span class="p_add">+ * The p*d_populate functions call virt_to_phys implicitly so they can&#39;t be used</span>
<span class="p_add">+ * directly on kernel symbols (bm_p*d). All the early functions are called too</span>
<span class="p_add">+ * early to use lm_alias so __p*d_populate functions must be used to populate</span>
<span class="p_add">+ * with the physical address from __pa_symbol.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
 static void __init kasan_early_pte_populate(pmd_t *pmd, unsigned long addr,
 					unsigned long end)
 {
<span class="p_chunk">@@ -33,12 +40,12 @@</span> <span class="p_context"> static void __init kasan_early_pte_populate(pmd_t *pmd, unsigned long addr,</span>
 	unsigned long next;
 
 	if (pmd_none(*pmd))
<span class="p_del">-		pmd_populate_kernel(&amp;init_mm, pmd, kasan_zero_pte);</span>
<span class="p_add">+		__pmd_populate(pmd, __pa_symbol(kasan_zero_pte), PMD_TYPE_TABLE);</span>
 
 	pte = pte_offset_kimg(pmd, addr);
 	do {
 		next = addr + PAGE_SIZE;
<span class="p_del">-		set_pte(pte, pfn_pte(virt_to_pfn(kasan_zero_page),</span>
<span class="p_add">+		set_pte(pte, pfn_pte(sym_to_pfn(kasan_zero_page),</span>
 					PAGE_KERNEL));
 	} while (pte++, addr = next, addr != end &amp;&amp; pte_none(*pte));
 }
<span class="p_chunk">@@ -51,7 +58,7 @@</span> <span class="p_context"> static void __init kasan_early_pmd_populate(pud_t *pud,</span>
 	unsigned long next;
 
 	if (pud_none(*pud))
<span class="p_del">-		pud_populate(&amp;init_mm, pud, kasan_zero_pmd);</span>
<span class="p_add">+		__pud_populate(pud, __pa_symbol(kasan_zero_pmd), PMD_TYPE_TABLE);</span>
 
 	pmd = pmd_offset_kimg(pud, addr);
 	do {
<span class="p_chunk">@@ -68,7 +75,7 @@</span> <span class="p_context"> static void __init kasan_early_pud_populate(pgd_t *pgd,</span>
 	unsigned long next;
 
 	if (pgd_none(*pgd))
<span class="p_del">-		pgd_populate(&amp;init_mm, pgd, kasan_zero_pud);</span>
<span class="p_add">+		__pgd_populate(pgd, __pa_symbol(kasan_zero_pud), PUD_TYPE_TABLE);</span>
 
 	pud = pud_offset_kimg(pgd, addr);
 	edo {
<span class="p_chunk">@@ -148,7 +155,7 @@</span> <span class="p_context"> void __init kasan_init(void)</span>
 	 */
 	memcpy(tmp_pg_dir, swapper_pg_dir, sizeof(tmp_pg_dir));
 	dsb(ishst);
<span class="p_del">-	cpu_replace_ttbr1(tmp_pg_dir);</span>
<span class="p_add">+	cpu_replace_ttbr1(lm_alias(tmp_pg_dir));</span>
 
 	clear_pgds(KASAN_SHADOW_START, KASAN_SHADOW_END);
 
<span class="p_chunk">@@ -199,10 +206,10 @@</span> <span class="p_context"> void __init kasan_init(void)</span>
 	 */
 	for (i = 0; i &lt; PTRS_PER_PTE; i++)
 		set_pte(&amp;kasan_zero_pte[i],
<span class="p_del">-			pfn_pte(virt_to_pfn(kasan_zero_page), PAGE_KERNEL_RO));</span>
<span class="p_add">+			pfn_pte(sym_to_pfn(kasan_zero_page), PAGE_KERNEL_RO));</span>
 
 	memset(kasan_zero_page, 0, PAGE_SIZE);
<span class="p_del">-	cpu_replace_ttbr1(swapper_pg_dir);</span>
<span class="p_add">+	cpu_replace_ttbr1(lm_alias(swapper_pg_dir));</span>
 
 	/* At this point kasan is fully initialized. Enable error messages */
 	init_task.kasan_depth = 0;
<span class="p_header">diff --git a/arch/arm64/mm/mmu.c b/arch/arm64/mm/mmu.c</span>
<span class="p_header">index 05615a3..7498ebd 100644</span>
<span class="p_header">--- a/arch/arm64/mm/mmu.c</span>
<span class="p_header">+++ b/arch/arm64/mm/mmu.c</span>
<span class="p_chunk">@@ -319,8 +319,8 @@</span> <span class="p_context"> static void create_mapping_late(phys_addr_t phys, unsigned long virt,</span>
 
 static void __init __map_memblock(pgd_t *pgd, phys_addr_t start, phys_addr_t end)
 {
<span class="p_del">-	unsigned long kernel_start = __pa(_text);</span>
<span class="p_del">-	unsigned long kernel_end = __pa(__init_begin);</span>
<span class="p_add">+	unsigned long kernel_start = __pa_symbol(_text);</span>
<span class="p_add">+	unsigned long kernel_end = __pa_symbol(__init_begin);</span>
 
 	/*
 	 * Take care not to create a writable alias for the
<span class="p_chunk">@@ -387,21 +387,21 @@</span> <span class="p_context"> void mark_rodata_ro(void)</span>
 	unsigned long section_size;
 
 	section_size = (unsigned long)_etext - (unsigned long)_text;
<span class="p_del">-	create_mapping_late(__pa(_text), (unsigned long)_text,</span>
<span class="p_add">+	create_mapping_late(__pa_symbol(_text), (unsigned long)_text,</span>
 			    section_size, PAGE_KERNEL_ROX);
 	/*
 	 * mark .rodata as read only. Use __init_begin rather than __end_rodata
 	 * to cover NOTES and EXCEPTION_TABLE.
 	 */
 	section_size = (unsigned long)__init_begin - (unsigned long)__start_rodata;
<span class="p_del">-	create_mapping_late(__pa(__start_rodata), (unsigned long)__start_rodata,</span>
<span class="p_add">+	create_mapping_late(__pa_symbol(__start_rodata), (unsigned long)__start_rodata,</span>
 			    section_size, PAGE_KERNEL_RO);
 }
 
 static void __init map_kernel_segment(pgd_t *pgd, void *va_start, void *va_end,
 				      pgprot_t prot, struct vm_struct *vma)
 {
<span class="p_del">-	phys_addr_t pa_start = __pa(va_start);</span>
<span class="p_add">+	phys_addr_t pa_start = __pa_symbol(va_start);</span>
 	unsigned long size = va_end - va_start;
 
 	BUG_ON(!PAGE_ALIGNED(pa_start));
<span class="p_chunk">@@ -449,7 +449,7 @@</span> <span class="p_context"> static void __init map_kernel(pgd_t *pgd)</span>
 		 */
 		BUG_ON(!IS_ENABLED(CONFIG_ARM64_16K_PAGES));
 		set_pud(pud_set_fixmap_offset(pgd, FIXADDR_START),
<span class="p_del">-			__pud(__pa(bm_pmd) | PUD_TYPE_TABLE));</span>
<span class="p_add">+			__pud(__pa_symbol(bm_pmd) | PUD_TYPE_TABLE));</span>
 		pud_clear_fixmap();
 	} else {
 		BUG();
<span class="p_chunk">@@ -480,7 +480,7 @@</span> <span class="p_context"> void __init paging_init(void)</span>
 	 */
 	cpu_replace_ttbr1(__va(pgd_phys));
 	memcpy(swapper_pg_dir, pgd, PAGE_SIZE);
<span class="p_del">-	cpu_replace_ttbr1(swapper_pg_dir);</span>
<span class="p_add">+	cpu_replace_ttbr1(lm_alias(swapper_pg_dir));</span>
 
 	pgd_clear_fixmap();
 	memblock_free(pgd_phys, PAGE_SIZE);
<span class="p_chunk">@@ -489,7 +489,7 @@</span> <span class="p_context"> void __init paging_init(void)</span>
 	 * We only reuse the PGD from the swapper_pg_dir, not the pud + pmd
 	 * allocated with it.
 	 */
<span class="p_del">-	memblock_free(__pa(swapper_pg_dir) + PAGE_SIZE,</span>
<span class="p_add">+	memblock_free(__pa_symbol(swapper_pg_dir) + PAGE_SIZE,</span>
 		      SWAPPER_DIR_SIZE - PAGE_SIZE);
 }
 
<span class="p_chunk">@@ -600,6 +600,12 @@</span> <span class="p_context"> static inline pte_t * fixmap_pte(unsigned long addr)</span>
 	return &amp;bm_pte[pte_index(addr)];
 }
 
<span class="p_add">+/*</span>
<span class="p_add">+ * The p*d_populate functions call virt_to_phys implicitly so they can&#39;t be used</span>
<span class="p_add">+ * directly on kernel symbols (bm_p*d). This function is called too early to use</span>
<span class="p_add">+ * lm_alias so __p*d_populate functions must be used to populate with the</span>
<span class="p_add">+ * physical address from __pa_symbol.</span>
<span class="p_add">+ */</span>
 void __init early_fixmap_init(void)
 {
 	pgd_t *pgd;
<span class="p_chunk">@@ -609,7 +615,7 @@</span> <span class="p_context"> void __init early_fixmap_init(void)</span>
 
 	pgd = pgd_offset_k(addr);
 	if (CONFIG_PGTABLE_LEVELS &gt; 3 &amp;&amp;
<span class="p_del">-	    !(pgd_none(*pgd) || pgd_page_paddr(*pgd) == __pa(bm_pud))) {</span>
<span class="p_add">+	    !(pgd_none(*pgd) || pgd_page_paddr(*pgd) == __pa_symbol(bm_pud))) {</span>
 		/*
 		 * We only end up here if the kernel mapping and the fixmap
 		 * share the top level pgd entry, which should only happen on
<span class="p_chunk">@@ -618,12 +624,14 @@</span> <span class="p_context"> void __init early_fixmap_init(void)</span>
 		BUG_ON(!IS_ENABLED(CONFIG_ARM64_16K_PAGES));
 		pud = pud_offset_kimg(pgd, addr);
 	} else {
<span class="p_del">-		pgd_populate(&amp;init_mm, pgd, bm_pud);</span>
<span class="p_add">+		if (pgd_none(*pgd))</span>
<span class="p_add">+			__pgd_populate(pgd, __pa_symbol(bm_pud), PUD_TYPE_TABLE);</span>
 		pud = fixmap_pud(addr);
 	}
<span class="p_del">-	pud_populate(&amp;init_mm, pud, bm_pmd);</span>
<span class="p_add">+	if (pud_none(*pud))</span>
<span class="p_add">+		__pud_populate(pud, __pa_symbol(bm_pmd), PMD_TYPE_TABLE);</span>
 	pmd = fixmap_pmd(addr);
<span class="p_del">-	pmd_populate_kernel(&amp;init_mm, pmd, bm_pte);</span>
<span class="p_add">+	__pmd_populate(pmd, __pa_symbol(bm_pte), PMD_TYPE_TABLE);</span>
 
 	/*
 	 * The boot-ioremap range spans multiple pmds, for which
<span class="p_header">diff --git a/drivers/firmware/psci.c b/drivers/firmware/psci.c</span>
<span class="p_header">index 8263429..9defbe2 100644</span>
<span class="p_header">--- a/drivers/firmware/psci.c</span>
<span class="p_header">+++ b/drivers/firmware/psci.c</span>
<span class="p_chunk">@@ -383,7 +383,7 @@</span> <span class="p_context"> static int psci_suspend_finisher(unsigned long index)</span>
 	u32 *state = __this_cpu_read(psci_power_state);
 
 	return psci_ops.cpu_suspend(state[index - 1],
<span class="p_del">-				    virt_to_phys(cpu_resume));</span>
<span class="p_add">+				    __pa_symbol(cpu_resume));</span>
 }
 
 int psci_cpu_suspend_enter(unsigned long index)
<span class="p_header">diff --git a/include/linux/mm.h b/include/linux/mm.h</span>
<span class="p_header">index a92c8d7..88556b8 100644</span>
<span class="p_header">--- a/include/linux/mm.h</span>
<span class="p_header">+++ b/include/linux/mm.h</span>
<span class="p_chunk">@@ -76,6 +76,10 @@</span> <span class="p_context"> extern int mmap_rnd_compat_bits __read_mostly;</span>
 #define page_to_virt(x)	__va(PFN_PHYS(page_to_pfn(x)))
 #endif
 
<span class="p_add">+#ifndef lm_alias</span>
<span class="p_add">+#define lm_alias(x)	__va(__pa_symbol(x))</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 /*
  * To prevent common memory management code establishing
  * a zero page mapping on a read fault.

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



