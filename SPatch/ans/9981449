
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[RFC,v2,05/31] KVM: arm/arm64: Support mmu for the virtual EL2 execution - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [RFC,v2,05/31] KVM: arm/arm64: Support mmu for the virtual EL2 execution</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=174677">Jintack Lim</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Oct. 3, 2017, 3:10 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1507000273-3735-3-git-send-email-jintack.lim@linaro.org&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9981449/mbox/"
   >mbox</a>
|
   <a href="/patch/9981449/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9981449/">/patch/9981449/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	6E95E60384 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  3 Oct 2017 03:11:51 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 613842881D
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  3 Oct 2017 03:11:51 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 5646728822; Tue,  3 Oct 2017 03:11:51 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.5 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	DKIM_VALID,DKIM_VALID_AU,RCVD_IN_DNSWL_HI,RCVD_IN_SORBS_SPAM
	autolearn=unavailable version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 3F6322881D
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  3 Oct 2017 03:11:50 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1751893AbdJCDLr (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 2 Oct 2017 23:11:47 -0400
Received: from mail-io0-f178.google.com ([209.85.223.178]:46983 &quot;EHLO
	mail-io0-f178.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1751469AbdJCDLn (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 2 Oct 2017 23:11:43 -0400
Received: by mail-io0-f178.google.com with SMTP id d16so6450703ioj.3
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Mon, 02 Oct 2017 20:11:42 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=linaro.org; s=google;
	h=from:to:cc:subject:date:message-id:in-reply-to:references;
	bh=eAqx2xLxCExiluCRkqw3a6k08S4JY17Xyb7qWUk8dxk=;
	b=S8xzPk2Wq31DEaTbSDQYeG4CWBCMPuKFYDuzz2HU9FXhvPmUtg5DHHyrbPkeu3aiQt
	6rrnLYvufcAiHpiAlzodUICx4aOuhnJcvPVNw3iX1bWWq0lzJvjzNefhP9BmuMsBv4YM
	YrC7L2efBwhGBFZTgv0oF0s+6fxC2DJZgkM94=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20161025;
	h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
	:references;
	bh=eAqx2xLxCExiluCRkqw3a6k08S4JY17Xyb7qWUk8dxk=;
	b=hJTta4ozvbaoMcXobY28+ym21gX52Etv/HC1pWOHeLwHQVXuA0VghDY2EoZUwH6Rk9
	aTf65Mt2r5dpj1mkVy7RktGm3uFWozy0jsxxcxYLwV1/x5n7w9pFXCCms7Vy/TII+y6x
	uoW+0NZoyY2MYJKGY+J232L4OV5ubHZFZsTqtkEhawtuAkPxk1eNMA9/ALM/fV4OomUD
	JteOODDp7S5LmKs1H+l6KVKjzEP2tbG6yhStPubDpaacPqK8JdHlvvkF/6WI16Abnzyw
	WAWOO6fwdfrgI4XE3J2+nSskmLuZA0QKj79MZSd513Zk6QLOurIw3+9Dx6w724mh79AR
	kUiQ==
X-Gm-Message-State: AMCzsaXRbrz5BbjqwM3q+7gWa6p1OoVJaDZ0FRcM6McWDldVVvImInmp
	Xf7sqIJuQX5kVGOvPVj+AhlZMg==
X-Google-Smtp-Source: AOwi7QAIBwULKw9b0zd/HR8w5S8KEvT4t3oAigcFqJ7oB2XwtvUhoMEGnvWO1tz0o1vhUTdqx9R4Bg==
X-Received: by 10.107.204.5 with SMTP id c5mr26146495iog.66.1507000302318;
	Mon, 02 Oct 2017 20:11:42 -0700 (PDT)
Received: from node.jintackl-qv28633.kvmarm-pg0.wisc.cloudlab.us
	(c220g1-031126.wisc.cloudlab.us. [128.104.222.76])
	by smtp.gmail.com with ESMTPSA id
	h84sm5367193iod.72.2017.10.02.20.11.40
	(version=TLS1_2 cipher=ECDHE-RSA-AES128-SHA bits=128/128);
	Mon, 02 Oct 2017 20:11:41 -0700 (PDT)
From: Jintack Lim &lt;jintack.lim@linaro.org&gt;
To: christoffer.dall@linaro.org, marc.zyngier@arm.com,
	kvmarm@lists.cs.columbia.edu
Cc: jintack@cs.columbia.edu, pbonzini@redhat.com, rkrcmar@redhat.com,
	catalin.marinas@arm.com, will.deacon@arm.com,
	linux@armlinux.org.uk, mark.rutland@arm.com,
	linux-arm-kernel@lists.infradead.org, kvm@vger.kernel.org,
	linux-kernel@vger.kernel.org, Jintack Lim &lt;jintack.lim@linaro.org&gt;
Subject: [RFC PATCH v2 05/31] KVM: arm/arm64: Support mmu for the virtual
	EL2 execution
Date: Mon,  2 Oct 2017 22:10:47 -0500
Message-Id: &lt;1507000273-3735-3-git-send-email-jintack.lim@linaro.org&gt;
X-Mailer: git-send-email 1.9.1
In-Reply-To: &lt;1507000273-3735-1-git-send-email-jintack.lim@linaro.org&gt;
References: &lt;1507000273-3735-1-git-send-email-jintack.lim@linaro.org&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=174677">Jintack Lim</a> - Oct. 3, 2017, 3:10 a.m.</div>
<pre class="content">
<span class="from">From: Christoffer Dall &lt;christoffer.dall@linaro.org&gt;</span>

When running a guest hypervisor in virtual EL2, the translation context
has to be separate from the rest of the system, including the guest
EL1/0 translation regime, so we allocate a separate VMID for this mode.

Considering that we have two different vttbr values due to separate
VMIDs, it&#39;s racy to keep a vttbr value in a struct (kvm_s2_mmu) and
share it between multiple vcpus. So, remove the shared vttbr field, and
set up per-vcpu hw_vttbr field.

Hypercalls to flush tlb now have vttbr as a parameter instead of mmu,
since mmu structure does not have vttbr any more.
<span class="signed-off-by">
Signed-off-by: Christoffer Dall &lt;christoffer.dall@linaro.org&gt;</span>
<span class="signed-off-by">Signed-off-by: Jintack Lim &lt;jintack.lim@linaro.org&gt;</span>
---

Notes:
    v1--&gt;v2:
    Fixed a bug that hw_vttbr was not initialized correctly in kvm_arch_vcpu_init()
    where vmid is not allocated yet. This prevented the guest from booting on 32bit
    arm; hw_vttbr is set on each entry on aarch64, so it was fine.

 arch/arm/include/asm/kvm_asm.h       |  6 ++--
 arch/arm/include/asm/kvm_emulate.h   |  4 +++
 arch/arm/include/asm/kvm_host.h      | 14 +++++---
 arch/arm/include/asm/kvm_mmu.h       | 11 ++++++
 arch/arm/kvm/hyp/switch.c            |  4 +--
 arch/arm/kvm/hyp/tlb.c               | 15 ++++-----
 arch/arm64/include/asm/kvm_asm.h     |  6 ++--
 arch/arm64/include/asm/kvm_emulate.h |  8 +++++
 arch/arm64/include/asm/kvm_host.h    | 14 +++++---
 arch/arm64/include/asm/kvm_mmu.h     | 11 ++++++
 arch/arm64/kvm/hyp/switch.c          |  4 +--
 arch/arm64/kvm/hyp/tlb.c             | 34 +++++++++----------
 virt/kvm/arm/arm.c                   | 65 +++++++++++++++++++++---------------
 virt/kvm/arm/mmu.c                   |  9 +++--
 14 files changed, 128 insertions(+), 77 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/arm/include/asm/kvm_asm.h b/arch/arm/include/asm/kvm_asm.h</span>
<span class="p_header">index 71b7255..23a79bd 100644</span>
<span class="p_header">--- a/arch/arm/include/asm/kvm_asm.h</span>
<span class="p_header">+++ b/arch/arm/include/asm/kvm_asm.h</span>
<span class="p_chunk">@@ -65,9 +65,9 @@</span> <span class="p_context"></span>
 extern char __kvm_hyp_vector[];
 
 extern void __kvm_flush_vm_context(void);
<span class="p_del">-extern void __kvm_tlb_flush_vmid_ipa(struct kvm_s2_mmu *mmu, phys_addr_t ipa);</span>
<span class="p_del">-extern void __kvm_tlb_flush_vmid(struct kvm_s2_mmu *mmu);</span>
<span class="p_del">-extern void __kvm_tlb_flush_local_vmid(struct kvm_s2_mmu *mmu);</span>
<span class="p_add">+extern void __kvm_tlb_flush_vmid_ipa(u64 vttbr, phys_addr_t ipa);</span>
<span class="p_add">+extern void __kvm_tlb_flush_vmid(u64 vttbr);</span>
<span class="p_add">+extern void __kvm_tlb_flush_local_vmid(u64 vttbr);</span>
 
 extern int __kvm_vcpu_run(struct kvm_vcpu *vcpu);
 
<span class="p_header">diff --git a/arch/arm/include/asm/kvm_emulate.h b/arch/arm/include/asm/kvm_emulate.h</span>
<span class="p_header">index 29a4dec..24a3fbf 100644</span>
<span class="p_header">--- a/arch/arm/include/asm/kvm_emulate.h</span>
<span class="p_header">+++ b/arch/arm/include/asm/kvm_emulate.h</span>
<span class="p_chunk">@@ -293,4 +293,8 @@</span> <span class="p_context"> static inline unsigned long vcpu_data_host_to_guest(struct kvm_vcpu *vcpu,</span>
 	}
 }
 
<span class="p_add">+static inline struct kvm_s2_vmid *vcpu_get_active_vmid(struct kvm_vcpu *vcpu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return &amp;vcpu-&gt;kvm-&gt;arch.mmu.vmid;</span>
<span class="p_add">+}</span>
 #endif /* __ARM_KVM_EMULATE_H__ */
<span class="p_header">diff --git a/arch/arm/include/asm/kvm_host.h b/arch/arm/include/asm/kvm_host.h</span>
<span class="p_header">index 78d826e..33ccdbe 100644</span>
<span class="p_header">--- a/arch/arm/include/asm/kvm_host.h</span>
<span class="p_header">+++ b/arch/arm/include/asm/kvm_host.h</span>
<span class="p_chunk">@@ -53,16 +53,18 @@</span> <span class="p_context"></span>
 int kvm_reset_vcpu(struct kvm_vcpu *vcpu);
 void kvm_reset_coprocs(struct kvm_vcpu *vcpu);
 
<span class="p_del">-struct kvm_s2_mmu {</span>
<span class="p_add">+struct kvm_s2_vmid {</span>
 	/* The VMID generation used for the virt. memory system */
 	u64    vmid_gen;
 	u32    vmid;
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+struct kvm_s2_mmu {</span>
<span class="p_add">+	struct kvm_s2_vmid vmid;</span>
<span class="p_add">+	struct kvm_s2_vmid el2_vmid;</span>
 
 	/* Stage-2 page table */
 	pgd_t *pgd;
<span class="p_del">-</span>
<span class="p_del">-	/* VTTBR value associated with above pgd and vmid */</span>
<span class="p_del">-	u64    vttbr;</span>
 };
 
 struct kvm_arch {
<span class="p_chunk">@@ -193,6 +195,9 @@</span> <span class="p_context"> struct kvm_vcpu_arch {</span>
 
 	/* Stage 2 paging state used by the hardware on next switch */
 	struct kvm_s2_mmu *hw_mmu;
<span class="p_add">+</span>
<span class="p_add">+	/* VTTBR value used by the hardware on next switch */</span>
<span class="p_add">+	u64 hw_vttbr;</span>
 };
 
 struct kvm_vm_stat {
<span class="p_chunk">@@ -239,6 +244,7 @@</span> <span class="p_context"> static inline void kvm_arch_mmu_notifier_invalidate_page(struct kvm *kvm,</span>
 {
 }
 
<span class="p_add">+unsigned int get_kvm_vmid_bits(void);</span>
 struct kvm_vcpu *kvm_arm_get_running_vcpu(void);
 struct kvm_vcpu __percpu **kvm_get_running_vcpus(void);
 void kvm_arm_halt_guest(struct kvm *kvm);
<span class="p_header">diff --git a/arch/arm/include/asm/kvm_mmu.h b/arch/arm/include/asm/kvm_mmu.h</span>
<span class="p_header">index fa6f217..86fdc70 100644</span>
<span class="p_header">--- a/arch/arm/include/asm/kvm_mmu.h</span>
<span class="p_header">+++ b/arch/arm/include/asm/kvm_mmu.h</span>
<span class="p_chunk">@@ -221,6 +221,17 @@</span> <span class="p_context"> static inline unsigned int kvm_get_vmid_bits(void)</span>
 	return 8;
 }
 
<span class="p_add">+static inline u64 kvm_get_vttbr(struct kvm_s2_vmid *vmid,</span>
<span class="p_add">+				struct kvm_s2_mmu *mmu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	u64 vmid_field, baddr;</span>
<span class="p_add">+</span>
<span class="p_add">+	baddr = virt_to_phys(mmu-&gt;pgd);</span>
<span class="p_add">+	vmid_field = ((u64)vmid-&gt;vmid &lt;&lt; VTTBR_VMID_SHIFT) &amp;</span>
<span class="p_add">+		VTTBR_VMID_MASK(get_kvm_vmid_bits());</span>
<span class="p_add">+	return baddr | vmid_field;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 #endif	/* !__ASSEMBLY__ */
 
 #endif /* __ARM_KVM_MMU_H__ */
<span class="p_header">diff --git a/arch/arm/kvm/hyp/switch.c b/arch/arm/kvm/hyp/switch.c</span>
<span class="p_header">index 4814671..4798e39 100644</span>
<span class="p_header">--- a/arch/arm/kvm/hyp/switch.c</span>
<span class="p_header">+++ b/arch/arm/kvm/hyp/switch.c</span>
<span class="p_chunk">@@ -75,9 +75,7 @@</span> <span class="p_context"> static void __hyp_text __deactivate_traps(struct kvm_vcpu *vcpu)</span>
 
 static void __hyp_text __activate_vm(struct kvm_vcpu *vcpu)
 {
<span class="p_del">-	struct kvm_s2_mmu *mmu = kern_hyp_va(vcpu-&gt;arch.hw_mmu);</span>
<span class="p_del">-</span>
<span class="p_del">-	write_sysreg(mmu-&gt;vttbr, VTTBR);</span>
<span class="p_add">+	write_sysreg(vcpu-&gt;arch.hw_vttbr, VTTBR);</span>
 	write_sysreg(vcpu-&gt;arch.midr, VPIDR);
 }
 
<span class="p_header">diff --git a/arch/arm/kvm/hyp/tlb.c b/arch/arm/kvm/hyp/tlb.c</span>
<span class="p_header">index 56f0a49..562ad0b 100644</span>
<span class="p_header">--- a/arch/arm/kvm/hyp/tlb.c</span>
<span class="p_header">+++ b/arch/arm/kvm/hyp/tlb.c</span>
<span class="p_chunk">@@ -34,13 +34,12 @@</span> <span class="p_context"></span>
  * As v7 does not support flushing per IPA, just nuke the whole TLB
  * instead, ignoring the ipa value.
  */
<span class="p_del">-void __hyp_text __kvm_tlb_flush_vmid(struct kvm_s2_mmu *mmu)</span>
<span class="p_add">+void __hyp_text __kvm_tlb_flush_vmid(u64 vttbr)</span>
 {
 	dsb(ishst);
 
 	/* Switch to requested VMID */
<span class="p_del">-	mmu = kern_hyp_va(mmu);</span>
<span class="p_del">-	write_sysreg(mmu-&gt;vttbr, VTTBR);</span>
<span class="p_add">+	write_sysreg(vttbr, VTTBR);</span>
 	isb();
 
 	write_sysreg(0, TLBIALLIS);
<span class="p_chunk">@@ -50,17 +49,15 @@</span> <span class="p_context"> void __hyp_text __kvm_tlb_flush_vmid(struct kvm_s2_mmu *mmu)</span>
 	write_sysreg(0, VTTBR);
 }
 
<span class="p_del">-void __hyp_text __kvm_tlb_flush_vmid_ipa(struct kvm_s2_mmu *mmu,</span>
<span class="p_del">-					 phys_addr_t ipa)</span>
<span class="p_add">+void __hyp_text __kvm_tlb_flush_vmid_ipa(u64 vttbr, phys_addr_t ipa)</span>
 {
<span class="p_del">-	__kvm_tlb_flush_vmid(mmu);</span>
<span class="p_add">+	__kvm_tlb_flush_vmid(vttbr);</span>
 }
 
<span class="p_del">-void __hyp_text __kvm_tlb_flush_local_vmid(struct kvm_s2_mmu *mmu)</span>
<span class="p_add">+void __hyp_text __kvm_tlb_flush_local_vmid(u64 vttbr)</span>
 {
 	/* Switch to requested VMID */
<span class="p_del">-	mmu = kern_hyp_va(mmu);</span>
<span class="p_del">-	write_sysreg(mmu-&gt;vttbr, VTTBR);</span>
<span class="p_add">+	write_sysreg(vttbr, VTTBR);</span>
 	isb();
 
 	write_sysreg(0, TLBIALL);
<span class="p_header">diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h</span>
<span class="p_header">index ff6244f..e492749 100644</span>
<span class="p_header">--- a/arch/arm64/include/asm/kvm_asm.h</span>
<span class="p_header">+++ b/arch/arm64/include/asm/kvm_asm.h</span>
<span class="p_chunk">@@ -52,9 +52,9 @@</span> <span class="p_context"></span>
 extern char __kvm_hyp_vector[];
 
 extern void __kvm_flush_vm_context(void);
<span class="p_del">-extern void __kvm_tlb_flush_vmid_ipa(struct kvm_s2_mmu *mmu, phys_addr_t ipa);</span>
<span class="p_del">-extern void __kvm_tlb_flush_vmid(struct kvm_s2_mmu *mmu);</span>
<span class="p_del">-extern void __kvm_tlb_flush_local_vmid(struct kvm_s2_mmu *mmu);</span>
<span class="p_add">+extern void __kvm_tlb_flush_vmid_ipa(u64 vttbr, phys_addr_t ipa);</span>
<span class="p_add">+extern void __kvm_tlb_flush_vmid(u64 vttbr);</span>
<span class="p_add">+extern void __kvm_tlb_flush_local_vmid(u64 vttbr);</span>
 
 extern int __kvm_vcpu_run(struct kvm_vcpu *vcpu);
 
<span class="p_header">diff --git a/arch/arm64/include/asm/kvm_emulate.h b/arch/arm64/include/asm/kvm_emulate.h</span>
<span class="p_header">index 4776bfc..71a3a04 100644</span>
<span class="p_header">--- a/arch/arm64/include/asm/kvm_emulate.h</span>
<span class="p_header">+++ b/arch/arm64/include/asm/kvm_emulate.h</span>
<span class="p_chunk">@@ -385,4 +385,12 @@</span> <span class="p_context"> static inline unsigned long vcpu_data_host_to_guest(struct kvm_vcpu *vcpu,</span>
 	return data;		/* Leave LE untouched */
 }
 
<span class="p_add">+static inline struct kvm_s2_vmid *vcpu_get_active_vmid(struct kvm_vcpu *vcpu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (unlikely(is_hyp_ctxt(vcpu)))</span>
<span class="p_add">+		return &amp;vcpu-&gt;kvm-&gt;arch.mmu.el2_vmid;</span>
<span class="p_add">+</span>
<span class="p_add">+	return &amp;vcpu-&gt;kvm-&gt;arch.mmu.vmid;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 #endif /* __ARM64_KVM_EMULATE_H__ */
<span class="p_header">diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h</span>
<span class="p_header">index e7e9f70..a7edf0e 100644</span>
<span class="p_header">--- a/arch/arm64/include/asm/kvm_host.h</span>
<span class="p_header">+++ b/arch/arm64/include/asm/kvm_host.h</span>
<span class="p_chunk">@@ -50,17 +50,19 @@</span> <span class="p_context"></span>
 int kvm_arch_dev_ioctl_check_extension(struct kvm *kvm, long ext);
 void __extended_idmap_trampoline(phys_addr_t boot_pgd, phys_addr_t idmap_start);
 
<span class="p_del">-struct kvm_s2_mmu {</span>
<span class="p_add">+struct kvm_s2_vmid {</span>
 	/* The VMID generation used for the virt. memory system */
 	u64    vmid_gen;
 	u32    vmid;
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+struct kvm_s2_mmu {</span>
<span class="p_add">+	struct kvm_s2_vmid vmid;</span>
<span class="p_add">+	struct kvm_s2_vmid el2_vmid;</span>
 
 	/* 1-level 2nd stage table and lock */
 	spinlock_t pgd_lock;
 	pgd_t *pgd;
<span class="p_del">-</span>
<span class="p_del">-	/* VTTBR value associated with above pgd and vmid */</span>
<span class="p_del">-	u64    vttbr;</span>
 };
 
 struct kvm_arch {
<span class="p_chunk">@@ -337,6 +339,9 @@</span> <span class="p_context"> struct kvm_vcpu_arch {</span>
 
 	/* Stage 2 paging state used by the hardware on next switch */
 	struct kvm_s2_mmu *hw_mmu;
<span class="p_add">+</span>
<span class="p_add">+	/* VTTBR value used by the hardware on next switch */</span>
<span class="p_add">+	u64 hw_vttbr;</span>
 };
 
 #define vcpu_gp_regs(v)		(&amp;(v)-&gt;arch.ctxt.gp_regs)
<span class="p_chunk">@@ -394,6 +399,7 @@</span> <span class="p_context"> static inline void kvm_arch_mmu_notifier_invalidate_page(struct kvm *kvm,</span>
 {
 }
 
<span class="p_add">+unsigned int get_kvm_vmid_bits(void);</span>
 struct kvm_vcpu *kvm_arm_get_running_vcpu(void);
 struct kvm_vcpu * __percpu *kvm_get_running_vcpus(void);
 void kvm_arm_halt_guest(struct kvm *kvm);
<span class="p_header">diff --git a/arch/arm64/include/asm/kvm_mmu.h b/arch/arm64/include/asm/kvm_mmu.h</span>
<span class="p_header">index a89cc22..21c0299 100644</span>
<span class="p_header">--- a/arch/arm64/include/asm/kvm_mmu.h</span>
<span class="p_header">+++ b/arch/arm64/include/asm/kvm_mmu.h</span>
<span class="p_chunk">@@ -312,5 +312,16 @@</span> <span class="p_context"> static inline unsigned int kvm_get_vmid_bits(void)</span>
 	return (cpuid_feature_extract_unsigned_field(reg, ID_AA64MMFR1_VMIDBITS_SHIFT) == 2) ? 16 : 8;
 }
 
<span class="p_add">+static inline u64 kvm_get_vttbr(struct kvm_s2_vmid *vmid,</span>
<span class="p_add">+				struct kvm_s2_mmu *mmu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	u64 vmid_field, baddr;</span>
<span class="p_add">+</span>
<span class="p_add">+	baddr = virt_to_phys(mmu-&gt;pgd);</span>
<span class="p_add">+	vmid_field = ((u64)vmid-&gt;vmid &lt;&lt; VTTBR_VMID_SHIFT) &amp;</span>
<span class="p_add">+		VTTBR_VMID_MASK(get_kvm_vmid_bits());</span>
<span class="p_add">+	return baddr | vmid_field;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 #endif /* __ASSEMBLY__ */
 #endif /* __ARM64_KVM_MMU_H__ */
<span class="p_header">diff --git a/arch/arm64/kvm/hyp/switch.c b/arch/arm64/kvm/hyp/switch.c</span>
<span class="p_header">index 8b1b3e9..3626e76 100644</span>
<span class="p_header">--- a/arch/arm64/kvm/hyp/switch.c</span>
<span class="p_header">+++ b/arch/arm64/kvm/hyp/switch.c</span>
<span class="p_chunk">@@ -181,9 +181,7 @@</span> <span class="p_context"> static void __hyp_text __deactivate_traps(struct kvm_vcpu *vcpu)</span>
 
 static void __hyp_text __activate_vm(struct kvm_vcpu *vcpu)
 {
<span class="p_del">-	struct kvm_s2_mmu *mmu = kern_hyp_va(vcpu-&gt;arch.hw_mmu);</span>
<span class="p_del">-</span>
<span class="p_del">-	write_sysreg(mmu-&gt;vttbr, vttbr_el2);</span>
<span class="p_add">+	write_sysreg(vcpu-&gt;arch.hw_vttbr, vttbr_el2);</span>
 }
 
 static void __hyp_text __deactivate_vm(struct kvm_vcpu *vcpu)
<span class="p_header">diff --git a/arch/arm64/kvm/hyp/tlb.c b/arch/arm64/kvm/hyp/tlb.c</span>
<span class="p_header">index 0897678..680b960 100644</span>
<span class="p_header">--- a/arch/arm64/kvm/hyp/tlb.c</span>
<span class="p_header">+++ b/arch/arm64/kvm/hyp/tlb.c</span>
<span class="p_chunk">@@ -18,7 +18,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/kvm_hyp.h&gt;
 #include &lt;asm/tlbflush.h&gt;
 
<span class="p_del">-static void __hyp_text __tlb_switch_to_guest_vhe(struct kvm_s2_mmu *mmu)</span>
<span class="p_add">+static void __hyp_text __tlb_switch_to_guest_vhe(u64 vttbr)</span>
 {
 	u64 val;
 
<span class="p_chunk">@@ -29,16 +29,16 @@</span> <span class="p_context"> static void __hyp_text __tlb_switch_to_guest_vhe(struct kvm_s2_mmu *mmu)</span>
 	 * bits. Changing E2H is impossible (goodbye TTBR1_EL2), so
 	 * let&#39;s flip TGE before executing the TLB operation.
 	 */
<span class="p_del">-	write_sysreg(mmu-&gt;vttbr, vttbr_el2);</span>
<span class="p_add">+	write_sysreg(vttbr, vttbr_el2);</span>
 	val = read_sysreg(hcr_el2);
 	val &amp;= ~HCR_TGE;
 	write_sysreg(val, hcr_el2);
 	isb();
 }
 
<span class="p_del">-static void __hyp_text __tlb_switch_to_guest_nvhe(struct kvm_s2_mmu *mmu)</span>
<span class="p_add">+static void __hyp_text __tlb_switch_to_guest_nvhe(u64 vttbr)</span>
 {
<span class="p_del">-	write_sysreg(mmu-&gt;vttbr, vttbr_el2);</span>
<span class="p_add">+	write_sysreg(vttbr, vttbr_el2);</span>
 	isb();
 }
 
<span class="p_chunk">@@ -47,7 +47,7 @@</span> <span class="p_context"> static hyp_alternate_select(__tlb_switch_to_guest,</span>
 			    __tlb_switch_to_guest_vhe,
 			    ARM64_HAS_VIRT_HOST_EXTN);
 
<span class="p_del">-static void __hyp_text __tlb_switch_to_host_vhe(struct kvm_s2_mmu *mmu)</span>
<span class="p_add">+static void __hyp_text __tlb_switch_to_host_vhe(void)</span>
 {
 	/*
 	 * We&#39;re done with the TLB operation, let&#39;s restore the host&#39;s
<span class="p_chunk">@@ -57,7 +57,7 @@</span> <span class="p_context"> static void __hyp_text __tlb_switch_to_host_vhe(struct kvm_s2_mmu *mmu)</span>
 	write_sysreg(HCR_HOST_VHE_FLAGS, hcr_el2);
 }
 
<span class="p_del">-static void __hyp_text __tlb_switch_to_host_nvhe(struct kvm_s2_mmu *mmu)</span>
<span class="p_add">+static void __hyp_text __tlb_switch_to_host_nvhe(void)</span>
 {
 	write_sysreg(0, vttbr_el2);
 }
<span class="p_chunk">@@ -67,14 +67,12 @@</span> <span class="p_context"> static hyp_alternate_select(__tlb_switch_to_host,</span>
 			    __tlb_switch_to_host_vhe,
 			    ARM64_HAS_VIRT_HOST_EXTN);
 
<span class="p_del">-void __hyp_text __kvm_tlb_flush_vmid_ipa(struct kvm_s2_mmu *mmu,</span>
<span class="p_del">-					 phys_addr_t ipa)</span>
<span class="p_add">+void __hyp_text __kvm_tlb_flush_vmid_ipa(u64 vttbr, phys_addr_t ipa)</span>
 {
 	dsb(ishst);
 
 	/* Switch to requested VMID */
<span class="p_del">-	mmu = kern_hyp_va(mmu);</span>
<span class="p_del">-	__tlb_switch_to_guest()(mmu);</span>
<span class="p_add">+	__tlb_switch_to_guest()(vttbr);</span>
 
 	/*
 	 * We could do so much better if we had the VA as well.
<span class="p_chunk">@@ -117,35 +115,33 @@</span> <span class="p_context"> void __hyp_text __kvm_tlb_flush_vmid_ipa(struct kvm_s2_mmu *mmu,</span>
 	if (!has_vhe() &amp;&amp; icache_is_vpipt())
 		__flush_icache_all();
 
<span class="p_del">-	__tlb_switch_to_host()(mmu);</span>
<span class="p_add">+	__tlb_switch_to_host()();</span>
 }
 
<span class="p_del">-void __hyp_text __kvm_tlb_flush_vmid(struct kvm_s2_mmu *mmu)</span>
<span class="p_add">+void __hyp_text __kvm_tlb_flush_vmid(u64 vttbr)</span>
 {
 	dsb(ishst);
 
 	/* Switch to requested VMID */
<span class="p_del">-	mmu = kern_hyp_va(mmu);</span>
<span class="p_del">-	__tlb_switch_to_guest()(mmu);</span>
<span class="p_add">+	__tlb_switch_to_guest()(vttbr);</span>
 
 	__tlbi(vmalls12e1is);
 	dsb(ish);
 	isb();
 
<span class="p_del">-	__tlb_switch_to_host()(mmu);</span>
<span class="p_add">+	__tlb_switch_to_host()();</span>
 }
 
<span class="p_del">-void __hyp_text __kvm_tlb_flush_local_vmid(struct kvm_s2_mmu *mmu)</span>
<span class="p_add">+void __hyp_text __kvm_tlb_flush_local_vmid(u64 vttbr)</span>
 {
 	/* Switch to requested VMID */
<span class="p_del">-	mmu = kern_hyp_va(mmu);</span>
<span class="p_del">-	__tlb_switch_to_guest()(mmu);</span>
<span class="p_add">+	__tlb_switch_to_guest()(vttbr);</span>
 
 	__tlbi(vmalle1);
 	dsb(nsh);
 	isb();
 
<span class="p_del">-	__tlb_switch_to_host()(mmu);</span>
<span class="p_add">+	__tlb_switch_to_host()();</span>
 }
 
 void __hyp_text __kvm_flush_vm_context(void)
<span class="p_header">diff --git a/virt/kvm/arm/arm.c b/virt/kvm/arm/arm.c</span>
<span class="p_header">index bee27bb..41e0654 100644</span>
<span class="p_header">--- a/virt/kvm/arm/arm.c</span>
<span class="p_header">+++ b/virt/kvm/arm/arm.c</span>
<span class="p_chunk">@@ -75,6 +75,11 @@</span> <span class="p_context"> static void kvm_arm_set_running_vcpu(struct kvm_vcpu *vcpu)</span>
 	__this_cpu_write(kvm_arm_running_vcpu, vcpu);
 }
 
<span class="p_add">+unsigned int get_kvm_vmid_bits(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return kvm_vmid_bits;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /**
  * kvm_arm_get_running_vcpu - get the vcpu running on the current CPU.
  * Must be called from non-preemptible context
<span class="p_chunk">@@ -138,7 +143,8 @@</span> <span class="p_context"> int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)</span>
 	kvm_vgic_early_init(kvm);
 
 	/* Mark the initial VMID generation invalid */
<span class="p_del">-	kvm-&gt;arch.mmu.vmid_gen = 0;</span>
<span class="p_add">+	kvm-&gt;arch.mmu.vmid.vmid_gen = 0;</span>
<span class="p_add">+	kvm-&gt;arch.mmu.el2_vmid.vmid_gen = 0;</span>
 
 	/* The maximum number of VCPUs is limited by the host&#39;s GIC model */
 	kvm-&gt;arch.max_vcpus = vgic_present ?
<span class="p_chunk">@@ -325,6 +331,8 @@</span> <span class="p_context"> void kvm_arch_vcpu_unblocking(struct kvm_vcpu *vcpu)</span>
 
 int kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)
 {
<span class="p_add">+	struct kvm_s2_mmu *mmu = &amp;vcpu-&gt;kvm-&gt;arch.mmu;</span>
<span class="p_add">+</span>
 	/* Force users to call KVM_ARM_VCPU_INIT */
 	vcpu-&gt;arch.target = -1;
 	bitmap_zero(vcpu-&gt;arch.features, KVM_VCPU_MAX_FEATURES);
<span class="p_chunk">@@ -334,7 +342,7 @@</span> <span class="p_context"> int kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)</span>
 
 	kvm_arm_reset_debug_ptr(vcpu);
 
<span class="p_del">-	vcpu-&gt;arch.hw_mmu = &amp;vcpu-&gt;kvm-&gt;arch.mmu;</span>
<span class="p_add">+	vcpu-&gt;arch.hw_mmu = mmu;</span>
 
 	return kvm_vgic_vcpu_init(vcpu);
 }
<span class="p_chunk">@@ -350,7 +358,10 @@</span> <span class="p_context"> void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)</span>
 	 * over-invalidation doesn&#39;t affect correctness.
 	 */
 	if (*last_ran != vcpu-&gt;vcpu_id) {
<span class="p_del">-		kvm_call_hyp(__kvm_tlb_flush_local_vmid, &amp;vcpu-&gt;kvm-&gt;arch.mmu);</span>
<span class="p_add">+		struct kvm_s2_mmu *mmu = &amp;vcpu-&gt;kvm-&gt;arch.mmu;</span>
<span class="p_add">+		u64 vttbr = kvm_get_vttbr(&amp;mmu-&gt;vmid, mmu);</span>
<span class="p_add">+</span>
<span class="p_add">+		kvm_call_hyp(__kvm_tlb_flush_local_vmid, vttbr);</span>
 		*last_ran = vcpu-&gt;vcpu_id;
 	}
 
<span class="p_chunk">@@ -434,36 +445,38 @@</span> <span class="p_context"> void force_vm_exit(const cpumask_t *mask)</span>
 
 /**
  * need_new_vmid_gen - check that the VMID is still valid
<span class="p_del">- * @kvm: The VM&#39;s VMID to check</span>
<span class="p_add">+ * @vmid: The VMID to check</span>
  *
  * return true if there is a new generation of VMIDs being used
  *
<span class="p_del">- * The hardware supports only 256 values with the value zero reserved for the</span>
<span class="p_del">- * host, so we check if an assigned value belongs to a previous generation,</span>
<span class="p_del">- * which which requires us to assign a new value. If we&#39;re the first to use a</span>
<span class="p_del">- * VMID for the new generation, we must flush necessary caches and TLBs on all</span>
<span class="p_del">- * CPUs.</span>
<span class="p_add">+ * The hardware supports a limited set of values with the value zero reserved</span>
<span class="p_add">+ * for the host, so we check if an assigned value belongs to a previous</span>
<span class="p_add">+ * generation, which which requires us to assign a new value. If we&#39;re the</span>
<span class="p_add">+ * first to use a VMID for the new generation, we must flush necessary caches</span>
<span class="p_add">+ * and TLBs on all CPUs.</span>
  */
<span class="p_del">-static bool need_new_vmid_gen(struct kvm_s2_mmu *mmu)</span>
<span class="p_add">+static bool need_new_vmid_gen(struct kvm_s2_vmid *vmid)</span>
 {
<span class="p_del">-	return unlikely(mmu-&gt;vmid_gen != atomic64_read(&amp;kvm_vmid_gen));</span>
<span class="p_add">+	return unlikely(vmid-&gt;vmid_gen != atomic64_read(&amp;kvm_vmid_gen));</span>
 }
 
 /**
  * update_vttbr - Update the VTTBR with a valid VMID before the guest runs
  * @kvm: The guest that we are about to run
<span class="p_del">- * @mmu: The stage-2 translation context to update</span>
<span class="p_add">+ * @vmid: The stage-2 VMID information struct</span>
  *
  * Called from kvm_arch_vcpu_ioctl_run before entering the guest to ensure the
  * VM has a valid VMID, otherwise assigns a new one and flushes corresponding
  * caches and TLBs.
  */
<span class="p_del">-static void update_vttbr(struct kvm *kvm, struct kvm_s2_mmu *mmu)</span>
<span class="p_add">+static void update_vttbr(struct kvm *kvm, struct kvm_s2_vmid *vmid)</span>
 {
<span class="p_del">-	phys_addr_t pgd_phys;</span>
<span class="p_del">-	u64 vmid;</span>
<span class="p_add">+	struct kvm_s2_mmu *mmu = &amp;kvm-&gt;arch.mmu;</span>
<span class="p_add">+	struct kvm_vcpu *vcpu;</span>
<span class="p_add">+	int i = 0;</span>
<span class="p_add">+	u64 new_vttbr;</span>
 
<span class="p_del">-	if (!need_new_vmid_gen(mmu))</span>
<span class="p_add">+	if (!need_new_vmid_gen(vmid))</span>
 		return;
 
 	spin_lock(&amp;kvm_vmid_lock);
<span class="p_chunk">@@ -473,7 +486,7 @@</span> <span class="p_context"> static void update_vttbr(struct kvm *kvm, struct kvm_s2_mmu *mmu)</span>
 	 * already allocated a valid vmid for this vm, then this vcpu should
 	 * use the same vmid.
 	 */
<span class="p_del">-	if (!need_new_vmid_gen(mmu)) {</span>
<span class="p_add">+	if (!need_new_vmid_gen(vmid)) {</span>
 		spin_unlock(&amp;kvm_vmid_lock);
 		return;
 	}
<span class="p_chunk">@@ -497,17 +510,15 @@</span> <span class="p_context"> static void update_vttbr(struct kvm *kvm, struct kvm_s2_mmu *mmu)</span>
 		kvm_call_hyp(__kvm_flush_vm_context);
 	}
 
<span class="p_del">-	mmu-&gt;vmid_gen = atomic64_read(&amp;kvm_vmid_gen);</span>
<span class="p_del">-	mmu-&gt;vmid = kvm_next_vmid;</span>
<span class="p_add">+	vmid-&gt;vmid_gen = atomic64_read(&amp;kvm_vmid_gen);</span>
<span class="p_add">+	vmid-&gt;vmid = kvm_next_vmid;</span>
 	kvm_next_vmid++;
 	kvm_next_vmid &amp;= (1 &lt;&lt; kvm_vmid_bits) - 1;
 
<span class="p_del">-	/* update vttbr to be used with the new vmid */</span>
<span class="p_del">-	pgd_phys = virt_to_phys(mmu-&gt;pgd);</span>
<span class="p_del">-	BUG_ON(pgd_phys &amp; ~VTTBR_BADDR_MASK);</span>
<span class="p_del">-	vmid = ((u64)(mmu-&gt;vmid) &lt;&lt; VTTBR_VMID_SHIFT) &amp;</span>
<span class="p_del">-	       VTTBR_VMID_MASK(kvm_vmid_bits);</span>
<span class="p_del">-	mmu-&gt;vttbr = pgd_phys | vmid;</span>
<span class="p_add">+	new_vttbr = kvm_get_vttbr(&amp;mmu-&gt;vmid, mmu);</span>
<span class="p_add">+	kvm_for_each_vcpu(i, vcpu, kvm) {</span>
<span class="p_add">+		vcpu-&gt;arch.hw_vttbr = new_vttbr;</span>
<span class="p_add">+	}</span>
 
 	spin_unlock(&amp;kvm_vmid_lock);
 }
<span class="p_chunk">@@ -642,7 +653,7 @@</span> <span class="p_context"> int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *run)</span>
 		 */
 		cond_resched();
 
<span class="p_del">-		update_vttbr(vcpu-&gt;kvm, vcpu-&gt;arch.hw_mmu);</span>
<span class="p_add">+		update_vttbr(vcpu-&gt;kvm, vcpu_get_active_vmid(vcpu));</span>
 
 		check_vcpu_requests(vcpu);
 
<span class="p_chunk">@@ -681,7 +692,7 @@</span> <span class="p_context"> int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *run)</span>
 		 */
 		smp_store_mb(vcpu-&gt;mode, IN_GUEST_MODE);
 
<span class="p_del">-		if (ret &lt;= 0 || need_new_vmid_gen(vcpu-&gt;arch.hw_mmu) ||</span>
<span class="p_add">+		if (ret &lt;= 0 || need_new_vmid_gen(vcpu_get_active_vmid(vcpu)) ||</span>
 		    kvm_request_pending(vcpu)) {
 			vcpu-&gt;mode = OUTSIDE_GUEST_MODE;
 			local_irq_enable();
<span class="p_header">diff --git a/virt/kvm/arm/mmu.c b/virt/kvm/arm/mmu.c</span>
<span class="p_header">index d8ea1f9..0edcf23 100644</span>
<span class="p_header">--- a/virt/kvm/arm/mmu.c</span>
<span class="p_header">+++ b/virt/kvm/arm/mmu.c</span>
<span class="p_chunk">@@ -61,12 +61,17 @@</span> <span class="p_context"> static bool memslot_is_logging(struct kvm_memory_slot *memslot)</span>
  */
 void kvm_flush_remote_tlbs(struct kvm *kvm)
 {
<span class="p_del">-	kvm_call_hyp(__kvm_tlb_flush_vmid, kvm);</span>
<span class="p_add">+	struct kvm_s2_mmu *mmu = &amp;kvm-&gt;arch.mmu;</span>
<span class="p_add">+	u64 vttbr = kvm_get_vttbr(&amp;mmu-&gt;vmid, mmu);</span>
<span class="p_add">+</span>
<span class="p_add">+	kvm_call_hyp(__kvm_tlb_flush_vmid, vttbr);</span>
 }
 
 static void kvm_tlb_flush_vmid_ipa(struct kvm_s2_mmu *mmu, phys_addr_t ipa)
 {
<span class="p_del">-	kvm_call_hyp(__kvm_tlb_flush_vmid_ipa, mmu, ipa);</span>
<span class="p_add">+	u64 vttbr = kvm_get_vttbr(&amp;mmu-&gt;vmid, mmu);</span>
<span class="p_add">+</span>
<span class="p_add">+	kvm_call_hyp(__kvm_tlb_flush_vmid_ipa, vttbr, ipa);</span>
 }
 
 /*

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



