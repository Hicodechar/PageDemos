
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[V2] mm/madvise: Enable (soft|hard) offline of HugeTLB pages at PGD level - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [V2] mm/madvise: Enable (soft|hard) offline of HugeTLB pages at PGD level</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=36302">Anshuman Khandual</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>April 26, 2017, 3:57 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20170426035731.6924-1-khandual@linux.vnet.ibm.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9700265/mbox/"
   >mbox</a>
|
   <a href="/patch/9700265/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9700265/">/patch/9700265/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	DA8FF60245 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 26 Apr 2017 03:58:56 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id C3F3A284EA
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 26 Apr 2017 03:58:56 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id B711B2857F; Wed, 26 Apr 2017 03:58:56 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 15C7C284EA
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 26 Apr 2017 03:58:56 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1947722AbdDZD6p (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 25 Apr 2017 23:58:45 -0400
Received: from mx0a-001b2d01.pphosted.com ([148.163.156.1]:51688 &quot;EHLO
	mx0a-001b2d01.pphosted.com&quot; rhost-flags-OK-OK-OK-OK)
	by vger.kernel.org with ESMTP id S1434861AbdDZD6i (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 25 Apr 2017 23:58:38 -0400
Received: from pps.filterd (m0098409.ppops.net [127.0.0.1])
	by mx0a-001b2d01.pphosted.com (8.16.0.20/8.16.0.20) with SMTP id
	v3Q3rgkZ051400
	for &lt;linux-kernel@vger.kernel.org&gt;; Tue, 25 Apr 2017 23:58:37 -0400
Received: from e23smtp05.au.ibm.com (e23smtp05.au.ibm.com [202.81.31.147])
	by mx0a-001b2d01.pphosted.com with ESMTP id 2a2ebtuny0-1
	(version=TLSv1.2 cipher=AES256-SHA bits=256 verify=NOT)
	for &lt;linux-kernel@vger.kernel.org&gt;; Tue, 25 Apr 2017 23:58:37 -0400
Received: from localhost
	by e23smtp05.au.ibm.com with IBM ESMTP SMTP Gateway: Authorized Use
	Only! Violators will be prosecuted
	for &lt;linux-kernel@vger.kernel.org&gt; from &lt;khandual@linux.vnet.ibm.com&gt;;
	Wed, 26 Apr 2017 13:58:31 +1000
Received: from d23relay10.au.ibm.com (202.81.31.229)
	by e23smtp05.au.ibm.com (202.81.31.211) with IBM ESMTP SMTP Gateway:
	Authorized Use Only! Violators will be prosecuted; 
	Wed, 26 Apr 2017 13:58:28 +1000
Received: from d23av03.au.ibm.com (d23av03.au.ibm.com [9.190.234.97])
	by d23relay10.au.ibm.com (8.14.9/8.14.9/NCO v10.0) with ESMTP id
	v3Q3wKsU4194738
	for &lt;linux-kernel@vger.kernel.org&gt;; Wed, 26 Apr 2017 13:58:28 +1000
Received: from d23av03.au.ibm.com (localhost [127.0.0.1])
	by d23av03.au.ibm.com (8.14.4/8.14.4/NCO v10.0 AVout) with ESMTP id
	v3Q3voOY011211
	for &lt;linux-kernel@vger.kernel.org&gt;; Wed, 26 Apr 2017 13:57:50 +1000
Received: from dhcp-9-202-26-78.in.ibm.com ([9.78.202.34])
	by d23av03.au.ibm.com (8.14.4/8.14.4/NCO v10.0 AVin) with ESMTP id
	v3Q3vmFE010694; Wed, 26 Apr 2017 13:57:48 +1000
From: Anshuman Khandual &lt;khandual@linux.vnet.ibm.com&gt;
To: linux-kernel@vger.kernel.org, linux-mm@kvack.org
Cc: n-horiguchi@ah.jp.nec.com, akpm@linux-foundation.org,
	aneesh.kumar@linux.vnet.ibm.com
Subject: [PATCH V2] mm/madvise: Enable (soft|hard) offline of HugeTLB pages
	at PGD level
Date: Wed, 26 Apr 2017 09:27:31 +0530
X-Mailer: git-send-email 2.9.3
X-TM-AS-MML: disable
x-cbid: 17042603-0016-0000-0000-00000233FD70
X-IBM-AV-DETECTION: SAVI=unused REMOTE=unused XFE=unused
x-cbparentid: 17042603-0017-0000-0000-000006AF7394
Message-Id: &lt;20170426035731.6924-1-khandual@linux.vnet.ibm.com&gt;
X-Proofpoint-Virus-Version: vendor=fsecure engine=2.50.10432:, ,
	definitions=2017-04-26_02:, , signatures=0
X-Proofpoint-Spam-Details: rule=outbound_notspam policy=outbound score=0
	spamscore=0 suspectscore=10
	malwarescore=0 phishscore=0 adultscore=0 bulkscore=0 classifier=spam
	adjust=0 reason=mlx scancount=1 engine=8.0.1-1703280000
	definitions=main-1704260067
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36302">Anshuman Khandual</a> - April 26, 2017, 3:57 a.m.</div>
<pre class="content">
Though migrating gigantic HugeTLB pages does not sound much like real
world use case, they can be affected by memory errors. Hence migration
at the PGD level HugeTLB pages should be supported just to enable soft
and hard offline use cases.

While allocating the new gigantic HugeTLB page, it should not matter
whether new page comes from the same node or not. There would be very
few gigantic pages on the system afterall, we should not be bothered
about node locality when trying to save a big page from crashing.

This introduces a new HugeTLB allocator called alloc_huge_page_nonid()
which will scan over all online nodes on the system and allocate a
single HugeTLB page.
<span class="signed-off-by">
Signed-off-by: Anshuman Khandual &lt;khandual@linux.vnet.ibm.com&gt;</span>
---
Tested on a POWER8 machine with 16GB pages along with Aneesh&#39;s
recent HugeTLB enablement patch series on powerpc which can
be found here.

https://lkml.org/lkml/2017/4/17/225

Here, we directly call alloc_huge_page_nonid() which ignores the
node locality. But we can also first call normal alloc_huge_page()
with the node number and if that fails to allocate only then call
alloc_huge_page_nonid() as a fallback option.

Aneesh mentioned about the waste of memory if we just have to
soft offline a single page. The problem persists both on PGD
as well as PMD level HugeTLB pages. Tried solving the problem
with https://patchwork.kernel.org/patch/9690119/ but right now
madvise splits the entire range of HugeTLB pages (if the page
is HugeTLB one) and calls soft_offline_page() on the head page
of each HugeTLB page as soft_offline_page() acts on the entire
HugeTLB range not just the individual pages. Changing the iterator
in madvise() scan over individual pages solves the problem but
then it creates multiple HugeTLB migrations (HUGE_PAGE_SIZE /
PAGE_SIZE times to be precise) if we really have to soft offline
a single HugeTLB page which is not optimal.

Hence for now, lets just enable PGD level HugeTLB soft offline
at par with the PMD level HugeTLB before we can go back and
address the memory wastage problem comprehensively for both
PGD and PMD level HugeTLB page as mentioned above.

Changes in V2:
 * Added hstate_is_gigantic() definition when !CONFIG_HUGETLB_PAGE
   which takes care of the build failure reported earlier.

 mm/hugetlb.c            | 17 +++++++++++++++++
 mm/memory-failure.c     |  8 ++++++--
 3 files changed, 31 insertions(+), 3 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=41">Andrew Morton</a> - May 12, 2017, 9:35 p.m.</div>
<pre class="content">
On Wed, 26 Apr 2017 09:27:31 +0530 Anshuman Khandual &lt;khandual@linux.vnet.ibm.com&gt; wrote:
<span class="quote">
&gt; Though migrating gigantic HugeTLB pages does not sound much like real</span>
<span class="quote">&gt; world use case, they can be affected by memory errors. Hence migration</span>
<span class="quote">&gt; at the PGD level HugeTLB pages should be supported just to enable soft</span>
<span class="quote">&gt; and hard offline use cases.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; While allocating the new gigantic HugeTLB page, it should not matter</span>
<span class="quote">&gt; whether new page comes from the same node or not. There would be very</span>
<span class="quote">&gt; few gigantic pages on the system afterall, we should not be bothered</span>
<span class="quote">&gt; about node locality when trying to save a big page from crashing.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This introduces a new HugeTLB allocator called alloc_huge_page_nonid()</span>
<span class="quote">&gt; which will scan over all online nodes on the system and allocate a</span>
<span class="quote">&gt; single HugeTLB page.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; ...</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; --- a/mm/hugetlb.c</span>
<span class="quote">&gt; +++ b/mm/hugetlb.c</span>
<span class="quote">&gt; @@ -1669,6 +1669,23 @@ struct page *__alloc_buddy_huge_page_with_mpol(struct hstate *h,</span>
<span class="quote">&gt;  	return __alloc_buddy_huge_page(h, vma, addr, NUMA_NO_NODE);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +struct page *alloc_huge_page_nonid(struct hstate *h)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct page *page = NULL;</span>
<span class="quote">&gt; +	int nid = 0;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	spin_lock(&amp;hugetlb_lock);</span>
<span class="quote">&gt; +	if (h-&gt;free_huge_pages - h-&gt;resv_huge_pages &gt; 0) {</span>
<span class="quote">&gt; +		for_each_online_node(nid) {</span>
<span class="quote">&gt; +			page = dequeue_huge_page_node(h, nid);</span>
<span class="quote">&gt; +			if (page)</span>
<span class="quote">&gt; +				break;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +	spin_unlock(&amp;hugetlb_lock);</span>
<span class="quote">&gt; +	return page;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt;   * This allocation function is useful in the context where vma is irrelevant.</span>
<span class="quote">&gt;   * E.g. soft-offlining uses this function because it only cares physical</span>
<span class="quote">&gt; diff --git a/mm/memory-failure.c b/mm/memory-failure.c</span>
<span class="quote">&gt; index fe64d7729a8e..d4f5710cf3f7 100644</span>
<span class="quote">&gt; --- a/mm/memory-failure.c</span>
<span class="quote">&gt; +++ b/mm/memory-failure.c</span>
<span class="quote">&gt; @@ -1481,11 +1481,15 @@ EXPORT_SYMBOL(unpoison_memory);</span>
<span class="quote">&gt;  static struct page *new_page(struct page *p, unsigned long private, int **x)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	int nid = page_to_nid(p);</span>
<span class="quote">&gt; -	if (PageHuge(p))</span>
<span class="quote">&gt; +	if (PageHuge(p)) {</span>
<span class="quote">&gt; +		if (hstate_is_gigantic(page_hstate(compound_head(p))))</span>
<span class="quote">&gt; +			return alloc_huge_page_nonid(page_hstate(compound_head(p)));</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  		return alloc_huge_page_node(page_hstate(compound_head(p)),</span>
<span class="quote">&gt;  						   nid);</span>
<span class="quote">&gt; -	else</span>
<span class="quote">&gt; +	} else {</span>
<span class="quote">&gt;  		return __alloc_pages_node(nid, GFP_HIGHUSER_MOVABLE, 0);</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt;  }</span>

Rather than adding alloc_huge_page_nonid(), would it be neater to teach
alloc_huge_page_node() (actually dequeue_huge_page_node()) to understand
nid==NUMA_NO_NODE?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36302">Anshuman Khandual</a> - May 14, 2017, 4:11 a.m.</div>
<pre class="content">
On 05/13/2017 03:05 AM, Andrew Morton wrote:
<span class="quote">&gt; On Wed, 26 Apr 2017 09:27:31 +0530 Anshuman Khandual &lt;khandual@linux.vnet.ibm.com&gt; wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; Though migrating gigantic HugeTLB pages does not sound much like real</span>
<span class="quote">&gt;&gt; world use case, they can be affected by memory errors. Hence migration</span>
<span class="quote">&gt;&gt; at the PGD level HugeTLB pages should be supported just to enable soft</span>
<span class="quote">&gt;&gt; and hard offline use cases.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; While allocating the new gigantic HugeTLB page, it should not matter</span>
<span class="quote">&gt;&gt; whether new page comes from the same node or not. There would be very</span>
<span class="quote">&gt;&gt; few gigantic pages on the system afterall, we should not be bothered</span>
<span class="quote">&gt;&gt; about node locality when trying to save a big page from crashing.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; This introduces a new HugeTLB allocator called alloc_huge_page_nonid()</span>
<span class="quote">&gt;&gt; which will scan over all online nodes on the system and allocate a</span>
<span class="quote">&gt;&gt; single HugeTLB page.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; ...</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; --- a/mm/hugetlb.c</span>
<span class="quote">&gt;&gt; +++ b/mm/hugetlb.c</span>
<span class="quote">&gt;&gt; @@ -1669,6 +1669,23 @@ struct page *__alloc_buddy_huge_page_with_mpol(struct hstate *h,</span>
<span class="quote">&gt;&gt;  	return __alloc_buddy_huge_page(h, vma, addr, NUMA_NO_NODE);</span>
<span class="quote">&gt;&gt;  }</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; +struct page *alloc_huge_page_nonid(struct hstate *h)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	struct page *page = NULL;</span>
<span class="quote">&gt;&gt; +	int nid = 0;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	spin_lock(&amp;hugetlb_lock);</span>
<span class="quote">&gt;&gt; +	if (h-&gt;free_huge_pages - h-&gt;resv_huge_pages &gt; 0) {</span>
<span class="quote">&gt;&gt; +		for_each_online_node(nid) {</span>
<span class="quote">&gt;&gt; +			page = dequeue_huge_page_node(h, nid);</span>
<span class="quote">&gt;&gt; +			if (page)</span>
<span class="quote">&gt;&gt; +				break;</span>
<span class="quote">&gt;&gt; +		}</span>
<span class="quote">&gt;&gt; +	}</span>
<span class="quote">&gt;&gt; +	spin_unlock(&amp;hugetlb_lock);</span>
<span class="quote">&gt;&gt; +	return page;</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt;  /*</span>
<span class="quote">&gt;&gt;   * This allocation function is useful in the context where vma is irrelevant.</span>
<span class="quote">&gt;&gt;   * E.g. soft-offlining uses this function because it only cares physical</span>
<span class="quote">&gt;&gt; diff --git a/mm/memory-failure.c b/mm/memory-failure.c</span>
<span class="quote">&gt;&gt; index fe64d7729a8e..d4f5710cf3f7 100644</span>
<span class="quote">&gt;&gt; --- a/mm/memory-failure.c</span>
<span class="quote">&gt;&gt; +++ b/mm/memory-failure.c</span>
<span class="quote">&gt;&gt; @@ -1481,11 +1481,15 @@ EXPORT_SYMBOL(unpoison_memory);</span>
<span class="quote">&gt;&gt;  static struct page *new_page(struct page *p, unsigned long private, int **x)</span>
<span class="quote">&gt;&gt;  {</span>
<span class="quote">&gt;&gt;  	int nid = page_to_nid(p);</span>
<span class="quote">&gt;&gt; -	if (PageHuge(p))</span>
<span class="quote">&gt;&gt; +	if (PageHuge(p)) {</span>
<span class="quote">&gt;&gt; +		if (hstate_is_gigantic(page_hstate(compound_head(p))))</span>
<span class="quote">&gt;&gt; +			return alloc_huge_page_nonid(page_hstate(compound_head(p)));</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt;  		return alloc_huge_page_node(page_hstate(compound_head(p)),</span>
<span class="quote">&gt;&gt;  						   nid);</span>
<span class="quote">&gt;&gt; -	else</span>
<span class="quote">&gt;&gt; +	} else {</span>
<span class="quote">&gt;&gt;  		return __alloc_pages_node(nid, GFP_HIGHUSER_MOVABLE, 0);</span>
<span class="quote">&gt;&gt; +	}</span>
<span class="quote">&gt;&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Rather than adding alloc_huge_page_nonid(), would it be neater to teach</span>
<span class="quote">&gt; alloc_huge_page_node() (actually dequeue_huge_page_node()) to understand</span>
<span class="quote">&gt; nid==NUMA_NO_NODE?</span>

Sure, will change dequeue_huge_page_node() to accommodate NUMA_NO_NODE and
let soft offline call with NUMA_NO_NODE in case of gigantic huge pages.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h</span>
<span class="p_header">index 04b73a9c8b4b..964d964f22c8 100644</span>
<span class="p_header">--- a/include/linux/hugetlb.h</span>
<span class="p_header">+++ b/include/linux/hugetlb.h</span>
<span class="p_chunk">@@ -347,6 +347,7 @@</span> <span class="p_context"> struct huge_bootmem_page {</span>
 
 struct page *alloc_huge_page(struct vm_area_struct *vma,
 				unsigned long addr, int avoid_reserve);
<span class="p_add">+struct page *alloc_huge_page_nonid(struct hstate *h);</span>
 struct page *alloc_huge_page_node(struct hstate *h, int nid);
 struct page *alloc_huge_page_noerr(struct vm_area_struct *vma,
 				unsigned long addr, int avoid_reserve);
<span class="p_chunk">@@ -473,7 +474,11 @@</span> <span class="p_context"> extern int dissolve_free_huge_pages(unsigned long start_pfn,</span>
 static inline bool hugepage_migration_supported(struct hstate *h)
 {
 #ifdef CONFIG_ARCH_ENABLE_HUGEPAGE_MIGRATION
<span class="p_del">-	return huge_page_shift(h) == PMD_SHIFT;</span>
<span class="p_add">+	if ((huge_page_shift(h) == PMD_SHIFT) ||</span>
<span class="p_add">+		(huge_page_shift(h) == PGDIR_SHIFT))</span>
<span class="p_add">+		return true;</span>
<span class="p_add">+	else</span>
<span class="p_add">+		return false;</span>
 #else
 	return false;
 #endif
<span class="p_chunk">@@ -511,6 +516,7 @@</span> <span class="p_context"> static inline void hugetlb_count_sub(long l, struct mm_struct *mm)</span>
 #else	/* CONFIG_HUGETLB_PAGE */
 struct hstate {};
 #define alloc_huge_page(v, a, r) NULL
<span class="p_add">+#define alloc_huge_page_nonid(h) NULL</span>
 #define alloc_huge_page_node(h, nid) NULL
 #define alloc_huge_page_noerr(v, a, r) NULL
 #define alloc_bootmem_huge_page(h) NULL
<span class="p_chunk">@@ -525,6 +531,7 @@</span> <span class="p_context"> struct hstate {};</span>
 #define vma_mmu_pagesize(v) PAGE_SIZE
 #define huge_page_order(h) 0
 #define huge_page_shift(h) PAGE_SHIFT
<span class="p_add">+#define hstate_is_gigantic(h) 0</span>
 static inline unsigned int pages_per_huge_page(struct hstate *h)
 {
 	return 1;
<span class="p_header">diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="p_header">index 97a44db06850..bd96fff2bc09 100644</span>
<span class="p_header">--- a/mm/hugetlb.c</span>
<span class="p_header">+++ b/mm/hugetlb.c</span>
<span class="p_chunk">@@ -1669,6 +1669,23 @@</span> <span class="p_context"> struct page *__alloc_buddy_huge_page_with_mpol(struct hstate *h,</span>
 	return __alloc_buddy_huge_page(h, vma, addr, NUMA_NO_NODE);
 }
 
<span class="p_add">+struct page *alloc_huge_page_nonid(struct hstate *h)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct page *page = NULL;</span>
<span class="p_add">+	int nid = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	spin_lock(&amp;hugetlb_lock);</span>
<span class="p_add">+	if (h-&gt;free_huge_pages - h-&gt;resv_huge_pages &gt; 0) {</span>
<span class="p_add">+		for_each_online_node(nid) {</span>
<span class="p_add">+			page = dequeue_huge_page_node(h, nid);</span>
<span class="p_add">+			if (page)</span>
<span class="p_add">+				break;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+	spin_unlock(&amp;hugetlb_lock);</span>
<span class="p_add">+	return page;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /*
  * This allocation function is useful in the context where vma is irrelevant.
  * E.g. soft-offlining uses this function because it only cares physical
<span class="p_header">diff --git a/mm/memory-failure.c b/mm/memory-failure.c</span>
<span class="p_header">index fe64d7729a8e..d4f5710cf3f7 100644</span>
<span class="p_header">--- a/mm/memory-failure.c</span>
<span class="p_header">+++ b/mm/memory-failure.c</span>
<span class="p_chunk">@@ -1481,11 +1481,15 @@</span> <span class="p_context"> EXPORT_SYMBOL(unpoison_memory);</span>
 static struct page *new_page(struct page *p, unsigned long private, int **x)
 {
 	int nid = page_to_nid(p);
<span class="p_del">-	if (PageHuge(p))</span>
<span class="p_add">+	if (PageHuge(p)) {</span>
<span class="p_add">+		if (hstate_is_gigantic(page_hstate(compound_head(p))))</span>
<span class="p_add">+			return alloc_huge_page_nonid(page_hstate(compound_head(p)));</span>
<span class="p_add">+</span>
 		return alloc_huge_page_node(page_hstate(compound_head(p)),
 						   nid);
<span class="p_del">-	else</span>
<span class="p_add">+	} else {</span>
 		return __alloc_pages_node(nid, GFP_HIGHUSER_MOVABLE, 0);
<span class="p_add">+	}</span>
 }
 
 /*

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



