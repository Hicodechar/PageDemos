
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[3.2,2/4] mm: larger stack guard gap, between vmas - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [3.2,2/4] mm: larger stack guard gap, between vmas</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=131">Ben Hutchings</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>June 27, 2017, 11:09 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;lsq.1498561776.670399876@decadent.org.uk&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9811413/mbox/"
   >mbox</a>
|
   <a href="/patch/9811413/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9811413/">/patch/9811413/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	9CB60603D7 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 27 Jun 2017 11:11:38 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 8730D2808F
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 27 Jun 2017 11:11:38 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 7954C28553; Tue, 27 Jun 2017 11:11:38 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 088102808F
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 27 Jun 2017 11:11:36 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752971AbdF0LL2 (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 27 Jun 2017 07:11:28 -0400
Received: from shadbolt.e.decadent.org.uk ([88.96.1.126]:34807 &quot;EHLO
	shadbolt.e.decadent.org.uk&quot; rhost-flags-OK-OK-OK-OK)
	by vger.kernel.org with ESMTP id S1752704AbdF0LKW (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 27 Jun 2017 07:10:22 -0400
Received: from [2a02:8011:400e:2:6f00:88c8:c921:d332] (helo=deadeye)
	by shadbolt.decadent.org.uk with esmtps
	(TLS1.2:ECDHE_RSA_AES_256_GCM_SHA384:256) (Exim 4.84_2)
	(envelope-from &lt;ben@decadent.org.uk&gt;)
	id 1dPoNu-00069c-PK; Tue, 27 Jun 2017 12:10:19 +0100
Received: from ben by deadeye with local (Exim 4.89)
	(envelope-from &lt;ben@decadent.org.uk&gt;)
	id 1dPoNp-0001DI-GJ; Tue, 27 Jun 2017 12:10:13 +0100
Content-Type: text/plain; charset=&quot;UTF-8&quot;
Content-Disposition: inline
Content-Transfer-Encoding: 8bit
MIME-Version: 1.0
From: Ben Hutchings &lt;ben@decadent.org.uk&gt;
To: linux-kernel@vger.kernel.org, stable@vger.kernel.org
CC: akpm@linux-foundation.org, &quot;Michal Hocko&quot; &lt;mhocko@suse.com&gt;,
	&quot;Linus Torvalds&quot; &lt;torvalds@linux-foundation.org&gt;,
	&quot;Helge Deller&quot; &lt;deller@gmx.de&gt;, &quot;Hugh Dickins&quot; &lt;hughd@google.com&gt;
Date: Tue, 27 Jun 2017 12:09:36 +0100
Message-ID: &lt;lsq.1498561776.670399876@decadent.org.uk&gt;
X-Mailer: LinuxStableQueue (scripts by bwh)
Subject: [PATCH 3.2 2/4] mm: larger stack guard gap, between vmas
In-Reply-To: &lt;lsq.1498561775.303580185@decadent.org.uk&gt;
X-SA-Exim-Connect-IP: 2a02:8011:400e:2:6f00:88c8:c921:d332
X-SA-Exim-Mail-From: ben@decadent.org.uk
X-SA-Exim-Scanned: No (on shadbolt.decadent.org.uk);
	SAEximRunCond expanded to false
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=131">Ben Hutchings</a> - June 27, 2017, 11:09 a.m.</div>
<pre class="content">
3.2.90-rc1 review patch.  If anyone has any objections, please let me know.

------------------
<span class="from">
From: Hugh Dickins &lt;hughd@google.com&gt;</span>

commit 1be7107fbe18eed3e319a6c3e83c78254b693acb upstream.

Stack guard page is a useful feature to reduce a risk of stack smashing
into a different mapping. We have been using a single page gap which
is sufficient to prevent having stack adjacent to a different mapping.
But this seems to be insufficient in the light of the stack usage in
userspace. E.g. glibc uses as large as 64kB alloca() in many commonly
used functions. Others use constructs liks gid_t buffer[NGROUPS_MAX]
which is 256kB or stack strings with MAX_ARG_STRLEN.

This will become especially dangerous for suid binaries and the default
no limit for the stack size limit because those applications can be
tricked to consume a large portion of the stack and a single glibc call
could jump over the guard page. These attacks are not theoretical,
unfortunatelly.

Make those attacks less probable by increasing the stack guard gap
to 1MB (on systems with 4k pages; but make it depend on the page size
because systems with larger base pages might cap stack allocations in
the PAGE_SIZE units) which should cover larger alloca() and VLA stack
allocations. It is obviously not a full fix because the problem is
somehow inherent, but it should reduce attack space a lot.

One could argue that the gap size should be configurable from userspace,
but that can be done later when somebody finds that the new 1MB is wrong
for some special case applications.  For now, add a kernel command line
option (stack_guard_gap) to specify the stack gap size (in page units).

Implementation wise, first delete all the old code for stack guard page:
because although we could get away with accounting one extra page in a
stack vma, accounting a larger gap can break userspace - case in point,
a program run with &quot;ulimit -S -v 20000&quot; failed when the 1MB gap was
counted for RLIMIT_AS; similar problems could come with RLIMIT_MLOCK
and strict non-overcommit mode.

Instead of keeping gap inside the stack vma, maintain the stack guard
gap as a gap between vmas: using vm_start_gap() in place of vm_start
(or vm_end_gap() in place of vm_end if VM_GROWSUP) in just those few
places which need to respect the gap - mainly arch_get_unmapped_area(),
and and the vma tree&#39;s subtree_gap support for that.

Original-patch-by: Oleg Nesterov &lt;oleg@redhat.com&gt;
Original-patch-by: Michal Hocko &lt;mhocko@suse.com&gt;
<span class="signed-off-by">Signed-off-by: Hugh Dickins &lt;hughd@google.com&gt;</span>
<span class="acked-by">Acked-by: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="tested-by">Tested-by: Helge Deller &lt;deller@gmx.de&gt; # parisc</span>
<span class="signed-off-by">Signed-off-by: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;</span>
[Hugh Dickins: Backported to 3.2]
[bwh: Fix more instances of vma-&gt;vm_start in sparc64 impl. of
 arch_get_unmapped_area_topdown() and generic impl. of
 hugetlb_get_unmapped_area()]
<span class="signed-off-by">Signed-off-by: Ben Hutchings &lt;ben@decadent.org.uk&gt;</span>
---
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">--- a/Documentation/kernel-parameters.txt</span>
<span class="p_header">+++ b/Documentation/kernel-parameters.txt</span>
<span class="p_chunk">@@ -2457,6 +2457,13 @@</span> <span class="p_context"> bytes respectively. Such letter suffixes</span>
 	spia_pedr=
 	spia_peddr=
 
<span class="p_add">+	stack_guard_gap=	[MM]</span>
<span class="p_add">+			override the default stack gap protection. The value</span>
<span class="p_add">+			is in page units and it defines how many pages prior</span>
<span class="p_add">+			to (for stacks growing down) resp. after (for stacks</span>
<span class="p_add">+			growing up) the main stack are reserved for no other</span>
<span class="p_add">+			mapping. Default value is 256 pages.</span>
<span class="p_add">+</span>
 	stacktrace	[FTRACE]
 			Enabled the stack tracer on boot up.
 
<span class="p_header">--- a/arch/alpha/kernel/osf_sys.c</span>
<span class="p_header">+++ b/arch/alpha/kernel/osf_sys.c</span>
<span class="p_chunk">@@ -1147,7 +1147,7 @@</span> <span class="p_context"> arch_get_unmapped_area_1(unsigned long a</span>
 		/* At this point:  (!vma || addr &lt; vma-&gt;vm_end). */
 		if (limit - len &lt; addr)
 			return -ENOMEM;
<span class="p_del">-		if (!vma || addr + len &lt;= vma-&gt;vm_start)</span>
<span class="p_add">+		if (!vma || addr + len &lt;= vm_start_gap(vma))</span>
 			return addr;
 		addr = vma-&gt;vm_end;
 		vma = vma-&gt;vm_next;
<span class="p_header">--- a/arch/arm/mm/mmap.c</span>
<span class="p_header">+++ b/arch/arm/mm/mmap.c</span>
<span class="p_chunk">@@ -30,7 +30,7 @@</span> <span class="p_context"> arch_get_unmapped_area(struct file *filp</span>
 {
 	struct mm_struct *mm = current-&gt;mm;
 	struct vm_area_struct *vma;
<span class="p_del">-	unsigned long start_addr;</span>
<span class="p_add">+	unsigned long start_addr, vm_start;</span>
 	int do_align = 0;
 	int aliasing = cache_is_vipt_aliasing();
 
<span class="p_chunk">@@ -62,7 +62,7 @@</span> <span class="p_context"> arch_get_unmapped_area(struct file *filp</span>
 
 		vma = find_vma(mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 	if (len &gt; mm-&gt;cached_hole_size) {
<span class="p_chunk">@@ -96,15 +96,17 @@</span> <span class="p_context"> full_search:</span>
 			}
 			return -ENOMEM;
 		}
<span class="p_del">-		if (!vma || addr + len &lt;= vma-&gt;vm_start) {</span>
<span class="p_add">+		if (vma)</span>
<span class="p_add">+			vm_start = vm_start_gap(vma);</span>
<span class="p_add">+		if (!vma || addr + len &lt;= vm_start) {</span>
 			/*
 			 * Remember the place where we stopped the search:
 			 */
 			mm-&gt;free_area_cache = addr + len;
 			return addr;
 		}
<span class="p_del">-		if (addr + mm-&gt;cached_hole_size &lt; vma-&gt;vm_start)</span>
<span class="p_del">-		        mm-&gt;cached_hole_size = vma-&gt;vm_start - addr;</span>
<span class="p_add">+		if (addr + mm-&gt;cached_hole_size &lt; vm_start)</span>
<span class="p_add">+			mm-&gt;cached_hole_size = vm_start - addr;</span>
 		addr = vma-&gt;vm_end;
 		if (do_align)
 			addr = COLOUR_ALIGN(addr, pgoff);
<span class="p_header">--- a/arch/frv/mm/elf-fdpic.c</span>
<span class="p_header">+++ b/arch/frv/mm/elf-fdpic.c</span>
<span class="p_chunk">@@ -74,7 +74,7 @@</span> <span class="p_context"> unsigned long arch_get_unmapped_area(str</span>
 		addr = PAGE_ALIGN(addr);
 		vma = find_vma(current-&gt;mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			goto success;
 	}
 
<span class="p_chunk">@@ -89,7 +89,7 @@</span> <span class="p_context"> unsigned long arch_get_unmapped_area(str</span>
 			for (; vma; vma = vma-&gt;vm_next) {
 				if (addr &gt; limit)
 					break;
<span class="p_del">-				if (addr + len &lt;= vma-&gt;vm_start)</span>
<span class="p_add">+				if (addr + len &lt;= vm_start_gap(vma))</span>
 					goto success;
 				addr = vma-&gt;vm_end;
 			}
<span class="p_chunk">@@ -104,7 +104,7 @@</span> <span class="p_context"> unsigned long arch_get_unmapped_area(str</span>
 		for (; vma; vma = vma-&gt;vm_next) {
 			if (addr &gt; limit)
 				break;
<span class="p_del">-			if (addr + len &lt;= vma-&gt;vm_start)</span>
<span class="p_add">+			if (addr + len &lt;= vm_start_gap(vma))</span>
 				goto success;
 			addr = vma-&gt;vm_end;
 		}
<span class="p_header">--- a/arch/ia64/kernel/sys_ia64.c</span>
<span class="p_header">+++ b/arch/ia64/kernel/sys_ia64.c</span>
<span class="p_chunk">@@ -27,7 +27,8 @@</span> <span class="p_context"> arch_get_unmapped_area (struct file *fil</span>
 	long map_shared = (flags &amp; MAP_SHARED);
 	unsigned long start_addr, align_mask = PAGE_SIZE - 1;
 	struct mm_struct *mm = current-&gt;mm;
<span class="p_del">-	struct vm_area_struct *vma;</span>
<span class="p_add">+	struct vm_area_struct *vma, *prev;</span>
<span class="p_add">+	unsigned long prev_end;</span>
 
 	if (len &gt; RGN_MAP_LIMIT)
 		return -ENOMEM;
<span class="p_chunk">@@ -58,7 +59,17 @@</span> <span class="p_context"> arch_get_unmapped_area (struct file *fil</span>
   full_search:
 	start_addr = addr = (addr + align_mask) &amp; ~align_mask;
 
<span class="p_del">-	for (vma = find_vma(mm, addr); ; vma = vma-&gt;vm_next) {</span>
<span class="p_add">+	for (vma = find_vma_prev(mm, addr, &amp;prev); ; prev = vma,</span>
<span class="p_add">+						vma = vma-&gt;vm_next) {</span>
<span class="p_add">+		if (prev) {</span>
<span class="p_add">+			prev_end = vm_end_gap(prev);</span>
<span class="p_add">+			if (addr &lt; prev_end) {</span>
<span class="p_add">+				addr = (prev_end + align_mask) &amp; ~align_mask;</span>
<span class="p_add">+				/* If vma already violates gap, forget it */</span>
<span class="p_add">+				if (vma &amp;&amp; addr &gt; vma-&gt;vm_start)</span>
<span class="p_add">+					addr = vma-&gt;vm_start;</span>
<span class="p_add">+			}</span>
<span class="p_add">+		}</span>
 		/* At this point:  (!vma || addr &lt; vma-&gt;vm_end). */
 		if (TASK_SIZE - len &lt; addr || RGN_MAP_LIMIT - len &lt; REGION_OFFSET(addr)) {
 			if (start_addr != TASK_UNMAPPED_BASE) {
<span class="p_chunk">@@ -68,12 +79,11 @@</span> <span class="p_context"> arch_get_unmapped_area (struct file *fil</span>
 			}
 			return -ENOMEM;
 		}
<span class="p_del">-		if (!vma || addr + len &lt;= vma-&gt;vm_start) {</span>
<span class="p_add">+		if (!vma || addr + len &lt;= vm_start_gap(vma)) {</span>
 			/* Remember the address where we stopped this search:  */
 			mm-&gt;free_area_cache = addr + len;
 			return addr;
 		}
<span class="p_del">-		addr = (vma-&gt;vm_end + align_mask) &amp; ~align_mask;</span>
 	}
 }
 
<span class="p_header">--- a/arch/ia64/mm/hugetlbpage.c</span>
<span class="p_header">+++ b/arch/ia64/mm/hugetlbpage.c</span>
<span class="p_chunk">@@ -171,9 +171,9 @@</span> <span class="p_context"> unsigned long hugetlb_get_unmapped_area(</span>
 		/* At this point:  (!vmm || addr &lt; vmm-&gt;vm_end). */
 		if (REGION_OFFSET(addr) + len &gt; RGN_MAP_LIMIT)
 			return -ENOMEM;
<span class="p_del">-		if (!vmm || (addr + len) &lt;= vmm-&gt;vm_start)</span>
<span class="p_add">+		if (!vmm || (addr + len) &lt;= vm_start_gap(vmm))</span>
 			return addr;
<span class="p_del">-		addr = ALIGN(vmm-&gt;vm_end, HPAGE_SIZE);</span>
<span class="p_add">+		addr = ALIGN(vm_end_gap(vmm), HPAGE_SIZE);</span>
 	}
 }
 
<span class="p_header">--- a/arch/mips/mm/mmap.c</span>
<span class="p_header">+++ b/arch/mips/mm/mmap.c</span>
<span class="p_chunk">@@ -70,6 +70,7 @@</span> <span class="p_context"> static unsigned long arch_get_unmapped_a</span>
 	struct mm_struct *mm = current-&gt;mm;
 	struct vm_area_struct *vma;
 	unsigned long addr = addr0;
<span class="p_add">+	unsigned long vm_start;</span>
 	int do_color_align;
 
 	if (unlikely(len &gt; TASK_SIZE))
<span class="p_chunk">@@ -103,7 +104,7 @@</span> <span class="p_context"> static unsigned long arch_get_unmapped_a</span>
 
 		vma = find_vma(mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 
<span class="p_chunk">@@ -118,7 +119,7 @@</span> <span class="p_context"> static unsigned long arch_get_unmapped_a</span>
 			/* At this point:  (!vma || addr &lt; vma-&gt;vm_end). */
 			if (TASK_SIZE - len &lt; addr)
 				return -ENOMEM;
<span class="p_del">-			if (!vma || addr + len &lt;= vma-&gt;vm_start)</span>
<span class="p_add">+			if (!vma || addr + len &lt;= vm_start_gap(vma))</span>
 				return addr;
 			addr = vma-&gt;vm_end;
 			if (do_color_align)
<span class="p_chunk">@@ -145,7 +146,7 @@</span> <span class="p_context"> static unsigned long arch_get_unmapped_a</span>
 		/* make sure it can fit in the remaining address space */
 		if (likely(addr &gt; len)) {
 			vma = find_vma(mm, addr - len);
<span class="p_del">-			if (!vma || addr &lt;= vma-&gt;vm_start) {</span>
<span class="p_add">+			if (!vma || addr &lt;= vm_start_gap(vma)) {</span>
 				/* cache the address as a hint for next time */
 				return mm-&gt;free_area_cache = addr - len;
 			}
<span class="p_chunk">@@ -165,20 +166,22 @@</span> <span class="p_context"> static unsigned long arch_get_unmapped_a</span>
 			 * return with success:
 			 */
 			vma = find_vma(mm, addr);
<span class="p_del">-			if (likely(!vma || addr + len &lt;= vma-&gt;vm_start)) {</span>
<span class="p_add">+			if (vma)</span>
<span class="p_add">+				vm_start = vm_start_gap(vma);</span>
<span class="p_add">+			if (likely(!vma || addr + len &lt;= vm_start)) {</span>
 				/* cache the address as a hint for next time */
 				return mm-&gt;free_area_cache = addr;
 			}
 
 			/* remember the largest hole we saw so far */
<span class="p_del">-			if (addr + mm-&gt;cached_hole_size &lt; vma-&gt;vm_start)</span>
<span class="p_del">-				mm-&gt;cached_hole_size = vma-&gt;vm_start - addr;</span>
<span class="p_add">+			if (addr + mm-&gt;cached_hole_size &lt; vm_start)</span>
<span class="p_add">+				mm-&gt;cached_hole_size = vm_start - addr;</span>
 
 			/* try just below the current vma-&gt;vm_start */
<span class="p_del">-			addr = vma-&gt;vm_start - len;</span>
<span class="p_add">+			addr = vm_start - len;</span>
 			if (do_color_align)
 				addr = COLOUR_ALIGN_DOWN(addr, pgoff);
<span class="p_del">-		} while (likely(len &lt; vma-&gt;vm_start));</span>
<span class="p_add">+		} while (likely(len &lt; vm_start));</span>
 
 bottomup:
 		/*
<span class="p_header">--- a/arch/parisc/kernel/sys_parisc.c</span>
<span class="p_header">+++ b/arch/parisc/kernel/sys_parisc.c</span>
<span class="p_chunk">@@ -35,17 +35,27 @@</span> <span class="p_context"></span>
 
 static unsigned long get_unshared_area(unsigned long addr, unsigned long len)
 {
<span class="p_del">-	struct vm_area_struct *vma;</span>
<span class="p_add">+	struct vm_area_struct *vma, *prev;</span>
<span class="p_add">+	unsigned long prev_end;</span>
 
 	addr = PAGE_ALIGN(addr);
 
<span class="p_del">-	for (vma = find_vma(current-&gt;mm, addr); ; vma = vma-&gt;vm_next) {</span>
<span class="p_add">+	for (vma = find_vma_prev(current-&gt;mm, addr, &amp;prev); ; prev = vma,</span>
<span class="p_add">+							vma = vma-&gt;vm_next) {</span>
<span class="p_add">+		if (prev) {</span>
<span class="p_add">+			prev_end = vm_end_gap(prev);</span>
<span class="p_add">+			if (addr &lt; prev_end) {</span>
<span class="p_add">+				addr = prev_end;</span>
<span class="p_add">+				/* If vma already violates gap, forget it */</span>
<span class="p_add">+				if (vma &amp;&amp; addr &gt; vma-&gt;vm_start)</span>
<span class="p_add">+					addr = vma-&gt;vm_start;</span>
<span class="p_add">+			}</span>
<span class="p_add">+		}</span>
 		/* At this point:  (!vma || addr &lt; vma-&gt;vm_end). */
 		if (TASK_SIZE - len &lt; addr)
 			return -ENOMEM;
<span class="p_del">-		if (!vma || addr + len &lt;= vma-&gt;vm_start)</span>
<span class="p_add">+		if (!vma || addr + len &lt;= vm_start_gap(vma))</span>
 			return addr;
<span class="p_del">-		addr = vma-&gt;vm_end;</span>
 	}
 }
 
<span class="p_chunk">@@ -70,22 +80,32 @@</span> <span class="p_context"> static int get_offset(struct address_spa</span>
 static unsigned long get_shared_area(struct address_space *mapping,
 		unsigned long addr, unsigned long len, unsigned long pgoff)
 {
<span class="p_del">-	struct vm_area_struct *vma;</span>
<span class="p_add">+	struct vm_area_struct *vma, *prev;</span>
<span class="p_add">+	unsigned long prev_end;</span>
 	int offset = mapping ? get_offset(mapping) : 0;
 
 	offset = (offset + (pgoff &lt;&lt; PAGE_SHIFT)) &amp; 0x3FF000;
 
 	addr = DCACHE_ALIGN(addr - offset) + offset;
 
<span class="p_del">-	for (vma = find_vma(current-&gt;mm, addr); ; vma = vma-&gt;vm_next) {</span>
<span class="p_add">+	for (vma = find_vma_prev(current-&gt;mm, addr, &amp;prev); ; prev = vma,</span>
<span class="p_add">+							vma = vma-&gt;vm_next) {</span>
<span class="p_add">+		if (prev) {</span>
<span class="p_add">+			prev_end = vm_end_gap(prev);</span>
<span class="p_add">+			if (addr &lt; prev_end) {</span>
<span class="p_add">+				addr = DCACHE_ALIGN(prev_end - offset) + offset;</span>
<span class="p_add">+				if (addr &lt; prev_end)	/* handle wraparound */</span>
<span class="p_add">+					return -ENOMEM;</span>
<span class="p_add">+				/* If vma already violates gap, forget it */</span>
<span class="p_add">+				if (vma &amp;&amp; addr &gt; vma-&gt;vm_start)</span>
<span class="p_add">+					addr = vma-&gt;vm_start;</span>
<span class="p_add">+			}</span>
<span class="p_add">+		}</span>
 		/* At this point:  (!vma || addr &lt; vma-&gt;vm_end). */
 		if (TASK_SIZE - len &lt; addr)
 			return -ENOMEM;
<span class="p_del">-		if (!vma || addr + len &lt;= vma-&gt;vm_start)</span>
<span class="p_add">+		if (!vma || addr + len &lt;= vm_start_gap(vma))</span>
 			return addr;
<span class="p_del">-		addr = DCACHE_ALIGN(vma-&gt;vm_end - offset) + offset;</span>
<span class="p_del">-		if (addr &lt; vma-&gt;vm_end) /* handle wraparound */</span>
<span class="p_del">-			return -ENOMEM;</span>
 	}
 }
 
<span class="p_header">--- a/arch/powerpc/mm/slice.c</span>
<span class="p_header">+++ b/arch/powerpc/mm/slice.c</span>
<span class="p_chunk">@@ -98,7 +98,7 @@</span> <span class="p_context"> static int slice_area_is_free(struct mm_</span>
 	if ((mm-&gt;task_size - len) &lt; addr)
 		return 0;
 	vma = find_vma(mm, addr);
<span class="p_del">-	return (!vma || (addr + len) &lt;= vma-&gt;vm_start);</span>
<span class="p_add">+	return (!vma || (addr + len) &lt;= vm_start_gap(vma));</span>
 }
 
 static int slice_low_has_vma(struct mm_struct *mm, unsigned long slice)
<span class="p_chunk">@@ -227,7 +227,7 @@</span> <span class="p_context"> static unsigned long slice_find_area_bot</span>
 					      int psize, int use_cache)
 {
 	struct vm_area_struct *vma;
<span class="p_del">-	unsigned long start_addr, addr;</span>
<span class="p_add">+	unsigned long start_addr, addr, vm_start;</span>
 	struct slice_mask mask;
 	int pshift = max_t(int, mmu_psize_defs[psize].shift, PAGE_SHIFT);
 
<span class="p_chunk">@@ -256,7 +256,9 @@</span> <span class="p_context"> full_search:</span>
 				addr = _ALIGN_UP(addr + 1,  1ul &lt;&lt; SLICE_HIGH_SHIFT);
 			continue;
 		}
<span class="p_del">-		if (!vma || addr + len &lt;= vma-&gt;vm_start) {</span>
<span class="p_add">+		if (vma)</span>
<span class="p_add">+			vm_start = vm_start_gap(vma);</span>
<span class="p_add">+		if (!vma || addr + len &lt;= vm_start) {</span>
 			/*
 			 * Remember the place where we stopped the search:
 			 */
<span class="p_chunk">@@ -264,8 +266,8 @@</span> <span class="p_context"> full_search:</span>
 				mm-&gt;free_area_cache = addr + len;
 			return addr;
 		}
<span class="p_del">-		if (use_cache &amp;&amp; (addr + mm-&gt;cached_hole_size) &lt; vma-&gt;vm_start)</span>
<span class="p_del">-		        mm-&gt;cached_hole_size = vma-&gt;vm_start - addr;</span>
<span class="p_add">+		if (use_cache &amp;&amp; (addr + mm-&gt;cached_hole_size) &lt; vm_start)</span>
<span class="p_add">+			mm-&gt;cached_hole_size = vm_start - addr;</span>
 		addr = vma-&gt;vm_end;
 	}
 
<span class="p_chunk">@@ -284,7 +286,7 @@</span> <span class="p_context"> static unsigned long slice_find_area_top</span>
 					     int psize, int use_cache)
 {
 	struct vm_area_struct *vma;
<span class="p_del">-	unsigned long addr;</span>
<span class="p_add">+	unsigned long addr, vm_start;</span>
 	struct slice_mask mask;
 	int pshift = max_t(int, mmu_psize_defs[psize].shift, PAGE_SHIFT);
 
<span class="p_chunk">@@ -336,7 +338,9 @@</span> <span class="p_context"> static unsigned long slice_find_area_top</span>
 		 * return with success:
 		 */
 		vma = find_vma(mm, addr);
<span class="p_del">-		if (!vma || (addr + len) &lt;= vma-&gt;vm_start) {</span>
<span class="p_add">+		if (vma)</span>
<span class="p_add">+			vm_start = vm_start_gap(vma);</span>
<span class="p_add">+		if (!vma || (addr + len) &lt;= vm_start) {</span>
 			/* remember the address as a hint for next time */
 			if (use_cache)
 				mm-&gt;free_area_cache = addr;
<span class="p_chunk">@@ -344,11 +348,11 @@</span> <span class="p_context"> static unsigned long slice_find_area_top</span>
 		}
 
 		/* remember the largest hole we saw so far */
<span class="p_del">-		if (use_cache &amp;&amp; (addr + mm-&gt;cached_hole_size) &lt; vma-&gt;vm_start)</span>
<span class="p_del">-		        mm-&gt;cached_hole_size = vma-&gt;vm_start - addr;</span>
<span class="p_add">+		if (use_cache &amp;&amp; (addr + mm-&gt;cached_hole_size) &lt; vm_start)</span>
<span class="p_add">+			mm-&gt;cached_hole_size = vm_start - addr;</span>
 
 		/* try just below the current vma-&gt;vm_start */
<span class="p_del">-		addr = vma-&gt;vm_start;</span>
<span class="p_add">+		addr = vm_start;</span>
 	}
 
 	/*
<span class="p_header">--- a/arch/sh/mm/mmap.c</span>
<span class="p_header">+++ b/arch/sh/mm/mmap.c</span>
<span class="p_chunk">@@ -47,7 +47,7 @@</span> <span class="p_context"> unsigned long arch_get_unmapped_area(str</span>
 {
 	struct mm_struct *mm = current-&gt;mm;
 	struct vm_area_struct *vma;
<span class="p_del">-	unsigned long start_addr;</span>
<span class="p_add">+	unsigned long start_addr, vm_start;</span>
 	int do_colour_align;
 
 	if (flags &amp; MAP_FIXED) {
<span class="p_chunk">@@ -75,7 +75,7 @@</span> <span class="p_context"> unsigned long arch_get_unmapped_area(str</span>
 
 		vma = find_vma(mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 
<span class="p_chunk">@@ -106,15 +106,17 @@</span> <span class="p_context"> full_search:</span>
 			}
 			return -ENOMEM;
 		}
<span class="p_del">-		if (likely(!vma || addr + len &lt;= vma-&gt;vm_start)) {</span>
<span class="p_add">+		if (vma)</span>
<span class="p_add">+			vm_start = vm_start_gap(vma);</span>
<span class="p_add">+		if (likely(!vma || addr + len &lt;= vm_start)) {</span>
 			/*
 			 * Remember the place where we stopped the search:
 			 */
 			mm-&gt;free_area_cache = addr + len;
 			return addr;
 		}
<span class="p_del">-		if (addr + mm-&gt;cached_hole_size &lt; vma-&gt;vm_start)</span>
<span class="p_del">-		        mm-&gt;cached_hole_size = vma-&gt;vm_start - addr;</span>
<span class="p_add">+		if (addr + mm-&gt;cached_hole_size &lt; vm_start)</span>
<span class="p_add">+			mm-&gt;cached_hole_size = vm_start - addr;</span>
 
 		addr = vma-&gt;vm_end;
 		if (do_colour_align)
<span class="p_chunk">@@ -130,6 +132,7 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct fi</span>
 	struct vm_area_struct *vma;
 	struct mm_struct *mm = current-&gt;mm;
 	unsigned long addr = addr0;
<span class="p_add">+	unsigned long vm_start;</span>
 	int do_colour_align;
 
 	if (flags &amp; MAP_FIXED) {
<span class="p_chunk">@@ -158,7 +161,7 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct fi</span>
 
 		vma = find_vma(mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 
<span class="p_chunk">@@ -179,7 +182,7 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct fi</span>
 	/* make sure it can fit in the remaining address space */
 	if (likely(addr &gt; len)) {
 		vma = find_vma(mm, addr-len);
<span class="p_del">-		if (!vma || addr &lt;= vma-&gt;vm_start) {</span>
<span class="p_add">+		if (!vma || addr &lt;= vm_start_gap(vma)) {</span>
 			/* remember the address as a hint for next time */
 			return (mm-&gt;free_area_cache = addr-len);
 		}
<span class="p_chunk">@@ -199,20 +202,22 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct fi</span>
 		 * return with success:
 		 */
 		vma = find_vma(mm, addr);
<span class="p_del">-		if (likely(!vma || addr+len &lt;= vma-&gt;vm_start)) {</span>
<span class="p_add">+		if (vma)</span>
<span class="p_add">+			vm_start = vm_start_gap(vma);</span>
<span class="p_add">+		if (likely(!vma || addr + len &lt;= vm_start)) {</span>
 			/* remember the address as a hint for next time */
 			return (mm-&gt;free_area_cache = addr);
 		}
 
 		/* remember the largest hole we saw so far */
<span class="p_del">-		if (addr + mm-&gt;cached_hole_size &lt; vma-&gt;vm_start)</span>
<span class="p_del">-		        mm-&gt;cached_hole_size = vma-&gt;vm_start - addr;</span>
<span class="p_add">+		if (addr + mm-&gt;cached_hole_size &lt; vm_start)</span>
<span class="p_add">+			mm-&gt;cached_hole_size = vm_start - addr;</span>
 
 		/* try just below the current vma-&gt;vm_start */
<span class="p_del">-		addr = vma-&gt;vm_start-len;</span>
<span class="p_add">+		addr = vm_start-len;</span>
 		if (do_colour_align)
 			addr = COLOUR_ALIGN_DOWN(addr, pgoff);
<span class="p_del">-	} while (likely(len &lt; vma-&gt;vm_start));</span>
<span class="p_add">+	} while (likely(len &lt; vm_start));</span>
 
 bottomup:
 	/*
<span class="p_header">--- a/arch/sparc/kernel/sys_sparc_32.c</span>
<span class="p_header">+++ b/arch/sparc/kernel/sys_sparc_32.c</span>
<span class="p_chunk">@@ -71,7 +71,7 @@</span> <span class="p_context"> unsigned long arch_get_unmapped_area(str</span>
 		}
 		if (TASK_SIZE - PAGE_SIZE - len &lt; addr)
 			return -ENOMEM;
<span class="p_del">-		if (!vmm || addr + len &lt;= vmm-&gt;vm_start)</span>
<span class="p_add">+		if (!vmm || addr + len &lt;= vm_start_gap(vmm))</span>
 			return addr;
 		addr = vmm-&gt;vm_end;
 		if (flags &amp; MAP_SHARED)
<span class="p_header">--- a/arch/sparc/kernel/sys_sparc_64.c</span>
<span class="p_header">+++ b/arch/sparc/kernel/sys_sparc_64.c</span>
<span class="p_chunk">@@ -117,7 +117,7 @@</span> <span class="p_context"> unsigned long arch_get_unmapped_area(str</span>
 	struct mm_struct *mm = current-&gt;mm;
 	struct vm_area_struct * vma;
 	unsigned long task_size = TASK_SIZE;
<span class="p_del">-	unsigned long start_addr;</span>
<span class="p_add">+	unsigned long start_addr, vm_start;</span>
 	int do_color_align;
 
 	if (flags &amp; MAP_FIXED) {
<span class="p_chunk">@@ -147,7 +147,7 @@</span> <span class="p_context"> unsigned long arch_get_unmapped_area(str</span>
 
 		vma = find_vma(mm, addr);
 		if (task_size - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 
<span class="p_chunk">@@ -181,15 +181,17 @@</span> <span class="p_context"> full_search:</span>
 			}
 			return -ENOMEM;
 		}
<span class="p_del">-		if (likely(!vma || addr + len &lt;= vma-&gt;vm_start)) {</span>
<span class="p_add">+		if (vma)</span>
<span class="p_add">+			vm_start = vm_start_gap(vma);</span>
<span class="p_add">+		if (likely(!vma || addr + len &lt;= vm_start)) {</span>
 			/*
 			 * Remember the place where we stopped the search:
 			 */
 			mm-&gt;free_area_cache = addr + len;
 			return addr;
 		}
<span class="p_del">-		if (addr + mm-&gt;cached_hole_size &lt; vma-&gt;vm_start)</span>
<span class="p_del">-		        mm-&gt;cached_hole_size = vma-&gt;vm_start - addr;</span>
<span class="p_add">+		if (addr + mm-&gt;cached_hole_size &lt; vm_start)</span>
<span class="p_add">+			mm-&gt;cached_hole_size = vm_start - addr;</span>
 
 		addr = vma-&gt;vm_end;
 		if (do_color_align)
<span class="p_chunk">@@ -205,7 +207,7 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct fi</span>
 	struct vm_area_struct *vma;
 	struct mm_struct *mm = current-&gt;mm;
 	unsigned long task_size = STACK_TOP32;
<span class="p_del">-	unsigned long addr = addr0;</span>
<span class="p_add">+	unsigned long addr = addr0, vm_start;</span>
 	int do_color_align;
 
 	/* This should only ever run for 32-bit processes.  */
<span class="p_chunk">@@ -237,7 +239,7 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct fi</span>
 
 		vma = find_vma(mm, addr);
 		if (task_size - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 
<span class="p_chunk">@@ -258,7 +260,7 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct fi</span>
 	/* make sure it can fit in the remaining address space */
 	if (likely(addr &gt; len)) {
 		vma = find_vma(mm, addr-len);
<span class="p_del">-		if (!vma || addr &lt;= vma-&gt;vm_start) {</span>
<span class="p_add">+		if (!vma || addr &lt;= vm_start_gap(vma)) {</span>
 			/* remember the address as a hint for next time */
 			return (mm-&gt;free_area_cache = addr-len);
 		}
<span class="p_chunk">@@ -278,20 +280,22 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct fi</span>
 		 * return with success:
 		 */
 		vma = find_vma(mm, addr);
<span class="p_del">-		if (likely(!vma || addr+len &lt;= vma-&gt;vm_start)) {</span>
<span class="p_add">+		if (vma)</span>
<span class="p_add">+			vm_start = vm_start_gap(vma);</span>
<span class="p_add">+		if (likely(!vma || addr + len &lt;= vm_start)) {</span>
 			/* remember the address as a hint for next time */
 			return (mm-&gt;free_area_cache = addr);
 		}
 
  		/* remember the largest hole we saw so far */
<span class="p_del">- 		if (addr + mm-&gt;cached_hole_size &lt; vma-&gt;vm_start)</span>
<span class="p_del">- 		        mm-&gt;cached_hole_size = vma-&gt;vm_start - addr;</span>
<span class="p_add">+ 		if (addr + mm-&gt;cached_hole_size &lt; vm_start)</span>
<span class="p_add">+ 		        mm-&gt;cached_hole_size = vm_start - addr;</span>
 
 		/* try just below the current vma-&gt;vm_start */
<span class="p_del">-		addr = vma-&gt;vm_start-len;</span>
<span class="p_add">+		addr = vm_start - len;</span>
 		if (do_color_align)
 			addr = COLOUR_ALIGN_DOWN(addr, pgoff);
<span class="p_del">-	} while (likely(len &lt; vma-&gt;vm_start));</span>
<span class="p_add">+	} while (likely(len &lt; vm_start));</span>
 
 bottomup:
 	/*
<span class="p_header">--- a/arch/sparc/mm/hugetlbpage.c</span>
<span class="p_header">+++ b/arch/sparc/mm/hugetlbpage.c</span>
<span class="p_chunk">@@ -33,7 +33,7 @@</span> <span class="p_context"> static unsigned long hugetlb_get_unmappe</span>
 	struct mm_struct *mm = current-&gt;mm;
 	struct vm_area_struct * vma;
 	unsigned long task_size = TASK_SIZE;
<span class="p_del">-	unsigned long start_addr;</span>
<span class="p_add">+	unsigned long start_addr, vm_start;</span>
 
 	if (test_thread_flag(TIF_32BIT))
 		task_size = STACK_TOP32;
<span class="p_chunk">@@ -67,15 +67,17 @@</span> <span class="p_context"> full_search:</span>
 			}
 			return -ENOMEM;
 		}
<span class="p_del">-		if (likely(!vma || addr + len &lt;= vma-&gt;vm_start)) {</span>
<span class="p_add">+		if (vma)</span>
<span class="p_add">+			vm_start = vm_start_gap(vma);</span>
<span class="p_add">+		if (likely(!vma || addr + len &lt;= vm_start)) {</span>
 			/*
 			 * Remember the place where we stopped the search:
 			 */
 			mm-&gt;free_area_cache = addr + len;
 			return addr;
 		}
<span class="p_del">-		if (addr + mm-&gt;cached_hole_size &lt; vma-&gt;vm_start)</span>
<span class="p_del">-		        mm-&gt;cached_hole_size = vma-&gt;vm_start - addr;</span>
<span class="p_add">+		if (addr + mm-&gt;cached_hole_size &lt; vm_start)</span>
<span class="p_add">+			mm-&gt;cached_hole_size = vm_start - addr;</span>
 
 		addr = ALIGN(vma-&gt;vm_end, HPAGE_SIZE);
 	}
<span class="p_chunk">@@ -90,6 +92,7 @@</span> <span class="p_context"> hugetlb_get_unmapped_area_topdown(struct</span>
 	struct vm_area_struct *vma;
 	struct mm_struct *mm = current-&gt;mm;
 	unsigned long addr = addr0;
<span class="p_add">+	unsigned long vm_start;</span>
 
 	/* This should only ever run for 32-bit processes.  */
 	BUG_ON(!test_thread_flag(TIF_32BIT));
<span class="p_chunk">@@ -106,7 +109,7 @@</span> <span class="p_context"> hugetlb_get_unmapped_area_topdown(struct</span>
 	/* make sure it can fit in the remaining address space */
 	if (likely(addr &gt; len)) {
 		vma = find_vma(mm, addr-len);
<span class="p_del">-		if (!vma || addr &lt;= vma-&gt;vm_start) {</span>
<span class="p_add">+		if (!vma || addr &lt;= vm_start_gap(vma)) {</span>
 			/* remember the address as a hint for next time */
 			return (mm-&gt;free_area_cache = addr-len);
 		}
<span class="p_chunk">@@ -124,18 +127,20 @@</span> <span class="p_context"> hugetlb_get_unmapped_area_topdown(struct</span>
 		 * return with success:
 		 */
 		vma = find_vma(mm, addr);
<span class="p_del">-		if (likely(!vma || addr+len &lt;= vma-&gt;vm_start)) {</span>
<span class="p_add">+		if (vma)</span>
<span class="p_add">+			vm_start = vm_start_gap(vma);</span>
<span class="p_add">+		if (likely(!vma || addr + len &lt;= vm_start)) {</span>
 			/* remember the address as a hint for next time */
 			return (mm-&gt;free_area_cache = addr);
 		}
 
  		/* remember the largest hole we saw so far */
<span class="p_del">- 		if (addr + mm-&gt;cached_hole_size &lt; vma-&gt;vm_start)</span>
<span class="p_del">- 		        mm-&gt;cached_hole_size = vma-&gt;vm_start - addr;</span>
<span class="p_add">+		if (addr + mm-&gt;cached_hole_size &lt; vm_start)</span>
<span class="p_add">+			mm-&gt;cached_hole_size = vm_start - addr;</span>
 
 		/* try just below the current vma-&gt;vm_start */
<span class="p_del">-		addr = (vma-&gt;vm_start-len) &amp; HPAGE_MASK;</span>
<span class="p_del">-	} while (likely(len &lt; vma-&gt;vm_start));</span>
<span class="p_add">+		addr = (vm_start - len) &amp; HPAGE_MASK;</span>
<span class="p_add">+	} while (likely(len &lt; vm_start));</span>
 
 bottomup:
 	/*
<span class="p_chunk">@@ -182,7 +187,7 @@</span> <span class="p_context"> hugetlb_get_unmapped_area(struct file *f</span>
 		addr = ALIGN(addr, HPAGE_SIZE);
 		vma = find_vma(mm, addr);
 		if (task_size - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 	if (mm-&gt;get_unmapped_area == arch_get_unmapped_area)
<span class="p_header">--- a/arch/tile/mm/hugetlbpage.c</span>
<span class="p_header">+++ b/arch/tile/mm/hugetlbpage.c</span>
<span class="p_chunk">@@ -159,7 +159,7 @@</span> <span class="p_context"> static unsigned long hugetlb_get_unmappe</span>
 	struct hstate *h = hstate_file(file);
 	struct mm_struct *mm = current-&gt;mm;
 	struct vm_area_struct *vma;
<span class="p_del">-	unsigned long start_addr;</span>
<span class="p_add">+	unsigned long start_addr, vm_start;</span>
 
 	if (len &gt; mm-&gt;cached_hole_size) {
 		start_addr = mm-&gt;free_area_cache;
<span class="p_chunk">@@ -185,12 +185,14 @@</span> <span class="p_context"> full_search:</span>
 			}
 			return -ENOMEM;
 		}
<span class="p_del">-		if (!vma || addr + len &lt;= vma-&gt;vm_start) {</span>
<span class="p_add">+		if (vma)</span>
<span class="p_add">+			vm_start = vm_start_gap(vma);</span>
<span class="p_add">+		if (!vma || addr + len &lt;= vm_start) {</span>
 			mm-&gt;free_area_cache = addr + len;
 			return addr;
 		}
<span class="p_del">-		if (addr + mm-&gt;cached_hole_size &lt; vma-&gt;vm_start)</span>
<span class="p_del">-			mm-&gt;cached_hole_size = vma-&gt;vm_start - addr;</span>
<span class="p_add">+		if (addr + mm-&gt;cached_hole_size &lt; vm_start)</span>
<span class="p_add">+			mm-&gt;cached_hole_size = vm_start - addr;</span>
 		addr = ALIGN(vma-&gt;vm_end, huge_page_size(h));
 	}
 }
<span class="p_chunk">@@ -204,6 +206,7 @@</span> <span class="p_context"> static unsigned long hugetlb_get_unmappe</span>
 	struct vm_area_struct *vma, *prev_vma;
 	unsigned long base = mm-&gt;mmap_base, addr = addr0;
 	unsigned long largest_hole = mm-&gt;cached_hole_size;
<span class="p_add">+	unsigned long vm_start;</span>
 	int first_time = 1;
 
 	/* don&#39;t allow allocations above current base */
<span class="p_chunk">@@ -234,9 +237,10 @@</span> <span class="p_context"> try_again:</span>
 
 		/*
 		 * new region fits between prev_vma-&gt;vm_end and
<span class="p_del">-		 * vma-&gt;vm_start, use it:</span>
<span class="p_add">+		 * vm_start, use it:</span>
 		 */
<span class="p_del">-		if (addr + len &lt;= vma-&gt;vm_start &amp;&amp;</span>
<span class="p_add">+		vm_start = vm_start_gap(vma);</span>
<span class="p_add">+		if (addr + len &lt;= vm_start &amp;&amp;</span>
 			    (!prev_vma || (addr &gt;= prev_vma-&gt;vm_end))) {
 			/* remember the address as a hint for next time */
 			mm-&gt;cached_hole_size = largest_hole;
<span class="p_chunk">@@ -251,13 +255,13 @@</span> <span class="p_context"> try_again:</span>
 		}
 
 		/* remember the largest hole we saw so far */
<span class="p_del">-		if (addr + largest_hole &lt; vma-&gt;vm_start)</span>
<span class="p_del">-			largest_hole = vma-&gt;vm_start - addr;</span>
<span class="p_add">+		if (addr + largest_hole &lt; vm_start)</span>
<span class="p_add">+			largest_hole = vm_start - addr;</span>
 
 		/* try just below the current vma-&gt;vm_start */
<span class="p_del">-		addr = (vma-&gt;vm_start - len) &amp; huge_page_mask(h);</span>
<span class="p_add">+		addr = (vm_start - len) &amp; huge_page_mask(h);</span>
 
<span class="p_del">-	} while (len &lt;= vma-&gt;vm_start);</span>
<span class="p_add">+	} while (len &lt;= vm_start);</span>
 
 fail:
 	/*
<span class="p_chunk">@@ -312,7 +316,7 @@</span> <span class="p_context"> unsigned long hugetlb_get_unmapped_area(</span>
 		addr = ALIGN(addr, huge_page_size(h));
 		vma = find_vma(mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 	if (current-&gt;mm-&gt;get_unmapped_area == arch_get_unmapped_area)
<span class="p_header">--- a/arch/x86/kernel/sys_x86_64.c</span>
<span class="p_header">+++ b/arch/x86/kernel/sys_x86_64.c</span>
<span class="p_chunk">@@ -126,7 +126,7 @@</span> <span class="p_context"> arch_get_unmapped_area(struct file *filp</span>
 {
 	struct mm_struct *mm = current-&gt;mm;
 	struct vm_area_struct *vma;
<span class="p_del">-	unsigned long start_addr;</span>
<span class="p_add">+	unsigned long start_addr, vm_start;</span>
 	unsigned long begin, end;
 
 	if (flags &amp; MAP_FIXED)
<span class="p_chunk">@@ -141,7 +141,7 @@</span> <span class="p_context"> arch_get_unmapped_area(struct file *filp</span>
 		addr = PAGE_ALIGN(addr);
 		vma = find_vma(mm, addr);
 		if (end - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 	if (((flags &amp; MAP_32BIT) || test_thread_flag(TIF_IA32))
<span class="p_chunk">@@ -172,15 +172,17 @@</span> <span class="p_context"> full_search:</span>
 			}
 			return -ENOMEM;
 		}
<span class="p_del">-		if (!vma || addr + len &lt;= vma-&gt;vm_start) {</span>
<span class="p_add">+		if (vma)</span>
<span class="p_add">+			vm_start = vm_start_gap(vma);</span>
<span class="p_add">+		if (!vma || addr + len &lt;= vm_start) {</span>
 			/*
 			 * Remember the place where we stopped the search:
 			 */
 			mm-&gt;free_area_cache = addr + len;
 			return addr;
 		}
<span class="p_del">-		if (addr + mm-&gt;cached_hole_size &lt; vma-&gt;vm_start)</span>
<span class="p_del">-			mm-&gt;cached_hole_size = vma-&gt;vm_start - addr;</span>
<span class="p_add">+		if (addr + mm-&gt;cached_hole_size &lt; vm_start)</span>
<span class="p_add">+			mm-&gt;cached_hole_size = vm_start - addr;</span>
 
 		addr = vma-&gt;vm_end;
 		addr = align_addr(addr, filp, 0);
<span class="p_chunk">@@ -196,6 +198,7 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct fi</span>
 	struct vm_area_struct *vma;
 	struct mm_struct *mm = current-&gt;mm;
 	unsigned long addr = addr0;
<span class="p_add">+	unsigned long vm_start;</span>
 
 	/* requested length too big for entire address space */
 	if (len &gt; TASK_SIZE)
<span class="p_chunk">@@ -213,7 +216,7 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct fi</span>
 		addr = PAGE_ALIGN(addr);
 		vma = find_vma(mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-				(!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+				(!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 
<span class="p_chunk">@@ -232,7 +235,7 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct fi</span>
 						    ALIGN_TOPDOWN);
 
 		vma = find_vma(mm, tmp_addr);
<span class="p_del">-		if (!vma || tmp_addr + len &lt;= vma-&gt;vm_start)</span>
<span class="p_add">+		if (!vma || tmp_addr + len &lt;= vm_start_gap(vma))</span>
 			/* remember the address as a hint for next time */
 			return mm-&gt;free_area_cache = tmp_addr;
 	}
<span class="p_chunk">@@ -251,17 +254,19 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct fi</span>
 		 * return with success:
 		 */
 		vma = find_vma(mm, addr);
<span class="p_del">-		if (!vma || addr+len &lt;= vma-&gt;vm_start)</span>
<span class="p_add">+		if (vma)</span>
<span class="p_add">+			vm_start = vm_start_gap(vma);</span>
<span class="p_add">+		if (!vma || addr + len &lt;= vm_start)</span>
 			/* remember the address as a hint for next time */
 			return mm-&gt;free_area_cache = addr;
 
 		/* remember the largest hole we saw so far */
<span class="p_del">-		if (addr + mm-&gt;cached_hole_size &lt; vma-&gt;vm_start)</span>
<span class="p_del">-			mm-&gt;cached_hole_size = vma-&gt;vm_start - addr;</span>
<span class="p_add">+		if (addr + mm-&gt;cached_hole_size &lt; vm_start)</span>
<span class="p_add">+			mm-&gt;cached_hole_size = vm_start - addr;</span>
 
 		/* try just below the current vma-&gt;vm_start */
<span class="p_del">-		addr = vma-&gt;vm_start-len;</span>
<span class="p_del">-	} while (len &lt; vma-&gt;vm_start);</span>
<span class="p_add">+		addr = vm_start - len;</span>
<span class="p_add">+	} while (len &lt; vm_start);</span>
 
 bottomup:
 	/*
<span class="p_header">--- a/arch/x86/mm/hugetlbpage.c</span>
<span class="p_header">+++ b/arch/x86/mm/hugetlbpage.c</span>
<span class="p_chunk">@@ -277,7 +277,7 @@</span> <span class="p_context"> static unsigned long hugetlb_get_unmappe</span>
 	struct hstate *h = hstate_file(file);
 	struct mm_struct *mm = current-&gt;mm;
 	struct vm_area_struct *vma;
<span class="p_del">-	unsigned long start_addr;</span>
<span class="p_add">+	unsigned long start_addr, vm_start;</span>
 
 	if (len &gt; mm-&gt;cached_hole_size) {
 	        start_addr = mm-&gt;free_area_cache;
<span class="p_chunk">@@ -303,12 +303,14 @@</span> <span class="p_context"> full_search:</span>
 			}
 			return -ENOMEM;
 		}
<span class="p_del">-		if (!vma || addr + len &lt;= vma-&gt;vm_start) {</span>
<span class="p_add">+		if (vma)</span>
<span class="p_add">+			vm_start = vm_start_gap(vma);</span>
<span class="p_add">+		if (!vma || addr + len &lt;= vm_start) {</span>
 			mm-&gt;free_area_cache = addr + len;
 			return addr;
 		}
<span class="p_del">-		if (addr + mm-&gt;cached_hole_size &lt; vma-&gt;vm_start)</span>
<span class="p_del">-		        mm-&gt;cached_hole_size = vma-&gt;vm_start - addr;</span>
<span class="p_add">+		if (addr + mm-&gt;cached_hole_size &lt; vm_start)</span>
<span class="p_add">+			mm-&gt;cached_hole_size = vm_start - addr;</span>
 		addr = ALIGN(vma-&gt;vm_end, huge_page_size(h));
 	}
 }
<span class="p_chunk">@@ -322,6 +324,7 @@</span> <span class="p_context"> static unsigned long hugetlb_get_unmappe</span>
 	struct vm_area_struct *vma, *prev_vma;
 	unsigned long base = mm-&gt;mmap_base, addr = addr0;
 	unsigned long largest_hole = mm-&gt;cached_hole_size;
<span class="p_add">+	unsigned long vm_start;</span>
 	int first_time = 1;
 
 	/* don&#39;t allow allocations above current base */
<span class="p_chunk">@@ -351,7 +354,8 @@</span> <span class="p_context"> try_again:</span>
 		 * new region fits between prev_vma-&gt;vm_end and
 		 * vma-&gt;vm_start, use it:
 		 */
<span class="p_del">-		if (addr + len &lt;= vma-&gt;vm_start &amp;&amp;</span>
<span class="p_add">+		vm_start = vm_start_gap(vma);</span>
<span class="p_add">+		if (addr + len &lt;= vm_start &amp;&amp;</span>
 		            (!prev_vma || (addr &gt;= prev_vma-&gt;vm_end))) {
 			/* remember the address as a hint for next time */
 		        mm-&gt;cached_hole_size = largest_hole;
<span class="p_chunk">@@ -365,12 +369,12 @@</span> <span class="p_context"> try_again:</span>
 		}
 
 		/* remember the largest hole we saw so far */
<span class="p_del">-		if (addr + largest_hole &lt; vma-&gt;vm_start)</span>
<span class="p_del">-		        largest_hole = vma-&gt;vm_start - addr;</span>
<span class="p_add">+		if (addr + largest_hole &lt; vm_start)</span>
<span class="p_add">+			largest_hole = vm_start - addr;</span>
 
 		/* try just below the current vma-&gt;vm_start */
<span class="p_del">-		addr = (vma-&gt;vm_start - len) &amp; huge_page_mask(h);</span>
<span class="p_del">-	} while (len &lt;= vma-&gt;vm_start);</span>
<span class="p_add">+		addr = (vm_start - len) &amp; huge_page_mask(h);</span>
<span class="p_add">+	} while (len &lt;= vm_start);</span>
 
 fail:
 	/*
<span class="p_chunk">@@ -426,7 +430,7 @@</span> <span class="p_context"> hugetlb_get_unmapped_area(struct file *f</span>
 		addr = ALIGN(addr, huge_page_size(h));
 		vma = find_vma(mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 	if (mm-&gt;get_unmapped_area == arch_get_unmapped_area)
<span class="p_header">--- a/fs/hugetlbfs/inode.c</span>
<span class="p_header">+++ b/fs/hugetlbfs/inode.c</span>
<span class="p_chunk">@@ -150,7 +150,7 @@</span> <span class="p_context"> hugetlb_get_unmapped_area(struct file *f</span>
 		addr = ALIGN(addr, huge_page_size(h));
 		vma = find_vma(mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 
<span class="p_chunk">@@ -176,7 +176,7 @@</span> <span class="p_context"> full_search:</span>
 			return -ENOMEM;
 		}
 
<span class="p_del">-		if (!vma || addr + len &lt;= vma-&gt;vm_start)</span>
<span class="p_add">+		if (!vma || addr + len &lt;= vm_start_gap(vma))</span>
 			return addr;
 		addr = ALIGN(vma-&gt;vm_end, huge_page_size(h));
 	}
<span class="p_header">--- a/fs/proc/task_mmu.c</span>
<span class="p_header">+++ b/fs/proc/task_mmu.c</span>
<span class="p_chunk">@@ -230,11 +230,7 @@</span> <span class="p_context"> static void show_map_vma(struct seq_file</span>
 
 	/* We don&#39;t show the stack guard page in /proc/maps */
 	start = vma-&gt;vm_start;
<span class="p_del">-	if (stack_guard_page_start(vma, start))</span>
<span class="p_del">-		start += PAGE_SIZE;</span>
 	end = vma-&gt;vm_end;
<span class="p_del">-	if (stack_guard_page_end(vma, end))</span>
<span class="p_del">-		end -= PAGE_SIZE;</span>
 
 	seq_printf(m, &quot;%08lx-%08lx %c%c%c%c %08llx %02x:%02x %lu %n&quot;,
 			start,
<span class="p_header">--- a/include/linux/mm.h</span>
<span class="p_header">+++ b/include/linux/mm.h</span>
<span class="p_chunk">@@ -1015,34 +1015,6 @@</span> <span class="p_context"> int set_page_dirty(struct page *page);</span>
 int set_page_dirty_lock(struct page *page);
 int clear_page_dirty_for_io(struct page *page);
 
<span class="p_del">-/* Is the vma a continuation of the stack vma above it? */</span>
<span class="p_del">-static inline int vma_growsdown(struct vm_area_struct *vma, unsigned long addr)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return vma &amp;&amp; (vma-&gt;vm_end == addr) &amp;&amp; (vma-&gt;vm_flags &amp; VM_GROWSDOWN);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline int stack_guard_page_start(struct vm_area_struct *vma,</span>
<span class="p_del">-					     unsigned long addr)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return (vma-&gt;vm_flags &amp; VM_GROWSDOWN) &amp;&amp;</span>
<span class="p_del">-		(vma-&gt;vm_start == addr) &amp;&amp;</span>
<span class="p_del">-		!vma_growsdown(vma-&gt;vm_prev, addr);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-/* Is the vma a continuation of the stack vma below it? */</span>
<span class="p_del">-static inline int vma_growsup(struct vm_area_struct *vma, unsigned long addr)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return vma &amp;&amp; (vma-&gt;vm_start == addr) &amp;&amp; (vma-&gt;vm_flags &amp; VM_GROWSUP);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline int stack_guard_page_end(struct vm_area_struct *vma,</span>
<span class="p_del">-					   unsigned long addr)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return (vma-&gt;vm_flags &amp; VM_GROWSUP) &amp;&amp;</span>
<span class="p_del">-		(vma-&gt;vm_end == addr) &amp;&amp;</span>
<span class="p_del">-		!vma_growsup(vma-&gt;vm_next, addr);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 extern unsigned long move_page_tables(struct vm_area_struct *vma,
 		unsigned long old_addr, struct vm_area_struct *new_vma,
 		unsigned long new_addr, unsigned long len);
<span class="p_chunk">@@ -1462,6 +1434,7 @@</span> <span class="p_context"> unsigned long ra_submit(struct file_ra_s</span>
 			struct address_space *mapping,
 			struct file *filp);
 
<span class="p_add">+extern unsigned long stack_guard_gap;</span>
 /* Generic expand stack which grows the stack according to GROWS{UP,DOWN} */
 extern int expand_stack(struct vm_area_struct *vma, unsigned long address);
 
<span class="p_chunk">@@ -1490,6 +1463,30 @@</span> <span class="p_context"> static inline struct vm_area_struct * fi</span>
 	return vma;
 }
 
<span class="p_add">+static inline unsigned long vm_start_gap(struct vm_area_struct *vma)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long vm_start = vma-&gt;vm_start;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (vma-&gt;vm_flags &amp; VM_GROWSDOWN) {</span>
<span class="p_add">+		vm_start -= stack_guard_gap;</span>
<span class="p_add">+		if (vm_start &gt; vma-&gt;vm_start)</span>
<span class="p_add">+			vm_start = 0;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return vm_start;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline unsigned long vm_end_gap(struct vm_area_struct *vma)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long vm_end = vma-&gt;vm_end;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (vma-&gt;vm_flags &amp; VM_GROWSUP) {</span>
<span class="p_add">+		vm_end += stack_guard_gap;</span>
<span class="p_add">+		if (vm_end &lt; vma-&gt;vm_end)</span>
<span class="p_add">+			vm_end = -PAGE_SIZE;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return vm_end;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static inline unsigned long vma_pages(struct vm_area_struct *vma)
 {
 	return (vma-&gt;vm_end - vma-&gt;vm_start) &gt;&gt; PAGE_SHIFT;
<span class="p_header">--- a/mm/memory.c</span>
<span class="p_header">+++ b/mm/memory.c</span>
<span class="p_chunk">@@ -1605,12 +1605,6 @@</span> <span class="p_context"> no_page_table:</span>
 	return page;
 }
 
<span class="p_del">-static inline int stack_guard_page(struct vm_area_struct *vma, unsigned long addr)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return stack_guard_page_start(vma, addr) ||</span>
<span class="p_del">-	       stack_guard_page_end(vma, addr+PAGE_SIZE);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 /**
  * __get_user_pages() - pin user pages in memory
  * @tsk:	task_struct of target task
<span class="p_chunk">@@ -1761,11 +1755,6 @@</span> <span class="p_context"> int __get_user_pages(struct task_struct</span>
 				int ret;
 				unsigned int fault_flags = 0;
 
<span class="p_del">-				/* For mlock, just skip the stack guard page. */</span>
<span class="p_del">-				if (foll_flags &amp; FOLL_MLOCK) {</span>
<span class="p_del">-					if (stack_guard_page(vma, start))</span>
<span class="p_del">-						goto next_page;</span>
<span class="p_del">-				}</span>
 				if (foll_flags &amp; FOLL_WRITE)
 					fault_flags |= FAULT_FLAG_WRITE;
 				if (nonblocking)
<span class="p_chunk">@@ -3122,40 +3111,6 @@</span> <span class="p_context"> out_release:</span>
 }
 
 /*
<span class="p_del">- * This is like a special single-page &quot;expand_{down|up}wards()&quot;,</span>
<span class="p_del">- * except we must first make sure that &#39;address{-|+}PAGE_SIZE&#39;</span>
<span class="p_del">- * doesn&#39;t hit another vma.</span>
<span class="p_del">- */</span>
<span class="p_del">-static inline int check_stack_guard_page(struct vm_area_struct *vma, unsigned long address)</span>
<span class="p_del">-{</span>
<span class="p_del">-	address &amp;= PAGE_MASK;</span>
<span class="p_del">-	if ((vma-&gt;vm_flags &amp; VM_GROWSDOWN) &amp;&amp; address == vma-&gt;vm_start) {</span>
<span class="p_del">-		struct vm_area_struct *prev = vma-&gt;vm_prev;</span>
<span class="p_del">-</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * Is there a mapping abutting this one below?</span>
<span class="p_del">-		 *</span>
<span class="p_del">-		 * That&#39;s only ok if it&#39;s the same stack mapping</span>
<span class="p_del">-		 * that has gotten split..</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		if (prev &amp;&amp; prev-&gt;vm_end == address)</span>
<span class="p_del">-			return prev-&gt;vm_flags &amp; VM_GROWSDOWN ? 0 : -ENOMEM;</span>
<span class="p_del">-</span>
<span class="p_del">-		return expand_downwards(vma, address - PAGE_SIZE);</span>
<span class="p_del">-	}</span>
<span class="p_del">-	if ((vma-&gt;vm_flags &amp; VM_GROWSUP) &amp;&amp; address + PAGE_SIZE == vma-&gt;vm_end) {</span>
<span class="p_del">-		struct vm_area_struct *next = vma-&gt;vm_next;</span>
<span class="p_del">-</span>
<span class="p_del">-		/* As VM_GROWSDOWN but s/below/above/ */</span>
<span class="p_del">-		if (next &amp;&amp; next-&gt;vm_start == address + PAGE_SIZE)</span>
<span class="p_del">-			return next-&gt;vm_flags &amp; VM_GROWSUP ? 0 : -ENOMEM;</span>
<span class="p_del">-</span>
<span class="p_del">-		return expand_upwards(vma, address + PAGE_SIZE);</span>
<span class="p_del">-	}</span>
<span class="p_del">-	return 0;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
  * We enter with non-exclusive mmap_sem (to exclude vma changes,
  * but allow concurrent faults), and pte mapped but not yet locked.
  * We return with mmap_sem still held, but pte unmapped and unlocked.
<span class="p_chunk">@@ -3174,10 +3129,6 @@</span> <span class="p_context"> static int do_anonymous_page(struct mm_s</span>
 	if (vma-&gt;vm_flags &amp; VM_SHARED)
 		return VM_FAULT_SIGBUS;
 
<span class="p_del">-	/* Check if we need to add a guard page to the stack */</span>
<span class="p_del">-	if (check_stack_guard_page(vma, address) &lt; 0)</span>
<span class="p_del">-		return VM_FAULT_SIGSEGV;</span>
<span class="p_del">-</span>
 	/* Use the zero-page for reads */
 	if (!(flags &amp; FAULT_FLAG_WRITE)) {
 		entry = pte_mkspecial(pfn_pte(my_zero_pfn(address),
<span class="p_header">--- a/mm/mmap.c</span>
<span class="p_header">+++ b/mm/mmap.c</span>
<span class="p_chunk">@@ -245,6 +245,7 @@</span> <span class="p_context"> SYSCALL_DEFINE1(brk, unsigned long, brk)</span>
 	unsigned long rlim, retval;
 	unsigned long newbrk, oldbrk;
 	struct mm_struct *mm = current-&gt;mm;
<span class="p_add">+	struct vm_area_struct *next;</span>
 	unsigned long min_brk;
 
 	down_write(&amp;mm-&gt;mmap_sem);
<span class="p_chunk">@@ -289,7 +290,8 @@</span> <span class="p_context"> SYSCALL_DEFINE1(brk, unsigned long, brk)</span>
 	}
 
 	/* Check against existing mmap mappings. */
<span class="p_del">-	if (find_vma_intersection(mm, oldbrk, newbrk+PAGE_SIZE))</span>
<span class="p_add">+	next = find_vma(mm, oldbrk);</span>
<span class="p_add">+	if (next &amp;&amp; newbrk + PAGE_SIZE &gt; vm_start_gap(next))</span>
 		goto out;
 
 	/* Ok, looks good - let it rip. */
<span class="p_chunk">@@ -1368,8 +1370,8 @@</span> <span class="p_context"> arch_get_unmapped_area(struct file *filp</span>
 		unsigned long len, unsigned long pgoff, unsigned long flags)
 {
 	struct mm_struct *mm = current-&gt;mm;
<span class="p_del">-	struct vm_area_struct *vma;</span>
<span class="p_del">-	unsigned long start_addr;</span>
<span class="p_add">+	struct vm_area_struct *vma, *prev;</span>
<span class="p_add">+	unsigned long start_addr, vm_start, prev_end;</span>
 
 	if (len &gt; TASK_SIZE - mmap_min_addr)
 		return -ENOMEM;
<span class="p_chunk">@@ -1379,9 +1381,10 @@</span> <span class="p_context"> arch_get_unmapped_area(struct file *filp</span>
 
 	if (addr) {
 		addr = PAGE_ALIGN(addr);
<span class="p_del">-		vma = find_vma(mm, addr);</span>
<span class="p_add">+		vma = find_vma_prev(mm, addr, &amp;prev);</span>
 		if (TASK_SIZE - len &gt;= addr &amp;&amp; addr &gt;= mmap_min_addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)) &amp;&amp;</span>
<span class="p_add">+		    (!prev || addr &gt;= vm_end_gap(prev)))</span>
 			return addr;
 	}
 	if (len &gt; mm-&gt;cached_hole_size) {
<span class="p_chunk">@@ -1392,7 +1395,17 @@</span> <span class="p_context"> arch_get_unmapped_area(struct file *filp</span>
 	}
 
 full_search:
<span class="p_del">-	for (vma = find_vma(mm, addr); ; vma = vma-&gt;vm_next) {</span>
<span class="p_add">+	for (vma = find_vma_prev(mm, addr, &amp;prev); ; prev = vma,</span>
<span class="p_add">+						vma = vma-&gt;vm_next) {</span>
<span class="p_add">+		if (prev) {</span>
<span class="p_add">+			prev_end = vm_end_gap(prev);</span>
<span class="p_add">+			if (addr &lt; prev_end) {</span>
<span class="p_add">+				addr = prev_end;</span>
<span class="p_add">+				/* If vma already violates gap, forget it */</span>
<span class="p_add">+				if (vma &amp;&amp; addr &gt; vma-&gt;vm_start)</span>
<span class="p_add">+					addr = vma-&gt;vm_start;</span>
<span class="p_add">+			}</span>
<span class="p_add">+		}</span>
 		/* At this point:  (!vma || addr &lt; vma-&gt;vm_end). */
 		if (TASK_SIZE - len &lt; addr) {
 			/*
<span class="p_chunk">@@ -1407,16 +1420,16 @@</span> <span class="p_context"> full_search:</span>
 			}
 			return -ENOMEM;
 		}
<span class="p_del">-		if (!vma || addr + len &lt;= vma-&gt;vm_start) {</span>
<span class="p_add">+		vm_start = vma ? vm_start_gap(vma) : TASK_SIZE;</span>
<span class="p_add">+		if (addr + len &lt;= vm_start) {</span>
 			/*
 			 * Remember the place where we stopped the search:
 			 */
 			mm-&gt;free_area_cache = addr + len;
 			return addr;
 		}
<span class="p_del">-		if (addr + mm-&gt;cached_hole_size &lt; vma-&gt;vm_start)</span>
<span class="p_del">-		        mm-&gt;cached_hole_size = vma-&gt;vm_start - addr;</span>
<span class="p_del">-		addr = vma-&gt;vm_end;</span>
<span class="p_add">+		if (addr + mm-&gt;cached_hole_size &lt; vm_start)</span>
<span class="p_add">+			mm-&gt;cached_hole_size = vm_start - addr;</span>
 	}
 }
 #endif	
<span class="p_chunk">@@ -1442,9 +1455,10 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct fi</span>
 			  const unsigned long len, const unsigned long pgoff,
 			  const unsigned long flags)
 {
<span class="p_del">-	struct vm_area_struct *vma;</span>
<span class="p_add">+	struct vm_area_struct *vma, *prev;</span>
 	struct mm_struct *mm = current-&gt;mm;
 	unsigned long addr = addr0;
<span class="p_add">+	unsigned long vm_start, prev_end;</span>
 	unsigned long low_limit = max(PAGE_SIZE, mmap_min_addr);
 
 	/* requested length too big for entire address space */
<span class="p_chunk">@@ -1457,9 +1471,10 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct fi</span>
 	/* requesting a specific address */
 	if (addr) {
 		addr = PAGE_ALIGN(addr);
<span class="p_del">-		vma = find_vma(mm, addr);</span>
<span class="p_add">+		vma = find_vma_prev(mm, addr, &amp;prev);</span>
 		if (TASK_SIZE - len &gt;= addr &amp;&amp; addr &gt;= mmap_min_addr &amp;&amp;
<span class="p_del">-				(!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+				(!vma || addr + len &lt;= vm_start_gap(vma)) &amp;&amp;</span>
<span class="p_add">+				(!prev || addr &gt;= vm_end_gap(prev)))</span>
 			return addr;
 	}
 
<span class="p_chunk">@@ -1474,8 +1489,9 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct fi</span>
 
 	/* make sure it can fit in the remaining address space */
 	if (addr &gt;= low_limit + len) {
<span class="p_del">-		vma = find_vma(mm, addr-len);</span>
<span class="p_del">-		if (!vma || addr &lt;= vma-&gt;vm_start)</span>
<span class="p_add">+		vma = find_vma_prev(mm, addr-len, &amp;prev);</span>
<span class="p_add">+		if ((!vma || addr &lt;= vm_start_gap(vma)) &amp;&amp;</span>
<span class="p_add">+		    (!prev || addr-len &gt;= vm_end_gap(prev)))</span>
 			/* remember the address as a hint for next time */
 			return (mm-&gt;free_area_cache = addr-len);
 	}
<span class="p_chunk">@@ -1491,18 +1507,21 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct fi</span>
 		 * else if new region fits below vma-&gt;vm_start,
 		 * return with success:
 		 */
<span class="p_del">-		vma = find_vma(mm, addr);</span>
<span class="p_del">-		if (!vma || addr+len &lt;= vma-&gt;vm_start)</span>
<span class="p_add">+		vma = find_vma_prev(mm, addr, &amp;prev);</span>
<span class="p_add">+		vm_start = vma ? vm_start_gap(vma) : mm-&gt;mmap_base;</span>
<span class="p_add">+		prev_end = prev ? vm_end_gap(prev) : low_limit;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (addr + len &lt;= vm_start &amp;&amp; addr &gt;= prev_end)</span>
 			/* remember the address as a hint for next time */
 			return (mm-&gt;free_area_cache = addr);
 
  		/* remember the largest hole we saw so far */
<span class="p_del">- 		if (addr + mm-&gt;cached_hole_size &lt; vma-&gt;vm_start)</span>
<span class="p_del">- 		        mm-&gt;cached_hole_size = vma-&gt;vm_start - addr;</span>
<span class="p_add">+		if (addr + mm-&gt;cached_hole_size &lt; vm_start)</span>
<span class="p_add">+			mm-&gt;cached_hole_size = vm_start - addr;</span>
 
 		/* try just below the current vma-&gt;vm_start */
<span class="p_del">-		addr = vma-&gt;vm_start-len;</span>
<span class="p_del">-	} while (vma-&gt;vm_start &gt;= low_limit + len);</span>
<span class="p_add">+		addr = vm_start - len;</span>
<span class="p_add">+	} while (vm_start &gt;= low_limit + len);</span>
 
 bottomup:
 	/*
<span class="p_chunk">@@ -1647,21 +1666,19 @@</span> <span class="p_context"> out:</span>
  * update accounting. This is shared with both the
  * grow-up and grow-down cases.
  */
<span class="p_del">-static int acct_stack_growth(struct vm_area_struct *vma, unsigned long size, unsigned long grow)</span>
<span class="p_add">+static int acct_stack_growth(struct vm_area_struct *vma,</span>
<span class="p_add">+			     unsigned long size, unsigned long grow)</span>
 {
 	struct mm_struct *mm = vma-&gt;vm_mm;
 	struct rlimit *rlim = current-&gt;signal-&gt;rlim;
<span class="p_del">-	unsigned long new_start, actual_size;</span>
<span class="p_add">+	unsigned long new_start;</span>
 
 	/* address space limit tests */
 	if (!may_expand_vm(mm, grow))
 		return -ENOMEM;
 
 	/* Stack limit test */
<span class="p_del">-	actual_size = size;</span>
<span class="p_del">-	if (size &amp;&amp; (vma-&gt;vm_flags &amp; (VM_GROWSUP | VM_GROWSDOWN)))</span>
<span class="p_del">-		actual_size -= PAGE_SIZE;</span>
<span class="p_del">-	if (actual_size &gt; ACCESS_ONCE(rlim[RLIMIT_STACK].rlim_cur))</span>
<span class="p_add">+	if (size &gt; ACCESS_ONCE(rlim[RLIMIT_STACK].rlim_cur))</span>
 		return -ENOMEM;
 
 	/* mlock limit tests */
<span class="p_chunk">@@ -1703,32 +1720,40 @@</span> <span class="p_context"> static int acct_stack_growth(struct vm_a</span>
  */
 int expand_upwards(struct vm_area_struct *vma, unsigned long address)
 {
<span class="p_del">-	int error;</span>
<span class="p_add">+	struct vm_area_struct *next;</span>
<span class="p_add">+	unsigned long gap_addr;</span>
<span class="p_add">+	int error = 0;</span>
 
 	if (!(vma-&gt;vm_flags &amp; VM_GROWSUP))
 		return -EFAULT;
 
<span class="p_del">-	/*</span>
<span class="p_del">-	 * We must make sure the anon_vma is allocated</span>
<span class="p_del">-	 * so that the anon_vma locking is not a noop.</span>
<span class="p_del">-	 */</span>
<span class="p_add">+	/* Guard against wrapping around to address 0. */</span>
<span class="p_add">+	address &amp;= PAGE_MASK;</span>
<span class="p_add">+	address += PAGE_SIZE;</span>
<span class="p_add">+	if (!address)</span>
<span class="p_add">+		return -ENOMEM;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Enforce stack_guard_gap */</span>
<span class="p_add">+	gap_addr = address + stack_guard_gap;</span>
<span class="p_add">+	if (gap_addr &lt; address)</span>
<span class="p_add">+		return -ENOMEM;</span>
<span class="p_add">+	next = vma-&gt;vm_next;</span>
<span class="p_add">+	if (next &amp;&amp; next-&gt;vm_start &lt; gap_addr) {</span>
<span class="p_add">+		if (!(next-&gt;vm_flags &amp; VM_GROWSUP))</span>
<span class="p_add">+			return -ENOMEM;</span>
<span class="p_add">+		/* Check that both stack segments have the same anon_vma? */</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/* We must make sure the anon_vma is allocated. */</span>
 	if (unlikely(anon_vma_prepare(vma)))
 		return -ENOMEM;
<span class="p_del">-	vma_lock_anon_vma(vma);</span>
 
 	/*
 	 * vma-&gt;vm_start/vm_end cannot change under us because the caller
 	 * is required to hold the mmap_sem in read mode.  We need the
 	 * anon_vma lock to serialize against concurrent expand_stacks.
<span class="p_del">-	 * Also guard against wrapping around to address 0.</span>
 	 */
<span class="p_del">-	if (address &lt; PAGE_ALIGN(address+4))</span>
<span class="p_del">-		address = PAGE_ALIGN(address+4);</span>
<span class="p_del">-	else {</span>
<span class="p_del">-		vma_unlock_anon_vma(vma);</span>
<span class="p_del">-		return -ENOMEM;</span>
<span class="p_del">-	}</span>
<span class="p_del">-	error = 0;</span>
<span class="p_add">+	vma_lock_anon_vma(vma);</span>
 
 	/* Somebody else might have raced and expanded it already */
 	if (address &gt; vma-&gt;vm_end) {
<span class="p_chunk">@@ -1758,27 +1783,36 @@</span> <span class="p_context"> int expand_upwards(struct vm_area_struct</span>
 int expand_downwards(struct vm_area_struct *vma,
 				   unsigned long address)
 {
<span class="p_add">+	struct vm_area_struct *prev;</span>
<span class="p_add">+	unsigned long gap_addr;</span>
 	int error;
 
<span class="p_del">-	/*</span>
<span class="p_del">-	 * We must make sure the anon_vma is allocated</span>
<span class="p_del">-	 * so that the anon_vma locking is not a noop.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	if (unlikely(anon_vma_prepare(vma)))</span>
<span class="p_del">-		return -ENOMEM;</span>
<span class="p_del">-</span>
 	address &amp;= PAGE_MASK;
 	error = security_file_mmap(NULL, 0, 0, 0, address, 1);
 	if (error)
 		return error;
 
<span class="p_del">-	vma_lock_anon_vma(vma);</span>
<span class="p_add">+	/* Enforce stack_guard_gap */</span>
<span class="p_add">+	gap_addr = address - stack_guard_gap;</span>
<span class="p_add">+	if (gap_addr &gt; address)</span>
<span class="p_add">+		return -ENOMEM;</span>
<span class="p_add">+	prev = vma-&gt;vm_prev;</span>
<span class="p_add">+	if (prev &amp;&amp; prev-&gt;vm_end &gt; gap_addr) {</span>
<span class="p_add">+		if (!(prev-&gt;vm_flags &amp; VM_GROWSDOWN))</span>
<span class="p_add">+			return -ENOMEM;</span>
<span class="p_add">+		/* Check that both stack segments have the same anon_vma? */</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/* We must make sure the anon_vma is allocated. */</span>
<span class="p_add">+	if (unlikely(anon_vma_prepare(vma)))</span>
<span class="p_add">+		return -ENOMEM;</span>
 
 	/*
 	 * vma-&gt;vm_start/vm_end cannot change under us because the caller
 	 * is required to hold the mmap_sem in read mode.  We need the
 	 * anon_vma lock to serialize against concurrent expand_stacks.
 	 */
<span class="p_add">+	vma_lock_anon_vma(vma);</span>
 
 	/* Somebody else might have raced and expanded it already */
 	if (address &lt; vma-&gt;vm_start) {
<span class="p_chunk">@@ -1802,28 +1836,25 @@</span> <span class="p_context"> int expand_downwards(struct vm_area_stru</span>
 	return error;
 }
 
<span class="p_del">-/*</span>
<span class="p_del">- * Note how expand_stack() refuses to expand the stack all the way to</span>
<span class="p_del">- * abut the next virtual mapping, *unless* that mapping itself is also</span>
<span class="p_del">- * a stack mapping. We want to leave room for a guard page, after all</span>
<span class="p_del">- * (the guard page itself is not added here, that is done by the</span>
<span class="p_del">- * actual page faulting logic)</span>
<span class="p_del">- *</span>
<span class="p_del">- * This matches the behavior of the guard page logic (see mm/memory.c:</span>
<span class="p_del">- * check_stack_guard_page()), which only allows the guard page to be</span>
<span class="p_del">- * removed under these circumstances.</span>
<span class="p_del">- */</span>
<span class="p_add">+/* enforced gap between the expanding stack and other mappings. */</span>
<span class="p_add">+unsigned long stack_guard_gap = 256UL&lt;&lt;PAGE_SHIFT;</span>
<span class="p_add">+</span>
<span class="p_add">+static int __init cmdline_parse_stack_guard_gap(char *p)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long val;</span>
<span class="p_add">+	char *endptr;</span>
<span class="p_add">+</span>
<span class="p_add">+	val = simple_strtoul(p, &amp;endptr, 10);</span>
<span class="p_add">+	if (!*endptr)</span>
<span class="p_add">+		stack_guard_gap = val &lt;&lt; PAGE_SHIFT;</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+__setup(&quot;stack_guard_gap=&quot;, cmdline_parse_stack_guard_gap);</span>
<span class="p_add">+</span>
 #ifdef CONFIG_STACK_GROWSUP
 int expand_stack(struct vm_area_struct *vma, unsigned long address)
 {
<span class="p_del">-	struct vm_area_struct *next;</span>
<span class="p_del">-</span>
<span class="p_del">-	address &amp;= PAGE_MASK;</span>
<span class="p_del">-	next = vma-&gt;vm_next;</span>
<span class="p_del">-	if (next &amp;&amp; next-&gt;vm_start == address + PAGE_SIZE) {</span>
<span class="p_del">-		if (!(next-&gt;vm_flags &amp; VM_GROWSUP))</span>
<span class="p_del">-			return -ENOMEM;</span>
<span class="p_del">-	}</span>
 	return expand_upwards(vma, address);
 }
 
<span class="p_chunk">@@ -1846,14 +1877,6 @@</span> <span class="p_context"> find_extend_vma(struct mm_struct *mm, un</span>
 #else
 int expand_stack(struct vm_area_struct *vma, unsigned long address)
 {
<span class="p_del">-	struct vm_area_struct *prev;</span>
<span class="p_del">-</span>
<span class="p_del">-	address &amp;= PAGE_MASK;</span>
<span class="p_del">-	prev = vma-&gt;vm_prev;</span>
<span class="p_del">-	if (prev &amp;&amp; prev-&gt;vm_end == address) {</span>
<span class="p_del">-		if (!(prev-&gt;vm_flags &amp; VM_GROWSDOWN))</span>
<span class="p_del">-			return -ENOMEM;</span>
<span class="p_del">-	}</span>
 	return expand_downwards(vma, address);
 }
 

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



