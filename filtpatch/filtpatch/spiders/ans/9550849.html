
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v6] sparc64: Multi-page size support - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v6] sparc64: Multi-page size support</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=145751">Nitin Gupta</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Feb. 2, 2017, 12:16 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1485994656-115830-1-git-send-email-nitin.m.gupta@oracle.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9550849/mbox/"
   >mbox</a>
|
   <a href="/patch/9550849/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9550849/">/patch/9550849/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	7C75760415 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  2 Feb 2017 00:18:11 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 5EEE2283E8
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  2 Feb 2017 00:18:11 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 533A528456; Thu,  2 Feb 2017 00:18:11 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.4 required=2.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	RCVD_IN_SORBS_SPAM,
	UNPARSEABLE_RELAY autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 0B21E283E8
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  2 Feb 2017 00:18:10 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1751229AbdBBASI (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Wed, 1 Feb 2017 19:18:08 -0500
Received: from aserp1040.oracle.com ([141.146.126.69]:35705 &quot;EHLO
	aserp1040.oracle.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1750925AbdBBASG (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Wed, 1 Feb 2017 19:18:06 -0500
Received: from userv0021.oracle.com (userv0021.oracle.com [156.151.31.71])
	by aserp1040.oracle.com (Sentrion-MTA-4.3.2/Sentrion-MTA-4.3.2) with
	ESMTP id v120Hksw013127
	(version=TLSv1.2 cipher=ECDHE-RSA-AES256-GCM-SHA384 bits=256
	verify=OK); Thu, 2 Feb 2017 00:17:47 GMT
Received: from userv0122.oracle.com (userv0122.oracle.com [156.151.31.75])
	by userv0021.oracle.com (8.14.4/8.14.4) with ESMTP id v120Hkmo022522
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-GCM-SHA384 bits=256
	verify=OK); Thu, 2 Feb 2017 00:17:46 GMT
Received: from abhmp0005.oracle.com (abhmp0005.oracle.com [141.146.116.11])
	by userv0122.oracle.com (8.14.4/8.14.4) with ESMTP id
	v120HjoP000760; Thu, 2 Feb 2017 00:17:45 GMT
Received: from ca-qasparc20.us.oracle.com (/10.147.24.73)
	by default (Oracle Beehive Gateway v4.0)
	with ESMTP ; Wed, 01 Feb 2017 16:17:44 -0800
From: Nitin Gupta &lt;nitin.m.gupta@oracle.com&gt;
To: &quot;David S. Miller&quot; &lt;davem@davemloft.net&gt;
Cc: Nitin Gupta &lt;nitin.m.gupta@oracle.com&gt;,
	&quot;David S. Miller&quot; &lt;davem@davemloft.net&gt;,
	Mike Kravetz &lt;mike.kravetz@oracle.com&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	Julian Calaby &lt;julian.calaby@gmail.com&gt;, Hugh Dickins &lt;hughd@google.com&gt;,
	Adam Buchbinder &lt;adam.buchbinder@gmail.com&gt;,
	&quot;Kirill A. Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt;,
	Stephen Rothwell &lt;sfr@canb.auug.org.au&gt;,
	Paul Gortmaker &lt;paul.gortmaker@windriver.com&gt;,
	Thomas Tai &lt;thomas.tai@oracle.com&gt;, Chris Hyser &lt;chris.hyser@oracle.com&gt;,
	Khalid Aziz &lt;khalid.aziz@oracle.com&gt;,
	Atish Patra &lt;atish.patra@oracle.com&gt;,
	Michal Hocko &lt;mhocko@suse.com&gt;, sparclinux@vger.kernel.org,
	linux-kernel@vger.kernel.org
Subject: [PATCH v6] sparc64: Multi-page size support
Date: Wed,  1 Feb 2017 16:16:36 -0800
Message-Id: &lt;1485994656-115830-1-git-send-email-nitin.m.gupta@oracle.com&gt;
X-Mailer: git-send-email 1.7.1
X-Source-IP: userv0021.oracle.com [156.151.31.71]
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=145751">Nitin Gupta</a> - Feb. 2, 2017, 12:16 a.m.</div>
<pre class="content">
Add support for using multiple hugepage sizes simultaneously
on mainline. Currently, support for 256M has been added which
can be used along with 8M pages.

Page tables are set like this (e.g. for 256M page):
    VA + (8M * x) -&gt; PA + (8M * x) (sz bit = 256M) where x in [0, 31]

and TSB is set similarly:
    VA + (4M * x) -&gt; PA + (4M * x) (sz bit = 256M) where x in [0, 63]

- Testing

Tested on Sonoma (which supports 256M pages) by running stream
benchmark instances in parallel: one instance uses 8M pages and
another uses 256M pages, consuming 48G each.

Boot params used:

default_hugepagesz=256M hugepagesz=256M hugepages=300 hugepagesz=8M
hugepages=10000
<span class="signed-off-by">
Signed-off-by: Nitin Gupta &lt;nitin.m.gupta@oracle.com&gt;</span>
---
Changelog v6 vs v5:
 - Fix _flush_huge_tsb_one_entry: add correct offset to base vaddr
Changelog v4 vs v5:
 - Enable hugepage initialization on sun4u
Changelog v3 vs v4:
 - Remove incorrect WARN_ON in __flush_huge_tsb_one_entry()

Changelog v2 vs v3:
 - Remove unused label in tsb.S (David)
 - Order local variables from longest to shortest line (David)

Changelog v1 vs v2:
 - Fix warning due to unused __flush_huge_tsb_one() when
   CONFIG_HUGETLB is not defined.
---
 arch/sparc/include/asm/page_64.h     |   3 +-
 arch/sparc/include/asm/pgtable_64.h  |  23 +++--
 arch/sparc/include/asm/tlbflush_64.h |   5 +-
 arch/sparc/kernel/tsb.S              |  21 +----
 arch/sparc/mm/hugetlbpage.c          | 160 +++++++++++++++++++++++++++++++----
 arch/sparc/mm/init_64.c              |  42 ++++++++-
 arch/sparc/mm/tlb.c                  |  17 ++--
 arch/sparc/mm/tsb.c                  |  44 ++++++++--
 8 files changed, 253 insertions(+), 62 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=129">David Miller</a> - Feb. 23, 2017, 4:32 p.m.</div>
<pre class="content">
<span class="from">From: Nitin Gupta &lt;nitin.m.gupta@oracle.com&gt;</span>
Date: Wed,  1 Feb 2017 16:16:36 -0800
<span class="quote">
&gt; Add support for using multiple hugepage sizes simultaneously</span>
<span class="quote">&gt; on mainline. Currently, support for 256M has been added which</span>
<span class="quote">&gt; can be used along with 8M pages.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Page tables are set like this (e.g. for 256M page):</span>
<span class="quote">&gt;     VA + (8M * x) -&gt; PA + (8M * x) (sz bit = 256M) where x in [0, 31]</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; and TSB is set similarly:</span>
<span class="quote">&gt;     VA + (4M * x) -&gt; PA + (4M * x) (sz bit = 256M) where x in [0, 63]</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; - Testing</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Tested on Sonoma (which supports 256M pages) by running stream</span>
<span class="quote">&gt; benchmark instances in parallel: one instance uses 8M pages and</span>
<span class="quote">&gt; another uses 256M pages, consuming 48G each.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Boot params used:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; default_hugepagesz=256M hugepagesz=256M hugepages=300 hugepagesz=8M</span>
<span class="quote">&gt; hugepages=10000</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Nitin Gupta &lt;nitin.m.gupta@oracle.com&gt;</span>

Applied, thanks.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/sparc/include/asm/page_64.h b/arch/sparc/include/asm/page_64.h</span>
<span class="p_header">index c1263fc..d76f38d 100644</span>
<span class="p_header">--- a/arch/sparc/include/asm/page_64.h</span>
<span class="p_header">+++ b/arch/sparc/include/asm/page_64.h</span>
<span class="p_chunk">@@ -17,7 +17,7 @@</span> <span class="p_context"></span>
 
 #define HPAGE_SHIFT		23
 #define REAL_HPAGE_SHIFT	22
<span class="p_del">-</span>
<span class="p_add">+#define HPAGE_256MB_SHIFT	28</span>
 #define REAL_HPAGE_SIZE		(_AC(1,UL) &lt;&lt; REAL_HPAGE_SHIFT)
 
 #if defined(CONFIG_HUGETLB_PAGE) || defined(CONFIG_TRANSPARENT_HUGEPAGE)
<span class="p_chunk">@@ -26,6 +26,7 @@</span> <span class="p_context"></span>
 #define HUGETLB_PAGE_ORDER	(HPAGE_SHIFT - PAGE_SHIFT)
 #define HAVE_ARCH_HUGETLB_UNMAPPED_AREA
 #define REAL_HPAGE_PER_HPAGE	(_AC(1,UL) &lt;&lt; (HPAGE_SHIFT - REAL_HPAGE_SHIFT))
<span class="p_add">+#define HUGE_MAX_HSTATE		2</span>
 #endif
 
 #ifndef __ASSEMBLY__
<span class="p_header">diff --git a/arch/sparc/include/asm/pgtable_64.h b/arch/sparc/include/asm/pgtable_64.h</span>
<span class="p_header">index 314b668..7932a4a 100644</span>
<span class="p_header">--- a/arch/sparc/include/asm/pgtable_64.h</span>
<span class="p_header">+++ b/arch/sparc/include/asm/pgtable_64.h</span>
<span class="p_chunk">@@ -375,7 +375,10 @@</span> <span class="p_context"> static inline pgprot_t pgprot_noncached(pgprot_t prot)</span>
 #define pgprot_noncached pgprot_noncached
 
 #if defined(CONFIG_HUGETLB_PAGE) || defined(CONFIG_TRANSPARENT_HUGEPAGE)
<span class="p_del">-static inline unsigned long __pte_huge_mask(void)</span>
<span class="p_add">+extern pte_t arch_make_huge_pte(pte_t entry, struct vm_area_struct *vma,</span>
<span class="p_add">+				struct page *page, int writable);</span>
<span class="p_add">+#define arch_make_huge_pte arch_make_huge_pte</span>
<span class="p_add">+static inline unsigned long __pte_default_huge_mask(void)</span>
 {
 	unsigned long mask;
 
<span class="p_chunk">@@ -395,12 +398,14 @@</span> <span class="p_context"> static inline unsigned long __pte_huge_mask(void)</span>
 
 static inline pte_t pte_mkhuge(pte_t pte)
 {
<span class="p_del">-	return __pte(pte_val(pte) | _PAGE_PMD_HUGE | __pte_huge_mask());</span>
<span class="p_add">+	return __pte(pte_val(pte) | __pte_default_huge_mask());</span>
 }
 
<span class="p_del">-static inline bool is_hugetlb_pte(pte_t pte)</span>
<span class="p_add">+static inline bool is_default_hugetlb_pte(pte_t pte)</span>
 {
<span class="p_del">-	return !!(pte_val(pte) &amp; __pte_huge_mask());</span>
<span class="p_add">+	unsigned long mask = __pte_default_huge_mask();</span>
<span class="p_add">+</span>
<span class="p_add">+	return (pte_val(pte) &amp; mask) == mask;</span>
 }
 
 static inline bool is_hugetlb_pmd(pmd_t pmd)
<span class="p_chunk">@@ -875,10 +880,12 @@</span> <span class="p_context"> static inline unsigned long pud_pfn(pud_t pud)</span>
 
 /* Actual page table PTE updates.  */
 void tlb_batch_add(struct mm_struct *mm, unsigned long vaddr,
<span class="p_del">-		   pte_t *ptep, pte_t orig, int fullmm);</span>
<span class="p_add">+		   pte_t *ptep, pte_t orig, int fullmm,</span>
<span class="p_add">+		   unsigned int hugepage_shift);</span>
 
 static void maybe_tlb_batch_add(struct mm_struct *mm, unsigned long vaddr,
<span class="p_del">-				pte_t *ptep, pte_t orig, int fullmm)</span>
<span class="p_add">+				pte_t *ptep, pte_t orig, int fullmm,</span>
<span class="p_add">+				unsigned int hugepage_shift)</span>
 {
 	/* It is more efficient to let flush_tlb_kernel_range()
 	 * handle init_mm tlb flushes.
<span class="p_chunk">@@ -887,7 +894,7 @@</span> <span class="p_context"> static void maybe_tlb_batch_add(struct mm_struct *mm, unsigned long vaddr,</span>
 	 *             and SUN4V pte layout, so this inline test is fine.
 	 */
 	if (likely(mm != &amp;init_mm) &amp;&amp; pte_accessible(mm, orig))
<span class="p_del">-		tlb_batch_add(mm, vaddr, ptep, orig, fullmm);</span>
<span class="p_add">+		tlb_batch_add(mm, vaddr, ptep, orig, fullmm, hugepage_shift);</span>
 }
 
 #define __HAVE_ARCH_PMDP_HUGE_GET_AND_CLEAR
<span class="p_chunk">@@ -906,7 +913,7 @@</span> <span class="p_context"> static inline void __set_pte_at(struct mm_struct *mm, unsigned long addr,</span>
 	pte_t orig = *ptep;
 
 	*ptep = pte;
<span class="p_del">-	maybe_tlb_batch_add(mm, addr, ptep, orig, fullmm);</span>
<span class="p_add">+	maybe_tlb_batch_add(mm, addr, ptep, orig, fullmm, PAGE_SHIFT);</span>
 }
 
 #define set_pte_at(mm,addr,ptep,pte)	\
<span class="p_header">diff --git a/arch/sparc/include/asm/tlbflush_64.h b/arch/sparc/include/asm/tlbflush_64.h</span>
<span class="p_header">index a8e192e..54be88a 100644</span>
<span class="p_header">--- a/arch/sparc/include/asm/tlbflush_64.h</span>
<span class="p_header">+++ b/arch/sparc/include/asm/tlbflush_64.h</span>
<span class="p_chunk">@@ -8,7 +8,7 @@</span> <span class="p_context"></span>
 #define TLB_BATCH_NR	192
 
 struct tlb_batch {
<span class="p_del">-	bool huge;</span>
<span class="p_add">+	unsigned int hugepage_shift;</span>
 	struct mm_struct *mm;
 	unsigned long tlb_nr;
 	unsigned long active;
<span class="p_chunk">@@ -17,7 +17,8 @@</span> <span class="p_context"> struct tlb_batch {</span>
 
 void flush_tsb_kernel_range(unsigned long start, unsigned long end);
 void flush_tsb_user(struct tlb_batch *tb);
<span class="p_del">-void flush_tsb_user_page(struct mm_struct *mm, unsigned long vaddr, bool huge);</span>
<span class="p_add">+void flush_tsb_user_page(struct mm_struct *mm, unsigned long vaddr,</span>
<span class="p_add">+			 unsigned int hugepage_shift);</span>
 
 /* TLB flush operations. */
 
<span class="p_header">diff --git a/arch/sparc/kernel/tsb.S b/arch/sparc/kernel/tsb.S</span>
<span class="p_header">index d568c82..10689cf 100644</span>
<span class="p_header">--- a/arch/sparc/kernel/tsb.S</span>
<span class="p_header">+++ b/arch/sparc/kernel/tsb.S</span>
<span class="p_chunk">@@ -117,26 +117,11 @@</span> <span class="p_context"> tsb_miss_page_table_walk_sun4v_fastpath:</span>
 	/* Valid PTE is now in %g5.  */
 
 #if defined(CONFIG_HUGETLB_PAGE) || defined(CONFIG_TRANSPARENT_HUGEPAGE)
<span class="p_del">-661:	sethi		%uhi(_PAGE_SZALL_4U), %g7</span>
<span class="p_add">+	sethi		%uhi(_PAGE_PMD_HUGE), %g7</span>
 	sllx		%g7, 32, %g7
<span class="p_del">-	.section	.sun4v_2insn_patch, &quot;ax&quot;</span>
<span class="p_del">-	.word		661b</span>
<span class="p_del">-	mov		_PAGE_SZALL_4V, %g7</span>
<span class="p_del">-	nop</span>
<span class="p_del">-	.previous</span>
<span class="p_del">-</span>
<span class="p_del">-	and		%g5, %g7, %g2</span>
<span class="p_del">-</span>
<span class="p_del">-661:	sethi		%uhi(_PAGE_SZHUGE_4U), %g7</span>
<span class="p_del">-	sllx		%g7, 32, %g7</span>
<span class="p_del">-	.section	.sun4v_2insn_patch, &quot;ax&quot;</span>
<span class="p_del">-	.word		661b</span>
<span class="p_del">-	mov		_PAGE_SZHUGE_4V, %g7</span>
<span class="p_del">-	nop</span>
<span class="p_del">-	.previous</span>
 
<span class="p_del">-	cmp		%g2, %g7</span>
<span class="p_del">-	bne,pt		%xcc, 60f</span>
<span class="p_add">+	andcc		%g5, %g7, %g0</span>
<span class="p_add">+	be,pt		%xcc, 60f</span>
 	 nop
 
 	/* It is a huge page, use huge page TSB entry address we
<span class="p_header">diff --git a/arch/sparc/mm/hugetlbpage.c b/arch/sparc/mm/hugetlbpage.c</span>
<span class="p_header">index 988acc8..618a568 100644</span>
<span class="p_header">--- a/arch/sparc/mm/hugetlbpage.c</span>
<span class="p_header">+++ b/arch/sparc/mm/hugetlbpage.c</span>
<span class="p_chunk">@@ -28,6 +28,7 @@</span> <span class="p_context"> static unsigned long hugetlb_get_unmapped_area_bottomup(struct file *filp,</span>
 							unsigned long pgoff,
 							unsigned long flags)
 {
<span class="p_add">+	struct hstate *h = hstate_file(filp);</span>
 	unsigned long task_size = TASK_SIZE;
 	struct vm_unmapped_area_info info;
 
<span class="p_chunk">@@ -38,7 +39,7 @@</span> <span class="p_context"> static unsigned long hugetlb_get_unmapped_area_bottomup(struct file *filp,</span>
 	info.length = len;
 	info.low_limit = TASK_UNMAPPED_BASE;
 	info.high_limit = min(task_size, VA_EXCLUDE_START);
<span class="p_del">-	info.align_mask = PAGE_MASK &amp; ~HPAGE_MASK;</span>
<span class="p_add">+	info.align_mask = PAGE_MASK &amp; ~huge_page_mask(h);</span>
 	info.align_offset = 0;
 	addr = vm_unmapped_area(&amp;info);
 
<span class="p_chunk">@@ -58,6 +59,7 @@</span> <span class="p_context"> hugetlb_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
 				  const unsigned long pgoff,
 				  const unsigned long flags)
 {
<span class="p_add">+	struct hstate *h = hstate_file(filp);</span>
 	struct mm_struct *mm = current-&gt;mm;
 	unsigned long addr = addr0;
 	struct vm_unmapped_area_info info;
<span class="p_chunk">@@ -69,7 +71,7 @@</span> <span class="p_context"> hugetlb_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
 	info.length = len;
 	info.low_limit = PAGE_SIZE;
 	info.high_limit = mm-&gt;mmap_base;
<span class="p_del">-	info.align_mask = PAGE_MASK &amp; ~HPAGE_MASK;</span>
<span class="p_add">+	info.align_mask = PAGE_MASK &amp; ~huge_page_mask(h);</span>
 	info.align_offset = 0;
 	addr = vm_unmapped_area(&amp;info);
 
<span class="p_chunk">@@ -94,6 +96,7 @@</span> <span class="p_context"> unsigned long</span>
 hugetlb_get_unmapped_area(struct file *file, unsigned long addr,
 		unsigned long len, unsigned long pgoff, unsigned long flags)
 {
<span class="p_add">+	struct hstate *h = hstate_file(file);</span>
 	struct mm_struct *mm = current-&gt;mm;
 	struct vm_area_struct *vma;
 	unsigned long task_size = TASK_SIZE;
<span class="p_chunk">@@ -101,7 +104,7 @@</span> <span class="p_context"> hugetlb_get_unmapped_area(struct file *file, unsigned long addr,</span>
 	if (test_thread_flag(TIF_32BIT))
 		task_size = STACK_TOP32;
 
<span class="p_del">-	if (len &amp; ~HPAGE_MASK)</span>
<span class="p_add">+	if (len &amp; ~huge_page_mask(h))</span>
 		return -EINVAL;
 	if (len &gt; task_size)
 		return -ENOMEM;
<span class="p_chunk">@@ -113,7 +116,7 @@</span> <span class="p_context"> hugetlb_get_unmapped_area(struct file *file, unsigned long addr,</span>
 	}
 
 	if (addr) {
<span class="p_del">-		addr = ALIGN(addr, HPAGE_SIZE);</span>
<span class="p_add">+		addr = ALIGN(addr, huge_page_size(h));</span>
 		vma = find_vma(mm, addr);
 		if (task_size - len &gt;= addr &amp;&amp;
 		    (!vma || addr + len &lt;= vma-&gt;vm_start))
<span class="p_chunk">@@ -127,6 +130,112 @@</span> <span class="p_context"> hugetlb_get_unmapped_area(struct file *file, unsigned long addr,</span>
 				pgoff, flags);
 }
 
<span class="p_add">+static pte_t sun4u_hugepage_shift_to_tte(pte_t entry, unsigned int shift)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return entry;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static pte_t sun4v_hugepage_shift_to_tte(pte_t entry, unsigned int shift)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long hugepage_size = _PAGE_SZ4MB_4V;</span>
<span class="p_add">+</span>
<span class="p_add">+	pte_val(entry) = pte_val(entry) &amp; ~_PAGE_SZALL_4V;</span>
<span class="p_add">+</span>
<span class="p_add">+	switch (shift) {</span>
<span class="p_add">+	case HPAGE_256MB_SHIFT:</span>
<span class="p_add">+		hugepage_size = _PAGE_SZ256MB_4V;</span>
<span class="p_add">+		pte_val(entry) |= _PAGE_PMD_HUGE;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case HPAGE_SHIFT:</span>
<span class="p_add">+		pte_val(entry) |= _PAGE_PMD_HUGE;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	default:</span>
<span class="p_add">+		WARN_ONCE(1, &quot;unsupported hugepage shift=%u\n&quot;, shift);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	pte_val(entry) = pte_val(entry) | hugepage_size;</span>
<span class="p_add">+	return entry;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static pte_t hugepage_shift_to_tte(pte_t entry, unsigned int shift)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (tlb_type == hypervisor)</span>
<span class="p_add">+		return sun4v_hugepage_shift_to_tte(entry, shift);</span>
<span class="p_add">+	else</span>
<span class="p_add">+		return sun4u_hugepage_shift_to_tte(entry, shift);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+pte_t arch_make_huge_pte(pte_t entry, struct vm_area_struct *vma,</span>
<span class="p_add">+			 struct page *page, int writeable)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned int shift = huge_page_shift(hstate_vma(vma));</span>
<span class="p_add">+</span>
<span class="p_add">+	return hugepage_shift_to_tte(entry, shift);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static unsigned int sun4v_huge_tte_to_shift(pte_t entry)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long tte_szbits = pte_val(entry) &amp; _PAGE_SZALL_4V;</span>
<span class="p_add">+	unsigned int shift;</span>
<span class="p_add">+</span>
<span class="p_add">+	switch (tte_szbits) {</span>
<span class="p_add">+	case _PAGE_SZ256MB_4V:</span>
<span class="p_add">+		shift = HPAGE_256MB_SHIFT;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case _PAGE_SZ4MB_4V:</span>
<span class="p_add">+		shift = REAL_HPAGE_SHIFT;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	default:</span>
<span class="p_add">+		shift = PAGE_SHIFT;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return shift;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static unsigned int sun4u_huge_tte_to_shift(pte_t entry)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long tte_szbits = pte_val(entry) &amp; _PAGE_SZALL_4U;</span>
<span class="p_add">+	unsigned int shift;</span>
<span class="p_add">+</span>
<span class="p_add">+	switch (tte_szbits) {</span>
<span class="p_add">+	case _PAGE_SZ256MB_4U:</span>
<span class="p_add">+		shift = HPAGE_256MB_SHIFT;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case _PAGE_SZ4MB_4U:</span>
<span class="p_add">+		shift = REAL_HPAGE_SHIFT;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	default:</span>
<span class="p_add">+		shift = PAGE_SHIFT;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return shift;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static unsigned int huge_tte_to_shift(pte_t entry)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long shift;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (tlb_type == hypervisor)</span>
<span class="p_add">+		shift = sun4v_huge_tte_to_shift(entry);</span>
<span class="p_add">+	else</span>
<span class="p_add">+		shift = sun4u_huge_tte_to_shift(entry);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (shift == PAGE_SHIFT)</span>
<span class="p_add">+		WARN_ONCE(1, &quot;tto_to_shift: invalid hugepage tte=0x%lx\n&quot;,</span>
<span class="p_add">+			  pte_val(entry));</span>
<span class="p_add">+</span>
<span class="p_add">+	return shift;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static unsigned long huge_tte_to_size(pte_t pte)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long size = 1UL &lt;&lt; huge_tte_to_shift(pte);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (size == REAL_HPAGE_SIZE)</span>
<span class="p_add">+		size = HPAGE_SIZE;</span>
<span class="p_add">+	return size;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 pte_t *huge_pte_alloc(struct mm_struct *mm,
 			unsigned long addr, unsigned long sz)
 {
<span class="p_chunk">@@ -160,35 +269,54 @@</span> <span class="p_context"> pte_t *huge_pte_offset(struct mm_struct *mm, unsigned long addr)</span>
 void set_huge_pte_at(struct mm_struct *mm, unsigned long addr,
 		     pte_t *ptep, pte_t entry)
 {
<span class="p_add">+	unsigned int i, nptes, hugepage_shift;</span>
<span class="p_add">+	unsigned long size;</span>
 	pte_t orig;
 
<span class="p_add">+	size = huge_tte_to_size(entry);</span>
<span class="p_add">+	nptes = size &gt;&gt; PMD_SHIFT;</span>
<span class="p_add">+</span>
 	if (!pte_present(*ptep) &amp;&amp; pte_present(entry))
<span class="p_del">-		mm-&gt;context.hugetlb_pte_count++;</span>
<span class="p_add">+		mm-&gt;context.hugetlb_pte_count += nptes;</span>
 
<span class="p_del">-	addr &amp;= HPAGE_MASK;</span>
<span class="p_add">+	addr &amp;= ~(size - 1);</span>
 	orig = *ptep;
<span class="p_del">-	*ptep = entry;</span>
<span class="p_add">+	hugepage_shift = pte_none(orig) ? PAGE_SIZE : huge_tte_to_shift(orig);</span>
 
<span class="p_del">-	/* Issue TLB flush at REAL_HPAGE_SIZE boundaries */</span>
<span class="p_del">-	maybe_tlb_batch_add(mm, addr, ptep, orig, 0);</span>
<span class="p_del">-	maybe_tlb_batch_add(mm, addr + REAL_HPAGE_SIZE, ptep, orig, 0);</span>
<span class="p_add">+	for (i = 0; i &lt; nptes; i++)</span>
<span class="p_add">+		ptep[i] = __pte(pte_val(entry) + (i &lt;&lt; PMD_SHIFT));</span>
<span class="p_add">+</span>
<span class="p_add">+	maybe_tlb_batch_add(mm, addr, ptep, orig, 0, hugepage_shift);</span>
<span class="p_add">+	/* An HPAGE_SIZE&#39;ed page is composed of two REAL_HPAGE_SIZE&#39;ed pages */</span>
<span class="p_add">+	if (size == HPAGE_SIZE)</span>
<span class="p_add">+		maybe_tlb_batch_add(mm, addr + REAL_HPAGE_SIZE, ptep, orig, 0,</span>
<span class="p_add">+				    hugepage_shift);</span>
 }
 
 pte_t huge_ptep_get_and_clear(struct mm_struct *mm, unsigned long addr,
 			      pte_t *ptep)
 {
<span class="p_add">+	unsigned int i, nptes, hugepage_shift;</span>
<span class="p_add">+	unsigned long size;</span>
 	pte_t entry;
 
 	entry = *ptep;
<span class="p_add">+	size = huge_tte_to_size(entry);</span>
<span class="p_add">+	nptes = size &gt;&gt; PMD_SHIFT;</span>
<span class="p_add">+	hugepage_shift = pte_none(entry) ? PAGE_SIZE : huge_tte_to_shift(entry);</span>
<span class="p_add">+</span>
 	if (pte_present(entry))
<span class="p_del">-		mm-&gt;context.hugetlb_pte_count--;</span>
<span class="p_add">+		mm-&gt;context.hugetlb_pte_count -= nptes;</span>
 
<span class="p_del">-	addr &amp;= HPAGE_MASK;</span>
<span class="p_del">-	*ptep = __pte(0UL);</span>
<span class="p_add">+	addr &amp;= ~(size - 1);</span>
<span class="p_add">+	for (i = 0; i &lt; nptes; i++)</span>
<span class="p_add">+		ptep[i] = __pte(0UL);</span>
 
<span class="p_del">-	/* Issue TLB flush at REAL_HPAGE_SIZE boundaries */</span>
<span class="p_del">-	maybe_tlb_batch_add(mm, addr, ptep, entry, 0);</span>
<span class="p_del">-	maybe_tlb_batch_add(mm, addr + REAL_HPAGE_SIZE, ptep, entry, 0);</span>
<span class="p_add">+	maybe_tlb_batch_add(mm, addr, ptep, entry, 0, hugepage_shift);</span>
<span class="p_add">+	/* An HPAGE_SIZE&#39;ed page is composed of two REAL_HPAGE_SIZE&#39;ed pages */</span>
<span class="p_add">+	if (size == HPAGE_SIZE)</span>
<span class="p_add">+		maybe_tlb_batch_add(mm, addr + REAL_HPAGE_SIZE, ptep, entry, 0,</span>
<span class="p_add">+				    hugepage_shift);</span>
 
 	return entry;
 }
<span class="p_header">diff --git a/arch/sparc/mm/init_64.c b/arch/sparc/mm/init_64.c</span>
<span class="p_header">index 5d2f915..7ed3975 100644</span>
<span class="p_header">--- a/arch/sparc/mm/init_64.c</span>
<span class="p_header">+++ b/arch/sparc/mm/init_64.c</span>
<span class="p_chunk">@@ -324,6 +324,46 @@</span> <span class="p_context"> static void __update_mmu_tsb_insert(struct mm_struct *mm, unsigned long tsb_inde</span>
 	tsb_insert(tsb, tag, tte);
 }
 
<span class="p_add">+#ifdef CONFIG_HUGETLB_PAGE</span>
<span class="p_add">+static int __init setup_hugepagesz(char *string)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long long hugepage_size;</span>
<span class="p_add">+	unsigned int hugepage_shift;</span>
<span class="p_add">+	unsigned short hv_pgsz_idx;</span>
<span class="p_add">+	unsigned int hv_pgsz_mask;</span>
<span class="p_add">+	int rc = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	hugepage_size = memparse(string, &amp;string);</span>
<span class="p_add">+	hugepage_shift = ilog2(hugepage_size);</span>
<span class="p_add">+</span>
<span class="p_add">+	switch (hugepage_shift) {</span>
<span class="p_add">+	case HPAGE_256MB_SHIFT:</span>
<span class="p_add">+		hv_pgsz_mask = HV_PGSZ_MASK_256MB;</span>
<span class="p_add">+		hv_pgsz_idx = HV_PGSZ_IDX_256MB;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case HPAGE_SHIFT:</span>
<span class="p_add">+		hv_pgsz_mask = HV_PGSZ_MASK_4MB;</span>
<span class="p_add">+		hv_pgsz_idx = HV_PGSZ_IDX_4MB;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	default:</span>
<span class="p_add">+		hv_pgsz_mask = 0;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if ((hv_pgsz_mask &amp; cpu_pgsz_mask) == 0U) {</span>
<span class="p_add">+		pr_warn(&quot;hugepagesz=%llu not supported by MMU.\n&quot;,</span>
<span class="p_add">+			hugepage_size);</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	hugetlb_add_hstate(hugepage_shift - PAGE_SHIFT);</span>
<span class="p_add">+	rc = 1;</span>
<span class="p_add">+</span>
<span class="p_add">+out:</span>
<span class="p_add">+	return rc;</span>
<span class="p_add">+}</span>
<span class="p_add">+__setup(&quot;hugepagesz=&quot;, setup_hugepagesz);</span>
<span class="p_add">+#endif	/* CONFIG_HUGETLB_PAGE */</span>
<span class="p_add">+</span>
 void update_mmu_cache(struct vm_area_struct *vma, unsigned long address, pte_t *ptep)
 {
 	struct mm_struct *mm;
<span class="p_chunk">@@ -347,7 +387,7 @@</span> <span class="p_context"> void update_mmu_cache(struct vm_area_struct *vma, unsigned long address, pte_t *</span>
 
 #if defined(CONFIG_HUGETLB_PAGE) || defined(CONFIG_TRANSPARENT_HUGEPAGE)
 	if ((mm-&gt;context.hugetlb_pte_count || mm-&gt;context.thp_pte_count) &amp;&amp;
<span class="p_del">-	    is_hugetlb_pte(pte)) {</span>
<span class="p_add">+	    is_hugetlb_pmd(__pmd(pte_val(pte)))) {</span>
 		/* We are fabricating 8MB pages using 4MB real hw pages.  */
 		pte_val(pte) |= (address &amp; (1UL &lt;&lt; REAL_HPAGE_SHIFT));
 		__update_mmu_tsb_insert(mm, MM_TSB_HUGE, REAL_HPAGE_SHIFT,
<span class="p_header">diff --git a/arch/sparc/mm/tlb.c b/arch/sparc/mm/tlb.c</span>
<span class="p_header">index c56a195..afda3bb 100644</span>
<span class="p_header">--- a/arch/sparc/mm/tlb.c</span>
<span class="p_header">+++ b/arch/sparc/mm/tlb.c</span>
<span class="p_chunk">@@ -67,7 +67,7 @@</span> <span class="p_context"> void arch_leave_lazy_mmu_mode(void)</span>
 }
 
 static void tlb_batch_add_one(struct mm_struct *mm, unsigned long vaddr,
<span class="p_del">-			      bool exec, bool huge)</span>
<span class="p_add">+			      bool exec, unsigned int hugepage_shift)</span>
 {
 	struct tlb_batch *tb = &amp;get_cpu_var(tlb_batch);
 	unsigned long nr;
<span class="p_chunk">@@ -84,19 +84,19 @@</span> <span class="p_context"> static void tlb_batch_add_one(struct mm_struct *mm, unsigned long vaddr,</span>
 	}
 
 	if (!tb-&gt;active) {
<span class="p_del">-		flush_tsb_user_page(mm, vaddr, huge);</span>
<span class="p_add">+		flush_tsb_user_page(mm, vaddr, hugepage_shift);</span>
 		global_flush_tlb_page(mm, vaddr);
 		goto out;
 	}
 
 	if (nr == 0) {
 		tb-&gt;mm = mm;
<span class="p_del">-		tb-&gt;huge = huge;</span>
<span class="p_add">+		tb-&gt;hugepage_shift = hugepage_shift;</span>
 	}
 
<span class="p_del">-	if (tb-&gt;huge != huge) {</span>
<span class="p_add">+	if (tb-&gt;hugepage_shift != hugepage_shift) {</span>
 		flush_tlb_pending();
<span class="p_del">-		tb-&gt;huge = huge;</span>
<span class="p_add">+		tb-&gt;hugepage_shift = hugepage_shift;</span>
 		nr = 0;
 	}
 
<span class="p_chunk">@@ -110,10 +110,9 @@</span> <span class="p_context"> static void tlb_batch_add_one(struct mm_struct *mm, unsigned long vaddr,</span>
 }
 
 void tlb_batch_add(struct mm_struct *mm, unsigned long vaddr,
<span class="p_del">-		   pte_t *ptep, pte_t orig, int fullmm)</span>
<span class="p_add">+		   pte_t *ptep, pte_t orig, int fullmm,</span>
<span class="p_add">+		   unsigned int hugepage_shift)</span>
 {
<span class="p_del">-	bool huge = is_hugetlb_pte(orig);</span>
<span class="p_del">-</span>
 	if (tlb_type != hypervisor &amp;&amp;
 	    pte_dirty(orig)) {
 		unsigned long paddr, pfn = pte_pfn(orig);
<span class="p_chunk">@@ -139,7 +138,7 @@</span> <span class="p_context"> void tlb_batch_add(struct mm_struct *mm, unsigned long vaddr,</span>
 
 no_cache_flush:
 	if (!fullmm)
<span class="p_del">-		tlb_batch_add_one(mm, vaddr, pte_exec(orig), huge);</span>
<span class="p_add">+		tlb_batch_add_one(mm, vaddr, pte_exec(orig), hugepage_shift);</span>
 }
 
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE
<span class="p_header">diff --git a/arch/sparc/mm/tsb.c b/arch/sparc/mm/tsb.c</span>
<span class="p_header">index e20fbba..4ccca32 100644</span>
<span class="p_header">--- a/arch/sparc/mm/tsb.c</span>
<span class="p_header">+++ b/arch/sparc/mm/tsb.c</span>
<span class="p_chunk">@@ -86,6 +86,33 @@</span> <span class="p_context"> static void __flush_tsb_one(struct tlb_batch *tb, unsigned long hash_shift,</span>
 		__flush_tsb_one_entry(tsb, tb-&gt;vaddrs[i], hash_shift, nentries);
 }
 
<span class="p_add">+#if defined(CONFIG_HUGETLB_PAGE) || defined(CONFIG_TRANSPARENT_HUGEPAGE)</span>
<span class="p_add">+static void __flush_huge_tsb_one_entry(unsigned long tsb, unsigned long v,</span>
<span class="p_add">+				       unsigned long hash_shift,</span>
<span class="p_add">+				       unsigned long nentries,</span>
<span class="p_add">+				       unsigned int hugepage_shift)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned int hpage_entries;</span>
<span class="p_add">+	unsigned int i;</span>
<span class="p_add">+</span>
<span class="p_add">+	hpage_entries = 1 &lt;&lt; (hugepage_shift - hash_shift);</span>
<span class="p_add">+	for (i = 0; i &lt; hpage_entries; i++)</span>
<span class="p_add">+		__flush_tsb_one_entry(tsb, v + (i &lt;&lt; hash_shift), hash_shift,</span>
<span class="p_add">+				      nentries);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void __flush_huge_tsb_one(struct tlb_batch *tb, unsigned long hash_shift,</span>
<span class="p_add">+				 unsigned long tsb, unsigned long nentries,</span>
<span class="p_add">+				 unsigned int hugepage_shift)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long i;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i &lt; tb-&gt;tlb_nr; i++)</span>
<span class="p_add">+		__flush_huge_tsb_one_entry(tsb, tb-&gt;vaddrs[i], hash_shift,</span>
<span class="p_add">+					   nentries, hugepage_shift);</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 void flush_tsb_user(struct tlb_batch *tb)
 {
 	struct mm_struct *mm = tb-&gt;mm;
<span class="p_chunk">@@ -93,7 +120,7 @@</span> <span class="p_context"> void flush_tsb_user(struct tlb_batch *tb)</span>
 
 	spin_lock_irqsave(&amp;mm-&gt;context.lock, flags);
 
<span class="p_del">-	if (!tb-&gt;huge) {</span>
<span class="p_add">+	if (tb-&gt;hugepage_shift == PAGE_SHIFT) {</span>
 		base = (unsigned long) mm-&gt;context.tsb_block[MM_TSB_BASE].tsb;
 		nentries = mm-&gt;context.tsb_block[MM_TSB_BASE].tsb_nentries;
 		if (tlb_type == cheetah_plus || tlb_type == hypervisor)
<span class="p_chunk">@@ -101,24 +128,26 @@</span> <span class="p_context"> void flush_tsb_user(struct tlb_batch *tb)</span>
 		__flush_tsb_one(tb, PAGE_SHIFT, base, nentries);
 	}
 #if defined(CONFIG_HUGETLB_PAGE) || defined(CONFIG_TRANSPARENT_HUGEPAGE)
<span class="p_del">-	if (tb-&gt;huge &amp;&amp; mm-&gt;context.tsb_block[MM_TSB_HUGE].tsb) {</span>
<span class="p_add">+	else if (mm-&gt;context.tsb_block[MM_TSB_HUGE].tsb) {</span>
 		base = (unsigned long) mm-&gt;context.tsb_block[MM_TSB_HUGE].tsb;
 		nentries = mm-&gt;context.tsb_block[MM_TSB_HUGE].tsb_nentries;
 		if (tlb_type == cheetah_plus || tlb_type == hypervisor)
 			base = __pa(base);
<span class="p_del">-		__flush_tsb_one(tb, REAL_HPAGE_SHIFT, base, nentries);</span>
<span class="p_add">+		__flush_huge_tsb_one(tb, REAL_HPAGE_SHIFT, base, nentries,</span>
<span class="p_add">+				     tb-&gt;hugepage_shift);</span>
 	}
 #endif
 	spin_unlock_irqrestore(&amp;mm-&gt;context.lock, flags);
 }
 
<span class="p_del">-void flush_tsb_user_page(struct mm_struct *mm, unsigned long vaddr, bool huge)</span>
<span class="p_add">+void flush_tsb_user_page(struct mm_struct *mm, unsigned long vaddr,</span>
<span class="p_add">+			 unsigned int hugepage_shift)</span>
 {
 	unsigned long nentries, base, flags;
 
 	spin_lock_irqsave(&amp;mm-&gt;context.lock, flags);
 
<span class="p_del">-	if (!huge) {</span>
<span class="p_add">+	if (hugepage_shift == PAGE_SHIFT) {</span>
 		base = (unsigned long) mm-&gt;context.tsb_block[MM_TSB_BASE].tsb;
 		nentries = mm-&gt;context.tsb_block[MM_TSB_BASE].tsb_nentries;
 		if (tlb_type == cheetah_plus || tlb_type == hypervisor)
<span class="p_chunk">@@ -126,12 +155,13 @@</span> <span class="p_context"> void flush_tsb_user_page(struct mm_struct *mm, unsigned long vaddr, bool huge)</span>
 		__flush_tsb_one_entry(base, vaddr, PAGE_SHIFT, nentries);
 	}
 #if defined(CONFIG_HUGETLB_PAGE) || defined(CONFIG_TRANSPARENT_HUGEPAGE)
<span class="p_del">-	if (huge &amp;&amp; mm-&gt;context.tsb_block[MM_TSB_HUGE].tsb) {</span>
<span class="p_add">+	else if (mm-&gt;context.tsb_block[MM_TSB_HUGE].tsb) {</span>
 		base = (unsigned long) mm-&gt;context.tsb_block[MM_TSB_HUGE].tsb;
 		nentries = mm-&gt;context.tsb_block[MM_TSB_HUGE].tsb_nentries;
 		if (tlb_type == cheetah_plus || tlb_type == hypervisor)
 			base = __pa(base);
<span class="p_del">-		__flush_tsb_one_entry(base, vaddr, REAL_HPAGE_SHIFT, nentries);</span>
<span class="p_add">+		__flush_huge_tsb_one_entry(base, vaddr, REAL_HPAGE_SHIFT,</span>
<span class="p_add">+					   nentries, hugepage_shift);</span>
 	}
 #endif
 	spin_unlock_irqrestore(&amp;mm-&gt;context.lock, flags);

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



