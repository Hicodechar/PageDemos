
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[HMM-v25,15/19] mm/migrate: migrate_vma() unmap page from vma while collecting pages - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [HMM-v25,15/19] mm/migrate: migrate_vma() unmap page from vma while collecting pages</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=2554">Jerome Glisse</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Aug. 17, 2017, 12:05 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20170817000548.32038-16-jglisse@redhat.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9904749/mbox/"
   >mbox</a>
|
   <a href="/patch/9904749/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9904749/">/patch/9904749/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	1957260231 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 17 Aug 2017 00:06:28 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 0B98C28955
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 17 Aug 2017 00:06:28 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 004A628A71; Thu, 17 Aug 2017 00:06:27 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 6D74028955
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 17 Aug 2017 00:06:27 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752407AbdHQAGX (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Wed, 16 Aug 2017 20:06:23 -0400
Received: from mx1.redhat.com ([209.132.183.28]:41750 &quot;EHLO mx1.redhat.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1752347AbdHQAGU (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Wed, 16 Aug 2017 20:06:20 -0400
Received: from smtp.corp.redhat.com
	(int-mx01.intmail.prod.int.phx2.redhat.com [10.5.11.11])
	(using TLSv1.2 with cipher AECDH-AES256-SHA (256/256 bits))
	(No client certificate requested)
	by mx1.redhat.com (Postfix) with ESMTPS id 8F228C01C118;
	Thu, 17 Aug 2017 00:06:20 +0000 (UTC)
DMARC-Filter: OpenDMARC Filter v1.3.2 mx1.redhat.com 8F228C01C118
Authentication-Results: ext-mx07.extmail.prod.ext.phx2.redhat.com;
	dmarc=none (p=none dis=none) header.from=redhat.com
Authentication-Results: ext-mx07.extmail.prod.ext.phx2.redhat.com;
	spf=fail smtp.mailfrom=jglisse@redhat.com
Received: from localhost.localdomain.com (ovpn-120-132.rdu2.redhat.com
	[10.10.120.132])
	by smtp.corp.redhat.com (Postfix) with ESMTP id 0D2EA5C7C0;
	Thu, 17 Aug 2017 00:06:17 +0000 (UTC)
From: =?UTF-8?q?J=C3=A9r=C3=B4me=20Glisse?= &lt;jglisse@redhat.com&gt;
To: akpm@linux-foundation.org, linux-kernel@vger.kernel.org,
	linux-mm@kvack.org
Cc: John Hubbard &lt;jhubbard@nvidia.com&gt;,
	Dan Williams &lt;dan.j.williams@intel.com&gt;,
	David Nellans &lt;dnellans@nvidia.com&gt;,
	Balbir Singh &lt;bsingharora@gmail.com&gt;,
	=?UTF-8?q?J=C3=A9r=C3=B4me=20Glisse?= &lt;jglisse@redhat.com&gt;,
	Evgeny Baskakov &lt;ebaskakov@nvidia.com&gt;,
	Mark Hairgrove &lt;mhairgrove@nvidia.com&gt;,
	Sherry Cheung &lt;SCheung@nvidia.com&gt;, Subhash Gutti &lt;sgutti@nvidia.com&gt;
Subject: [HMM-v25 15/19] mm/migrate: migrate_vma() unmap page from vma while
	collecting pages
Date: Wed, 16 Aug 2017 20:05:44 -0400
Message-Id: &lt;20170817000548.32038-16-jglisse@redhat.com&gt;
In-Reply-To: &lt;20170817000548.32038-1-jglisse@redhat.com&gt;
References: &lt;20170817000548.32038-1-jglisse@redhat.com&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit
X-Scanned-By: MIMEDefang 2.79 on 10.5.11.11
X-Greylist: Sender IP whitelisted, not delayed by milter-greylist-4.5.16
	(mx1.redhat.com [10.5.110.31]);
	Thu, 17 Aug 2017 00:06:20 +0000 (UTC)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2554">Jerome Glisse</a> - Aug. 17, 2017, 12:05 a.m.</div>
<pre class="content">
Common case for migration of virtual address range is page are map
only once inside the vma in which migration is taking place. Because
we already walk the CPU page table for that range we can directly do
the unmap there and setup special migration swap entry.
<span class="signed-off-by">
Signed-off-by: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Evgeny Baskakov &lt;ebaskakov@nvidia.com&gt;</span>
<span class="signed-off-by">Signed-off-by: John Hubbard &lt;jhubbard@nvidia.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Mark Hairgrove &lt;mhairgrove@nvidia.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Sherry Cheung &lt;SCheung@nvidia.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Subhash Gutti &lt;sgutti@nvidia.com&gt;</span>
---
 mm/migrate.c | 141 +++++++++++++++++++++++++++++++++++++++++++++++------------
 1 file changed, 112 insertions(+), 29 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/mm/migrate.c b/mm/migrate.c</span>
<span class="p_header">index 60e2f8369cd7..57d1fa7a8e62 100644</span>
<span class="p_header">--- a/mm/migrate.c</span>
<span class="p_header">+++ b/mm/migrate.c</span>
<span class="p_chunk">@@ -2160,7 +2160,7 @@</span> <span class="p_context"> static int migrate_vma_collect_pmd(pmd_t *pmdp,</span>
 	struct migrate_vma *migrate = walk-&gt;private;
 	struct vm_area_struct *vma = walk-&gt;vma;
 	struct mm_struct *mm = vma-&gt;vm_mm;
<span class="p_del">-	unsigned long addr = start;</span>
<span class="p_add">+	unsigned long addr = start, unmapped = 0;</span>
 	spinlock_t *ptl;
 	pte_t *ptep;
 
<span class="p_chunk">@@ -2205,9 +2205,12 @@</span> <span class="p_context"> static int migrate_vma_collect_pmd(pmd_t *pmdp,</span>
 		return migrate_vma_collect_hole(start, end, walk);
 
 	ptep = pte_offset_map_lock(mm, pmdp, addr, &amp;ptl);
<span class="p_add">+	arch_enter_lazy_mmu_mode();</span>
<span class="p_add">+</span>
 	for (; addr &lt; end; addr += PAGE_SIZE, ptep++) {
 		unsigned long mpfn, pfn;
 		struct page *page;
<span class="p_add">+		swp_entry_t entry;</span>
 		pte_t pte;
 
 		pte = *ptep;
<span class="p_chunk">@@ -2239,11 +2242,44 @@</span> <span class="p_context"> static int migrate_vma_collect_pmd(pmd_t *pmdp,</span>
 		mpfn = migrate_pfn(pfn) | MIGRATE_PFN_MIGRATE;
 		mpfn |= pte_write(pte) ? MIGRATE_PFN_WRITE : 0;
 
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Optimize for the common case where page is only mapped once</span>
<span class="p_add">+		 * in one process. If we can lock the page, then we can safely</span>
<span class="p_add">+		 * set up a special migration page table entry now.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (trylock_page(page)) {</span>
<span class="p_add">+			pte_t swp_pte;</span>
<span class="p_add">+</span>
<span class="p_add">+			mpfn |= MIGRATE_PFN_LOCKED;</span>
<span class="p_add">+			ptep_get_and_clear(mm, addr, ptep);</span>
<span class="p_add">+</span>
<span class="p_add">+			/* Setup special migration page table entry */</span>
<span class="p_add">+			entry = make_migration_entry(page, pte_write(pte));</span>
<span class="p_add">+			swp_pte = swp_entry_to_pte(entry);</span>
<span class="p_add">+			if (pte_soft_dirty(pte))</span>
<span class="p_add">+				swp_pte = pte_swp_mksoft_dirty(swp_pte);</span>
<span class="p_add">+			set_pte_at(mm, addr, ptep, swp_pte);</span>
<span class="p_add">+</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * This is like regular unmap: we remove the rmap and</span>
<span class="p_add">+			 * drop page refcount. Page won&#39;t be freed, as we took</span>
<span class="p_add">+			 * a reference just above.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			page_remove_rmap(page, false);</span>
<span class="p_add">+			put_page(page);</span>
<span class="p_add">+			unmapped++;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
 next:
 		migrate-&gt;src[migrate-&gt;npages++] = mpfn;
 	}
<span class="p_add">+	arch_leave_lazy_mmu_mode();</span>
 	pte_unmap_unlock(ptep - 1, ptl);
 
<span class="p_add">+	/* Only flush the TLB if we actually modified any entries */</span>
<span class="p_add">+	if (unmapped)</span>
<span class="p_add">+		flush_tlb_range(walk-&gt;vma, start, end);</span>
<span class="p_add">+</span>
 	return 0;
 }
 
<span class="p_chunk">@@ -2268,7 +2304,13 @@</span> <span class="p_context"> static void migrate_vma_collect(struct migrate_vma *migrate)</span>
 	mm_walk.mm = migrate-&gt;vma-&gt;vm_mm;
 	mm_walk.private = migrate;
 
<span class="p_add">+	mmu_notifier_invalidate_range_start(mm_walk.mm,</span>
<span class="p_add">+					    migrate-&gt;start,</span>
<span class="p_add">+					    migrate-&gt;end);</span>
 	walk_page_range(migrate-&gt;start, migrate-&gt;end, &amp;mm_walk);
<span class="p_add">+	mmu_notifier_invalidate_range_end(mm_walk.mm,</span>
<span class="p_add">+					  migrate-&gt;start,</span>
<span class="p_add">+					  migrate-&gt;end);</span>
 
 	migrate-&gt;end = migrate-&gt;start + (migrate-&gt;npages &lt;&lt; PAGE_SHIFT);
 }
<span class="p_chunk">@@ -2316,32 +2358,37 @@</span> <span class="p_context"> static bool migrate_vma_check_page(struct page *page)</span>
 static void migrate_vma_prepare(struct migrate_vma *migrate)
 {
 	const unsigned long npages = migrate-&gt;npages;
<span class="p_add">+	const unsigned long start = migrate-&gt;start;</span>
<span class="p_add">+	unsigned long addr, i, restore = 0;</span>
 	bool allow_drain = true;
<span class="p_del">-	unsigned long i;</span>
 
 	lru_add_drain();
 
 	for (i = 0; (i &lt; npages) &amp;&amp; migrate-&gt;cpages; i++) {
 		struct page *page = migrate_pfn_to_page(migrate-&gt;src[i]);
<span class="p_add">+		bool remap = true;</span>
 
 		if (!page)
 			continue;
 
<span class="p_del">-		/*</span>
<span class="p_del">-		 * Because we are migrating several pages there can be</span>
<span class="p_del">-		 * a deadlock between 2 concurrent migration where each</span>
<span class="p_del">-		 * are waiting on each other page lock.</span>
<span class="p_del">-		 *</span>
<span class="p_del">-		 * Make migrate_vma() a best effort thing and backoff</span>
<span class="p_del">-		 * for any page we can not lock right away.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		if (!trylock_page(page)) {</span>
<span class="p_del">-			migrate-&gt;src[i] = 0;</span>
<span class="p_del">-			migrate-&gt;cpages--;</span>
<span class="p_del">-			put_page(page);</span>
<span class="p_del">-			continue;</span>
<span class="p_add">+		if (!(migrate-&gt;src[i] &amp; MIGRATE_PFN_LOCKED)) {</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * Because we are migrating several pages there can be</span>
<span class="p_add">+			 * a deadlock between 2 concurrent migration where each</span>
<span class="p_add">+			 * are waiting on each other page lock.</span>
<span class="p_add">+			 *</span>
<span class="p_add">+			 * Make migrate_vma() a best effort thing and backoff</span>
<span class="p_add">+			 * for any page we can not lock right away.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			if (!trylock_page(page)) {</span>
<span class="p_add">+				migrate-&gt;src[i] = 0;</span>
<span class="p_add">+				migrate-&gt;cpages--;</span>
<span class="p_add">+				put_page(page);</span>
<span class="p_add">+				continue;</span>
<span class="p_add">+			}</span>
<span class="p_add">+			remap = false;</span>
<span class="p_add">+			migrate-&gt;src[i] |= MIGRATE_PFN_LOCKED;</span>
 		}
<span class="p_del">-		migrate-&gt;src[i] |= MIGRATE_PFN_LOCKED;</span>
 
 		if (!PageLRU(page) &amp;&amp; allow_drain) {
 			/* Drain CPU&#39;s pagevec */
<span class="p_chunk">@@ -2350,21 +2397,50 @@</span> <span class="p_context"> static void migrate_vma_prepare(struct migrate_vma *migrate)</span>
 		}
 
 		if (isolate_lru_page(page)) {
<span class="p_del">-			migrate-&gt;src[i] = 0;</span>
<span class="p_del">-			unlock_page(page);</span>
<span class="p_del">-			migrate-&gt;cpages--;</span>
<span class="p_del">-			put_page(page);</span>
<span class="p_add">+			if (remap) {</span>
<span class="p_add">+				migrate-&gt;src[i] &amp;= ~MIGRATE_PFN_MIGRATE;</span>
<span class="p_add">+				migrate-&gt;cpages--;</span>
<span class="p_add">+				restore++;</span>
<span class="p_add">+			} else {</span>
<span class="p_add">+				migrate-&gt;src[i] = 0;</span>
<span class="p_add">+				unlock_page(page);</span>
<span class="p_add">+				migrate-&gt;cpages--;</span>
<span class="p_add">+				put_page(page);</span>
<span class="p_add">+			}</span>
 			continue;
 		}
 
 		if (!migrate_vma_check_page(page)) {
<span class="p_del">-			migrate-&gt;src[i] = 0;</span>
<span class="p_del">-			unlock_page(page);</span>
<span class="p_del">-			migrate-&gt;cpages--;</span>
<span class="p_add">+			if (remap) {</span>
<span class="p_add">+				migrate-&gt;src[i] &amp;= ~MIGRATE_PFN_MIGRATE;</span>
<span class="p_add">+				migrate-&gt;cpages--;</span>
<span class="p_add">+				restore++;</span>
 
<span class="p_del">-			putback_lru_page(page);</span>
<span class="p_add">+				get_page(page);</span>
<span class="p_add">+				putback_lru_page(page);</span>
<span class="p_add">+			} else {</span>
<span class="p_add">+				migrate-&gt;src[i] = 0;</span>
<span class="p_add">+				unlock_page(page);</span>
<span class="p_add">+				migrate-&gt;cpages--;</span>
<span class="p_add">+</span>
<span class="p_add">+				putback_lru_page(page);</span>
<span class="p_add">+			}</span>
 		}
 	}
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0, addr = start; i &lt; npages &amp;&amp; restore; i++, addr += PAGE_SIZE) {</span>
<span class="p_add">+		struct page *page = migrate_pfn_to_page(migrate-&gt;src[i]);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!page || (migrate-&gt;src[i] &amp; MIGRATE_PFN_MIGRATE))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		remove_migration_pte(page, migrate-&gt;vma, addr, page);</span>
<span class="p_add">+</span>
<span class="p_add">+		migrate-&gt;src[i] = 0;</span>
<span class="p_add">+		unlock_page(page);</span>
<span class="p_add">+		put_page(page);</span>
<span class="p_add">+		restore--;</span>
<span class="p_add">+	}</span>
 }
 
 /*
<span class="p_chunk">@@ -2391,12 +2467,19 @@</span> <span class="p_context"> static void migrate_vma_unmap(struct migrate_vma *migrate)</span>
 		if (!page || !(migrate-&gt;src[i] &amp; MIGRATE_PFN_MIGRATE))
 			continue;
 
<span class="p_del">-		try_to_unmap(page, flags);</span>
<span class="p_del">-		if (page_mapped(page) || !migrate_vma_check_page(page)) {</span>
<span class="p_del">-			migrate-&gt;src[i] &amp;= ~MIGRATE_PFN_MIGRATE;</span>
<span class="p_del">-			migrate-&gt;cpages--;</span>
<span class="p_del">-			restore++;</span>
<span class="p_add">+		if (page_mapped(page)) {</span>
<span class="p_add">+			try_to_unmap(page, flags);</span>
<span class="p_add">+			if (page_mapped(page))</span>
<span class="p_add">+				goto restore;</span>
 		}
<span class="p_add">+</span>
<span class="p_add">+		if (migrate_vma_check_page(page))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+restore:</span>
<span class="p_add">+		migrate-&gt;src[i] &amp;= ~MIGRATE_PFN_MIGRATE;</span>
<span class="p_add">+		migrate-&gt;cpages--;</span>
<span class="p_add">+		restore++;</span>
 	}
 
 	for (addr = start, i = 0; i &lt; npages &amp;&amp; restore; addr += PAGE_SIZE, i++) {

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



