
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v2,05/10] x86/mm: Rework lazy TLB mode and TLB freshness tracking - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v2,05/10] x86/mm: Rework lazy TLB mode and TLB freshness tracking</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=125831">Andrew Lutomirski</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>June 14, 2017, 4:56 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;039935bc914009103fdaa6f72f14980c19562de5.1497415951.git.luto@kernel.org&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9785335/mbox/"
   >mbox</a>
|
   <a href="/patch/9785335/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9785335/">/patch/9785335/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	26C2E602C9 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 14 Jun 2017 04:56:50 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 008A728575
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 14 Jun 2017 04:56:50 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id E92322858E; Wed, 14 Jun 2017 04:56:49 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id C35982858D
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 14 Jun 2017 04:56:48 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1754282AbdFNE4m (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Wed, 14 Jun 2017 00:56:42 -0400
Received: from mail.kernel.org ([198.145.29.99]:45806 &quot;EHLO mail.kernel.org&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1754223AbdFNE4k (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Wed, 14 Jun 2017 00:56:40 -0400
Received: from localhost (c-71-202-137-17.hsd1.ca.comcast.net
	[71.202.137.17])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256
	bits)) (No client certificate requested)
	by mail.kernel.org (Postfix) with ESMTPSA id 1CB8C239E0;
	Wed, 14 Jun 2017 04:56:39 +0000 (UTC)
DMARC-Filter: OpenDMARC Filter v1.3.2 mail.kernel.org 1CB8C239E0
Authentication-Results: mail.kernel.org;
	dmarc=none (p=none dis=none) header.from=kernel.org
Authentication-Results: mail.kernel.org;
	spf=none smtp.mailfrom=luto@kernel.org
From: Andy Lutomirski &lt;luto@kernel.org&gt;
To: x86@kernel.org
Cc: linux-kernel@vger.kernel.org, Borislav Petkov &lt;bp@alien8.de&gt;,
	Linus Torvalds &lt;torvalds@linux-foundation.org&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;, Mel Gorman &lt;mgorman@suse.de&gt;,
	&quot;linux-mm@kvack.org&quot; &lt;linux-mm@kvack.org&gt;,
	Nadav Amit &lt;nadav.amit@gmail.com&gt;, Rik van Riel &lt;riel@redhat.com&gt;,
	Dave Hansen &lt;dave.hansen@intel.com&gt;,
	Arjan van de Ven &lt;arjan@linux.intel.com&gt;,
	Peter Zijlstra &lt;peterz@infradead.org&gt;, Andy Lutomirski &lt;luto@kernel.org&gt;,
	Andrew Banman &lt;abanman@sgi.com&gt;, Mike Travis &lt;travis@sgi.com&gt;,
	Dimitri Sivanich &lt;sivanich@sgi.com&gt;, Juergen Gross &lt;jgross@suse.com&gt;,
	Boris Ostrovsky &lt;boris.ostrovsky@oracle.com&gt;
Subject: [PATCH v2 05/10] x86/mm: Rework lazy TLB mode and TLB freshness
	tracking
Date: Tue, 13 Jun 2017 21:56:23 -0700
Message-Id: &lt;039935bc914009103fdaa6f72f14980c19562de5.1497415951.git.luto@kernel.org&gt;
X-Mailer: git-send-email 2.9.4
In-Reply-To: &lt;cover.1497415951.git.luto@kernel.org&gt;
References: &lt;cover.1497415951.git.luto@kernel.org&gt;
In-Reply-To: &lt;cover.1497415951.git.luto@kernel.org&gt;
References: &lt;cover.1497415951.git.luto@kernel.org&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=125831">Andrew Lutomirski</a> - June 14, 2017, 4:56 a.m.</div>
<pre class="content">
x86&#39;s lazy TLB mode used to be fairly weak -- it would switch to
init_mm the first time it tried to flush a lazy TLB.  This meant an
unnecessary CR3 write and, if the flush was remote, an unnecessary
IPI.

Rewrite it entirely.  When we enter lazy mode, we simply remove the
cpu from mm_cpumask.  This means that we need a way to figure out
whether we&#39;ve missed a flush when we switch back out of lazy mode.
I use the tlb_gen machinery to track whether a context is up to
date.

Note to reviewers: this patch, my itself, looks a bit odd.  I&#39;m
using an array of length 1 containing (ctx_id, tlb_gen) rather than
just storing tlb_gen, and making it at array isn&#39;t necessary yet.
I&#39;m doing this because the next few patches add PCID support, and,
with PCID, we need ctx_id, and the array will end up with a length
greater than 1.  Making it an array now means that there will be
less churn and therefore less stress on your eyeballs.

NB: This is dubious but, AFAICT, still correct on Xen and UV.
xen_exit_mmap() uses mm_cpumask() for nefarious purposes and this
patch changes the way that mm_cpumask() works.  This should be okay,
since Xen *also* iterates all online CPUs to find all the CPUs it
needs to twiddle.

The UV tlbflush code is rather dated and should be changed.

Cc: Andrew Banman &lt;abanman@sgi.com&gt;
Cc: Mike Travis &lt;travis@sgi.com&gt;
Cc: Dimitri Sivanich &lt;sivanich@sgi.com&gt;
Cc: Juergen Gross &lt;jgross@suse.com&gt;
Cc: Boris Ostrovsky &lt;boris.ostrovsky@oracle.com&gt;
<span class="signed-off-by">Signed-off-by: Andy Lutomirski &lt;luto@kernel.org&gt;</span>
---
 arch/x86/include/asm/mmu_context.h |   6 +-
 arch/x86/include/asm/tlbflush.h    |   4 -
 arch/x86/mm/init.c                 |   1 -
 arch/x86/mm/tlb.c                  | 242 +++++++++++++++++++------------------
 4 files changed, 131 insertions(+), 122 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=100191">Juergen Gross</a> - June 14, 2017, 6:09 a.m.</div>
<pre class="content">
On 14/06/17 06:56, Andy Lutomirski wrote:
<span class="quote">&gt; x86&#39;s lazy TLB mode used to be fairly weak -- it would switch to</span>
<span class="quote">&gt; init_mm the first time it tried to flush a lazy TLB.  This meant an</span>
<span class="quote">&gt; unnecessary CR3 write and, if the flush was remote, an unnecessary</span>
<span class="quote">&gt; IPI.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Rewrite it entirely.  When we enter lazy mode, we simply remove the</span>
<span class="quote">&gt; cpu from mm_cpumask.  This means that we need a way to figure out</span>
<span class="quote">&gt; whether we&#39;ve missed a flush when we switch back out of lazy mode.</span>
<span class="quote">&gt; I use the tlb_gen machinery to track whether a context is up to</span>
<span class="quote">&gt; date.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Note to reviewers: this patch, my itself, looks a bit odd.  I&#39;m</span>
<span class="quote">&gt; using an array of length 1 containing (ctx_id, tlb_gen) rather than</span>
<span class="quote">&gt; just storing tlb_gen, and making it at array isn&#39;t necessary yet.</span>
<span class="quote">&gt; I&#39;m doing this because the next few patches add PCID support, and,</span>
<span class="quote">&gt; with PCID, we need ctx_id, and the array will end up with a length</span>
<span class="quote">&gt; greater than 1.  Making it an array now means that there will be</span>
<span class="quote">&gt; less churn and therefore less stress on your eyeballs.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; NB: This is dubious but, AFAICT, still correct on Xen and UV.</span>
<span class="quote">&gt; xen_exit_mmap() uses mm_cpumask() for nefarious purposes and this</span>
<span class="quote">&gt; patch changes the way that mm_cpumask() works.  This should be okay,</span>
<span class="quote">&gt; since Xen *also* iterates all online CPUs to find all the CPUs it</span>
<span class="quote">&gt; needs to twiddle.</span>

There is a allocation failure path in xen_drop_mm_ref() which might
be wrong with this patch. As this path should be taken only very
unlikely I&#39;d suggest to remove the test for mm_cpumask() bit zero in
this path.


Juergen
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=65121">Dave Hansen</a> - June 14, 2017, 10:33 p.m.</div>
<pre class="content">
On 06/13/2017 09:56 PM, Andy Lutomirski wrote:
<span class="quote">&gt; -	if (cpumask_test_cpu(cpu, &amp;batch-&gt;cpumask))</span>
<span class="quote">&gt; +	if (cpumask_test_cpu(cpu, &amp;batch-&gt;cpumask)) {</span>
<span class="quote">&gt; +		local_irq_disable();</span>
<span class="quote">&gt;  		flush_tlb_func_local(&amp;info, TLB_LOCAL_SHOOTDOWN);</span>
<span class="quote">&gt; +		local_irq_enable();</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>

Could you talk a little about why this needs to be local_irq_disable()
and not preempt_disable()?  Is it about the case where somebody is
trying to call flush_tlb_func_*() from an interrupt handler?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=125831">Andrew Lutomirski</a> - June 14, 2017, 10:42 p.m.</div>
<pre class="content">
On Wed, Jun 14, 2017 at 3:33 PM, Dave Hansen &lt;dave.hansen@intel.com&gt; wrote:
<span class="quote">&gt; On 06/13/2017 09:56 PM, Andy Lutomirski wrote:</span>
<span class="quote">&gt;&gt; -     if (cpumask_test_cpu(cpu, &amp;batch-&gt;cpumask))</span>
<span class="quote">&gt;&gt; +     if (cpumask_test_cpu(cpu, &amp;batch-&gt;cpumask)) {</span>
<span class="quote">&gt;&gt; +             local_irq_disable();</span>
<span class="quote">&gt;&gt;               flush_tlb_func_local(&amp;info, TLB_LOCAL_SHOOTDOWN);</span>
<span class="quote">&gt;&gt; +             local_irq_enable();</span>
<span class="quote">&gt;&gt; +     }</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Could you talk a little about why this needs to be local_irq_disable()</span>
<span class="quote">&gt; and not preempt_disable()?  Is it about the case where somebody is</span>
<span class="quote">&gt; trying to call flush_tlb_func_*() from an interrupt handler?</span>

It&#39;s to prevent flush_tlb_func_local() and flush_tlb_func_remote()
from being run concurrently, which would cause flush_tlb_func_common()
to be reentered.  Either we&#39;d need to be very careful in
flush_tlb_func_common() to avoid races if this happened, or we could
just disable interrupts around flush_tlb_func_local().  The latter is
fast and easy.

--Andy
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=55071">Nadav Amit</a> - June 18, 2017, 8:06 a.m.</div>
<pre class="content">
<span class="quote">&gt; On Jun 13, 2017, at 9:56 PM, Andy Lutomirski &lt;luto@kernel.org&gt; wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; x86&#39;s lazy TLB mode used to be fairly weak -- it would switch to</span>
<span class="quote">&gt; init_mm the first time it tried to flush a lazy TLB.  This meant an</span>
<span class="quote">&gt; unnecessary CR3 write and, if the flush was remote, an unnecessary</span>
<span class="quote">&gt; IPI.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Rewrite it entirely.  When we enter lazy mode, we simply remove the</span>
<span class="quote">&gt; cpu from mm_cpumask.  This means that we need a way to figure out</span>
<span class="quote">&gt; whether we&#39;ve missed a flush when we switch back out of lazy mode.</span>
<span class="quote">&gt; I use the tlb_gen machinery to track whether a context is up to</span>
<span class="quote">&gt; date.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Note to reviewers: this patch, my itself, looks a bit odd.  I&#39;m</span>
<span class="quote">&gt; using an array of length 1 containing (ctx_id, tlb_gen) rather than</span>
<span class="quote">&gt; just storing tlb_gen, and making it at array isn&#39;t necessary yet.</span>
<span class="quote">&gt; I&#39;m doing this because the next few patches add PCID support, and,</span>
<span class="quote">&gt; with PCID, we need ctx_id, and the array will end up with a length</span>
<span class="quote">&gt; greater than 1.  Making it an array now means that there will be</span>
<span class="quote">&gt; less churn and therefore less stress on your eyeballs.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; NB: This is dubious but, AFAICT, still correct on Xen and UV.</span>
<span class="quote">&gt; xen_exit_mmap() uses mm_cpumask() for nefarious purposes and this</span>
<span class="quote">&gt; patch changes the way that mm_cpumask() works.  This should be okay,</span>
<span class="quote">&gt; since Xen *also* iterates all online CPUs to find all the CPUs it</span>
<span class="quote">&gt; needs to twiddle.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The UV tlbflush code is rather dated and should be changed.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Cc: Andrew Banman &lt;abanman@sgi.com&gt;</span>
<span class="quote">&gt; Cc: Mike Travis &lt;travis@sgi.com&gt;</span>
<span class="quote">&gt; Cc: Dimitri Sivanich &lt;sivanich@sgi.com&gt;</span>
<span class="quote">&gt; Cc: Juergen Gross &lt;jgross@suse.com&gt;</span>
<span class="quote">&gt; Cc: Boris Ostrovsky &lt;boris.ostrovsky@oracle.com&gt;</span>
<span class="quote">&gt; Signed-off-by: Andy Lutomirski &lt;luto@kernel.org&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt; arch/x86/include/asm/mmu_context.h |   6 +-</span>
<span class="quote">&gt; arch/x86/include/asm/tlbflush.h    |   4 -</span>
<span class="quote">&gt; arch/x86/mm/init.c                 |   1 -</span>
<span class="quote">&gt; arch/x86/mm/tlb.c                  | 242 +++++++++++++++++++------------------</span>
<span class="quote">&gt; 4 files changed, 131 insertions(+), 122 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/arch/x86/include/asm/mmu_context.h b/arch/x86/include/asm/mmu_context.h</span>
<span class="quote">&gt; index e5295d485899..69a4f1ee86ac 100644</span>
<span class="quote">&gt; --- a/arch/x86/include/asm/mmu_context.h</span>
<span class="quote">&gt; +++ b/arch/x86/include/asm/mmu_context.h</span>
<span class="quote">&gt; @@ -125,8 +125,10 @@ static inline void switch_ldt(struct mm_struct *prev, struct mm_struct *next)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; static inline void enter_lazy_tlb(struct mm_struct *mm, struct task_struct *tsk)</span>
<span class="quote">&gt; {</span>
<span class="quote">&gt; -	if (this_cpu_read(cpu_tlbstate.state) == TLBSTATE_OK)</span>
<span class="quote">&gt; -		this_cpu_write(cpu_tlbstate.state, TLBSTATE_LAZY);</span>
<span class="quote">&gt; +	int cpu = smp_processor_id();</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (cpumask_test_cpu(cpu, mm_cpumask(mm)))</span>
<span class="quote">&gt; +		cpumask_clear_cpu(cpu, mm_cpumask(mm));</span>

The indication for laziness that was in cpu_tlbstate.state may be a better
indication whether the cpu needs to be cleared from the previous mm_cpumask().
If you kept this indication, you could have used this per-cpu information in
switch_mm_irqs_off() instead of &quot;cpumask_test_cpu(cpu, mm_cpumask(next))”,
which might have been accessed by another core.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=125831">Andrew Lutomirski</a> - June 19, 2017, 9:58 p.m.</div>
<pre class="content">
On Sun, Jun 18, 2017 at 1:06 AM, Nadav Amit &lt;nadav.amit@gmail.com&gt; wrote:
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; On Jun 13, 2017, at 9:56 PM, Andy Lutomirski &lt;luto@kernel.org&gt; wrote:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; x86&#39;s lazy TLB mode used to be fairly weak -- it would switch to</span>
<span class="quote">&gt;&gt; init_mm the first time it tried to flush a lazy TLB.  This meant an</span>
<span class="quote">&gt;&gt; unnecessary CR3 write and, if the flush was remote, an unnecessary</span>
<span class="quote">&gt;&gt; IPI.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Rewrite it entirely.  When we enter lazy mode, we simply remove the</span>
<span class="quote">&gt;&gt; cpu from mm_cpumask.  This means that we need a way to figure out</span>
<span class="quote">&gt;&gt; whether we&#39;ve missed a flush when we switch back out of lazy mode.</span>
<span class="quote">&gt;&gt; I use the tlb_gen machinery to track whether a context is up to</span>
<span class="quote">&gt;&gt; date.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Note to reviewers: this patch, my itself, looks a bit odd.  I&#39;m</span>
<span class="quote">&gt;&gt; using an array of length 1 containing (ctx_id, tlb_gen) rather than</span>
<span class="quote">&gt;&gt; just storing tlb_gen, and making it at array isn&#39;t necessary yet.</span>
<span class="quote">&gt;&gt; I&#39;m doing this because the next few patches add PCID support, and,</span>
<span class="quote">&gt;&gt; with PCID, we need ctx_id, and the array will end up with a length</span>
<span class="quote">&gt;&gt; greater than 1.  Making it an array now means that there will be</span>
<span class="quote">&gt;&gt; less churn and therefore less stress on your eyeballs.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; NB: This is dubious but, AFAICT, still correct on Xen and UV.</span>
<span class="quote">&gt;&gt; xen_exit_mmap() uses mm_cpumask() for nefarious purposes and this</span>
<span class="quote">&gt;&gt; patch changes the way that mm_cpumask() works.  This should be okay,</span>
<span class="quote">&gt;&gt; since Xen *also* iterates all online CPUs to find all the CPUs it</span>
<span class="quote">&gt;&gt; needs to twiddle.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; The UV tlbflush code is rather dated and should be changed.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Cc: Andrew Banman &lt;abanman@sgi.com&gt;</span>
<span class="quote">&gt;&gt; Cc: Mike Travis &lt;travis@sgi.com&gt;</span>
<span class="quote">&gt;&gt; Cc: Dimitri Sivanich &lt;sivanich@sgi.com&gt;</span>
<span class="quote">&gt;&gt; Cc: Juergen Gross &lt;jgross@suse.com&gt;</span>
<span class="quote">&gt;&gt; Cc: Boris Ostrovsky &lt;boris.ostrovsky@oracle.com&gt;</span>
<span class="quote">&gt;&gt; Signed-off-by: Andy Lutomirski &lt;luto@kernel.org&gt;</span>
<span class="quote">&gt;&gt; ---</span>
<span class="quote">&gt;&gt; arch/x86/include/asm/mmu_context.h |   6 +-</span>
<span class="quote">&gt;&gt; arch/x86/include/asm/tlbflush.h    |   4 -</span>
<span class="quote">&gt;&gt; arch/x86/mm/init.c                 |   1 -</span>
<span class="quote">&gt;&gt; arch/x86/mm/tlb.c                  | 242 +++++++++++++++++++------------------</span>
<span class="quote">&gt;&gt; 4 files changed, 131 insertions(+), 122 deletions(-)</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; diff --git a/arch/x86/include/asm/mmu_context.h b/arch/x86/include/asm/mmu_context.h</span>
<span class="quote">&gt;&gt; index e5295d485899..69a4f1ee86ac 100644</span>
<span class="quote">&gt;&gt; --- a/arch/x86/include/asm/mmu_context.h</span>
<span class="quote">&gt;&gt; +++ b/arch/x86/include/asm/mmu_context.h</span>
<span class="quote">&gt;&gt; @@ -125,8 +125,10 @@ static inline void switch_ldt(struct mm_struct *prev, struct mm_struct *next)</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; static inline void enter_lazy_tlb(struct mm_struct *mm, struct task_struct *tsk)</span>
<span class="quote">&gt;&gt; {</span>
<span class="quote">&gt;&gt; -     if (this_cpu_read(cpu_tlbstate.state) == TLBSTATE_OK)</span>
<span class="quote">&gt;&gt; -             this_cpu_write(cpu_tlbstate.state, TLBSTATE_LAZY);</span>
<span class="quote">&gt;&gt; +     int cpu = smp_processor_id();</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +     if (cpumask_test_cpu(cpu, mm_cpumask(mm)))</span>
<span class="quote">&gt;&gt; +             cpumask_clear_cpu(cpu, mm_cpumask(mm));</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; The indication for laziness that was in cpu_tlbstate.state may be a better</span>
<span class="quote">&gt; indication whether the cpu needs to be cleared from the previous mm_cpumask().</span>
<span class="quote">&gt; If you kept this indication, you could have used this per-cpu information in</span>
<span class="quote">&gt; switch_mm_irqs_off() instead of &quot;cpumask_test_cpu(cpu, mm_cpumask(next))”,</span>
<span class="quote">&gt; which might have been accessed by another core.</span>

Hmm, fair enough.  On the other hand, this is the least of our
problems in this particular case -- the scheduler&#39;s use of mmgrab()
and mmdrop() are probably at least as bad if not worse.  My preference
would be to get all this stuff merged and then see if we want to add
some scalability improvements on top.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=125831">Andrew Lutomirski</a> - June 19, 2017, 10 p.m.</div>
<pre class="content">
On Tue, Jun 13, 2017 at 11:09 PM, Juergen Gross &lt;jgross@suse.com&gt; wrote:
<span class="quote">&gt; On 14/06/17 06:56, Andy Lutomirski wrote:</span>
<span class="quote">&gt;&gt; x86&#39;s lazy TLB mode used to be fairly weak -- it would switch to</span>
<span class="quote">&gt;&gt; init_mm the first time it tried to flush a lazy TLB.  This meant an</span>
<span class="quote">&gt;&gt; unnecessary CR3 write and, if the flush was remote, an unnecessary</span>
<span class="quote">&gt;&gt; IPI.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Rewrite it entirely.  When we enter lazy mode, we simply remove the</span>
<span class="quote">&gt;&gt; cpu from mm_cpumask.  This means that we need a way to figure out</span>
<span class="quote">&gt;&gt; whether we&#39;ve missed a flush when we switch back out of lazy mode.</span>
<span class="quote">&gt;&gt; I use the tlb_gen machinery to track whether a context is up to</span>
<span class="quote">&gt;&gt; date.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Note to reviewers: this patch, my itself, looks a bit odd.  I&#39;m</span>
<span class="quote">&gt;&gt; using an array of length 1 containing (ctx_id, tlb_gen) rather than</span>
<span class="quote">&gt;&gt; just storing tlb_gen, and making it at array isn&#39;t necessary yet.</span>
<span class="quote">&gt;&gt; I&#39;m doing this because the next few patches add PCID support, and,</span>
<span class="quote">&gt;&gt; with PCID, we need ctx_id, and the array will end up with a length</span>
<span class="quote">&gt;&gt; greater than 1.  Making it an array now means that there will be</span>
<span class="quote">&gt;&gt; less churn and therefore less stress on your eyeballs.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; NB: This is dubious but, AFAICT, still correct on Xen and UV.</span>
<span class="quote">&gt;&gt; xen_exit_mmap() uses mm_cpumask() for nefarious purposes and this</span>
<span class="quote">&gt;&gt; patch changes the way that mm_cpumask() works.  This should be okay,</span>
<span class="quote">&gt;&gt; since Xen *also* iterates all online CPUs to find all the CPUs it</span>
<span class="quote">&gt;&gt; needs to twiddle.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; There is a allocation failure path in xen_drop_mm_ref() which might</span>
<span class="quote">&gt; be wrong with this patch. As this path should be taken only very</span>
<span class="quote">&gt; unlikely I&#39;d suggest to remove the test for mm_cpumask() bit zero in</span>
<span class="quote">&gt; this path.</span>
<span class="quote">&gt;</span>

Right, fixed.
<span class="quote">
&gt;</span>
<span class="quote">&gt; Juergen</span>
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/include/asm/mmu_context.h b/arch/x86/include/asm/mmu_context.h</span>
<span class="p_header">index e5295d485899..69a4f1ee86ac 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/mmu_context.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/mmu_context.h</span>
<span class="p_chunk">@@ -125,8 +125,10 @@</span> <span class="p_context"> static inline void switch_ldt(struct mm_struct *prev, struct mm_struct *next)</span>
 
 static inline void enter_lazy_tlb(struct mm_struct *mm, struct task_struct *tsk)
 {
<span class="p_del">-	if (this_cpu_read(cpu_tlbstate.state) == TLBSTATE_OK)</span>
<span class="p_del">-		this_cpu_write(cpu_tlbstate.state, TLBSTATE_LAZY);</span>
<span class="p_add">+	int cpu = smp_processor_id();</span>
<span class="p_add">+</span>
<span class="p_add">+	if (cpumask_test_cpu(cpu, mm_cpumask(mm)))</span>
<span class="p_add">+		cpumask_clear_cpu(cpu, mm_cpumask(mm));</span>
 }
 
 extern atomic64_t last_mm_ctx_id;
<span class="p_header">diff --git a/arch/x86/include/asm/tlbflush.h b/arch/x86/include/asm/tlbflush.h</span>
<span class="p_header">index 4f6c30d6ec39..87b13e51e867 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/tlbflush.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/tlbflush.h</span>
<span class="p_chunk">@@ -95,7 +95,6 @@</span> <span class="p_context"> struct tlb_state {</span>
 	 * mode even if we&#39;ve already switched back to swapper_pg_dir.
 	 */
 	struct mm_struct *loaded_mm;
<span class="p_del">-	int state;</span>
 
 	/*
 	 * Access to this CR4 shadow and to H/W CR4 is protected by
<span class="p_chunk">@@ -310,9 +309,6 @@</span> <span class="p_context"> static inline void flush_tlb_page(struct vm_area_struct *vma, unsigned long a)</span>
 void native_flush_tlb_others(const struct cpumask *cpumask,
 			     const struct flush_tlb_info *info);
 
<span class="p_del">-#define TLBSTATE_OK	1</span>
<span class="p_del">-#define TLBSTATE_LAZY	2</span>
<span class="p_del">-</span>
 static inline void arch_tlbbatch_add_mm(struct arch_tlbflush_unmap_batch *batch,
 					struct mm_struct *mm)
 {
<span class="p_header">diff --git a/arch/x86/mm/init.c b/arch/x86/mm/init.c</span>
<span class="p_header">index 88ee942cb47d..7d6fa4676af9 100644</span>
<span class="p_header">--- a/arch/x86/mm/init.c</span>
<span class="p_header">+++ b/arch/x86/mm/init.c</span>
<span class="p_chunk">@@ -812,7 +812,6 @@</span> <span class="p_context"> void __init zone_sizes_init(void)</span>
 
 DEFINE_PER_CPU_SHARED_ALIGNED(struct tlb_state, cpu_tlbstate) = {
 	.loaded_mm = &amp;init_mm,
<span class="p_del">-	.state = 0,</span>
 	.cr4 = ~0UL,	/* fail hard if we screw up cr4 shadow initialization */
 };
 EXPORT_SYMBOL_GPL(cpu_tlbstate);
<span class="p_header">diff --git a/arch/x86/mm/tlb.c b/arch/x86/mm/tlb.c</span>
<span class="p_header">index 3b19ba748e92..fea2b07ac7d8 100644</span>
<span class="p_header">--- a/arch/x86/mm/tlb.c</span>
<span class="p_header">+++ b/arch/x86/mm/tlb.c</span>
<span class="p_chunk">@@ -45,8 +45,8 @@</span> <span class="p_context"> void leave_mm(int cpu)</span>
 	if (loaded_mm == &amp;init_mm)
 		return;
 
<span class="p_del">-	if (this_cpu_read(cpu_tlbstate.state) == TLBSTATE_OK)</span>
<span class="p_del">-		BUG();</span>
<span class="p_add">+	/* Warn if we&#39;re not lazy. */</span>
<span class="p_add">+	WARN_ON(cpumask_test_cpu(smp_processor_id(), mm_cpumask(loaded_mm)));</span>
 
 	switch_mm(NULL, &amp;init_mm, NULL);
 }
<span class="p_chunk">@@ -67,133 +67,118 @@</span> <span class="p_context"> void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,</span>
 {
 	unsigned cpu = smp_processor_id();
 	struct mm_struct *real_prev = this_cpu_read(cpu_tlbstate.loaded_mm);
<span class="p_add">+	u64 next_tlb_gen;</span>
 
 	/*
<span class="p_del">-	 * NB: The scheduler will call us with prev == next when</span>
<span class="p_del">-	 * switching from lazy TLB mode to normal mode if active_mm</span>
<span class="p_del">-	 * isn&#39;t changing.  When this happens, there is no guarantee</span>
<span class="p_del">-	 * that CR3 (and hence cpu_tlbstate.loaded_mm) matches next.</span>
<span class="p_add">+	 * NB: The scheduler will call us with prev == next when switching</span>
<span class="p_add">+	 * from lazy TLB mode to normal mode if active_mm isn&#39;t changing.</span>
<span class="p_add">+	 * When this happens, we don&#39;t assume that CR3 (and hence</span>
<span class="p_add">+	 * cpu_tlbstate.loaded_mm) matches next.</span>
 	 *
 	 * NB: leave_mm() calls us with prev == NULL and tsk == NULL.
 	 */
 
<span class="p_del">-	this_cpu_write(cpu_tlbstate.state, TLBSTATE_OK);</span>
<span class="p_add">+	/* We don&#39;t want flush_tlb_func_* to run concurrently with us. */</span>
<span class="p_add">+	if (IS_ENABLED(CONFIG_PROVE_LOCKING))</span>
<span class="p_add">+		WARN_ON_ONCE(!irqs_disabled());</span>
<span class="p_add">+</span>
<span class="p_add">+	VM_BUG_ON(read_cr3_pa() != __pa(real_prev-&gt;pgd));</span>
 
 	if (real_prev == next) {
<span class="p_del">-		/*</span>
<span class="p_del">-		 * There&#39;s nothing to do: we always keep the per-mm control</span>
<span class="p_del">-		 * regs in sync with cpu_tlbstate.loaded_mm.  Just</span>
<span class="p_del">-		 * sanity-check mm_cpumask.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		if (WARN_ON_ONCE(!cpumask_test_cpu(cpu, mm_cpumask(next))))</span>
<span class="p_del">-			cpumask_set_cpu(cpu, mm_cpumask(next));</span>
<span class="p_del">-		return;</span>
<span class="p_del">-	}</span>
<span class="p_add">+		if (cpumask_test_cpu(cpu, mm_cpumask(next))) {</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * There&#39;s nothing to do: we weren&#39;t lazy, and we</span>
<span class="p_add">+			 * aren&#39;t changing our mm.  We don&#39;t need to flush</span>
<span class="p_add">+			 * anything, nor do we need to update CR3, CR4, or</span>
<span class="p_add">+			 * LDTR.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			return;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		/* Resume remote flushes and then read tlb_gen. */</span>
<span class="p_add">+		cpumask_set_cpu(cpu, mm_cpumask(next));</span>
<span class="p_add">+		next_tlb_gen = atomic64_read(&amp;next-&gt;context.tlb_gen);</span>
<span class="p_add">+</span>
<span class="p_add">+		VM_BUG_ON(this_cpu_read(cpu_tlbstate.ctxs[0].ctx_id) !=</span>
<span class="p_add">+			  next-&gt;context.ctx_id);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (this_cpu_read(cpu_tlbstate.ctxs[0].tlb_gen) &lt;</span>
<span class="p_add">+		    next_tlb_gen) {</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * Ideally, we&#39;d have a flush_tlb() variant that</span>
<span class="p_add">+			 * takes the known CR3 value as input.  This would</span>
<span class="p_add">+			 * be faster on Xen PV and on hypothetical CPUs</span>
<span class="p_add">+			 * on which INVPCID is fast.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			this_cpu_write(cpu_tlbstate.ctxs[0].tlb_gen,</span>
<span class="p_add">+				       next_tlb_gen);</span>
<span class="p_add">+			write_cr3(__pa(next-&gt;pgd));</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * This gets called via leave_mm() in the idle path</span>
<span class="p_add">+			 * where RCU functions differently.  Tracing normally</span>
<span class="p_add">+			 * uses RCU, so we have to call the tracepoint</span>
<span class="p_add">+			 * specially here.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			trace_tlb_flush_rcuidle(TLB_FLUSH_ON_TASK_SWITCH,</span>
<span class="p_add">+						TLB_FLUSH_ALL);</span>
<span class="p_add">+		}</span>
 
<span class="p_del">-	if (IS_ENABLED(CONFIG_VMAP_STACK)) {</span>
 		/*
<span class="p_del">-		 * If our current stack is in vmalloc space and isn&#39;t</span>
<span class="p_del">-		 * mapped in the new pgd, we&#39;ll double-fault.  Forcibly</span>
<span class="p_del">-		 * map it.</span>
<span class="p_add">+		 * We just exited lazy mode, which means that CR4 and/or LDTR</span>
<span class="p_add">+		 * may be stale.  (Changes to the required CR4 and LDTR states</span>
<span class="p_add">+		 * are not reflected in tlb_gen.)</span>
 		 */
<span class="p_del">-		unsigned int stack_pgd_index = pgd_index(current_stack_pointer());</span>
<span class="p_del">-</span>
<span class="p_del">-		pgd_t *pgd = next-&gt;pgd + stack_pgd_index;</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		if (IS_ENABLED(CONFIG_VMAP_STACK)) {</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * If our current stack is in vmalloc space and isn&#39;t</span>
<span class="p_add">+			 * mapped in the new pgd, we&#39;ll double-fault.  Forcibly</span>
<span class="p_add">+			 * map it.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			unsigned int stack_pgd_index =</span>
<span class="p_add">+				pgd_index(current_stack_pointer());</span>
<span class="p_add">+</span>
<span class="p_add">+			pgd_t *pgd = next-&gt;pgd + stack_pgd_index;</span>
<span class="p_add">+</span>
<span class="p_add">+			if (unlikely(pgd_none(*pgd)))</span>
<span class="p_add">+				set_pgd(pgd, init_mm.pgd[stack_pgd_index]);</span>
<span class="p_add">+		}</span>
 
<span class="p_del">-		if (unlikely(pgd_none(*pgd)))</span>
<span class="p_del">-			set_pgd(pgd, init_mm.pgd[stack_pgd_index]);</span>
<span class="p_del">-	}</span>
<span class="p_add">+		/* Stop remote flushes for the previous mm */</span>
<span class="p_add">+		if (cpumask_test_cpu(cpu, mm_cpumask(real_prev)))</span>
<span class="p_add">+			cpumask_clear_cpu(cpu, mm_cpumask(real_prev));</span>
 
<span class="p_del">-	this_cpu_write(cpu_tlbstate.loaded_mm, next);</span>
<span class="p_del">-	this_cpu_write(cpu_tlbstate.ctxs[0].ctx_id, next-&gt;context.ctx_id);</span>
<span class="p_del">-	this_cpu_write(cpu_tlbstate.ctxs[0].tlb_gen,</span>
<span class="p_del">-		       atomic64_read(&amp;next-&gt;context.tlb_gen));</span>
<span class="p_add">+		WARN_ON_ONCE(cpumask_test_cpu(cpu, mm_cpumask(next)));</span>
 
<span class="p_del">-	WARN_ON_ONCE(cpumask_test_cpu(cpu, mm_cpumask(next)));</span>
<span class="p_del">-	cpumask_set_cpu(cpu, mm_cpumask(next));</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Start remote flushes and then read tlb_gen.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		cpumask_set_cpu(cpu, mm_cpumask(next));</span>
<span class="p_add">+		next_tlb_gen = atomic64_read(&amp;next-&gt;context.tlb_gen);</span>
 
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Re-load page tables.</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 * This logic has an ordering constraint:</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 *  CPU 0: Write to a PTE for &#39;next&#39;</span>
<span class="p_del">-	 *  CPU 0: load bit 1 in mm_cpumask.  if nonzero, send IPI.</span>
<span class="p_del">-	 *  CPU 1: set bit 1 in next&#39;s mm_cpumask</span>
<span class="p_del">-	 *  CPU 1: load from the PTE that CPU 0 writes (implicit)</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 * We need to prevent an outcome in which CPU 1 observes</span>
<span class="p_del">-	 * the new PTE value and CPU 0 observes bit 1 clear in</span>
<span class="p_del">-	 * mm_cpumask.  (If that occurs, then the IPI will never</span>
<span class="p_del">-	 * be sent, and CPU 0&#39;s TLB will contain a stale entry.)</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 * The bad outcome can occur if either CPU&#39;s load is</span>
<span class="p_del">-	 * reordered before that CPU&#39;s store, so both CPUs must</span>
<span class="p_del">-	 * execute full barriers to prevent this from happening.</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 * Thus, switch_mm needs a full barrier between the</span>
<span class="p_del">-	 * store to mm_cpumask and any operation that could load</span>
<span class="p_del">-	 * from next-&gt;pgd.  TLB fills are special and can happen</span>
<span class="p_del">-	 * due to instruction fetches or for no reason at all,</span>
<span class="p_del">-	 * and neither LOCK nor MFENCE orders them.</span>
<span class="p_del">-	 * Fortunately, load_cr3() is serializing and gives the</span>
<span class="p_del">-	 * ordering guarantee we need.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	load_cr3(next-&gt;pgd);</span>
<span class="p_add">+		VM_BUG_ON(this_cpu_read(cpu_tlbstate.ctxs[0].ctx_id) ==</span>
<span class="p_add">+			  next-&gt;context.ctx_id);</span>
 
<span class="p_del">-	/*</span>
<span class="p_del">-	 * This gets called via leave_mm() in the idle path where RCU</span>
<span class="p_del">-	 * functions differently.  Tracing normally uses RCU, so we have to</span>
<span class="p_del">-	 * call the tracepoint specially here.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	trace_tlb_flush_rcuidle(TLB_FLUSH_ON_TASK_SWITCH, TLB_FLUSH_ALL);</span>
<span class="p_add">+		this_cpu_write(cpu_tlbstate.ctxs[0].ctx_id,</span>
<span class="p_add">+			       next-&gt;context.ctx_id);</span>
<span class="p_add">+		this_cpu_write(cpu_tlbstate.ctxs[0].tlb_gen,</span>
<span class="p_add">+			       next_tlb_gen);</span>
<span class="p_add">+		this_cpu_write(cpu_tlbstate.loaded_mm, next);</span>
<span class="p_add">+		write_cr3(__pa(next-&gt;pgd));</span>
 
<span class="p_del">-	/* Stop flush ipis for the previous mm */</span>
<span class="p_del">-	WARN_ON_ONCE(!cpumask_test_cpu(cpu, mm_cpumask(real_prev)) &amp;&amp;</span>
<span class="p_del">-		     real_prev != &amp;init_mm);</span>
<span class="p_del">-	cpumask_clear_cpu(cpu, mm_cpumask(real_prev));</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * This gets called via leave_mm() in the idle path where RCU</span>
<span class="p_add">+		 * functions differently.  Tracing normally uses RCU, so we</span>
<span class="p_add">+		 * have to call the tracepoint specially here.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		trace_tlb_flush_rcuidle(TLB_FLUSH_ON_TASK_SWITCH,</span>
<span class="p_add">+					TLB_FLUSH_ALL);</span>
<span class="p_add">+	}</span>
 
<span class="p_del">-	/* Load per-mm CR4 and LDTR state */</span>
 	load_mm_cr4(next);
 	switch_ldt(real_prev, next);
 }
 
<span class="p_del">-/*</span>
<span class="p_del">- * The flush IPI assumes that a thread switch happens in this order:</span>
<span class="p_del">- * [cpu0: the cpu that switches]</span>
<span class="p_del">- * 1) switch_mm() either 1a) or 1b)</span>
<span class="p_del">- * 1a) thread switch to a different mm</span>
<span class="p_del">- * 1a1) set cpu_tlbstate to TLBSTATE_OK</span>
<span class="p_del">- *	Now the tlb flush NMI handler flush_tlb_func won&#39;t call leave_mm</span>
<span class="p_del">- *	if cpu0 was in lazy tlb mode.</span>
<span class="p_del">- * 1a2) update cpu active_mm</span>
<span class="p_del">- *	Now cpu0 accepts tlb flushes for the new mm.</span>
<span class="p_del">- * 1a3) cpu_set(cpu, new_mm-&gt;cpu_vm_mask);</span>
<span class="p_del">- *	Now the other cpus will send tlb flush ipis.</span>
<span class="p_del">- * 1a4) change cr3.</span>
<span class="p_del">- * 1a5) cpu_clear(cpu, old_mm-&gt;cpu_vm_mask);</span>
<span class="p_del">- *	Stop ipi delivery for the old mm. This is not synchronized with</span>
<span class="p_del">- *	the other cpus, but flush_tlb_func ignore flush ipis for the wrong</span>
<span class="p_del">- *	mm, and in the worst case we perform a superfluous tlb flush.</span>
<span class="p_del">- * 1b) thread switch without mm change</span>
<span class="p_del">- *	cpu active_mm is correct, cpu0 already handles flush ipis.</span>
<span class="p_del">- * 1b1) set cpu_tlbstate to TLBSTATE_OK</span>
<span class="p_del">- * 1b2) test_and_set the cpu bit in cpu_vm_mask.</span>
<span class="p_del">- *	Atomically set the bit [other cpus will start sending flush ipis],</span>
<span class="p_del">- *	and test the bit.</span>
<span class="p_del">- * 1b3) if the bit was 0: leave_mm was called, flush the tlb.</span>
<span class="p_del">- * 2) switch %%esp, ie current</span>
<span class="p_del">- *</span>
<span class="p_del">- * The interrupt must handle 2 special cases:</span>
<span class="p_del">- * - cr3 is changed before %%esp, ie. it cannot use current-&gt;{active_,}mm.</span>
<span class="p_del">- * - the cpu performs speculative tlb reads, i.e. even if the cpu only</span>
<span class="p_del">- *   runs in kernel space, the cpu could load tlb entries for user space</span>
<span class="p_del">- *   pages.</span>
<span class="p_del">- *</span>
<span class="p_del">- * The good news is that cpu_tlbstate is local to each cpu, no</span>
<span class="p_del">- * write/read ordering problems.</span>
<span class="p_del">- */</span>
<span class="p_del">-</span>
 static void flush_tlb_func_common(const struct flush_tlb_info *f,
 				  bool local, enum tlb_flush_reason reason)
 {
<span class="p_chunk">@@ -209,15 +194,19 @@</span> <span class="p_context"> static void flush_tlb_func_common(const struct flush_tlb_info *f,</span>
 	u64 mm_tlb_gen = atomic64_read(&amp;loaded_mm-&gt;context.tlb_gen);
 	u64 local_tlb_gen = this_cpu_read(cpu_tlbstate.ctxs[0].tlb_gen);
 
<span class="p_add">+	/* This code cannot presently handle being reentered. */</span>
<span class="p_add">+	VM_WARN_ON(!irqs_disabled());</span>
<span class="p_add">+</span>
 	VM_WARN_ON(this_cpu_read(cpu_tlbstate.ctxs[0].ctx_id) !=
 		   loaded_mm-&gt;context.ctx_id);
 
<span class="p_del">-	if (this_cpu_read(cpu_tlbstate.state) != TLBSTATE_OK) {</span>
<span class="p_add">+	if (!cpumask_test_cpu(smp_processor_id(), mm_cpumask(loaded_mm))) {</span>
 		/*
<span class="p_del">-		 * leave_mm() is adequate to handle any type of flush, and</span>
<span class="p_del">-		 * we would prefer not to receive further IPIs.</span>
<span class="p_add">+		 * We&#39;re in lazy mode -- don&#39;t flush.  We can get here on</span>
<span class="p_add">+		 * remote flushes due to races and on local flushes if a</span>
<span class="p_add">+		 * kernel thread coincidentally flushes the mm it&#39;s lazily</span>
<span class="p_add">+		 * still using.</span>
 		 */
<span class="p_del">-		leave_mm(smp_processor_id());</span>
 		return;
 	}
 
<span class="p_chunk">@@ -314,6 +303,21 @@</span> <span class="p_context"> void native_flush_tlb_others(const struct cpumask *cpumask,</span>
 				(info-&gt;end - info-&gt;start) &gt;&gt; PAGE_SHIFT);
 
 	if (is_uv_system()) {
<span class="p_add">+		/*</span>
<span class="p_add">+		 * This whole special case is confused.  UV has a &quot;Broadcast</span>
<span class="p_add">+		 * Assist Unit&quot;, which seems to be a fancy way to send IPIs.</span>
<span class="p_add">+		 * Back when x86 used an explicit TLB flush IPI, UV was</span>
<span class="p_add">+		 * optimized to use its own mechanism.  These days, x86 uses</span>
<span class="p_add">+		 * smp_call_function_many(), but UV still uses a manual IPI,</span>
<span class="p_add">+		 * and that IPI&#39;s action is out of date -- it does a manual</span>
<span class="p_add">+		 * flush instead of calling flush_tlb_func_remote().  This</span>
<span class="p_add">+		 * means that the percpu tlb_gen variables won&#39;t be updated</span>
<span class="p_add">+		 * and we&#39;ll do pointless flushes on future context switches.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * Rather than hooking native_flush_tlb_others() here, I think</span>
<span class="p_add">+		 * that UV should be updated so that smp_call_function_many(),</span>
<span class="p_add">+		 * etc, are optimal on UV.</span>
<span class="p_add">+		 */</span>
 		unsigned int cpu;
 
 		cpu = smp_processor_id();
<span class="p_chunk">@@ -364,10 +368,15 @@</span> <span class="p_context"> void flush_tlb_mm_range(struct mm_struct *mm, unsigned long start,</span>
 		info.end = TLB_FLUSH_ALL;
 	}
 
<span class="p_del">-	if (mm == this_cpu_read(cpu_tlbstate.loaded_mm))</span>
<span class="p_add">+	if (mm == this_cpu_read(cpu_tlbstate.loaded_mm)) {</span>
<span class="p_add">+		local_irq_disable();</span>
 		flush_tlb_func_local(&amp;info, TLB_LOCAL_MM_SHOOTDOWN);
<span class="p_add">+		local_irq_enable();</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	if (cpumask_any_but(mm_cpumask(mm), cpu) &lt; nr_cpu_ids)
 		flush_tlb_others(mm_cpumask(mm), &amp;info);
<span class="p_add">+</span>
 	put_cpu();
 }
 
<span class="p_chunk">@@ -376,8 +385,6 @@</span> <span class="p_context"> static void do_flush_tlb_all(void *info)</span>
 {
 	count_vm_tlb_event(NR_TLB_REMOTE_FLUSH_RECEIVED);
 	__flush_tlb_all();
<span class="p_del">-	if (this_cpu_read(cpu_tlbstate.state) == TLBSTATE_LAZY)</span>
<span class="p_del">-		leave_mm(smp_processor_id());</span>
 }
 
 void flush_tlb_all(void)
<span class="p_chunk">@@ -421,10 +428,15 @@</span> <span class="p_context"> void arch_tlbbatch_flush(struct arch_tlbflush_unmap_batch *batch)</span>
 
 	int cpu = get_cpu();
 
<span class="p_del">-	if (cpumask_test_cpu(cpu, &amp;batch-&gt;cpumask))</span>
<span class="p_add">+	if (cpumask_test_cpu(cpu, &amp;batch-&gt;cpumask)) {</span>
<span class="p_add">+		local_irq_disable();</span>
 		flush_tlb_func_local(&amp;info, TLB_LOCAL_SHOOTDOWN);
<span class="p_add">+		local_irq_enable();</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	if (cpumask_any_but(&amp;batch-&gt;cpumask, cpu) &lt; nr_cpu_ids)
 		flush_tlb_others(&amp;batch-&gt;cpumask, &amp;info);
<span class="p_add">+</span>
 	cpumask_clear(&amp;batch-&gt;cpumask);
 
 	put_cpu();

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



