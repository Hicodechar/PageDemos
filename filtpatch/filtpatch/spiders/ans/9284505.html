
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>Linux 3.14.76 - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    Linux 3.14.76</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=37061">gregkh@linuxfoundation.org</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Aug. 16, 2016, 8:06 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20160816200650.GB9413@kroah.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9284505/mbox/"
   >mbox</a>
|
   <a href="/patch/9284505/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9284505/">/patch/9284505/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	8416060839 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 16 Aug 2016 20:07:02 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 7074028712
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 16 Aug 2016 20:07:02 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 64F2D2875A; Tue, 16 Aug 2016 20:07:02 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id D4BB52876B
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 16 Aug 2016 20:06:59 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S932339AbcHPUGs (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 16 Aug 2016 16:06:48 -0400
Received: from mail.linuxfoundation.org ([140.211.169.12]:37485 &quot;EHLO
	mail.linuxfoundation.org&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1753709AbcHPUGm (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 16 Aug 2016 16:06:42 -0400
Received: from localhost (pes75-3-78-192-101-3.fbxo.proxad.net
	[78.192.101.3])
	by mail.linuxfoundation.org (Postfix) with ESMTPSA id 260819F3;
	Tue, 16 Aug 2016 20:06:40 +0000 (UTC)
Date: Tue, 16 Aug 2016 22:06:50 +0200
From: Greg KH &lt;gregkh@linuxfoundation.org&gt;
To: linux-kernel@vger.kernel.org, Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	torvalds@linux-foundation.org, stable@vger.kernel.org
Cc: lwn@lwn.net, Jiri Slaby &lt;jslaby@suse.cz&gt;
Subject: Re: Linux 3.14.76
Message-ID: &lt;20160816200650.GB9413@kroah.com&gt;
References: &lt;20160816200643.GA9413@kroah.com&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Disposition: inline
In-Reply-To: &lt;20160816200643.GA9413@kroah.com&gt;
User-Agent: Mutt/1.6.2 (2016-07-01)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=37061">gregkh@linuxfoundation.org</a> - Aug. 16, 2016, 8:06 p.m.</div>
<pre class="content">

</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/Makefile b/Makefile</span>
<span class="p_header">index 9e6e6131e986..306fd306906b 100644</span>
<span class="p_header">--- a/Makefile</span>
<span class="p_header">+++ b/Makefile</span>
<span class="p_chunk">@@ -1,6 +1,6 @@</span> <span class="p_context"></span>
 VERSION = 3
 PATCHLEVEL = 14
<span class="p_del">-SUBLEVEL = 75</span>
<span class="p_add">+SUBLEVEL = 76</span>
 EXTRAVERSION =
 NAME = Remembering Coco
 
<span class="p_header">diff --git a/arch/arm/kernel/sys_oabi-compat.c b/arch/arm/kernel/sys_oabi-compat.c</span>
<span class="p_header">index 3e94811690ce..a0aee80b608d 100644</span>
<span class="p_header">--- a/arch/arm/kernel/sys_oabi-compat.c</span>
<span class="p_header">+++ b/arch/arm/kernel/sys_oabi-compat.c</span>
<span class="p_chunk">@@ -275,8 +275,12 @@</span> <span class="p_context"> asmlinkage long sys_oabi_epoll_wait(int epfd,</span>
 	mm_segment_t fs;
 	long ret, err, i;
 
<span class="p_del">-	if (maxevents &lt;= 0 || maxevents &gt; (INT_MAX/sizeof(struct epoll_event)))</span>
<span class="p_add">+	if (maxevents &lt;= 0 ||</span>
<span class="p_add">+			maxevents &gt; (INT_MAX/sizeof(*kbuf)) ||</span>
<span class="p_add">+			maxevents &gt; (INT_MAX/sizeof(*events)))</span>
 		return -EINVAL;
<span class="p_add">+	if (!access_ok(VERIFY_WRITE, events, sizeof(*events) * maxevents))</span>
<span class="p_add">+		return -EFAULT;</span>
 	kbuf = kmalloc(sizeof(*kbuf) * maxevents, GFP_KERNEL);
 	if (!kbuf)
 		return -ENOMEM;
<span class="p_chunk">@@ -313,6 +317,8 @@</span> <span class="p_context"> asmlinkage long sys_oabi_semtimedop(int semid,</span>
 
 	if (nsops &lt; 1 || nsops &gt; SEMOPM)
 		return -EINVAL;
<span class="p_add">+	if (!access_ok(VERIFY_READ, tsops, sizeof(*tsops) * nsops))</span>
<span class="p_add">+		return -EFAULT;</span>
 	sops = kmalloc(sizeof(*sops) * nsops, GFP_KERNEL);
 	if (!sops)
 		return -ENOMEM;
<span class="p_header">diff --git a/arch/mips/kernel/scall64-n32.S b/arch/mips/kernel/scall64-n32.S</span>
<span class="p_header">index f7e5b72cf481..79747b85777a 100644</span>
<span class="p_header">--- a/arch/mips/kernel/scall64-n32.S</span>
<span class="p_header">+++ b/arch/mips/kernel/scall64-n32.S</span>
<span class="p_chunk">@@ -350,7 +350,7 @@</span> <span class="p_context"> EXPORT(sysn32_call_table)</span>
 	PTR	sys_ni_syscall			/* available, was setaltroot */
 	PTR	sys_add_key
 	PTR	sys_request_key
<span class="p_del">-	PTR	sys_keyctl			/* 6245 */</span>
<span class="p_add">+	PTR	compat_sys_keyctl		/* 6245 */</span>
 	PTR	sys_set_thread_area
 	PTR	sys_inotify_init
 	PTR	sys_inotify_add_watch
<span class="p_header">diff --git a/arch/mips/kernel/scall64-o32.S b/arch/mips/kernel/scall64-o32.S</span>
<span class="p_header">index 6788727d91af..af499022f3fb 100644</span>
<span class="p_header">--- a/arch/mips/kernel/scall64-o32.S</span>
<span class="p_header">+++ b/arch/mips/kernel/scall64-o32.S</span>
<span class="p_chunk">@@ -474,7 +474,7 @@</span> <span class="p_context"> EXPORT(sys32_call_table)</span>
 	PTR	sys_ni_syscall			/* available, was setaltroot */
 	PTR	sys_add_key			/* 4280 */
 	PTR	sys_request_key
<span class="p_del">-	PTR	sys_keyctl</span>
<span class="p_add">+	PTR	compat_sys_keyctl</span>
 	PTR	sys_set_thread_area
 	PTR	sys_inotify_init
 	PTR	sys_inotify_add_watch		/* 4285 */
<span class="p_header">diff --git a/arch/x86/include/asm/mmu_context.h b/arch/x86/include/asm/mmu_context.h</span>
<span class="p_header">index be12c534fd59..29a3d1b00ca9 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/mmu_context.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/mmu_context.h</span>
<span class="p_chunk">@@ -42,7 +42,34 @@</span> <span class="p_context"> static inline void switch_mm(struct mm_struct *prev, struct mm_struct *next,</span>
 #endif
 		cpumask_set_cpu(cpu, mm_cpumask(next));
 
<span class="p_del">-		/* Re-load page tables */</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Re-load page tables.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * This logic has an ordering constraint:</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 *  CPU 0: Write to a PTE for &#39;next&#39;</span>
<span class="p_add">+		 *  CPU 0: load bit 1 in mm_cpumask.  if nonzero, send IPI.</span>
<span class="p_add">+		 *  CPU 1: set bit 1 in next&#39;s mm_cpumask</span>
<span class="p_add">+		 *  CPU 1: load from the PTE that CPU 0 writes (implicit)</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * We need to prevent an outcome in which CPU 1 observes</span>
<span class="p_add">+		 * the new PTE value and CPU 0 observes bit 1 clear in</span>
<span class="p_add">+		 * mm_cpumask.  (If that occurs, then the IPI will never</span>
<span class="p_add">+		 * be sent, and CPU 0&#39;s TLB will contain a stale entry.)</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * The bad outcome can occur if either CPU&#39;s load is</span>
<span class="p_add">+		 * reordered before that CPU&#39;s store, so both CPUs must</span>
<span class="p_add">+		 * execute full barriers to prevent this from happening.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * Thus, switch_mm needs a full barrier between the</span>
<span class="p_add">+		 * store to mm_cpumask and any operation that could load</span>
<span class="p_add">+		 * from next-&gt;pgd.  TLB fills are special and can happen</span>
<span class="p_add">+		 * due to instruction fetches or for no reason at all,</span>
<span class="p_add">+		 * and neither LOCK nor MFENCE orders them.</span>
<span class="p_add">+		 * Fortunately, load_cr3() is serializing and gives the</span>
<span class="p_add">+		 * ordering guarantee we need.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 */</span>
 		load_cr3(next-&gt;pgd);
 
 		/* Stop flush ipis for the previous mm */
<span class="p_chunk">@@ -65,10 +92,14 @@</span> <span class="p_context"> static inline void switch_mm(struct mm_struct *prev, struct mm_struct *next,</span>
 			 * schedule, protecting us from simultaneous changes.
 			 */
 			cpumask_set_cpu(cpu, mm_cpumask(next));
<span class="p_add">+</span>
 			/*
 			 * We were in lazy tlb mode and leave_mm disabled
 			 * tlb flush IPI delivery. We must reload CR3
 			 * to make sure to use no freed page tables.
<span class="p_add">+			 *</span>
<span class="p_add">+			 * As above, load_cr3() is serializing and orders TLB</span>
<span class="p_add">+			 * fills with respect to the mm_cpumask write.</span>
 			 */
 			load_cr3(next-&gt;pgd);
 			load_LDT_nolock(&amp;next-&gt;context);
<span class="p_header">diff --git a/arch/x86/mm/tlb.c b/arch/x86/mm/tlb.c</span>
<span class="p_header">index dd8dda167a24..46e82e75192e 100644</span>
<span class="p_header">--- a/arch/x86/mm/tlb.c</span>
<span class="p_header">+++ b/arch/x86/mm/tlb.c</span>
<span class="p_chunk">@@ -152,7 +152,10 @@</span> <span class="p_context"> void flush_tlb_current_task(void)</span>
 	preempt_disable();
 
 	count_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ALL);
<span class="p_add">+</span>
<span class="p_add">+	/* This is an implicit full barrier that synchronizes with switch_mm. */</span>
 	local_flush_tlb();
<span class="p_add">+</span>
 	if (cpumask_any_but(mm_cpumask(mm), smp_processor_id()) &lt; nr_cpu_ids)
 		flush_tlb_others(mm_cpumask(mm), mm, 0UL, TLB_FLUSH_ALL);
 	preempt_enable();
<span class="p_chunk">@@ -166,11 +169,19 @@</span> <span class="p_context"> void flush_tlb_mm_range(struct mm_struct *mm, unsigned long start,</span>
 	unsigned long nr_base_pages;
 
 	preempt_disable();
<span class="p_del">-	if (current-&gt;active_mm != mm)</span>
<span class="p_add">+	if (current-&gt;active_mm != mm) {</span>
<span class="p_add">+		/* Synchronize with switch_mm. */</span>
<span class="p_add">+		smp_mb();</span>
<span class="p_add">+</span>
 		goto flush_all;
<span class="p_add">+	}</span>
 
 	if (!current-&gt;mm) {
 		leave_mm(smp_processor_id());
<span class="p_add">+</span>
<span class="p_add">+		/* Synchronize with switch_mm. */</span>
<span class="p_add">+		smp_mb();</span>
<span class="p_add">+</span>
 		goto flush_all;
 	}
 
<span class="p_chunk">@@ -222,10 +233,18 @@</span> <span class="p_context"> void flush_tlb_page(struct vm_area_struct *vma, unsigned long start)</span>
 	preempt_disable();
 
 	if (current-&gt;active_mm == mm) {
<span class="p_del">-		if (current-&gt;mm)</span>
<span class="p_add">+		if (current-&gt;mm) {</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * Implicit full barrier (INVLPG) that synchronizes</span>
<span class="p_add">+			 * with switch_mm.</span>
<span class="p_add">+			 */</span>
 			__flush_tlb_one(start);
<span class="p_del">-		else</span>
<span class="p_add">+		} else {</span>
 			leave_mm(smp_processor_id());
<span class="p_add">+</span>
<span class="p_add">+			/* Synchronize with switch_mm. */</span>
<span class="p_add">+			smp_mb();</span>
<span class="p_add">+		}</span>
 	}
 
 	if (cpumask_any_but(mm_cpumask(mm), smp_processor_id()) &lt; nr_cpu_ids)
<span class="p_header">diff --git a/block/genhd.c b/block/genhd.c</span>
<span class="p_header">index 9316f5fd416f..38d4ba122a43 100644</span>
<span class="p_header">--- a/block/genhd.c</span>
<span class="p_header">+++ b/block/genhd.c</span>
<span class="p_chunk">@@ -829,6 +829,7 @@</span> <span class="p_context"> static void disk_seqf_stop(struct seq_file *seqf, void *v)</span>
 	if (iter) {
 		class_dev_iter_exit(iter);
 		kfree(iter);
<span class="p_add">+		seqf-&gt;private = NULL;</span>
 	}
 }
 
<span class="p_header">diff --git a/crypto/gcm.c b/crypto/gcm.c</span>
<span class="p_header">index f0bd00b15f26..d2a0f7371cf0 100644</span>
<span class="p_header">--- a/crypto/gcm.c</span>
<span class="p_header">+++ b/crypto/gcm.c</span>
<span class="p_chunk">@@ -716,7 +716,9 @@</span> <span class="p_context"> static struct crypto_instance *crypto_gcm_alloc_common(struct rtattr **tb,</span>
 
 	ghash_alg = crypto_find_alg(ghash_name, &amp;crypto_ahash_type,
 				    CRYPTO_ALG_TYPE_HASH,
<span class="p_del">-				    CRYPTO_ALG_TYPE_AHASH_MASK);</span>
<span class="p_add">+				    CRYPTO_ALG_TYPE_AHASH_MASK |</span>
<span class="p_add">+				    crypto_requires_sync(algt-&gt;type,</span>
<span class="p_add">+							 algt-&gt;mask));</span>
 	if (IS_ERR(ghash_alg))
 		return ERR_CAST(ghash_alg);
 
<span class="p_header">diff --git a/crypto/scatterwalk.c b/crypto/scatterwalk.c</span>
<span class="p_header">index 79ca2278c2a3..0ec7a6fa3d4d 100644</span>
<span class="p_header">--- a/crypto/scatterwalk.c</span>
<span class="p_header">+++ b/crypto/scatterwalk.c</span>
<span class="p_chunk">@@ -68,7 +68,8 @@</span> <span class="p_context"> static void scatterwalk_pagedone(struct scatter_walk *walk, int out,</span>
 
 void scatterwalk_done(struct scatter_walk *walk, int out, int more)
 {
<span class="p_del">-	if (!(scatterwalk_pagelen(walk) &amp; (PAGE_SIZE - 1)) || !more)</span>
<span class="p_add">+	if (!more || walk-&gt;offset &gt;= walk-&gt;sg-&gt;offset + walk-&gt;sg-&gt;length ||</span>
<span class="p_add">+	    !(walk-&gt;offset &amp; (PAGE_SIZE - 1)))</span>
 		scatterwalk_pagedone(walk, out, more);
 }
 EXPORT_SYMBOL_GPL(scatterwalk_done);
<span class="p_header">diff --git a/drivers/char/random.c b/drivers/char/random.c</span>
<span class="p_header">index 8a64dbeae7b1..d20ac1997886 100644</span>
<span class="p_header">--- a/drivers/char/random.c</span>
<span class="p_header">+++ b/drivers/char/random.c</span>
<span class="p_chunk">@@ -698,15 +698,18 @@</span> <span class="p_context"> retry:</span>
 	}
 }
 
<span class="p_del">-static void credit_entropy_bits_safe(struct entropy_store *r, int nbits)</span>
<span class="p_add">+static int credit_entropy_bits_safe(struct entropy_store *r, int nbits)</span>
 {
 	const int nbits_max = (int)(~0U &gt;&gt; (ENTROPY_SHIFT + 1));
 
<span class="p_add">+	if (nbits &lt; 0)</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+</span>
 	/* Cap the value to avoid overflows */
 	nbits = min(nbits,  nbits_max);
<span class="p_del">-	nbits = max(nbits, -nbits_max);</span>
 
 	credit_entropy_bits(r, nbits);
<span class="p_add">+	return 0;</span>
 }
 
 /*********************************************************************
<span class="p_chunk">@@ -1420,8 +1423,7 @@</span> <span class="p_context"> static long random_ioctl(struct file *f, unsigned int cmd, unsigned long arg)</span>
 			return -EPERM;
 		if (get_user(ent_count, p))
 			return -EFAULT;
<span class="p_del">-		credit_entropy_bits_safe(&amp;input_pool, ent_count);</span>
<span class="p_del">-		return 0;</span>
<span class="p_add">+		return credit_entropy_bits_safe(&amp;input_pool, ent_count);</span>
 	case RNDADDENTROPY:
 		if (!capable(CAP_SYS_ADMIN))
 			return -EPERM;
<span class="p_chunk">@@ -1435,8 +1437,7 @@</span> <span class="p_context"> static long random_ioctl(struct file *f, unsigned int cmd, unsigned long arg)</span>
 				    size);
 		if (retval &lt; 0)
 			return retval;
<span class="p_del">-		credit_entropy_bits_safe(&amp;input_pool, ent_count);</span>
<span class="p_del">-		return 0;</span>
<span class="p_add">+		return credit_entropy_bits_safe(&amp;input_pool, ent_count);</span>
 	case RNDZAPENTCNT:
 	case RNDCLEARPOOL:
 		/*
<span class="p_header">diff --git a/drivers/infiniband/core/ucm.c b/drivers/infiniband/core/ucm.c</span>
<span class="p_header">index f2f63933e8a9..5befec118a18 100644</span>
<span class="p_header">--- a/drivers/infiniband/core/ucm.c</span>
<span class="p_header">+++ b/drivers/infiniband/core/ucm.c</span>
<span class="p_chunk">@@ -48,6 +48,7 @@</span> <span class="p_context"></span>
 
 #include &lt;asm/uaccess.h&gt;
 
<span class="p_add">+#include &lt;rdma/ib.h&gt;</span>
 #include &lt;rdma/ib_cm.h&gt;
 #include &lt;rdma/ib_user_cm.h&gt;
 #include &lt;rdma/ib_marshall.h&gt;
<span class="p_chunk">@@ -1104,6 +1105,9 @@</span> <span class="p_context"> static ssize_t ib_ucm_write(struct file *filp, const char __user *buf,</span>
 	struct ib_ucm_cmd_hdr hdr;
 	ssize_t result;
 
<span class="p_add">+	if (WARN_ON_ONCE(!ib_safe_file_access(filp)))</span>
<span class="p_add">+		return -EACCES;</span>
<span class="p_add">+</span>
 	if (len &lt; sizeof(hdr))
 		return -EINVAL;
 
<span class="p_header">diff --git a/drivers/infiniband/core/ucma.c b/drivers/infiniband/core/ucma.c</span>
<span class="p_header">index 45d67e9228d7..81dd84d0b68b 100644</span>
<span class="p_header">--- a/drivers/infiniband/core/ucma.c</span>
<span class="p_header">+++ b/drivers/infiniband/core/ucma.c</span>
<span class="p_chunk">@@ -1487,6 +1487,9 @@</span> <span class="p_context"> static ssize_t ucma_write(struct file *filp, const char __user *buf,</span>
 	struct rdma_ucm_cmd_hdr hdr;
 	ssize_t ret;
 
<span class="p_add">+	if (WARN_ON_ONCE(!ib_safe_file_access(filp)))</span>
<span class="p_add">+		return -EACCES;</span>
<span class="p_add">+</span>
 	if (len &lt; sizeof(hdr))
 		return -EINVAL;
 
<span class="p_header">diff --git a/drivers/infiniband/core/uverbs_main.c b/drivers/infiniband/core/uverbs_main.c</span>
<span class="p_header">index 8802d5ccd93d..f3ecfe4b9571 100644</span>
<span class="p_header">--- a/drivers/infiniband/core/uverbs_main.c</span>
<span class="p_header">+++ b/drivers/infiniband/core/uverbs_main.c</span>
<span class="p_chunk">@@ -48,6 +48,8 @@</span> <span class="p_context"></span>
 
 #include &lt;asm/uaccess.h&gt;
 
<span class="p_add">+#include &lt;rdma/ib.h&gt;</span>
<span class="p_add">+</span>
 #include &quot;uverbs.h&quot;
 
 MODULE_AUTHOR(&quot;Roland Dreier&quot;);
<span class="p_chunk">@@ -605,6 +607,9 @@</span> <span class="p_context"> static ssize_t ib_uverbs_write(struct file *filp, const char __user *buf,</span>
 	struct ib_uverbs_cmd_hdr hdr;
 	__u32 flags;
 
<span class="p_add">+	if (WARN_ON_ONCE(!ib_safe_file_access(filp)))</span>
<span class="p_add">+		return -EACCES;</span>
<span class="p_add">+</span>
 	if (count &lt; sizeof hdr)
 		return -EINVAL;
 
<span class="p_header">diff --git a/drivers/infiniband/hw/ipath/ipath_file_ops.c b/drivers/infiniband/hw/ipath/ipath_file_ops.c</span>
<span class="p_header">index 6d7f453b4d05..34aeb14f486a 100644</span>
<span class="p_header">--- a/drivers/infiniband/hw/ipath/ipath_file_ops.c</span>
<span class="p_header">+++ b/drivers/infiniband/hw/ipath/ipath_file_ops.c</span>
<span class="p_chunk">@@ -45,6 +45,8 @@</span> <span class="p_context"></span>
 #include &lt;linux/cpu.h&gt;
 #include &lt;asm/pgtable.h&gt;
 
<span class="p_add">+#include &quot;rdma/ib.h&quot;</span>
<span class="p_add">+</span>
 #include &quot;ipath_kernel.h&quot;
 #include &quot;ipath_common.h&quot;
 #include &quot;ipath_user_sdma.h&quot;
<span class="p_chunk">@@ -2240,6 +2242,9 @@</span> <span class="p_context"> static ssize_t ipath_write(struct file *fp, const char __user *data,</span>
 	ssize_t ret = 0;
 	void *dest;
 
<span class="p_add">+	if (WARN_ON_ONCE(!ib_safe_file_access(fp)))</span>
<span class="p_add">+		return -EACCES;</span>
<span class="p_add">+</span>
 	if (count &lt; sizeof(cmd.type)) {
 		ret = -EINVAL;
 		goto bail;
<span class="p_header">diff --git a/drivers/infiniband/hw/qib/qib_file_ops.c b/drivers/infiniband/hw/qib/qib_file_ops.c</span>
<span class="p_header">index 2023cd61b897..3c089ca85c64 100644</span>
<span class="p_header">--- a/drivers/infiniband/hw/qib/qib_file_ops.c</span>
<span class="p_header">+++ b/drivers/infiniband/hw/qib/qib_file_ops.c</span>
<span class="p_chunk">@@ -45,6 +45,8 @@</span> <span class="p_context"></span>
 #include &lt;linux/delay.h&gt;
 #include &lt;linux/export.h&gt;
 
<span class="p_add">+#include &lt;rdma/ib.h&gt;</span>
<span class="p_add">+</span>
 #include &quot;qib.h&quot;
 #include &quot;qib_common.h&quot;
 #include &quot;qib_user_sdma.h&quot;
<span class="p_chunk">@@ -2058,6 +2060,9 @@</span> <span class="p_context"> static ssize_t qib_write(struct file *fp, const char __user *data,</span>
 	ssize_t ret = 0;
 	void *dest;
 
<span class="p_add">+	if (WARN_ON_ONCE(!ib_safe_file_access(fp)))</span>
<span class="p_add">+		return -EACCES;</span>
<span class="p_add">+</span>
 	if (count &lt; sizeof(cmd.type)) {
 		ret = -EINVAL;
 		goto bail;
<span class="p_header">diff --git a/drivers/net/bonding/bond_netlink.c b/drivers/net/bonding/bond_netlink.c</span>
<span class="p_header">index 70651f8e8e3b..e6fc358add9f 100644</span>
<span class="p_header">--- a/drivers/net/bonding/bond_netlink.c</span>
<span class="p_header">+++ b/drivers/net/bonding/bond_netlink.c</span>
<span class="p_chunk">@@ -360,7 +360,11 @@</span> <span class="p_context"> static int bond_newlink(struct net *src_net, struct net_device *bond_dev,</span>
 	if (err &lt; 0)
 		return err;
 
<span class="p_del">-	return register_netdevice(bond_dev);</span>
<span class="p_add">+	err = register_netdevice(bond_dev);</span>
<span class="p_add">+</span>
<span class="p_add">+	netif_carrier_off(bond_dev);</span>
<span class="p_add">+</span>
<span class="p_add">+	return err;</span>
 }
 
 static size_t bond_get_size(const struct net_device *bond_dev)
<span class="p_header">diff --git a/drivers/net/usb/cdc_ncm.c b/drivers/net/usb/cdc_ncm.c</span>
<span class="p_header">index 584504e6e95c..df544c93735b 100644</span>
<span class="p_header">--- a/drivers/net/usb/cdc_ncm.c</span>
<span class="p_header">+++ b/drivers/net/usb/cdc_ncm.c</span>
<span class="p_chunk">@@ -571,24 +571,13 @@</span> <span class="p_context"> EXPORT_SYMBOL_GPL(cdc_ncm_select_altsetting);</span>
 
 static int cdc_ncm_bind(struct usbnet *dev, struct usb_interface *intf)
 {
<span class="p_del">-	int ret;</span>
<span class="p_del">-</span>
 	/* MBIM backwards compatible function? */
 	cdc_ncm_select_altsetting(dev, intf);
 	if (cdc_ncm_comm_intf_is_mbim(intf-&gt;cur_altsetting))
 		return -ENODEV;
 
 	/* NCM data altsetting is always 1 */
<span class="p_del">-	ret = cdc_ncm_bind_common(dev, intf, 1);</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * We should get an event when network connection is &quot;connected&quot; or</span>
<span class="p_del">-	 * &quot;disconnected&quot;. Set network connection in &quot;disconnected&quot; state</span>
<span class="p_del">-	 * (carrier is OFF) during attach, so the IP network stack does not</span>
<span class="p_del">-	 * start IPv6 negotiation and more.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	usbnet_link_change(dev, 0, 0);</span>
<span class="p_del">-	return ret;</span>
<span class="p_add">+	return cdc_ncm_bind_common(dev, intf, 1);</span>
 }
 
 static void cdc_ncm_align_tail(struct sk_buff *skb, size_t modulus, size_t remainder, size_t max)
<span class="p_chunk">@@ -1117,7 +1106,8 @@</span> <span class="p_context"> static int cdc_ncm_check_connect(struct usbnet *dev)</span>
 
 static const struct driver_info cdc_ncm_info = {
 	.description = &quot;CDC NCM&quot;,
<span class="p_del">-	.flags = FLAG_POINTTOPOINT | FLAG_NO_SETINT | FLAG_MULTI_PACKET,</span>
<span class="p_add">+	.flags = FLAG_POINTTOPOINT | FLAG_NO_SETINT | FLAG_MULTI_PACKET</span>
<span class="p_add">+			| FLAG_LINK_INTR,</span>
 	.bind = cdc_ncm_bind,
 	.unbind = cdc_ncm_unbind,
 	.check_connect = cdc_ncm_check_connect,
<span class="p_chunk">@@ -1131,7 +1121,7 @@</span> <span class="p_context"> static const struct driver_info cdc_ncm_info = {</span>
 static const struct driver_info wwan_info = {
 	.description = &quot;Mobile Broadband Network Device&quot;,
 	.flags = FLAG_POINTTOPOINT | FLAG_NO_SETINT | FLAG_MULTI_PACKET
<span class="p_del">-			| FLAG_WWAN,</span>
<span class="p_add">+			| FLAG_LINK_INTR | FLAG_WWAN,</span>
 	.bind = cdc_ncm_bind,
 	.unbind = cdc_ncm_unbind,
 	.check_connect = cdc_ncm_check_connect,
<span class="p_chunk">@@ -1145,7 +1135,7 @@</span> <span class="p_context"> static const struct driver_info wwan_info = {</span>
 static const struct driver_info wwan_noarp_info = {
 	.description = &quot;Mobile Broadband Network Device (NO ARP)&quot;,
 	.flags = FLAG_POINTTOPOINT | FLAG_NO_SETINT | FLAG_MULTI_PACKET
<span class="p_del">-			| FLAG_WWAN | FLAG_NOARP,</span>
<span class="p_add">+			| FLAG_LINK_INTR | FLAG_WWAN | FLAG_NOARP,</span>
 	.bind = cdc_ncm_bind,
 	.unbind = cdc_ncm_unbind,
 	.check_connect = cdc_ncm_check_connect,
<span class="p_header">diff --git a/drivers/scsi/scsi_lib.c b/drivers/scsi/scsi_lib.c</span>
<span class="p_header">index 719bd8257520..293155e0571d 100644</span>
<span class="p_header">--- a/drivers/scsi/scsi_lib.c</span>
<span class="p_header">+++ b/drivers/scsi/scsi_lib.c</span>
<span class="p_chunk">@@ -540,66 +540,6 @@</span> <span class="p_context"> void scsi_run_host_queues(struct Scsi_Host *shost)</span>
 
 static void __scsi_release_buffers(struct scsi_cmnd *, int);
 
<span class="p_del">-/*</span>
<span class="p_del">- * Function:    scsi_end_request()</span>
<span class="p_del">- *</span>
<span class="p_del">- * Purpose:     Post-processing of completed commands (usually invoked at end</span>
<span class="p_del">- *		of upper level post-processing and scsi_io_completion).</span>
<span class="p_del">- *</span>
<span class="p_del">- * Arguments:   cmd	 - command that is complete.</span>
<span class="p_del">- *              error    - 0 if I/O indicates success, &lt; 0 for I/O error.</span>
<span class="p_del">- *              bytes    - number of bytes of completed I/O</span>
<span class="p_del">- *		requeue  - indicates whether we should requeue leftovers.</span>
<span class="p_del">- *</span>
<span class="p_del">- * Lock status: Assumed that lock is not held upon entry.</span>
<span class="p_del">- *</span>
<span class="p_del">- * Returns:     cmd if requeue required, NULL otherwise.</span>
<span class="p_del">- *</span>
<span class="p_del">- * Notes:       This is called for block device requests in order to</span>
<span class="p_del">- *              mark some number of sectors as complete.</span>
<span class="p_del">- * </span>
<span class="p_del">- *		We are guaranteeing that the request queue will be goosed</span>
<span class="p_del">- *		at some point during this call.</span>
<span class="p_del">- * Notes:	If cmd was requeued, upon return it will be a stale pointer.</span>
<span class="p_del">- */</span>
<span class="p_del">-static struct scsi_cmnd *scsi_end_request(struct scsi_cmnd *cmd, int error,</span>
<span class="p_del">-					  int bytes, int requeue)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct request_queue *q = cmd-&gt;device-&gt;request_queue;</span>
<span class="p_del">-	struct request *req = cmd-&gt;request;</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * If there are blocks left over at the end, set up the command</span>
<span class="p_del">-	 * to queue the remainder of them.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	if (blk_end_request(req, error, bytes)) {</span>
<span class="p_del">-		/* kill remainder if no retrys */</span>
<span class="p_del">-		if (error &amp;&amp; scsi_noretry_cmd(cmd))</span>
<span class="p_del">-			blk_end_request_all(req, error);</span>
<span class="p_del">-		else {</span>
<span class="p_del">-			if (requeue) {</span>
<span class="p_del">-				/*</span>
<span class="p_del">-				 * Bleah.  Leftovers again.  Stick the</span>
<span class="p_del">-				 * leftovers in the front of the</span>
<span class="p_del">-				 * queue, and goose the queue again.</span>
<span class="p_del">-				 */</span>
<span class="p_del">-				scsi_release_buffers(cmd);</span>
<span class="p_del">-				scsi_requeue_command(q, cmd);</span>
<span class="p_del">-				cmd = NULL;</span>
<span class="p_del">-			}</span>
<span class="p_del">-			return cmd;</span>
<span class="p_del">-		}</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * This will goose the queue request function at the end, so we don&#39;t</span>
<span class="p_del">-	 * need to worry about launching another command.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	__scsi_release_buffers(cmd, 0);</span>
<span class="p_del">-	scsi_next_command(cmd);</span>
<span class="p_del">-	return NULL;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 static inline unsigned int scsi_sgtable_index(unsigned short nents)
 {
 	unsigned int index;
<span class="p_chunk">@@ -751,16 +691,9 @@</span> <span class="p_context"> static int __scsi_error_from_host_byte(struct scsi_cmnd *cmd, int result)</span>
  *
  * Returns:     Nothing
  *
<span class="p_del">- * Notes:       This function is matched in terms of capabilities to</span>
<span class="p_del">- *              the function that created the scatter-gather list.</span>
<span class="p_del">- *              In other words, if there are no bounce buffers</span>
<span class="p_del">- *              (the normal case for most drivers), we don&#39;t need</span>
<span class="p_del">- *              the logic to deal with cleaning up afterwards.</span>
<span class="p_del">- *</span>
<span class="p_del">- *		We must call scsi_end_request().  This will finish off</span>
<span class="p_del">- *		the specified number of sectors.  If we are done, the</span>
<span class="p_del">- *		command block will be released and the queue function</span>
<span class="p_del">- *		will be goosed.  If we are not done then we have to</span>
<span class="p_add">+ * Notes:       We will finish off the specified number of sectors.  If we</span>
<span class="p_add">+ *		are done, the command block will be released and the queue</span>
<span class="p_add">+ *		function will be goosed.  If we are not done then we have to</span>
  *		figure out what to do next:
  *
  *		a) We can call scsi_requeue_command().  The request
<span class="p_chunk">@@ -769,7 +702,7 @@</span> <span class="p_context"> static int __scsi_error_from_host_byte(struct scsi_cmnd *cmd, int result)</span>
  *		   be used if we made forward progress, or if we want
  *		   to switch from READ(10) to READ(6) for example.
  *
<span class="p_del">- *		b) We can call scsi_queue_insert().  The request will</span>
<span class="p_add">+ *		b) We can call __scsi_queue_insert().  The request will</span>
  *		   be put back on the queue and retried using the same
  *		   command as before, possibly after a delay.
  *
<span class="p_chunk">@@ -873,12 +806,28 @@</span> <span class="p_context"> void scsi_io_completion(struct scsi_cmnd *cmd, unsigned int good_bytes)</span>
 	}
 
 	/*
<span class="p_del">-	 * A number of bytes were successfully read.  If there</span>
<span class="p_del">-	 * are leftovers and there is some kind of error</span>
<span class="p_del">-	 * (result != 0), retry the rest.</span>
<span class="p_add">+	 * special case: failed zero length commands always need to</span>
<span class="p_add">+	 * drop down into the retry code. Otherwise, if we finished</span>
<span class="p_add">+	 * all bytes in the request we are done now.</span>
 	 */
<span class="p_del">-	if (scsi_end_request(cmd, error, good_bytes, result == 0) == NULL)</span>
<span class="p_del">-		return;</span>
<span class="p_add">+	if (!(blk_rq_bytes(req) == 0 &amp;&amp; error) &amp;&amp;</span>
<span class="p_add">+	    !blk_end_request(req, error, good_bytes))</span>
<span class="p_add">+		goto next_command;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Kill remainder if no retrys.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (error &amp;&amp; scsi_noretry_cmd(cmd)) {</span>
<span class="p_add">+		blk_end_request_all(req, error);</span>
<span class="p_add">+		goto next_command;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If there had been no error, but we have leftover bytes in the</span>
<span class="p_add">+	 * requeues just queue the command up again.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (result == 0)</span>
<span class="p_add">+		goto requeue;</span>
 
 	error = __scsi_error_from_host_byte(cmd, result);
 
<span class="p_chunk">@@ -1000,7 +949,6 @@</span> <span class="p_context"> void scsi_io_completion(struct scsi_cmnd *cmd, unsigned int good_bytes)</span>
 	switch (action) {
 	case ACTION_FAIL:
 		/* Give up and fail the remainder of the request */
<span class="p_del">-		scsi_release_buffers(cmd);</span>
 		if (!(req-&gt;cmd_flags &amp; REQ_QUIET)) {
 			if (description)
 				scmd_printk(KERN_INFO, cmd, &quot;%s\n&quot;,
<span class="p_chunk">@@ -1010,12 +958,11 @@</span> <span class="p_context"> void scsi_io_completion(struct scsi_cmnd *cmd, unsigned int good_bytes)</span>
 				scsi_print_sense(&quot;&quot;, cmd);
 			scsi_print_command(cmd);
 		}
<span class="p_del">-		if (blk_end_request_err(req, error))</span>
<span class="p_del">-			scsi_requeue_command(q, cmd);</span>
<span class="p_del">-		else</span>
<span class="p_del">-			scsi_next_command(cmd);</span>
<span class="p_del">-		break;</span>
<span class="p_add">+		if (!blk_end_request_err(req, error))</span>
<span class="p_add">+			goto next_command;</span>
<span class="p_add">+		/*FALLTHRU*/</span>
 	case ACTION_REPREP:
<span class="p_add">+	requeue:</span>
 		/* Unprep the request and put it back at the head of the queue.
 		 * A new command will be prepared and issued.
 		 */
<span class="p_chunk">@@ -1031,6 +978,11 @@</span> <span class="p_context"> void scsi_io_completion(struct scsi_cmnd *cmd, unsigned int good_bytes)</span>
 		__scsi_queue_insert(cmd, SCSI_MLQUEUE_DEVICE_BUSY, 0);
 		break;
 	}
<span class="p_add">+	return;</span>
<span class="p_add">+</span>
<span class="p_add">+next_command:</span>
<span class="p_add">+	__scsi_release_buffers(cmd, 0);</span>
<span class="p_add">+	scsi_next_command(cmd);</span>
 }
 
 static int scsi_init_sgtable(struct request *req, struct scsi_data_buffer *sdb,
<span class="p_header">diff --git a/drivers/usb/core/hub.c b/drivers/usb/core/hub.c</span>
<span class="p_header">index dcee3f09793d..f46ac929ef8a 100644</span>
<span class="p_header">--- a/drivers/usb/core/hub.c</span>
<span class="p_header">+++ b/drivers/usb/core/hub.c</span>
<span class="p_chunk">@@ -106,6 +106,7 @@</span> <span class="p_context"> EXPORT_SYMBOL_GPL(ehci_cf_port_reset_rwsem);</span>
 #define HUB_DEBOUNCE_STEP	  25
 #define HUB_DEBOUNCE_STABLE	 100
 
<span class="p_add">+static void hub_release(struct kref *kref);</span>
 static int usb_reset_and_verify_device(struct usb_device *udev);
 
 static inline char *portspeed(struct usb_hub *hub, int portstatus)
<span class="p_chunk">@@ -1023,10 +1024,20 @@</span> <span class="p_context"> static void hub_activate(struct usb_hub *hub, enum hub_activation_type type)</span>
 	unsigned delay;
 
 	/* Continue a partial initialization */
<span class="p_del">-	if (type == HUB_INIT2)</span>
<span class="p_del">-		goto init2;</span>
<span class="p_del">-	if (type == HUB_INIT3)</span>
<span class="p_add">+	if (type == HUB_INIT2 || type == HUB_INIT3) {</span>
<span class="p_add">+		device_lock(hub-&gt;intfdev);</span>
<span class="p_add">+</span>
<span class="p_add">+		/* Was the hub disconnected while we were waiting? */</span>
<span class="p_add">+		if (hub-&gt;disconnected) {</span>
<span class="p_add">+			device_unlock(hub-&gt;intfdev);</span>
<span class="p_add">+			kref_put(&amp;hub-&gt;kref, hub_release);</span>
<span class="p_add">+			return;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		if (type == HUB_INIT2)</span>
<span class="p_add">+			goto init2;</span>
 		goto init3;
<span class="p_add">+	}</span>
<span class="p_add">+	kref_get(&amp;hub-&gt;kref);</span>
 
 	/* The superspeed hub except for root hub has to use Hub Depth
 	 * value as an offset into the route string to locate the bits
<span class="p_chunk">@@ -1220,6 +1231,7 @@</span> <span class="p_context"> static void hub_activate(struct usb_hub *hub, enum hub_activation_type type)</span>
 			PREPARE_DELAYED_WORK(&amp;hub-&gt;init_work, hub_init_func3);
 			schedule_delayed_work(&amp;hub-&gt;init_work,
 					msecs_to_jiffies(delay));
<span class="p_add">+			device_unlock(hub-&gt;intfdev);</span>
 			return;		/* Continues at init3: below */
 		} else {
 			msleep(delay);
<span class="p_chunk">@@ -1240,6 +1252,11 @@</span> <span class="p_context"> static void hub_activate(struct usb_hub *hub, enum hub_activation_type type)</span>
 	/* Allow autosuspend if it was suppressed */
 	if (type &lt;= HUB_INIT3)
 		usb_autopm_put_interface_async(to_usb_interface(hub-&gt;intfdev));
<span class="p_add">+</span>
<span class="p_add">+	if (type == HUB_INIT2 || type == HUB_INIT3)</span>
<span class="p_add">+		device_unlock(hub-&gt;intfdev);</span>
<span class="p_add">+</span>
<span class="p_add">+	kref_put(&amp;hub-&gt;kref, hub_release);</span>
 }
 
 /* Implement the continuations for the delays above */
<span class="p_header">diff --git a/drivers/usb/core/quirks.c b/drivers/usb/core/quirks.c</span>
<span class="p_header">index b6c85fbd0a14..24af5b0b8d81 100644</span>
<span class="p_header">--- a/drivers/usb/core/quirks.c</span>
<span class="p_header">+++ b/drivers/usb/core/quirks.c</span>
<span class="p_chunk">@@ -164,6 +164,10 @@</span> <span class="p_context"> static const struct usb_device_id usb_quirk_list[] = {</span>
 	/* MAYA44USB sound device */
 	{ USB_DEVICE(0x0a92, 0x0091), .driver_info = USB_QUIRK_RESET_RESUME },
 
<span class="p_add">+	/* ASUS Base Station(T100) */</span>
<span class="p_add">+	{ USB_DEVICE(0x0b05, 0x17e0), .driver_info =</span>
<span class="p_add">+			USB_QUIRK_IGNORE_REMOTE_WAKEUP },</span>
<span class="p_add">+</span>
 	/* Action Semiconductor flash disk */
 	{ USB_DEVICE(0x10d6, 0x2200), .driver_info =
 			USB_QUIRK_STRING_FETCH_255 },
<span class="p_chunk">@@ -186,10 +190,6 @@</span> <span class="p_context"> static const struct usb_device_id usb_interface_quirk_list[] = {</span>
 	{ USB_VENDOR_AND_INTERFACE_INFO(0x046d, USB_CLASS_VIDEO, 1, 0),
 	  .driver_info = USB_QUIRK_RESET_RESUME },
 
<span class="p_del">-	/* ASUS Base Station(T100) */</span>
<span class="p_del">-	{ USB_DEVICE(0x0b05, 0x17e0), .driver_info =</span>
<span class="p_del">-			USB_QUIRK_IGNORE_REMOTE_WAKEUP },</span>
<span class="p_del">-</span>
 	{ }  /* terminating entry must be last */
 };
 
<span class="p_header">diff --git a/fs/ext4/extents.c b/fs/ext4/extents.c</span>
<span class="p_header">index a3be02e03021..0bffd9a0fbf2 100644</span>
<span class="p_header">--- a/fs/ext4/extents.c</span>
<span class="p_header">+++ b/fs/ext4/extents.c</span>
<span class="p_chunk">@@ -359,9 +359,13 @@</span> <span class="p_context"> static int ext4_valid_extent(struct inode *inode, struct ext4_extent *ext)</span>
 	ext4_fsblk_t block = ext4_ext_pblock(ext);
 	int len = ext4_ext_get_actual_len(ext);
 	ext4_lblk_t lblock = le32_to_cpu(ext-&gt;ee_block);
<span class="p_del">-	ext4_lblk_t last = lblock + len - 1;</span>
 
<span class="p_del">-	if (len == 0 || lblock &gt; last)</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We allow neither:</span>
<span class="p_add">+	 *  - zero length</span>
<span class="p_add">+	 *  - overflow/wrap-around</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (lblock + len &lt;= lblock)</span>
 		return 0;
 	return ext4_data_block_valid(EXT4_SB(inode-&gt;i_sb), block, len);
 }
<span class="p_header">diff --git a/fs/ext4/inode.c b/fs/ext4/inode.c</span>
<span class="p_header">index 58001fcff037..774cb09519cb 100644</span>
<span class="p_header">--- a/fs/ext4/inode.c</span>
<span class="p_header">+++ b/fs/ext4/inode.c</span>
<span class="p_chunk">@@ -204,9 +204,9 @@</span> <span class="p_context"> void ext4_evict_inode(struct inode *inode)</span>
 		 * Note that directories do not have this problem because they
 		 * don&#39;t use page cache.
 		 */
<span class="p_del">-		if (ext4_should_journal_data(inode) &amp;&amp;</span>
<span class="p_del">-		    (S_ISLNK(inode-&gt;i_mode) || S_ISREG(inode-&gt;i_mode)) &amp;&amp;</span>
<span class="p_del">-		    inode-&gt;i_ino != EXT4_JOURNAL_INO) {</span>
<span class="p_add">+		if (inode-&gt;i_ino != EXT4_JOURNAL_INO &amp;&amp;</span>
<span class="p_add">+		    ext4_should_journal_data(inode) &amp;&amp;</span>
<span class="p_add">+		    (S_ISLNK(inode-&gt;i_mode) || S_ISREG(inode-&gt;i_mode))) {</span>
 			journal_t *journal = EXT4_SB(inode-&gt;i_sb)-&gt;s_journal;
 			tid_t commit_tid = EXT4_I(inode)-&gt;i_datasync_tid;
 
<span class="p_chunk">@@ -2579,13 +2579,36 @@</span> <span class="p_context"> retry:</span>
 				done = true;
 			}
 		}
<span class="p_del">-		ext4_journal_stop(handle);</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Caution: If the handle is synchronous,</span>
<span class="p_add">+		 * ext4_journal_stop() can wait for transaction commit</span>
<span class="p_add">+		 * to finish which may depend on writeback of pages to</span>
<span class="p_add">+		 * complete or on page lock to be released.  In that</span>
<span class="p_add">+		 * case, we have to wait until after after we have</span>
<span class="p_add">+		 * submitted all the IO, released page locks we hold,</span>
<span class="p_add">+		 * and dropped io_end reference (for extent conversion</span>
<span class="p_add">+		 * to be able to complete) before stopping the handle.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (!ext4_handle_valid(handle) || handle-&gt;h_sync == 0) {</span>
<span class="p_add">+			ext4_journal_stop(handle);</span>
<span class="p_add">+			handle = NULL;</span>
<span class="p_add">+		}</span>
 		/* Submit prepared bio */
 		ext4_io_submit(&amp;mpd.io_submit);
 		/* Unlock pages we didn&#39;t use */
 		mpage_release_unused_pages(&amp;mpd, give_up_on_write);
<span class="p_del">-		/* Drop our io_end reference we got from init */</span>
<span class="p_del">-		ext4_put_io_end(mpd.io_submit.io_end);</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Drop our io_end reference we got from init. We have</span>
<span class="p_add">+		 * to be careful and use deferred io_end finishing if</span>
<span class="p_add">+		 * we are still holding the transaction as we can</span>
<span class="p_add">+		 * release the last reference to io_end which may end</span>
<span class="p_add">+		 * up doing unwritten extent conversion.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (handle) {</span>
<span class="p_add">+			ext4_put_io_end_defer(mpd.io_submit.io_end);</span>
<span class="p_add">+			ext4_journal_stop(handle);</span>
<span class="p_add">+		} else</span>
<span class="p_add">+			ext4_put_io_end(mpd.io_submit.io_end);</span>
 
 		if (ret == -ENOSPC &amp;&amp; sbi-&gt;s_journal) {
 			/*
<span class="p_header">diff --git a/fs/ext4/mballoc.c b/fs/ext4/mballoc.c</span>
<span class="p_header">index 4a79ce1ecaa1..fcb205f69ed6 100644</span>
<span class="p_header">--- a/fs/ext4/mballoc.c</span>
<span class="p_header">+++ b/fs/ext4/mballoc.c</span>
<span class="p_chunk">@@ -2897,7 +2897,7 @@</span> <span class="p_context"> ext4_mb_mark_diskspace_used(struct ext4_allocation_context *ac,</span>
 		ext4_error(sb, &quot;Allocating blocks %llu-%llu which overlap &quot;
 			   &quot;fs metadata&quot;, block, block+len);
 		/* File system mounted not to panic on error
<span class="p_del">-		 * Fix the bitmap and repeat the block allocation</span>
<span class="p_add">+		 * Fix the bitmap and return EUCLEAN</span>
 		 * We leak some of the blocks here.
 		 */
 		ext4_lock_group(sb, ac-&gt;ac_b_ex.fe_group);
<span class="p_chunk">@@ -2906,7 +2906,7 @@</span> <span class="p_context"> ext4_mb_mark_diskspace_used(struct ext4_allocation_context *ac,</span>
 		ext4_unlock_group(sb, ac-&gt;ac_b_ex.fe_group);
 		err = ext4_handle_dirty_metadata(handle, NULL, bitmap_bh);
 		if (!err)
<span class="p_del">-			err = -EAGAIN;</span>
<span class="p_add">+			err = -EUCLEAN;</span>
 		goto out_err;
 	}
 
<span class="p_chunk">@@ -4476,18 +4476,7 @@</span> <span class="p_context"> repeat:</span>
 	}
 	if (likely(ac-&gt;ac_status == AC_STATUS_FOUND)) {
 		*errp = ext4_mb_mark_diskspace_used(ac, handle, reserv_clstrs);
<span class="p_del">-		if (*errp == -EAGAIN) {</span>
<span class="p_del">-			/*</span>
<span class="p_del">-			 * drop the reference that we took</span>
<span class="p_del">-			 * in ext4_mb_use_best_found</span>
<span class="p_del">-			 */</span>
<span class="p_del">-			ext4_mb_release_context(ac);</span>
<span class="p_del">-			ac-&gt;ac_b_ex.fe_group = 0;</span>
<span class="p_del">-			ac-&gt;ac_b_ex.fe_start = 0;</span>
<span class="p_del">-			ac-&gt;ac_b_ex.fe_len = 0;</span>
<span class="p_del">-			ac-&gt;ac_status = AC_STATUS_CONTINUE;</span>
<span class="p_del">-			goto repeat;</span>
<span class="p_del">-		} else if (*errp) {</span>
<span class="p_add">+		if (*errp) {</span>
 			ext4_discard_allocated_blocks(ac);
 			goto errout;
 		} else {
<span class="p_header">diff --git a/fs/ext4/super.c b/fs/ext4/super.c</span>
<span class="p_header">index 64cd8114f75d..4ce824197b81 100644</span>
<span class="p_header">--- a/fs/ext4/super.c</span>
<span class="p_header">+++ b/fs/ext4/super.c</span>
<span class="p_chunk">@@ -2222,6 +2222,16 @@</span> <span class="p_context"> static void ext4_orphan_cleanup(struct super_block *sb,</span>
 	while (es-&gt;s_last_orphan) {
 		struct inode *inode;
 
<span class="p_add">+		/*</span>
<span class="p_add">+		 * We may have encountered an error during cleanup; if</span>
<span class="p_add">+		 * so, skip the rest.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (EXT4_SB(sb)-&gt;s_mount_state &amp; EXT4_ERROR_FS) {</span>
<span class="p_add">+			jbd_debug(1, &quot;Skipping orphan recovery on fs with errors.\n&quot;);</span>
<span class="p_add">+			es-&gt;s_last_orphan = 0;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
 		inode = ext4_orphan_get(sb, le32_to_cpu(es-&gt;s_last_orphan));
 		if (IS_ERR(inode)) {
 			es-&gt;s_last_orphan = 0;
<span class="p_header">diff --git a/fs/fuse/inode.c b/fs/fuse/inode.c</span>
<span class="p_header">index faf00af7f3d7..58737550a3f4 100644</span>
<span class="p_header">--- a/fs/fuse/inode.c</span>
<span class="p_header">+++ b/fs/fuse/inode.c</span>
<span class="p_chunk">@@ -911,7 +911,7 @@</span> <span class="p_context"> static void fuse_send_init(struct fuse_conn *fc, struct fuse_req *req)</span>
 	arg-&gt;flags |= FUSE_ASYNC_READ | FUSE_POSIX_LOCKS | FUSE_ATOMIC_O_TRUNC |
 		FUSE_EXPORT_SUPPORT | FUSE_BIG_WRITES | FUSE_DONT_MASK |
 		FUSE_SPLICE_WRITE | FUSE_SPLICE_MOVE | FUSE_SPLICE_READ |
<span class="p_del">-		FUSE_FLOCK_LOCKS | FUSE_IOCTL_DIR | FUSE_AUTO_INVAL_DATA |</span>
<span class="p_add">+		FUSE_FLOCK_LOCKS | FUSE_HAS_IOCTL_DIR | FUSE_AUTO_INVAL_DATA |</span>
 		FUSE_DO_READDIRPLUS | FUSE_READDIRPLUS_AUTO | FUSE_ASYNC_DIO;
 	req-&gt;in.h.opcode = FUSE_INIT;
 	req-&gt;in.numargs = 1;
<span class="p_header">diff --git a/include/linux/console.h b/include/linux/console.h</span>
<span class="p_header">index 7571a16bd653..ac1599bda9fc 100644</span>
<span class="p_header">--- a/include/linux/console.h</span>
<span class="p_header">+++ b/include/linux/console.h</span>
<span class="p_chunk">@@ -150,6 +150,7 @@</span> <span class="p_context"> extern int console_trylock(void);</span>
 extern void console_unlock(void);
 extern void console_conditional_schedule(void);
 extern void console_unblank(void);
<span class="p_add">+extern void console_flush_on_panic(void);</span>
 extern struct tty_driver *console_device(int *);
 extern void console_stop(struct console *);
 extern void console_start(struct console *);
<span class="p_header">diff --git a/include/rdma/ib.h b/include/rdma/ib.h</span>
<span class="p_header">index cf8f9e700e48..a6b93706b0fc 100644</span>
<span class="p_header">--- a/include/rdma/ib.h</span>
<span class="p_header">+++ b/include/rdma/ib.h</span>
<span class="p_chunk">@@ -34,6 +34,7 @@</span> <span class="p_context"></span>
 #define _RDMA_IB_H
 
 #include &lt;linux/types.h&gt;
<span class="p_add">+#include &lt;linux/sched.h&gt;</span>
 
 struct ib_addr {
 	union {
<span class="p_chunk">@@ -86,4 +87,19 @@</span> <span class="p_context"> struct sockaddr_ib {</span>
 	__u64			sib_scope_id;
 };
 
<span class="p_add">+/*</span>
<span class="p_add">+ * The IB interfaces that use write() as bi-directional ioctl() are</span>
<span class="p_add">+ * fundamentally unsafe, since there are lots of ways to trigger &quot;write()&quot;</span>
<span class="p_add">+ * calls from various contexts with elevated privileges. That includes the</span>
<span class="p_add">+ * traditional suid executable error message writes, but also various kernel</span>
<span class="p_add">+ * interfaces that can write to file descriptors.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This function provides protection for the legacy API by restricting the</span>
<span class="p_add">+ * calling context.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline bool ib_safe_file_access(struct file *filp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return filp-&gt;f_cred == current_cred() &amp;&amp; segment_eq(get_fs(), USER_DS);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 #endif /* _RDMA_IB_H */
<span class="p_header">diff --git a/ipc/msg.c b/ipc/msg.c</span>
<span class="p_header">index 4a036c619607..0d1449551e06 100644</span>
<span class="p_header">--- a/ipc/msg.c</span>
<span class="p_header">+++ b/ipc/msg.c</span>
<span class="p_chunk">@@ -745,7 +745,7 @@</span> <span class="p_context"> long do_msgsnd(int msqid, long mtype, void __user *mtext,</span>
 		rcu_read_lock();
 		ipc_lock_object(&amp;msq-&gt;q_perm);
 
<span class="p_del">-		ipc_rcu_putref(msq, ipc_rcu_free);</span>
<span class="p_add">+		ipc_rcu_putref(msq, msg_rcu_free);</span>
 		/* raced with RMID? */
 		if (!ipc_valid_object(&amp;msq-&gt;q_perm)) {
 			err = -EIDRM;
<span class="p_header">diff --git a/ipc/sem.c b/ipc/sem.c</span>
<span class="p_header">index e53c96f7db42..bd8cbb071166 100644</span>
<span class="p_header">--- a/ipc/sem.c</span>
<span class="p_header">+++ b/ipc/sem.c</span>
<span class="p_chunk">@@ -441,7 +441,7 @@</span> <span class="p_context"> static inline struct sem_array *sem_obtain_object_check(struct ipc_namespace *ns</span>
 static inline void sem_lock_and_putref(struct sem_array *sma)
 {
 	sem_lock(sma, NULL, -1);
<span class="p_del">-	ipc_rcu_putref(sma, ipc_rcu_free);</span>
<span class="p_add">+	ipc_rcu_putref(sma, sem_rcu_free);</span>
 }
 
 static inline void sem_rmid(struct ipc_namespace *ns, struct sem_array *s)
<span class="p_chunk">@@ -1371,7 +1371,7 @@</span> <span class="p_context"> static int semctl_main(struct ipc_namespace *ns, int semid, int semnum,</span>
 			rcu_read_unlock();
 			sem_io = ipc_alloc(sizeof(ushort)*nsems);
 			if (sem_io == NULL) {
<span class="p_del">-				ipc_rcu_putref(sma, ipc_rcu_free);</span>
<span class="p_add">+				ipc_rcu_putref(sma, sem_rcu_free);</span>
 				return -ENOMEM;
 			}
 
<span class="p_chunk">@@ -1405,20 +1405,20 @@</span> <span class="p_context"> static int semctl_main(struct ipc_namespace *ns, int semid, int semnum,</span>
 		if (nsems &gt; SEMMSL_FAST) {
 			sem_io = ipc_alloc(sizeof(ushort)*nsems);
 			if (sem_io == NULL) {
<span class="p_del">-				ipc_rcu_putref(sma, ipc_rcu_free);</span>
<span class="p_add">+				ipc_rcu_putref(sma, sem_rcu_free);</span>
 				return -ENOMEM;
 			}
 		}
 
 		if (copy_from_user(sem_io, p, nsems*sizeof(ushort))) {
<span class="p_del">-			ipc_rcu_putref(sma, ipc_rcu_free);</span>
<span class="p_add">+			ipc_rcu_putref(sma, sem_rcu_free);</span>
 			err = -EFAULT;
 			goto out_free;
 		}
 
 		for (i = 0; i &lt; nsems; i++) {
 			if (sem_io[i] &gt; SEMVMX) {
<span class="p_del">-				ipc_rcu_putref(sma, ipc_rcu_free);</span>
<span class="p_add">+				ipc_rcu_putref(sma, sem_rcu_free);</span>
 				err = -ERANGE;
 				goto out_free;
 			}
<span class="p_chunk">@@ -1708,7 +1708,7 @@</span> <span class="p_context"> static struct sem_undo *find_alloc_undo(struct ipc_namespace *ns, int semid)</span>
 	/* step 2: allocate new undo structure */
 	new = kzalloc(sizeof(struct sem_undo) + sizeof(short)*nsems, GFP_KERNEL);
 	if (!new) {
<span class="p_del">-		ipc_rcu_putref(sma, ipc_rcu_free);</span>
<span class="p_add">+		ipc_rcu_putref(sma, sem_rcu_free);</span>
 		return ERR_PTR(-ENOMEM);
 	}
 
<span class="p_header">diff --git a/kernel/panic.c b/kernel/panic.c</span>
<span class="p_header">index 6d6300375090..16458b37fadc 100644</span>
<span class="p_header">--- a/kernel/panic.c</span>
<span class="p_header">+++ b/kernel/panic.c</span>
<span class="p_chunk">@@ -23,6 +23,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/sysrq.h&gt;
 #include &lt;linux/init.h&gt;
 #include &lt;linux/nmi.h&gt;
<span class="p_add">+#include &lt;linux/console.h&gt;</span>
 
 #define PANIC_TIMER_STEP 100
 #define PANIC_BLINK_SPD 18
<span class="p_chunk">@@ -133,6 +134,8 @@</span> <span class="p_context"> void panic(const char *fmt, ...)</span>
 
 	bust_spinlocks(0);
 
<span class="p_add">+	console_flush_on_panic();</span>
<span class="p_add">+</span>
 	if (!panic_blink)
 		panic_blink = no_blink;
 
<span class="p_header">diff --git a/kernel/printk/printk.c b/kernel/printk/printk.c</span>
<span class="p_header">index 02e7fb4edb93..2e0406fe1105 100644</span>
<span class="p_header">--- a/kernel/printk/printk.c</span>
<span class="p_header">+++ b/kernel/printk/printk.c</span>
<span class="p_chunk">@@ -2011,13 +2011,24 @@</span> <span class="p_context"> void console_unlock(void)</span>
 	static u64 seen_seq;
 	unsigned long flags;
 	bool wake_klogd = false;
<span class="p_del">-	bool retry;</span>
<span class="p_add">+	bool do_cond_resched, retry;</span>
 
 	if (console_suspended) {
 		up(&amp;console_sem);
 		return;
 	}
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Console drivers are called under logbuf_lock, so</span>
<span class="p_add">+	 * @console_may_schedule should be cleared before; however, we may</span>
<span class="p_add">+	 * end up dumping a lot of lines, for example, if called from</span>
<span class="p_add">+	 * console registration path, and should invoke cond_resched()</span>
<span class="p_add">+	 * between lines if allowable.  Not doing so can cause a very long</span>
<span class="p_add">+	 * scheduling stall on a slow console leading to RCU stall and</span>
<span class="p_add">+	 * softlockup warnings which exacerbate the issue with more</span>
<span class="p_add">+	 * messages practically incapacitating the system.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	do_cond_resched = console_may_schedule;</span>
 	console_may_schedule = 0;
 
 	/* flush buffered message fragment immediately to console */
<span class="p_chunk">@@ -2074,6 +2085,9 @@</span> <span class="p_context"> skip:</span>
 		call_console_drivers(level, text, len);
 		start_critical_timings();
 		local_irq_restore(flags);
<span class="p_add">+</span>
<span class="p_add">+		if (do_cond_resched)</span>
<span class="p_add">+			cond_resched();</span>
 	}
 	console_locked = 0;
 	mutex_release(&amp;console_lock_dep_map, 1, _RET_IP_);
<span class="p_chunk">@@ -2142,6 +2156,25 @@</span> <span class="p_context"> void console_unblank(void)</span>
 	console_unlock();
 }
 
<span class="p_add">+/**</span>
<span class="p_add">+ * console_flush_on_panic - flush console content on panic</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Immediately output all pending messages no matter what.</span>
<span class="p_add">+ */</span>
<span class="p_add">+void console_flush_on_panic(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If someone else is holding the console lock, trylock will fail</span>
<span class="p_add">+	 * and may_schedule may be set.  Ignore and proceed to unlock so</span>
<span class="p_add">+	 * that messages are flushed out.  As this can be called from any</span>
<span class="p_add">+	 * context and we don&#39;t want to get preempted while flushing,</span>
<span class="p_add">+	 * ensure may_schedule is cleared.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	console_trylock();</span>
<span class="p_add">+	console_may_schedule = 0;</span>
<span class="p_add">+	console_unlock();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /*
  * Return the console tty driver structure and its associated index
  */
<span class="p_header">diff --git a/mm/migrate.c b/mm/migrate.c</span>
<span class="p_header">index 23ca861c93e9..ae10044bdfa5 100644</span>
<span class="p_header">--- a/mm/migrate.c</span>
<span class="p_header">+++ b/mm/migrate.c</span>
<span class="p_chunk">@@ -30,6 +30,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/mempolicy.h&gt;
 #include &lt;linux/vmalloc.h&gt;
 #include &lt;linux/security.h&gt;
<span class="p_add">+#include &lt;linux/backing-dev.h&gt;</span>
 #include &lt;linux/memcontrol.h&gt;
 #include &lt;linux/syscalls.h&gt;
 #include &lt;linux/hugetlb.h&gt;
<span class="p_chunk">@@ -344,6 +345,8 @@</span> <span class="p_context"> int migrate_page_move_mapping(struct address_space *mapping,</span>
 		struct buffer_head *head, enum migrate_mode mode,
 		int extra_count)
 {
<span class="p_add">+	struct zone *oldzone, *newzone;</span>
<span class="p_add">+	int dirty;</span>
 	int expected_count = 1 + extra_count;
 	void **pslot;
 
<span class="p_chunk">@@ -354,6 +357,9 @@</span> <span class="p_context"> int migrate_page_move_mapping(struct address_space *mapping,</span>
 		return MIGRATEPAGE_SUCCESS;
 	}
 
<span class="p_add">+	oldzone = page_zone(page);</span>
<span class="p_add">+	newzone = page_zone(newpage);</span>
<span class="p_add">+</span>
 	spin_lock_irq(&amp;mapping-&gt;tree_lock);
 
 	pslot = radix_tree_lookup_slot(&amp;mapping-&gt;page_tree,
<span class="p_chunk">@@ -394,6 +400,13 @@</span> <span class="p_context"> int migrate_page_move_mapping(struct address_space *mapping,</span>
 		set_page_private(newpage, page_private(page));
 	}
 
<span class="p_add">+	/* Move dirty while page refs frozen and newpage not yet exposed */</span>
<span class="p_add">+	dirty = PageDirty(page);</span>
<span class="p_add">+	if (dirty) {</span>
<span class="p_add">+		ClearPageDirty(page);</span>
<span class="p_add">+		SetPageDirty(newpage);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	radix_tree_replace_slot(pslot, newpage);
 
 	/*
<span class="p_chunk">@@ -403,6 +416,9 @@</span> <span class="p_context"> int migrate_page_move_mapping(struct address_space *mapping,</span>
 	 */
 	page_unfreeze_refs(page, expected_count - 1);
 
<span class="p_add">+	spin_unlock(&amp;mapping-&gt;tree_lock);</span>
<span class="p_add">+	/* Leave irq disabled to prevent preemption while updating stats */</span>
<span class="p_add">+</span>
 	/*
 	 * If moved to a different zone then also account
 	 * the page for that zone. Other VM counters will be
<span class="p_chunk">@@ -413,13 +429,19 @@</span> <span class="p_context"> int migrate_page_move_mapping(struct address_space *mapping,</span>
 	 * via NR_FILE_PAGES and NR_ANON_PAGES if they
 	 * are mapped to swap space.
 	 */
<span class="p_del">-	__dec_zone_page_state(page, NR_FILE_PAGES);</span>
<span class="p_del">-	__inc_zone_page_state(newpage, NR_FILE_PAGES);</span>
<span class="p_del">-	if (!PageSwapCache(page) &amp;&amp; PageSwapBacked(page)) {</span>
<span class="p_del">-		__dec_zone_page_state(page, NR_SHMEM);</span>
<span class="p_del">-		__inc_zone_page_state(newpage, NR_SHMEM);</span>
<span class="p_add">+	if (newzone != oldzone) {</span>
<span class="p_add">+		__dec_zone_state(oldzone, NR_FILE_PAGES);</span>
<span class="p_add">+		__inc_zone_state(newzone, NR_FILE_PAGES);</span>
<span class="p_add">+		if (PageSwapBacked(page) &amp;&amp; !PageSwapCache(page)) {</span>
<span class="p_add">+			__dec_zone_state(oldzone, NR_SHMEM);</span>
<span class="p_add">+			__inc_zone_state(newzone, NR_SHMEM);</span>
<span class="p_add">+		}</span>
<span class="p_add">+		if (dirty &amp;&amp; mapping_cap_account_dirty(mapping)) {</span>
<span class="p_add">+			__dec_zone_state(oldzone, NR_FILE_DIRTY);</span>
<span class="p_add">+			__inc_zone_state(newzone, NR_FILE_DIRTY);</span>
<span class="p_add">+		}</span>
 	}
<span class="p_del">-	spin_unlock_irq(&amp;mapping-&gt;tree_lock);</span>
<span class="p_add">+	local_irq_enable();</span>
 
 	return MIGRATEPAGE_SUCCESS;
 }
<span class="p_chunk">@@ -544,20 +566,9 @@</span> <span class="p_context"> void migrate_page_copy(struct page *newpage, struct page *page)</span>
 	if (PageMappedToDisk(page))
 		SetPageMappedToDisk(newpage);
 
<span class="p_del">-	if (PageDirty(page)) {</span>
<span class="p_del">-		clear_page_dirty_for_io(page);</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * Want to mark the page and the radix tree as dirty, and</span>
<span class="p_del">-		 * redo the accounting that clear_page_dirty_for_io undid,</span>
<span class="p_del">-		 * but we can&#39;t use set_page_dirty because that function</span>
<span class="p_del">-		 * is actually a signal that all of the page has become dirty.</span>
<span class="p_del">-		 * Whereas only part of our page may be dirty.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		if (PageSwapBacked(page))</span>
<span class="p_del">-			SetPageDirty(newpage);</span>
<span class="p_del">-		else</span>
<span class="p_del">-			__set_page_dirty_nobuffers(newpage);</span>
<span class="p_del">- 	}</span>
<span class="p_add">+	/* Move dirty on pages not done by migrate_page_move_mapping() */</span>
<span class="p_add">+	if (PageDirty(page))</span>
<span class="p_add">+		SetPageDirty(newpage);</span>
 
 	/*
 	 * Copy NUMA information to the new page, to prevent over-eager
<span class="p_header">diff --git a/net/ipv4/tcp_input.c b/net/ipv4/tcp_input.c</span>
<span class="p_header">index 5b10c59ba8a9..90f9d00a3fbc 100644</span>
<span class="p_header">--- a/net/ipv4/tcp_input.c</span>
<span class="p_header">+++ b/net/ipv4/tcp_input.c</span>
<span class="p_chunk">@@ -87,7 +87,7 @@</span> <span class="p_context"> int sysctl_tcp_adv_win_scale __read_mostly = 1;</span>
 EXPORT_SYMBOL(sysctl_tcp_adv_win_scale);
 
 /* rfc5961 challenge ack rate limiting */
<span class="p_del">-int sysctl_tcp_challenge_ack_limit = 100;</span>
<span class="p_add">+int sysctl_tcp_challenge_ack_limit = 1000;</span>
 
 int sysctl_tcp_stdurg __read_mostly;
 int sysctl_tcp_rfc1337 __read_mostly;
<span class="p_chunk">@@ -3293,12 +3293,18 @@</span> <span class="p_context"> static void tcp_send_challenge_ack(struct sock *sk)</span>
 	static u32 challenge_timestamp;
 	static unsigned int challenge_count;
 	u32 now = jiffies / HZ;
<span class="p_add">+	u32 count;</span>
 
 	if (now != challenge_timestamp) {
<span class="p_add">+		u32 half = (sysctl_tcp_challenge_ack_limit + 1) &gt;&gt; 1;</span>
<span class="p_add">+</span>
 		challenge_timestamp = now;
<span class="p_del">-		challenge_count = 0;</span>
<span class="p_add">+		challenge_count = half +</span>
<span class="p_add">+				  prandom_u32_max(sysctl_tcp_challenge_ack_limit);</span>
 	}
<span class="p_del">-	if (++challenge_count &lt;= sysctl_tcp_challenge_ack_limit) {</span>
<span class="p_add">+	count = challenge_count;</span>
<span class="p_add">+	if (count &gt; 0) {</span>
<span class="p_add">+		challenge_count = count - 1;</span>
 		NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_TCPCHALLENGEACK);
 		tcp_send_ack(sk);
 	}
<span class="p_header">diff --git a/net/ipv4/tcp_output.c b/net/ipv4/tcp_output.c</span>
<span class="p_header">index a68cd7100349..99d89783d1e6 100644</span>
<span class="p_header">--- a/net/ipv4/tcp_output.c</span>
<span class="p_header">+++ b/net/ipv4/tcp_output.c</span>
<span class="p_chunk">@@ -242,7 +242,8 @@</span> <span class="p_context"> void tcp_select_initial_window(int __space, __u32 mss,</span>
 		/* Set window scaling on max possible window
 		 * See RFC1323 for an explanation of the limit to 14
 		 */
<span class="p_del">-		space = max_t(u32, sysctl_tcp_rmem[2], sysctl_rmem_max);</span>
<span class="p_add">+		space = max_t(u32, space, sysctl_tcp_rmem[2]);</span>
<span class="p_add">+		space = max_t(u32, space, sysctl_rmem_max);</span>
 		space = min_t(u32, space, *window_clamp);
 		while (space &gt; 65535 &amp;&amp; (*rcv_wscale) &lt; 14) {
 			space &gt;&gt;= 1;
<span class="p_header">diff --git a/net/irda/af_irda.c b/net/irda/af_irda.c</span>
<span class="p_header">index f945293c17f0..033a7af5914e 100644</span>
<span class="p_header">--- a/net/irda/af_irda.c</span>
<span class="p_header">+++ b/net/irda/af_irda.c</span>
<span class="p_chunk">@@ -1037,8 +1037,11 @@</span> <span class="p_context"> static int irda_connect(struct socket *sock, struct sockaddr *uaddr,</span>
 	}
 
 	/* Check if we have opened a local TSAP */
<span class="p_del">-	if (!self-&gt;tsap)</span>
<span class="p_del">-		irda_open_tsap(self, LSAP_ANY, addr-&gt;sir_name);</span>
<span class="p_add">+	if (!self-&gt;tsap) {</span>
<span class="p_add">+		err = irda_open_tsap(self, LSAP_ANY, addr-&gt;sir_name);</span>
<span class="p_add">+		if (err)</span>
<span class="p_add">+			goto out;</span>
<span class="p_add">+	}</span>
 
 	/* Move to connecting socket, start sending Connect Requests */
 	sock-&gt;state = SS_CONNECTING;
<span class="p_header">diff --git a/net/sctp/sm_sideeffect.c b/net/sctp/sm_sideeffect.c</span>
<span class="p_header">index fef2acdf4a2e..ecae5561b912 100644</span>
<span class="p_header">--- a/net/sctp/sm_sideeffect.c</span>
<span class="p_header">+++ b/net/sctp/sm_sideeffect.c</span>
<span class="p_chunk">@@ -244,12 +244,13 @@</span> <span class="p_context"> void sctp_generate_t3_rtx_event(unsigned long peer)</span>
 	int error;
 	struct sctp_transport *transport = (struct sctp_transport *) peer;
 	struct sctp_association *asoc = transport-&gt;asoc;
<span class="p_del">-	struct net *net = sock_net(asoc-&gt;base.sk);</span>
<span class="p_add">+	struct sock *sk = asoc-&gt;base.sk;</span>
<span class="p_add">+	struct net *net = sock_net(sk);</span>
 
 	/* Check whether a task is in the sock.  */
 
<span class="p_del">-	bh_lock_sock(asoc-&gt;base.sk);</span>
<span class="p_del">-	if (sock_owned_by_user(asoc-&gt;base.sk)) {</span>
<span class="p_add">+	bh_lock_sock(sk);</span>
<span class="p_add">+	if (sock_owned_by_user(sk)) {</span>
 		pr_debug(&quot;%s: sock is busy\n&quot;, __func__);
 
 		/* Try again later.  */
<span class="p_chunk">@@ -272,10 +273,10 @@</span> <span class="p_context"> void sctp_generate_t3_rtx_event(unsigned long peer)</span>
 			   transport, GFP_ATOMIC);
 
 	if (error)
<span class="p_del">-		asoc-&gt;base.sk-&gt;sk_err = -error;</span>
<span class="p_add">+		sk-&gt;sk_err = -error;</span>
 
 out_unlock:
<span class="p_del">-	bh_unlock_sock(asoc-&gt;base.sk);</span>
<span class="p_add">+	bh_unlock_sock(sk);</span>
 	sctp_transport_put(transport);
 }
 
<span class="p_chunk">@@ -285,11 +286,12 @@</span> <span class="p_context"> out_unlock:</span>
 static void sctp_generate_timeout_event(struct sctp_association *asoc,
 					sctp_event_timeout_t timeout_type)
 {
<span class="p_del">-	struct net *net = sock_net(asoc-&gt;base.sk);</span>
<span class="p_add">+	struct sock *sk = asoc-&gt;base.sk;</span>
<span class="p_add">+	struct net *net = sock_net(sk);</span>
 	int error = 0;
 
<span class="p_del">-	bh_lock_sock(asoc-&gt;base.sk);</span>
<span class="p_del">-	if (sock_owned_by_user(asoc-&gt;base.sk)) {</span>
<span class="p_add">+	bh_lock_sock(sk);</span>
<span class="p_add">+	if (sock_owned_by_user(sk)) {</span>
 		pr_debug(&quot;%s: sock is busy: timer %d\n&quot;, __func__,
 			 timeout_type);
 
<span class="p_chunk">@@ -312,10 +314,10 @@</span> <span class="p_context"> static void sctp_generate_timeout_event(struct sctp_association *asoc,</span>
 			   (void *)timeout_type, GFP_ATOMIC);
 
 	if (error)
<span class="p_del">-		asoc-&gt;base.sk-&gt;sk_err = -error;</span>
<span class="p_add">+		sk-&gt;sk_err = -error;</span>
 
 out_unlock:
<span class="p_del">-	bh_unlock_sock(asoc-&gt;base.sk);</span>
<span class="p_add">+	bh_unlock_sock(sk);</span>
 	sctp_association_put(asoc);
 }
 
<span class="p_chunk">@@ -365,10 +367,11 @@</span> <span class="p_context"> void sctp_generate_heartbeat_event(unsigned long data)</span>
 	int error = 0;
 	struct sctp_transport *transport = (struct sctp_transport *) data;
 	struct sctp_association *asoc = transport-&gt;asoc;
<span class="p_del">-	struct net *net = sock_net(asoc-&gt;base.sk);</span>
<span class="p_add">+	struct sock *sk = asoc-&gt;base.sk;</span>
<span class="p_add">+	struct net *net = sock_net(sk);</span>
 
<span class="p_del">-	bh_lock_sock(asoc-&gt;base.sk);</span>
<span class="p_del">-	if (sock_owned_by_user(asoc-&gt;base.sk)) {</span>
<span class="p_add">+	bh_lock_sock(sk);</span>
<span class="p_add">+	if (sock_owned_by_user(sk)) {</span>
 		pr_debug(&quot;%s: sock is busy\n&quot;, __func__);
 
 		/* Try again later.  */
<span class="p_chunk">@@ -389,10 +392,10 @@</span> <span class="p_context"> void sctp_generate_heartbeat_event(unsigned long data)</span>
 			   transport, GFP_ATOMIC);
 
 	 if (error)
<span class="p_del">-		 asoc-&gt;base.sk-&gt;sk_err = -error;</span>
<span class="p_add">+		sk-&gt;sk_err = -error;</span>
 
 out_unlock:
<span class="p_del">-	bh_unlock_sock(asoc-&gt;base.sk);</span>
<span class="p_add">+	bh_unlock_sock(sk);</span>
 	sctp_transport_put(transport);
 }
 
<span class="p_chunk">@@ -403,10 +406,11 @@</span> <span class="p_context"> void sctp_generate_proto_unreach_event(unsigned long data)</span>
 {
 	struct sctp_transport *transport = (struct sctp_transport *) data;
 	struct sctp_association *asoc = transport-&gt;asoc;
<span class="p_del">-	struct net *net = sock_net(asoc-&gt;base.sk);</span>
<span class="p_add">+	struct sock *sk = asoc-&gt;base.sk;</span>
<span class="p_add">+	struct net *net = sock_net(sk);</span>
 
<span class="p_del">-	bh_lock_sock(asoc-&gt;base.sk);</span>
<span class="p_del">-	if (sock_owned_by_user(asoc-&gt;base.sk)) {</span>
<span class="p_add">+	bh_lock_sock(sk);</span>
<span class="p_add">+	if (sock_owned_by_user(sk)) {</span>
 		pr_debug(&quot;%s: sock is busy\n&quot;, __func__);
 
 		/* Try again later.  */
<span class="p_chunk">@@ -427,7 +431,7 @@</span> <span class="p_context"> void sctp_generate_proto_unreach_event(unsigned long data)</span>
 		   asoc-&gt;state, asoc-&gt;ep, asoc, transport, GFP_ATOMIC);
 
 out_unlock:
<span class="p_del">-	bh_unlock_sock(asoc-&gt;base.sk);</span>
<span class="p_add">+	bh_unlock_sock(sk);</span>
 	sctp_association_put(asoc);
 }
 
<span class="p_header">diff --git a/security/apparmor/apparmorfs.c b/security/apparmor/apparmorfs.c</span>
<span class="p_header">index 7db9954f1af2..b30489856741 100644</span>
<span class="p_header">--- a/security/apparmor/apparmorfs.c</span>
<span class="p_header">+++ b/security/apparmor/apparmorfs.c</span>
<span class="p_chunk">@@ -331,6 +331,7 @@</span> <span class="p_context"> static int aa_fs_seq_hash_show(struct seq_file *seq, void *v)</span>
 			seq_printf(seq, &quot;%.2x&quot;, profile-&gt;hash[i]);
 		seq_puts(seq, &quot;\n&quot;);
 	}
<span class="p_add">+	aa_put_profile(profile);</span>
 
 	return 0;
 }

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



