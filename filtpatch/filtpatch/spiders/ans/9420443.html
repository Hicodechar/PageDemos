
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[RFC,v3,13/20] x86: DMA support for memory encryption - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [RFC,v3,13/20] x86: DMA support for memory encryption</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=80801">Tom Lendacky</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Nov. 10, 2016, 12:37 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20161110003723.3280.62636.stgit@tlendack-t1.amdoffice.net&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9420443/mbox/"
   >mbox</a>
|
   <a href="/patch/9420443/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9420443/">/patch/9420443/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	AB1896048E for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 10 Nov 2016 00:53:25 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 9B8A22914F
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 10 Nov 2016 00:53:25 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 9009E2939E; Thu, 10 Nov 2016 00:53:25 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	DKIM_VALID,RCVD_IN_DNSWL_HI autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 8CB7C2914F
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 10 Nov 2016 00:53:24 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S932475AbcKJAxR (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Wed, 9 Nov 2016 19:53:17 -0500
Received: from mail-bl2nam02on0080.outbound.protection.outlook.com
	([104.47.38.80]:2016
	&quot;EHLO NAM02-BL2-obe.outbound.protection.outlook.com&quot;
	rhost-flags-OK-OK-OK-FAIL) by vger.kernel.org with ESMTP
	id S1754599AbcKJAxH (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Wed, 9 Nov 2016 19:53:07 -0500
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=amdcloud.onmicrosoft.com; s=selector1-amd-com;
	h=From:Date:Subject:Message-ID:Content-Type:MIME-Version;
	bh=SK6HL2VGalB81Hgl78b6Os+oIcRP3rsnVosGOfzJVVk=;
	b=GNZycNB0XC4dcO+oKTaC2p6dqvHsE9MtdSdr0/hIst7OG5VjXGCvdqkwPuM14Ff6IOhca+VCEYF91BuItSRSBEekPqTARofAr69lOkZRYBPANwQ/lQi75g3/Cq/eKxSTcMCoiXq+PgGIes+kar6fnICSBgayyYW4RIFz/GydJ7g=
Authentication-Results: spf=none (sender IP is )
	smtp.mailfrom=Thomas.Lendacky@amd.com; 
Received: from tlendack-t1.amdoffice.net (165.204.77.1) by
	CY4PR12MB1141.namprd12.prod.outlook.com (10.168.163.149) with
	Microsoft SMTP Server (version=TLS1_2,
	cipher=TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA384_P384) id
	15.1.707.6; Thu, 10 Nov 2016 00:37:25 +0000
From: Tom Lendacky &lt;thomas.lendacky@amd.com&gt;
Subject: [RFC PATCH v3 13/20] x86: DMA support for memory encryption
To: &lt;linux-arch@vger.kernel.org&gt;, &lt;linux-efi@vger.kernel.org&gt;,
	&lt;kvm@vger.kernel.org&gt;, &lt;linux-doc@vger.kernel.org&gt;,
	&lt;x86@kernel.org&gt;, &lt;linux-kernel@vger.kernel.org&gt;,
	&lt;kasan-dev@googlegroups.com&gt;, &lt;linux-mm@kvack.org&gt;,
	&lt;iommu@lists.linux-foundation.org&gt;
CC: Rik van Riel &lt;riel@redhat.com&gt;,
	Radim =?utf-8?b?S3LEjW3DocWZ?= &lt;rkrcmar@redhat.com&gt;,
	Arnd Bergmann &lt;arnd@arndb.de&gt;, Jonathan Corbet &lt;corbet@lwn.net&gt;,
	Matt Fleming &lt;matt@codeblueprint.co.uk&gt;, Joerg Roedel &lt;joro@8bytes.org&gt;,
	Konrad Rzeszutek Wilk &lt;konrad.wilk@oracle.com&gt;,
	&quot;Paolo Bonzini&quot; &lt;pbonzini@redhat.com&gt;,
	Larry Woodman &lt;lwoodman@redhat.com&gt;,
	&quot;Ingo Molnar&quot; &lt;mingo@redhat.com&gt;, Borislav Petkov &lt;bp@alien8.de&gt;,
	Andy Lutomirski &lt;luto@kernel.org&gt;, &quot;H. Peter Anvin&quot; &lt;hpa@zytor.com&gt;,
	Andrey Ryabinin &lt;aryabinin@virtuozzo.com&gt;,
	Alexander Potapenko &lt;glider@google.com&gt;,
	&quot;Thomas Gleixner&quot; &lt;tglx@linutronix.de&gt;,
	Dmitry Vyukov &lt;dvyukov@google.com&gt;
Date: Wed, 9 Nov 2016 18:37:23 -0600
Message-ID: &lt;20161110003723.3280.62636.stgit@tlendack-t1.amdoffice.net&gt;
In-Reply-To: &lt;20161110003426.3280.2999.stgit@tlendack-t1.amdoffice.net&gt;
References: &lt;20161110003426.3280.2999.stgit@tlendack-t1.amdoffice.net&gt;
User-Agent: StGit/0.17.1-dirty
MIME-Version: 1.0
Content-Type: text/plain; charset=&quot;utf-8&quot;
Content-Transfer-Encoding: 7bit
X-Originating-IP: [165.204.77.1]
X-ClientProxiedBy: DM5PR20CA0003.namprd20.prod.outlook.com (10.173.136.141)
	To
	CY4PR12MB1141.namprd12.prod.outlook.com (10.168.163.149)
X-MS-Office365-Filtering-Correlation-Id: b2c84936-9098-46dd-5e4b-08d40901bedf
X-Microsoft-Exchange-Diagnostics: 1; CY4PR12MB1141;
	2:sCwffxpursJqLRahPzpOQD/IJlNyUIFTbEQExXSEm4R0pmGiwo5jefV//SkrG7lPm+l8BENAf3WP5u+POj4ZAcEYeCtFZEDX3MiaveAL6uE7daOFHftk1KbXGrd2T/bCzwjoyDZHD7loIvtSwAGXQpTXvT56s89TldTWaRkknozCBAqSxzYPDyFgp0vP/r3IJil5vFr8CIcFnZW7Yqb+GQ==;
	3:/rx7J4rQFADKVjypOsHMlHAWEBwQsC+eQamQfOzFu8xUcHb8SjBezaFfjfktb0ft8P8jBztvAfmbq4aGTxMzBU37CKHbGTQLKtdZqFBV7R21QD8/5oFmO9qu9mTZ4H2w73B0kOMQi8x3BGVw70rbRA==;
	25:Wqw9aOI9UwmpUWvKz1n3kXuUJZmpafd6WaixnFH6RGM7YhB9aPwq5O3jdx/ER3kMLrdf4wUdTJGKwxW9tIJNFnRpIDwuzP44UjXzXSny/ZZR6JVQBJ/+116/sh65Voh3C17HX9YzyAcDyqtDXxLZU4XY14oDf9DrEMjKUDqxJtzacr9D7+OOG8Y8+ztRc+5QTcA+62oBTHP9DFD7wXLQv3dkCTqpn9Kb/i8OFWxqtdOkK4tRTaCGERPEAMGeFl4zdu4KE79qtDz/8OLe7JPsaUbcRosMWt6qrHaIMIrWYypMN/UUVCQ9+0hWRqNQk8fge8YDvh6CCnaG3g2Q+UUczlcQioOdoSVsvY6w6oWzl1Z8nZkqcZ0VBdMhXqhVKiolg3iWvWwEFl94ZacHGPlPUV3O4GgSG3w1VGbn4aWhSASTFSsOOMW8i+O5o7d2VlqP
X-Microsoft-Antispam: UriScan:;BCL:0;PCL:0;RULEID:;SRVR:CY4PR12MB1141;
X-Microsoft-Exchange-Diagnostics: 1; CY4PR12MB1141;
	31:JsbiN/pHHmkza10WNfErvy065LsbC6qCCbEAnP6IYMbRzIQAKNkpfFFrnPVMFYaF/fL+3xTk9p2e+CAzCjweM2mo1VSY61G/HkNk7wUtv+zHnKLS+NHbl1VUzHqwC03KRuPzsSuXNla/2yorlN2qooonk3l9+X9XWM341K5zhzSYxtK4bFHwVNOfmdl3QhV1/B6EqPbpCO6fepEc1iAV7gVtsPyFjvA/ak7ltsmOgUSXeMDT77oOQKwJxIoEF+YLeh1RES7zlRUnA+k7U05DJQ==;
	20:88MdMeXC6xk9cev4W6S5my9PqLJpO+piPvJnBYCsCLGR3kXo2qkVAdMsLwHm4bPoMiixV8tvaKjVh2bd+53B/xRYFOnvYtZM+lJtRGByTD5GpT8j8PsjZgI/u+lKMRc1TPJOpN6lum1Dz6sMQJKbcNdKjm8bm/LVTPZqrV/wKiSWRe55aTk6ph8i8OcGrJZnC753YMMW0xDES0scJG6If7fdf0sSY6mILUQ+V5wh+lNNKLoaKEb+7j+8d7pCvkPk54WBHIiK5HaU6w5CyuB0co01IB2R3q02MbQJUeocoo8RJ7A+PWyTVJOiNrZlfuKDXJnFiHqqFDH4EhUN5x3ngHJHb/PFc/gso+mKUnVhu1t3U1507THJjsVqkr/GkSLnkZhumL5D4zXOWlPv6MM3H+igaT9PjfFykgp4hhgR10w4I0KA/Rh5/LAVVEHL3/1nDcJj5wE9HqB5O0DUxm5AvmbcKCOKlgn8godde2l6iLZm/dpnz38tGP+ce27JUR5F
X-Microsoft-Antispam-PRVS: &lt;CY4PR12MB11413FA1C883F569A7EE614DECB80@CY4PR12MB1141.namprd12.prod.outlook.com&gt;
X-Exchange-Antispam-Report-Test: UriScan:(767451399110);
X-Exchange-Antispam-Report-CFA-Test: BCL:0; PCL:0;
	RULEID:(6040176)(601004)(2401047)(8121501046)(5005006)(3002001)(10201501046)(6055026);
	SRVR:CY4PR12MB1141; BCL:0; PCL:0; RULEID:; SRVR:CY4PR12MB1141;
X-Microsoft-Exchange-Diagnostics: 1; CY4PR12MB1141;
	4:8fMOFauBqtHn2NpyI0uN3gZsOhOOBSvgRM3Z8K9b9qmiv8NJhsGaY/7l92bqzvbuAp8HCBsJz0BvYbrvWxxrJb/2EEKYfxfTpy2ZxMEcnDxvHkcdVjiYMEJIOpnPduX5YyPV/J8bOjDg/emwHx5BvgD6qWsu9JtIWFv8+EJfMhwB9oDKJRhW8NqaYIJ8K2ZSxT+PLjgRoEG4BBuAvLMBzDq8ZMjIJeE5Ey1hF9knpFXuQrDddijQPe7/ASkh/FJv8aFhTJOa2sffdWD16cHEs+P4Lcx8HPYntoHVey4U/FEOP5O0JgzK2a//jQNx6T7aKLC23oCgSEl8mr1y2bksNC2qvRSh6kDPL6JV3c19vJ8NEMp9CZQpj4/tvu/eL4j7k8QH8AaiPSE9ZSMUx+hKrFEqZSsjQdezhP23+QkT0W54dbE3km2LzZhSdzoV2+vjex/hRcadgALY7rwf5EeEnQ==
X-Forefront-PRVS: 01221E3973
X-Forefront-Antispam-Report: SFV:NSPM;
	SFS:(10009020)(4630300001)(6009001)(7916002)(199003)(189002)(103116003)(92566002)(47776003)(23676002)(8676002)(7736002)(69596002)(50466002)(189998001)(81166006)(66066001)(9686002)(305945005)(5660300001)(77096005)(575784001)(7846002)(101416001)(86362001)(2201001)(2950100002)(5001770100001)(50986999)(4001350100001)(76176999)(54356999)(97736004)(1076002)(81156014)(53416004)(105586002)(42186005)(4326007)(83506001)(7416002)(97746001)(6116002)(230700001)(3846002)(586003)(68736007)(106356001)(33646002)(2906002)(71626007)(217873001);
	DIR:OUT; SFP:1101; SCL:1; SRVR:CY4PR12MB1141;
	H:tlendack-t1.amdoffice.net; FPR:; SPF:None; PTR:InfoNoRecords;
	A:1; MX:1; LANG:en; 
Received-SPF: None (protection.outlook.com: amd.com does not designate
	permitted sender hosts)
X-Microsoft-Exchange-Diagnostics: =?utf-8?B?MTtDWTRQUjEyTUIxMTQxOzIzOnpwWXo5OG9ha242MFpGWDZZcHZKdDUrblRX?=
	=?utf-8?B?UU1vWTdYdzMyd0hzamxRRjIzS1U4QklqQm83QzZOOHk4RlVlazl4ZitzUnBm?=
	=?utf-8?B?OUNvcVJXWkR6RDBQS2EvQmZncHNBSFJOWENJc3I3aDRBUFRGSVUwczlmVUdS?=
	=?utf-8?B?S1FxeTFHdE9FODducUZwb2hTZWNoNUtsa3huTUtKTGt3RlZhMm5TellLa0hi?=
	=?utf-8?B?SEtQWFp5TWxRdzVLTW1MQXVUczIvczV0RG95d0VnTnJIeVMwRnlOUGJYUXUy?=
	=?utf-8?B?S1J6d3JsKy91QlZsRGZ6QXVMVWFsdzdBcXJKVVJsdlN1UFVoZVZYSmZMRmJu?=
	=?utf-8?B?SGVkOW5xVjZTWVV3M2JpSWtrd2VudEZJVEhtV1IrVlREcmYwczdSRVVSU0ZH?=
	=?utf-8?B?amYwZTlYU0tkUk13aHk0dkNiWW9mVU02QXZOeXZTUnc1aW5zeHFWdWM1YmNm?=
	=?utf-8?B?QkJOQUtIeDZmcUhGSk9mdUdwTS9QQWp0dzVPT1NzOEw3eVdtNjdTZjFSUU1D?=
	=?utf-8?B?MTAwNEJRMzVKcFRXTUN3dHk1amFrQ1NTTllYNmlySUZ6MXdUemN3QWxTWUZH?=
	=?utf-8?B?NTZyeHpqcEZWeVVQNWxrVEI5V09oNTRIa1NIM294d0craXQxMWNuQUZydFJz?=
	=?utf-8?B?d0NRMHBEcnJ2VkdnbkFWNHlRR0pSNERZRExCQ1ZSemlBRXVxRHZ1UEQ1NGE0?=
	=?utf-8?B?RzlVMm41NzQvR1djNUlab1VFV1pwb3o3b0U5cVVNTXd4ak5aNEVwVFZZUDdv?=
	=?utf-8?B?WlYwMi9MbDRkbzYwaHQyaG4wdTRtbklDc2lwVE5rRDlncFJGZnoycUR4MXdt?=
	=?utf-8?B?Y2c0bXBxdjhsY3NCNk93c0lSejlWZmwxQWpRUFJOaXBVdmI2WU9QeXR4bGs5?=
	=?utf-8?B?VGNZVTh5bklPR0Q5ZHdjMHpCSG1SV3RDUS9UdTloZzhzRG9SeklIVFZJUGk2?=
	=?utf-8?B?VC94NjM3TDgrQVZ4L3B4TVdJdytEbGZVOHdNQXhtVlo5YUlnbXFmWXJiTnh2?=
	=?utf-8?B?Z2FxY09kR3VtTE9YZU9WT1FrY1c4bXpmUjBjcWlJZkNXVW04N0NWcE91bzFK?=
	=?utf-8?B?eWtHckZ1cklhayt2dFM3WUhwSGFoRkFvTURKYnphU1gzRUs5SURwUlphZGJv?=
	=?utf-8?B?M3F6czlwRk5lZ094SUdudWNzOTdXREhJQm43NGU0aTF0SkFFYXhLSUhiYVRN?=
	=?utf-8?B?QmJKaWlaMnEwOUZGMkhuL21RbUtyamV2VDdoaC9mNE02cjRCQkVWMUZsdE5v?=
	=?utf-8?B?MjFvbmRVTVU1aVkvWldmSy96OURYdFMwNWp1ZTRyMWUzY3UvRHpXWXlWUC9l?=
	=?utf-8?B?dXA0V0lDNkVma3NFNWk4anYrenoxR2Y4SEIzMFlUMU1LUVhaQzVEcGlYUHdG?=
	=?utf-8?B?WE9hOTFvMDhjdTNCQ1ZwRnlPdXEvLzd6NWVlMlRCRDZ6ZS9scUxwQlAwOXJC?=
	=?utf-8?B?bEVpYUFnUmo3U2pXTHErSWhiemVLNjR1NTdzZjZOVEUvVWhSZ0sxK3BqRTZD?=
	=?utf-8?B?dmV4TjFkTVVxVmZtVVh2SkpQdDhpQWtNWlk5WEc4djA3ZHNvcExhUTJMMEU3?=
	=?utf-8?B?OGNCSjBXV2tic2NaVUJvRStpa3R5N2UxcDU2ZW15WHNTcTk2VC92Ky96c2ZY?=
	=?utf-8?B?SDBDeEs1U2pQdFVBckR2Rk9STlZQcG4zVFFmdG1EM3pUMlMxcnp4T0lzZTB1?=
	=?utf-8?Q?RVT39NhhriSHiMWhs8pJPacI+PFZga2KaMBIorI?=
X-Microsoft-Exchange-Diagnostics: 1; CY4PR12MB1141;
	6:tCHKLn9J4Ra91SL9pr7kGkJDimWtkQZh0zKzmv+8Vf3r5wJcF+3PJRGYjdvbjAea16v3or5rQ3pLWlqMpIyaC7+TzP4/tblQ6Vp+dxCy25SwYzd/jR83fcPbvjmYd85y/EN7dkzuCgw6/qOAW7hfzVMwEJAcvu+HQwVn6qJDPlfZd8rYsth4mUtzJNwgmj9T9kpfEIyqB0+MYYu8NxK3MlIViiakzSx3RcQT3/K2HpVhhX+4iifl6qpGmO2C8IaNZRH81vl/FpR0AViR8R69EJuqHSCF5QHbuqHXab2C0h0AD64sxzarvKNzqGpVc6wsZ84MjPKop6XhQn5VViZH5x4wmaoatlrEOcCx+5T1v9E=;
	5:/lFwbYn9qgZxcCECnUewQ+1r3n//j0sjZMuNHmVi77v5oCEZS+6LPDDkAWAU+bwYe3gDnaQyu2qF0/3NA7sCxiGsiPFAP/3EIxVDuJoG53Pn69c2jbX+mCCQSYpNp1IcZpGwVE7HpUWy06jDuMkHoA==;
	24:kmhtqU04b0Ri41+S1oeeOnNAK6p1sSLcDPo5r7OsUHUqOmbjslC3/+IPgluzsufFOxwKDK5TMes0qF6wMs4bqhH5l5SsOLE5rWRFKmlQIYk=
SpamDiagnosticOutput: 1:99
SpamDiagnosticMetadata: NSPM
X-Microsoft-Exchange-Diagnostics: 1; CY4PR12MB1141;
	7:lCvnXg4ddKwmL74JxECgjQ6S40zu79MUZNWhfEXpt3XSwDfrzOE3W6VEZJvE7+HUKbNfsjtNebemoXVURC4rXFWruJD/yBAMF0W7iMLbL8+vNYevZRMcoN9RL8uoaTilnNEngPb4wwzLkoJ3jc1sN4tzQsp8SrnRNFJIrQRgP4Wez+y55A4JfdfZHHo8ywNsBh+LzYVKojplN6GZmcW5QMMs/0SkvPJqKauD4AFQA/FNbK7iEtWOV+Q1rwJAdhFDO61uaeEc9sXe6cObZthohGgi4WuadpezEobcDtSaqskrTuq6/QOeLDPWmoS11Zky1VAFiFGCVwFDc7IucLPKEiAebO3lGV1oLU1V8Msl0xs=;
	20:ZHmCnT16hW5WTGYikTqzoocE0cXDz6rST47lTJiJilAcuuZI5QClWHwEk3STmpCayKdGhDCctVBTvqI7lB76bD+nQohPLhcxDQQly4pV6h0oCanBbTv3B808nHhWqmuCVSjeh9m77O4wXgfoiBKwZsZ9fvIB670Lz76WPl/Cwzozah2OTQXEivtUUEEcUXQLwZTeeEenirIV15xEd91AP8TSkDddidbqtYNiwomYTt1enQW4nHN3LcbmzRwn7W1X
X-OriginatorOrg: amd.com
X-MS-Exchange-CrossTenant-OriginalArrivalTime: 10 Nov 2016 00:37:25.7080
	(UTC)
X-MS-Exchange-CrossTenant-FromEntityHeader: Hosted
X-MS-Exchange-Transport-CrossTenantHeadersStamped: CY4PR12MB1141
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=80801">Tom Lendacky</a> - Nov. 10, 2016, 12:37 a.m.</div>
<pre class="content">
Since DMA addresses will effectively look like 48-bit addresses when the
memory encryption mask is set, SWIOTLB is needed if the DMA mask of the
device performing the DMA does not support 48-bits. SWIOTLB will be
initialized to create un-encrypted bounce buffers for use by these devices.
<span class="signed-off-by">
Signed-off-by: Tom Lendacky &lt;thomas.lendacky@amd.com&gt;</span>
---
 arch/x86/include/asm/dma-mapping.h |    5 ++-
 arch/x86/include/asm/mem_encrypt.h |    5 +++
 arch/x86/kernel/pci-dma.c          |   11 ++++---
 arch/x86/kernel/pci-nommu.c        |    2 +
 arch/x86/kernel/pci-swiotlb.c      |    8 ++++-
 arch/x86/mm/mem_encrypt.c          |   17 +++++++++++
 include/linux/swiotlb.h            |    1 +
 init/main.c                        |   13 ++++++++
 lib/swiotlb.c                      |   58 +++++++++++++++++++++++++++++++-----
 9 files changed, 103 insertions(+), 17 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=69351">Radim Kr?má?</a> - Nov. 15, 2016, 2:39 p.m.</div>
<pre class="content">
2016-11-09 18:37-0600, Tom Lendacky:
<span class="quote">&gt; Since DMA addresses will effectively look like 48-bit addresses when the</span>
<span class="quote">&gt; memory encryption mask is set, SWIOTLB is needed if the DMA mask of the</span>
<span class="quote">&gt; device performing the DMA does not support 48-bits. SWIOTLB will be</span>
<span class="quote">&gt; initialized to create un-encrypted bounce buffers for use by these devices.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Tom Lendacky &lt;thomas.lendacky@amd.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt; diff --git a/arch/x86/kernel/pci-nommu.c b/arch/x86/kernel/pci-nommu.c</span>
<span class="quote">&gt; @@ -30,7 +30,7 @@ static dma_addr_t nommu_map_page(struct device *dev, struct page *page,</span>
<span class="quote">&gt;  				 enum dma_data_direction dir,</span>
<span class="quote">&gt;  				 unsigned long attrs)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	dma_addr_t bus = page_to_phys(page) + offset;</span>
<span class="quote">&gt; +	dma_addr_t bus = phys_to_dma(dev, page_to_phys(page)) + offset;</span>
<span class="quote">&gt;  	WARN_ON(size == 0);</span>
<span class="quote">&gt;  	if (!check_addr(&quot;map_single&quot;, dev, bus, size))</span>
<span class="quote">&gt;  		return DMA_ERROR_CODE;</span>
<span class="quote">&gt; diff --git a/arch/x86/kernel/pci-swiotlb.c b/arch/x86/kernel/pci-swiotlb.c</span>
<span class="quote">&gt; @@ -12,6 +12,8 @@</span>
<span class="quote">&gt;  int swiotlb __read_mostly;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  void *x86_swiotlb_alloc_coherent(struct device *hwdev, size_t size,</span>
<span class="quote">&gt; @@ -64,13 +66,15 @@ static struct dma_map_ops swiotlb_dma_ops = {</span>
<span class="quote">&gt;   * pci_swiotlb_detect_override - set swiotlb to 1 if necessary</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt;   * This returns non-zero if we are forced to use swiotlb (by the boot</span>
<span class="quote">&gt; - * option).</span>
<span class="quote">&gt; + * option). If memory encryption is enabled then swiotlb will be set</span>
<span class="quote">&gt; + * to 1 so that bounce buffers are allocated and used for devices that</span>
<span class="quote">&gt; + * do not support the addressing range required for the encryption mask.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  int __init pci_swiotlb_detect_override(void)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	int use_swiotlb = swiotlb | swiotlb_force;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (swiotlb_force)</span>
<span class="quote">&gt; +	if (swiotlb_force || sme_me_mask)</span>
<span class="quote">&gt;  		swiotlb = 1;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	return use_swiotlb;</span>

We want to return 1 even if only sme_me_mask is 1, because the return
value is used for detection.  The following would be less obscure, IMO:

	if (swiotlb_force || sme_me_mask)
		swiotlb = 1;

	return swiotlb;
<span class="quote">
&gt; diff --git a/init/main.c b/init/main.c</span>
<span class="quote">&gt; @@ -598,6 +602,15 @@ asmlinkage __visible void __init start_kernel(void)</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt;  	locking_selftest();</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * This needs to be called before any devices perform DMA</span>
<span class="quote">&gt; +	 * operations that might use the swiotlb bounce buffers.</span>
<span class="quote">&gt; +	 * This call will mark the bounce buffers as un-encrypted so</span>
<span class="quote">&gt; +	 * that their usage will not cause &quot;plain-text&quot; data to be</span>
<span class="quote">&gt; +	 * decrypted when accessed.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	mem_encrypt_init();</span>

(Comments below are connected to the reason why we call this.)
<span class="quote">
&gt; diff --git a/lib/swiotlb.c b/lib/swiotlb.c</span>
<span class="quote">&gt; @@ -159,6 +171,31 @@ void swiotlb_print_info(void)</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * If memory encryption is active, the DMA address for an encrypted page may</span>
<span class="quote">&gt; + * be beyond the range of the device. If bounce buffers are required be sure</span>
<span class="quote">&gt; + * that they are not on an encrypted page. This should be called before the</span>
<span class="quote">&gt; + * iotlb area is used.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +void __init swiotlb_clear_encryption(void)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	void *vaddr;</span>
<span class="quote">&gt; +	unsigned long bytes;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (no_iotlb_memory || !io_tlb_start || late_alloc)</span>

io_tlb_start seems redundant -- when can !no_iotlb_memory &amp;&amp;
!io_tlb_start happen?

Is the order of calls
  1) swiotlb init
  2) SME init
  3) swiotlb late init 
?

We setup encrypted swiotlb and then decrypt it, but sometimes set it up
decrypted (late_alloc) ... why isn&#39;t the swiotlb set up decrypted
directly?
<span class="quote">
&gt; +		return;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	vaddr = phys_to_virt(io_tlb_start);</span>
<span class="quote">&gt; +	bytes = PAGE_ALIGN(io_tlb_nslabs &lt;&lt; IO_TLB_SHIFT);</span>
<span class="quote">&gt; +	swiotlb_set_mem_unenc(vaddr, bytes);</span>
<span class="quote">&gt; +	memset(vaddr, 0, bytes);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	vaddr = phys_to_virt(io_tlb_overflow_buffer);</span>
<span class="quote">&gt; +	bytes = PAGE_ALIGN(io_tlb_overflow);</span>
<span class="quote">&gt; +	swiotlb_set_mem_unenc(vaddr, bytes);</span>
<span class="quote">&gt; +	memset(vaddr, 0, bytes);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; @@ -541,7 +583,7 @@ static phys_addr_t</span>
<span class="quote">&gt;  map_single(struct device *hwdev, phys_addr_t phys, size_t size,</span>
<span class="quote">&gt;  	   enum dma_data_direction dir)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	dma_addr_t start_dma_addr = phys_to_dma(hwdev, io_tlb_start);</span>
<span class="quote">&gt; +	dma_addr_t start_dma_addr = swiotlb_phys_to_dma(hwdev, io_tlb_start);</span>

We have decrypted io_tlb_start before, so shouldn&#39;t its physical address
be saved without the sme bit?  (Which changes a lot ...)

Thanks.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=1984">Michael S. Tsirkin</a> - Nov. 15, 2016, 3:16 p.m.</div>
<pre class="content">
On Wed, Nov 09, 2016 at 06:37:23PM -0600, Tom Lendacky wrote:
<span class="quote">&gt; Since DMA addresses will effectively look like 48-bit addresses when the</span>
<span class="quote">&gt; memory encryption mask is set, SWIOTLB is needed if the DMA mask of the</span>
<span class="quote">&gt; device performing the DMA does not support 48-bits. SWIOTLB will be</span>
<span class="quote">&gt; initialized to create un-encrypted bounce buffers for use by these devices.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Tom Lendacky &lt;thomas.lendacky@amd.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  arch/x86/include/asm/dma-mapping.h |    5 ++-</span>
<span class="quote">&gt;  arch/x86/include/asm/mem_encrypt.h |    5 +++</span>
<span class="quote">&gt;  arch/x86/kernel/pci-dma.c          |   11 ++++---</span>
<span class="quote">&gt;  arch/x86/kernel/pci-nommu.c        |    2 +</span>
<span class="quote">&gt;  arch/x86/kernel/pci-swiotlb.c      |    8 ++++-</span>
<span class="quote">&gt;  arch/x86/mm/mem_encrypt.c          |   17 +++++++++++</span>
<span class="quote">&gt;  include/linux/swiotlb.h            |    1 +</span>
<span class="quote">&gt;  init/main.c                        |   13 ++++++++</span>
<span class="quote">&gt;  lib/swiotlb.c                      |   58 +++++++++++++++++++++++++++++++-----</span>
<span class="quote">&gt;  9 files changed, 103 insertions(+), 17 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/arch/x86/include/asm/dma-mapping.h b/arch/x86/include/asm/dma-mapping.h</span>
<span class="quote">&gt; index 4446162..c9cdcae 100644</span>
<span class="quote">&gt; --- a/arch/x86/include/asm/dma-mapping.h</span>
<span class="quote">&gt; +++ b/arch/x86/include/asm/dma-mapping.h</span>
<span class="quote">&gt; @@ -12,6 +12,7 @@</span>
<span class="quote">&gt;  #include &lt;asm/io.h&gt;</span>
<span class="quote">&gt;  #include &lt;asm/swiotlb.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/dma-contiguous.h&gt;</span>
<span class="quote">&gt; +#include &lt;asm/mem_encrypt.h&gt;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #ifdef CONFIG_ISA</span>
<span class="quote">&gt;  # define ISA_DMA_BIT_MASK DMA_BIT_MASK(24)</span>
<span class="quote">&gt; @@ -69,12 +70,12 @@ static inline bool dma_capable(struct device *dev, dma_addr_t addr, size_t size)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static inline dma_addr_t phys_to_dma(struct device *dev, phys_addr_t paddr)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	return paddr;</span>
<span class="quote">&gt; +	return paddr | sme_me_mask;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static inline phys_addr_t dma_to_phys(struct device *dev, dma_addr_t daddr)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	return daddr;</span>
<span class="quote">&gt; +	return daddr &amp; ~sme_me_mask;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  #endif /* CONFIG_X86_DMA_REMAP */</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/arch/x86/include/asm/mem_encrypt.h b/arch/x86/include/asm/mem_encrypt.h</span>
<span class="quote">&gt; index d544481..a024451 100644</span>
<span class="quote">&gt; --- a/arch/x86/include/asm/mem_encrypt.h</span>
<span class="quote">&gt; +++ b/arch/x86/include/asm/mem_encrypt.h</span>
<span class="quote">&gt; @@ -35,6 +35,11 @@ void __init sme_encrypt_ramdisk(resource_size_t paddr,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  void __init sme_early_init(void);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +/* Architecture __weak replacement functions */</span>
<span class="quote">&gt; +void __init mem_encrypt_init(void);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +void swiotlb_set_mem_unenc(void *vaddr, unsigned long size);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  #define __sme_pa(x)		(__pa((x)) | sme_me_mask)</span>
<span class="quote">&gt;  #define __sme_pa_nodebug(x)	(__pa_nodebug((x)) | sme_me_mask)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/arch/x86/kernel/pci-dma.c b/arch/x86/kernel/pci-dma.c</span>
<span class="quote">&gt; index d30c377..0ce28df 100644</span>
<span class="quote">&gt; --- a/arch/x86/kernel/pci-dma.c</span>
<span class="quote">&gt; +++ b/arch/x86/kernel/pci-dma.c</span>
<span class="quote">&gt; @@ -92,9 +92,12 @@ again:</span>
<span class="quote">&gt;  	/* CMA can be used only in the context which permits sleeping */</span>
<span class="quote">&gt;  	if (gfpflags_allow_blocking(flag)) {</span>
<span class="quote">&gt;  		page = dma_alloc_from_contiguous(dev, count, get_order(size));</span>
<span class="quote">&gt; -		if (page &amp;&amp; page_to_phys(page) + size &gt; dma_mask) {</span>
<span class="quote">&gt; -			dma_release_from_contiguous(dev, page, count);</span>
<span class="quote">&gt; -			page = NULL;</span>
<span class="quote">&gt; +		if (page) {</span>
<span class="quote">&gt; +			addr = phys_to_dma(dev, page_to_phys(page));</span>
<span class="quote">&gt; +			if (addr + size &gt; dma_mask) {</span>
<span class="quote">&gt; +				dma_release_from_contiguous(dev, page, count);</span>
<span class="quote">&gt; +				page = NULL;</span>
<span class="quote">&gt; +			}</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  	/* fallback */</span>
<span class="quote">&gt; @@ -103,7 +106,7 @@ again:</span>
<span class="quote">&gt;  	if (!page)</span>
<span class="quote">&gt;  		return NULL;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	addr = page_to_phys(page);</span>
<span class="quote">&gt; +	addr = phys_to_dma(dev, page_to_phys(page));</span>
<span class="quote">&gt;  	if (addr + size &gt; dma_mask) {</span>
<span class="quote">&gt;  		__free_pages(page, get_order(size));</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/arch/x86/kernel/pci-nommu.c b/arch/x86/kernel/pci-nommu.c</span>
<span class="quote">&gt; index 00e71ce..922c10d 100644</span>
<span class="quote">&gt; --- a/arch/x86/kernel/pci-nommu.c</span>
<span class="quote">&gt; +++ b/arch/x86/kernel/pci-nommu.c</span>
<span class="quote">&gt; @@ -30,7 +30,7 @@ static dma_addr_t nommu_map_page(struct device *dev, struct page *page,</span>
<span class="quote">&gt;  				 enum dma_data_direction dir,</span>
<span class="quote">&gt;  				 unsigned long attrs)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	dma_addr_t bus = page_to_phys(page) + offset;</span>
<span class="quote">&gt; +	dma_addr_t bus = phys_to_dma(dev, page_to_phys(page)) + offset;</span>
<span class="quote">&gt;  	WARN_ON(size == 0);</span>
<span class="quote">&gt;  	if (!check_addr(&quot;map_single&quot;, dev, bus, size))</span>
<span class="quote">&gt;  		return DMA_ERROR_CODE;</span>
<span class="quote">&gt; diff --git a/arch/x86/kernel/pci-swiotlb.c b/arch/x86/kernel/pci-swiotlb.c</span>
<span class="quote">&gt; index b47edb8..34a9e524 100644</span>
<span class="quote">&gt; --- a/arch/x86/kernel/pci-swiotlb.c</span>
<span class="quote">&gt; +++ b/arch/x86/kernel/pci-swiotlb.c</span>
<span class="quote">&gt; @@ -12,6 +12,8 @@</span>
<span class="quote">&gt;  #include &lt;asm/dma.h&gt;</span>
<span class="quote">&gt;  #include &lt;asm/xen/swiotlb-xen.h&gt;</span>
<span class="quote">&gt;  #include &lt;asm/iommu_table.h&gt;</span>
<span class="quote">&gt; +#include &lt;asm/mem_encrypt.h&gt;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  int swiotlb __read_mostly;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  void *x86_swiotlb_alloc_coherent(struct device *hwdev, size_t size,</span>
<span class="quote">&gt; @@ -64,13 +66,15 @@ static struct dma_map_ops swiotlb_dma_ops = {</span>
<span class="quote">&gt;   * pci_swiotlb_detect_override - set swiotlb to 1 if necessary</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt;   * This returns non-zero if we are forced to use swiotlb (by the boot</span>
<span class="quote">&gt; - * option).</span>
<span class="quote">&gt; + * option). If memory encryption is enabled then swiotlb will be set</span>
<span class="quote">&gt; + * to 1 so that bounce buffers are allocated and used for devices that</span>
<span class="quote">&gt; + * do not support the addressing range required for the encryption mask.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  int __init pci_swiotlb_detect_override(void)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	int use_swiotlb = swiotlb | swiotlb_force;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (swiotlb_force)</span>
<span class="quote">&gt; +	if (swiotlb_force || sme_me_mask)</span>
<span class="quote">&gt;  		swiotlb = 1;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	return use_swiotlb;</span>
<span class="quote">&gt; diff --git a/arch/x86/mm/mem_encrypt.c b/arch/x86/mm/mem_encrypt.c</span>
<span class="quote">&gt; index 41cfdf9..e351003 100644</span>
<span class="quote">&gt; --- a/arch/x86/mm/mem_encrypt.c</span>
<span class="quote">&gt; +++ b/arch/x86/mm/mem_encrypt.c</span>
<span class="quote">&gt; @@ -13,6 +13,8 @@</span>
<span class="quote">&gt;  #include &lt;linux/linkage.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/init.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/mm.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/dma-mapping.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/swiotlb.h&gt;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #include &lt;asm/tlbflush.h&gt;</span>
<span class="quote">&gt;  #include &lt;asm/fixmap.h&gt;</span>
<span class="quote">&gt; @@ -240,3 +242,18 @@ void __init sme_early_init(void)</span>
<span class="quote">&gt;  	for (i = 0; i &lt; ARRAY_SIZE(protection_map); i++)</span>
<span class="quote">&gt;  		protection_map[i] = __pgprot(pgprot_val(protection_map[i]) | sme_me_mask);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/* Architecture __weak replacement functions */</span>
<span class="quote">&gt; +void __init mem_encrypt_init(void)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	if (!sme_me_mask)</span>
<span class="quote">&gt; +		return;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/* Make SWIOTLB use an unencrypted DMA area */</span>
<span class="quote">&gt; +	swiotlb_clear_encryption();</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +void swiotlb_set_mem_unenc(void *vaddr, unsigned long size)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	sme_set_mem_unenc(vaddr, size);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; diff --git a/include/linux/swiotlb.h b/include/linux/swiotlb.h</span>
<span class="quote">&gt; index 5f81f8a..5c909fc 100644</span>
<span class="quote">&gt; --- a/include/linux/swiotlb.h</span>
<span class="quote">&gt; +++ b/include/linux/swiotlb.h</span>
<span class="quote">&gt; @@ -29,6 +29,7 @@ int swiotlb_init_with_tbl(char *tlb, unsigned long nslabs, int verbose);</span>
<span class="quote">&gt;  extern unsigned long swiotlb_nr_tbl(void);</span>
<span class="quote">&gt;  unsigned long swiotlb_size_or_default(void);</span>
<span class="quote">&gt;  extern int swiotlb_late_init_with_tbl(char *tlb, unsigned long nslabs);</span>
<span class="quote">&gt; +extern void __init swiotlb_clear_encryption(void);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt;   * Enumeration for sync targets</span>
<span class="quote">&gt; diff --git a/init/main.c b/init/main.c</span>
<span class="quote">&gt; index a8a58e2..ae37f0d 100644</span>
<span class="quote">&gt; --- a/init/main.c</span>
<span class="quote">&gt; +++ b/init/main.c</span>
<span class="quote">&gt; @@ -458,6 +458,10 @@ void __init __weak thread_stack_cache_init(void)</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +void __init __weak mem_encrypt_init(void)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt;   * Set up kernel memory allocators</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt; @@ -598,6 +602,15 @@ asmlinkage __visible void __init start_kernel(void)</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt;  	locking_selftest();</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * This needs to be called before any devices perform DMA</span>
<span class="quote">&gt; +	 * operations that might use the swiotlb bounce buffers.</span>
<span class="quote">&gt; +	 * This call will mark the bounce buffers as un-encrypted so</span>
<span class="quote">&gt; +	 * that their usage will not cause &quot;plain-text&quot; data to be</span>
<span class="quote">&gt; +	 * decrypted when accessed.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	mem_encrypt_init();</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  #ifdef CONFIG_BLK_DEV_INITRD</span>
<span class="quote">&gt;  	if (initrd_start &amp;&amp; !initrd_below_start_ok &amp;&amp;</span>
<span class="quote">&gt;  	    page_to_pfn(virt_to_page((void *)initrd_start)) &lt; min_low_pfn) {</span>
<span class="quote">&gt; diff --git a/lib/swiotlb.c b/lib/swiotlb.c</span>
<span class="quote">&gt; index 22e13a0..638e99c 100644</span>
<span class="quote">&gt; --- a/lib/swiotlb.c</span>
<span class="quote">&gt; +++ b/lib/swiotlb.c</span>
<span class="quote">&gt; @@ -30,6 +30,7 @@</span>
<span class="quote">&gt;  #include &lt;linux/highmem.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/gfp.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/scatterlist.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/mem_encrypt.h&gt;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #include &lt;asm/io.h&gt;</span>
<span class="quote">&gt;  #include &lt;asm/dma.h&gt;</span>
<span class="quote">&gt; @@ -131,6 +132,17 @@ unsigned long swiotlb_size_or_default(void)</span>
<span class="quote">&gt;  	return size ? size : (IO_TLB_DEFAULT_SIZE);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +void __weak swiotlb_set_mem_unenc(void *vaddr, unsigned long size)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/* For swiotlb, clear memory encryption mask from dma addresses */</span>
<span class="quote">&gt; +static dma_addr_t swiotlb_phys_to_dma(struct device *hwdev,</span>
<span class="quote">&gt; +				      phys_addr_t address)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return phys_to_dma(hwdev, address) &amp; ~sme_me_mask;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  /* Note that this doesn&#39;t work with highmem page */</span>
<span class="quote">&gt;  static dma_addr_t swiotlb_virt_to_bus(struct device *hwdev,</span>
<span class="quote">&gt;  				      volatile void *address)</span>
<span class="quote">&gt; @@ -159,6 +171,31 @@ void swiotlb_print_info(void)</span>
<span class="quote">&gt;  	       bytes &gt;&gt; 20, vstart, vend - 1);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * If memory encryption is active, the DMA address for an encrypted page may</span>
<span class="quote">&gt; + * be beyond the range of the device. If bounce buffers are required be sure</span>
<span class="quote">&gt; + * that they are not on an encrypted page. This should be called before the</span>
<span class="quote">&gt; + * iotlb area is used.</span>

Makes sense, but I think at least a dmesg warning here
might be a good idea.

A boot flag that says &quot;don&#39;t enable devices that don&#39;t support
encryption&quot; might be a good idea, too, since most people
don&#39;t read dmesg output and won&#39;t notice the message.
<span class="quote">

&gt; + */</span>
<span class="quote">&gt; +void __init swiotlb_clear_encryption(void)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	void *vaddr;</span>
<span class="quote">&gt; +	unsigned long bytes;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (no_iotlb_memory || !io_tlb_start || late_alloc)</span>
<span class="quote">&gt; +		return;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	vaddr = phys_to_virt(io_tlb_start);</span>
<span class="quote">&gt; +	bytes = PAGE_ALIGN(io_tlb_nslabs &lt;&lt; IO_TLB_SHIFT);</span>
<span class="quote">&gt; +	swiotlb_set_mem_unenc(vaddr, bytes);</span>
<span class="quote">&gt; +	memset(vaddr, 0, bytes);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	vaddr = phys_to_virt(io_tlb_overflow_buffer);</span>
<span class="quote">&gt; +	bytes = PAGE_ALIGN(io_tlb_overflow);</span>
<span class="quote">&gt; +	swiotlb_set_mem_unenc(vaddr, bytes);</span>
<span class="quote">&gt; +	memset(vaddr, 0, bytes);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  int __init swiotlb_init_with_tbl(char *tlb, unsigned long nslabs, int verbose)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	void *v_overflow_buffer;</span>
<span class="quote">&gt; @@ -294,6 +331,8 @@ swiotlb_late_init_with_tbl(char *tlb, unsigned long nslabs)</span>
<span class="quote">&gt;  	io_tlb_start = virt_to_phys(tlb);</span>
<span class="quote">&gt;  	io_tlb_end = io_tlb_start + bytes;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +	/* Keep TLB in unencrypted memory if memory encryption is active */</span>
<span class="quote">&gt; +	swiotlb_set_mem_unenc(tlb, bytes);</span>
<span class="quote">&gt;  	memset(tlb, 0, bytes);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt; @@ -304,6 +343,9 @@ swiotlb_late_init_with_tbl(char *tlb, unsigned long nslabs)</span>
<span class="quote">&gt;  	if (!v_overflow_buffer)</span>
<span class="quote">&gt;  		goto cleanup2;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +	/* Keep overflow in unencrypted memory if memory encryption is active */</span>
<span class="quote">&gt; +	swiotlb_set_mem_unenc(v_overflow_buffer, io_tlb_overflow);</span>
<span class="quote">&gt; +	memset(v_overflow_buffer, 0, io_tlb_overflow);</span>
<span class="quote">&gt;  	io_tlb_overflow_buffer = virt_to_phys(v_overflow_buffer);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt; @@ -541,7 +583,7 @@ static phys_addr_t</span>
<span class="quote">&gt;  map_single(struct device *hwdev, phys_addr_t phys, size_t size,</span>
<span class="quote">&gt;  	   enum dma_data_direction dir)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	dma_addr_t start_dma_addr = phys_to_dma(hwdev, io_tlb_start);</span>
<span class="quote">&gt; +	dma_addr_t start_dma_addr = swiotlb_phys_to_dma(hwdev, io_tlb_start);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	return swiotlb_tbl_map_single(hwdev, start_dma_addr, phys, size, dir);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; @@ -659,7 +701,7 @@ swiotlb_alloc_coherent(struct device *hwdev, size_t size,</span>
<span class="quote">&gt;  			goto err_warn;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		ret = phys_to_virt(paddr);</span>
<span class="quote">&gt; -		dev_addr = phys_to_dma(hwdev, paddr);</span>
<span class="quote">&gt; +		dev_addr = swiotlb_phys_to_dma(hwdev, paddr);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		/* Confirm address can be DMA&#39;d by device */</span>
<span class="quote">&gt;  		if (dev_addr + size - 1 &gt; dma_mask) {</span>
<span class="quote">&gt; @@ -758,15 +800,15 @@ dma_addr_t swiotlb_map_page(struct device *dev, struct page *page,</span>
<span class="quote">&gt;  	map = map_single(dev, phys, size, dir);</span>
<span class="quote">&gt;  	if (map == SWIOTLB_MAP_ERROR) {</span>
<span class="quote">&gt;  		swiotlb_full(dev, size, dir, 1);</span>
<span class="quote">&gt; -		return phys_to_dma(dev, io_tlb_overflow_buffer);</span>
<span class="quote">&gt; +		return swiotlb_phys_to_dma(dev, io_tlb_overflow_buffer);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	dev_addr = phys_to_dma(dev, map);</span>
<span class="quote">&gt; +	dev_addr = swiotlb_phys_to_dma(dev, map);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/* Ensure that the address returned is DMA&#39;ble */</span>
<span class="quote">&gt;  	if (!dma_capable(dev, dev_addr, size)) {</span>
<span class="quote">&gt;  		swiotlb_tbl_unmap_single(dev, map, size, dir);</span>
<span class="quote">&gt; -		return phys_to_dma(dev, io_tlb_overflow_buffer);</span>
<span class="quote">&gt; +		return swiotlb_phys_to_dma(dev, io_tlb_overflow_buffer);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	return dev_addr;</span>
<span class="quote">&gt; @@ -901,7 +943,7 @@ swiotlb_map_sg_attrs(struct device *hwdev, struct scatterlist *sgl, int nelems,</span>
<span class="quote">&gt;  				sg_dma_len(sgl) = 0;</span>
<span class="quote">&gt;  				return 0;</span>
<span class="quote">&gt;  			}</span>
<span class="quote">&gt; -			sg-&gt;dma_address = phys_to_dma(hwdev, map);</span>
<span class="quote">&gt; +			sg-&gt;dma_address = swiotlb_phys_to_dma(hwdev, map);</span>
<span class="quote">&gt;  		} else</span>
<span class="quote">&gt;  			sg-&gt;dma_address = dev_addr;</span>
<span class="quote">&gt;  		sg_dma_len(sg) = sg-&gt;length;</span>
<span class="quote">&gt; @@ -985,7 +1027,7 @@ EXPORT_SYMBOL(swiotlb_sync_sg_for_device);</span>
<span class="quote">&gt;  int</span>
<span class="quote">&gt;  swiotlb_dma_mapping_error(struct device *hwdev, dma_addr_t dma_addr)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	return (dma_addr == phys_to_dma(hwdev, io_tlb_overflow_buffer));</span>
<span class="quote">&gt; +	return (dma_addr == swiotlb_phys_to_dma(hwdev, io_tlb_overflow_buffer));</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  EXPORT_SYMBOL(swiotlb_dma_mapping_error);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -998,6 +1040,6 @@ EXPORT_SYMBOL(swiotlb_dma_mapping_error);</span>
<span class="quote">&gt;  int</span>
<span class="quote">&gt;  swiotlb_dma_supported(struct device *hwdev, u64 mask)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	return phys_to_dma(hwdev, io_tlb_end - 1) &lt;= mask;</span>
<span class="quote">&gt; +	return swiotlb_phys_to_dma(hwdev, io_tlb_end - 1) &lt;= mask;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  EXPORT_SYMBOL(swiotlb_dma_supported);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; --</span>
<span class="quote">&gt; To unsubscribe from this list: send the line &quot;unsubscribe kvm&quot; in</span>
<span class="quote">&gt; the body of a message to majordomo@vger.kernel.org</span>
<span class="quote">&gt; More majordomo info at  http://vger.kernel.org/majordomo-info.html</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=80801">Tom Lendacky</a> - Nov. 15, 2016, 5:02 p.m.</div>
<pre class="content">
On 11/15/2016 8:39 AM, Radim Krčmář wrote:
<span class="quote">&gt; 2016-11-09 18:37-0600, Tom Lendacky:</span>
<span class="quote">&gt;&gt; Since DMA addresses will effectively look like 48-bit addresses when the</span>
<span class="quote">&gt;&gt; memory encryption mask is set, SWIOTLB is needed if the DMA mask of the</span>
<span class="quote">&gt;&gt; device performing the DMA does not support 48-bits. SWIOTLB will be</span>
<span class="quote">&gt;&gt; initialized to create un-encrypted bounce buffers for use by these devices.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Signed-off-by: Tom Lendacky &lt;thomas.lendacky@amd.com&gt;</span>
<span class="quote">&gt;&gt; ---</span>
<span class="quote">&gt;&gt; diff --git a/arch/x86/kernel/pci-nommu.c b/arch/x86/kernel/pci-nommu.c</span>
<span class="quote">&gt;&gt; @@ -30,7 +30,7 @@ static dma_addr_t nommu_map_page(struct device *dev, struct page *page,</span>
<span class="quote">&gt;&gt;  				 enum dma_data_direction dir,</span>
<span class="quote">&gt;&gt;  				 unsigned long attrs)</span>
<span class="quote">&gt;&gt;  {</span>
<span class="quote">&gt;&gt; -	dma_addr_t bus = page_to_phys(page) + offset;</span>
<span class="quote">&gt;&gt; +	dma_addr_t bus = phys_to_dma(dev, page_to_phys(page)) + offset;</span>
<span class="quote">&gt;&gt;  	WARN_ON(size == 0);</span>
<span class="quote">&gt;&gt;  	if (!check_addr(&quot;map_single&quot;, dev, bus, size))</span>
<span class="quote">&gt;&gt;  		return DMA_ERROR_CODE;</span>
<span class="quote">&gt;&gt; diff --git a/arch/x86/kernel/pci-swiotlb.c b/arch/x86/kernel/pci-swiotlb.c</span>
<span class="quote">&gt;&gt; @@ -12,6 +12,8 @@</span>
<span class="quote">&gt;&gt;  int swiotlb __read_mostly;</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  void *x86_swiotlb_alloc_coherent(struct device *hwdev, size_t size,</span>
<span class="quote">&gt;&gt; @@ -64,13 +66,15 @@ static struct dma_map_ops swiotlb_dma_ops = {</span>
<span class="quote">&gt;&gt;   * pci_swiotlb_detect_override - set swiotlb to 1 if necessary</span>
<span class="quote">&gt;&gt;   *</span>
<span class="quote">&gt;&gt;   * This returns non-zero if we are forced to use swiotlb (by the boot</span>
<span class="quote">&gt;&gt; - * option).</span>
<span class="quote">&gt;&gt; + * option). If memory encryption is enabled then swiotlb will be set</span>
<span class="quote">&gt;&gt; + * to 1 so that bounce buffers are allocated and used for devices that</span>
<span class="quote">&gt;&gt; + * do not support the addressing range required for the encryption mask.</span>
<span class="quote">&gt;&gt;   */</span>
<span class="quote">&gt;&gt;  int __init pci_swiotlb_detect_override(void)</span>
<span class="quote">&gt;&gt;  {</span>
<span class="quote">&gt;&gt;  	int use_swiotlb = swiotlb | swiotlb_force;</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; -	if (swiotlb_force)</span>
<span class="quote">&gt;&gt; +	if (swiotlb_force || sme_me_mask)</span>
<span class="quote">&gt;&gt;  		swiotlb = 1;</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  	return use_swiotlb;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; We want to return 1 even if only sme_me_mask is 1, because the return</span>
<span class="quote">&gt; value is used for detection.  The following would be less obscure, IMO:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	if (swiotlb_force || sme_me_mask)</span>
<span class="quote">&gt; 		swiotlb = 1;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	return swiotlb;</span>

If we do that then all DMA would go through the swiotlb bounce buffers.
By setting swiotlb to 1 we indicate that the bounce buffers will be
needed for those devices that can&#39;t support the addressing range when
the encryption bit is set (48 bit DMA). But if the device can support
the addressing range we won&#39;t use the bounce buffers.
<span class="quote">
&gt; </span>
<span class="quote">&gt;&gt; diff --git a/init/main.c b/init/main.c</span>
<span class="quote">&gt;&gt; @@ -598,6 +602,15 @@ asmlinkage __visible void __init start_kernel(void)</span>
<span class="quote">&gt;&gt;  	 */</span>
<span class="quote">&gt;&gt;  	locking_selftest();</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; +	/*</span>
<span class="quote">&gt;&gt; +	 * This needs to be called before any devices perform DMA</span>
<span class="quote">&gt;&gt; +	 * operations that might use the swiotlb bounce buffers.</span>
<span class="quote">&gt;&gt; +	 * This call will mark the bounce buffers as un-encrypted so</span>
<span class="quote">&gt;&gt; +	 * that their usage will not cause &quot;plain-text&quot; data to be</span>
<span class="quote">&gt;&gt; +	 * decrypted when accessed.</span>
<span class="quote">&gt;&gt; +	 */</span>
<span class="quote">&gt;&gt; +	mem_encrypt_init();</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; (Comments below are connected to the reason why we call this.)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; diff --git a/lib/swiotlb.c b/lib/swiotlb.c</span>
<span class="quote">&gt;&gt; @@ -159,6 +171,31 @@ void swiotlb_print_info(void)</span>
<span class="quote">&gt;&gt; +/*</span>
<span class="quote">&gt;&gt; + * If memory encryption is active, the DMA address for an encrypted page may</span>
<span class="quote">&gt;&gt; + * be beyond the range of the device. If bounce buffers are required be sure</span>
<span class="quote">&gt;&gt; + * that they are not on an encrypted page. This should be called before the</span>
<span class="quote">&gt;&gt; + * iotlb area is used.</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;&gt; +void __init swiotlb_clear_encryption(void)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	void *vaddr;</span>
<span class="quote">&gt;&gt; +	unsigned long bytes;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	if (no_iotlb_memory || !io_tlb_start || late_alloc)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; io_tlb_start seems redundant -- when can !no_iotlb_memory &amp;&amp;</span>
<span class="quote">&gt; !io_tlb_start happen?</span>

Yes, the io_tlb_start check can be removed.
<span class="quote">
&gt; </span>
<span class="quote">&gt; Is the order of calls</span>
<span class="quote">&gt;   1) swiotlb init</span>
<span class="quote">&gt;   2) SME init</span>
<span class="quote">&gt;   3) swiotlb late init </span>
<span class="quote">&gt; ?</span>

Yes, sort of.  The swiotlb late init may not be called.
<span class="quote">
&gt; </span>
<span class="quote">&gt; We setup encrypted swiotlb and then decrypt it, but sometimes set it up</span>
<span class="quote">&gt; decrypted (late_alloc) ... why isn&#39;t the swiotlb set up decrypted</span>
<span class="quote">&gt; directly?</span>

When swiotlb is allocated in swiotlb_init(), it is too early to make
use of the api to the change the page attributes. Because of this,
the callback to make those changes is needed.
<span class="quote">
&gt; </span>
<span class="quote">&gt;&gt; +		return;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	vaddr = phys_to_virt(io_tlb_start);</span>
<span class="quote">&gt;&gt; +	bytes = PAGE_ALIGN(io_tlb_nslabs &lt;&lt; IO_TLB_SHIFT);</span>
<span class="quote">&gt;&gt; +	swiotlb_set_mem_unenc(vaddr, bytes);</span>
<span class="quote">&gt;&gt; +	memset(vaddr, 0, bytes);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	vaddr = phys_to_virt(io_tlb_overflow_buffer);</span>
<span class="quote">&gt;&gt; +	bytes = PAGE_ALIGN(io_tlb_overflow);</span>
<span class="quote">&gt;&gt; +	swiotlb_set_mem_unenc(vaddr, bytes);</span>
<span class="quote">&gt;&gt; +	memset(vaddr, 0, bytes);</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; @@ -541,7 +583,7 @@ static phys_addr_t</span>
<span class="quote">&gt;&gt;  map_single(struct device *hwdev, phys_addr_t phys, size_t size,</span>
<span class="quote">&gt;&gt;  	   enum dma_data_direction dir)</span>
<span class="quote">&gt;&gt;  {</span>
<span class="quote">&gt;&gt; -	dma_addr_t start_dma_addr = phys_to_dma(hwdev, io_tlb_start);</span>
<span class="quote">&gt;&gt; +	dma_addr_t start_dma_addr = swiotlb_phys_to_dma(hwdev, io_tlb_start);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; We have decrypted io_tlb_start before, so shouldn&#39;t its physical address</span>
<span class="quote">&gt; be saved without the sme bit?  (Which changes a lot ...)</span>

I&#39;m not sure what you mean here, can you elaborate a bit more?

Thanks,
Tom
<span class="quote">
&gt; </span>
<span class="quote">&gt; Thanks.</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=69351">Radim Kr?má?</a> - Nov. 15, 2016, 6:17 p.m.</div>
<pre class="content">
2016-11-15 11:02-0600, Tom Lendacky:
<span class="quote">&gt; On 11/15/2016 8:39 AM, Radim Krčmář wrote:</span>
<span class="quote">&gt;&gt; 2016-11-09 18:37-0600, Tom Lendacky:</span>
<span class="quote">&gt;&gt;&gt; Since DMA addresses will effectively look like 48-bit addresses when the</span>
<span class="quote">&gt;&gt;&gt; memory encryption mask is set, SWIOTLB is needed if the DMA mask of the</span>
<span class="quote">&gt;&gt;&gt; device performing the DMA does not support 48-bits. SWIOTLB will be</span>
<span class="quote">&gt;&gt;&gt; initialized to create un-encrypted bounce buffers for use by these devices.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Signed-off-by: Tom Lendacky &lt;thomas.lendacky@amd.com&gt;</span>
<span class="quote">&gt;&gt;&gt; ---</span>
<span class="quote">&gt;&gt;&gt; diff --git a/arch/x86/kernel/pci-swiotlb.c b/arch/x86/kernel/pci-swiotlb.c</span>
<span class="quote">&gt;&gt;&gt; @@ -64,13 +66,15 @@ static struct dma_map_ops swiotlb_dma_ops = {</span>
<span class="quote">&gt;&gt;&gt;   * pci_swiotlb_detect_override - set swiotlb to 1 if necessary</span>
<span class="quote">&gt;&gt;&gt;   *</span>
<span class="quote">&gt;&gt;&gt;   * This returns non-zero if we are forced to use swiotlb (by the boot</span>
<span class="quote">&gt;&gt;&gt; - * option).</span>
<span class="quote">&gt;&gt;&gt; + * option). If memory encryption is enabled then swiotlb will be set</span>
<span class="quote">&gt;&gt;&gt; + * to 1 so that bounce buffers are allocated and used for devices that</span>
<span class="quote">&gt;&gt;&gt; + * do not support the addressing range required for the encryption mask.</span>
<span class="quote">&gt;&gt;&gt;   */</span>
<span class="quote">&gt;&gt;&gt;  int __init pci_swiotlb_detect_override(void)</span>
<span class="quote">&gt;&gt;&gt;  {</span>
<span class="quote">&gt;&gt;&gt;  	int use_swiotlb = swiotlb | swiotlb_force;</span>
<span class="quote">&gt;&gt;&gt;  </span>
<span class="quote">&gt;&gt;&gt; -	if (swiotlb_force)</span>
<span class="quote">&gt;&gt;&gt; +	if (swiotlb_force || sme_me_mask)</span>
<span class="quote">&gt;&gt;&gt;  		swiotlb = 1;</span>
<span class="quote">&gt;&gt;&gt;  </span>
<span class="quote">&gt;&gt;&gt;  	return use_swiotlb;</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; We want to return 1 even if only sme_me_mask is 1, because the return</span>
<span class="quote">&gt;&gt; value is used for detection.  The following would be less obscure, IMO:</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; 	if (swiotlb_force || sme_me_mask)</span>
<span class="quote">&gt;&gt; 		swiotlb = 1;</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; 	return swiotlb;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; If we do that then all DMA would go through the swiotlb bounce buffers.</span>

No, that is decided for example in swiotlb_map_page() and we need to
call pci_swiotlb_init() to register that function.
<span class="quote">
&gt; By setting swiotlb to 1 we indicate that the bounce buffers will be</span>
<span class="quote">&gt; needed for those devices that can&#39;t support the addressing range when</span>
<span class="quote">&gt; the encryption bit is set (48 bit DMA). But if the device can support</span>
<span class="quote">&gt; the addressing range we won&#39;t use the bounce buffers.</span>

If we return 0 here, then pci_swiotlb_init() will not be called =&gt;
dma_ops won&#39;t be set to swiotlb_dma_ops =&gt; we won&#39;t use bounce buffers.
<span class="quote">
&gt;&gt; We setup encrypted swiotlb and then decrypt it, but sometimes set it up</span>
<span class="quote">&gt;&gt; decrypted (late_alloc) ... why isn&#39;t the swiotlb set up decrypted</span>
<span class="quote">&gt;&gt; directly?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; When swiotlb is allocated in swiotlb_init(), it is too early to make</span>
<span class="quote">&gt; use of the api to the change the page attributes. Because of this,</span>
<span class="quote">&gt; the callback to make those changes is needed.</span>

Thanks. (I don&#39;t know page table setup enough to see a lesser evil. :])
<span class="quote">
&gt;&gt;&gt; @@ -541,7 +583,7 @@ static phys_addr_t</span>
<span class="quote">&gt;&gt;&gt;  map_single(struct device *hwdev, phys_addr_t phys, size_t size,</span>
<span class="quote">&gt;&gt;&gt;  	   enum dma_data_direction dir)</span>
<span class="quote">&gt;&gt;&gt;  {</span>
<span class="quote">&gt;&gt;&gt; -	dma_addr_t start_dma_addr = phys_to_dma(hwdev, io_tlb_start);</span>
<span class="quote">&gt;&gt;&gt; +	dma_addr_t start_dma_addr = swiotlb_phys_to_dma(hwdev, io_tlb_start);</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; We have decrypted io_tlb_start before, so shouldn&#39;t its physical address</span>
<span class="quote">&gt;&gt; be saved without the sme bit?  (Which changes a lot ...)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I&#39;m not sure what you mean here, can you elaborate a bit more?</span>

The C-bit (sme bit) is a part of the physical address.
If we know that a certain physical page should be accessed as
unencrypted (the bounce buffer) then the C-bit is 0.
I&#39;m wondering why we save the physical address with the C-bit set when
we know that it can&#39;t be accessed that way (because we remove it every
time).

The naming is a bit confusing, because physical addresses are actually
virtualized by SME -- maybe we should be calling them SME addresses?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=80801">Tom Lendacky</a> - Nov. 15, 2016, 6:29 p.m.</div>
<pre class="content">
On 11/15/2016 9:16 AM, Michael S. Tsirkin wrote:
<span class="quote">&gt; On Wed, Nov 09, 2016 at 06:37:23PM -0600, Tom Lendacky wrote:</span>
<span class="quote">&gt;&gt; Since DMA addresses will effectively look like 48-bit addresses when the</span>
<span class="quote">&gt;&gt; memory encryption mask is set, SWIOTLB is needed if the DMA mask of the</span>
<span class="quote">&gt;&gt; device performing the DMA does not support 48-bits. SWIOTLB will be</span>
<span class="quote">&gt;&gt; initialized to create un-encrypted bounce buffers for use by these devices.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Signed-off-by: Tom Lendacky &lt;thomas.lendacky@amd.com&gt;</span>
<span class="quote">&gt;&gt; ---</span>
<span class="quote">&gt;&gt;  arch/x86/include/asm/dma-mapping.h |    5 ++-</span>
<span class="quote">&gt;&gt;  arch/x86/include/asm/mem_encrypt.h |    5 +++</span>
<span class="quote">&gt;&gt;  arch/x86/kernel/pci-dma.c          |   11 ++++---</span>
<span class="quote">&gt;&gt;  arch/x86/kernel/pci-nommu.c        |    2 +</span>
<span class="quote">&gt;&gt;  arch/x86/kernel/pci-swiotlb.c      |    8 ++++-</span>
<span class="quote">&gt;&gt;  arch/x86/mm/mem_encrypt.c          |   17 +++++++++++</span>
<span class="quote">&gt;&gt;  include/linux/swiotlb.h            |    1 +</span>
<span class="quote">&gt;&gt;  init/main.c                        |   13 ++++++++</span>
<span class="quote">&gt;&gt;  lib/swiotlb.c                      |   58 +++++++++++++++++++++++++++++++-----</span>
<span class="quote">&gt;&gt;  9 files changed, 103 insertions(+), 17 deletions(-)</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; diff --git a/arch/x86/include/asm/dma-mapping.h b/arch/x86/include/asm/dma-mapping.h</span>
<span class="quote">&gt;&gt; index 4446162..c9cdcae 100644</span>
<span class="quote">&gt;&gt; --- a/arch/x86/include/asm/dma-mapping.h</span>
<span class="quote">&gt;&gt; +++ b/arch/x86/include/asm/dma-mapping.h</span>

..SNIP...
<span class="quote">
&gt;&gt;  </span>
<span class="quote">&gt;&gt; +/*</span>
<span class="quote">&gt;&gt; + * If memory encryption is active, the DMA address for an encrypted page may</span>
<span class="quote">&gt;&gt; + * be beyond the range of the device. If bounce buffers are required be sure</span>
<span class="quote">&gt;&gt; + * that they are not on an encrypted page. This should be called before the</span>
<span class="quote">&gt;&gt; + * iotlb area is used.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Makes sense, but I think at least a dmesg warning here</span>
<span class="quote">&gt; might be a good idea.</span>

Good idea.  Should it be a warning when it is first being set up or
a warning the first time the bounce buffers need to be used.  Or maybe
both?
<span class="quote">
&gt; </span>
<span class="quote">&gt; A boot flag that says &quot;don&#39;t enable devices that don&#39;t support</span>
<span class="quote">&gt; encryption&quot; might be a good idea, too, since most people</span>
<span class="quote">&gt; don&#39;t read dmesg output and won&#39;t notice the message.</span>

I&#39;ll look into this. It might be something that can be checked as
part of the device setting its DMA mask or the first time a DMA
API is used if the device doesn&#39;t explicitly set its mask.

Thanks,
Tom
<span class="quote">
&gt; </span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=1984">Michael S. Tsirkin</a> - Nov. 15, 2016, 7:16 p.m.</div>
<pre class="content">
On Tue, Nov 15, 2016 at 12:29:35PM -0600, Tom Lendacky wrote:
<span class="quote">&gt; On 11/15/2016 9:16 AM, Michael S. Tsirkin wrote:</span>
<span class="quote">&gt; &gt; On Wed, Nov 09, 2016 at 06:37:23PM -0600, Tom Lendacky wrote:</span>
<span class="quote">&gt; &gt;&gt; Since DMA addresses will effectively look like 48-bit addresses when the</span>
<span class="quote">&gt; &gt;&gt; memory encryption mask is set, SWIOTLB is needed if the DMA mask of the</span>
<span class="quote">&gt; &gt;&gt; device performing the DMA does not support 48-bits. SWIOTLB will be</span>
<span class="quote">&gt; &gt;&gt; initialized to create un-encrypted bounce buffers for use by these devices.</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; Signed-off-by: Tom Lendacky &lt;thomas.lendacky@amd.com&gt;</span>
<span class="quote">&gt; &gt;&gt; ---</span>
<span class="quote">&gt; &gt;&gt;  arch/x86/include/asm/dma-mapping.h |    5 ++-</span>
<span class="quote">&gt; &gt;&gt;  arch/x86/include/asm/mem_encrypt.h |    5 +++</span>
<span class="quote">&gt; &gt;&gt;  arch/x86/kernel/pci-dma.c          |   11 ++++---</span>
<span class="quote">&gt; &gt;&gt;  arch/x86/kernel/pci-nommu.c        |    2 +</span>
<span class="quote">&gt; &gt;&gt;  arch/x86/kernel/pci-swiotlb.c      |    8 ++++-</span>
<span class="quote">&gt; &gt;&gt;  arch/x86/mm/mem_encrypt.c          |   17 +++++++++++</span>
<span class="quote">&gt; &gt;&gt;  include/linux/swiotlb.h            |    1 +</span>
<span class="quote">&gt; &gt;&gt;  init/main.c                        |   13 ++++++++</span>
<span class="quote">&gt; &gt;&gt;  lib/swiotlb.c                      |   58 +++++++++++++++++++++++++++++++-----</span>
<span class="quote">&gt; &gt;&gt;  9 files changed, 103 insertions(+), 17 deletions(-)</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; diff --git a/arch/x86/include/asm/dma-mapping.h b/arch/x86/include/asm/dma-mapping.h</span>
<span class="quote">&gt; &gt;&gt; index 4446162..c9cdcae 100644</span>
<span class="quote">&gt; &gt;&gt; --- a/arch/x86/include/asm/dma-mapping.h</span>
<span class="quote">&gt; &gt;&gt; +++ b/arch/x86/include/asm/dma-mapping.h</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; ..SNIP...</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt;&gt;  </span>
<span class="quote">&gt; &gt;&gt; +/*</span>
<span class="quote">&gt; &gt;&gt; + * If memory encryption is active, the DMA address for an encrypted page may</span>
<span class="quote">&gt; &gt;&gt; + * be beyond the range of the device. If bounce buffers are required be sure</span>
<span class="quote">&gt; &gt;&gt; + * that they are not on an encrypted page. This should be called before the</span>
<span class="quote">&gt; &gt;&gt; + * iotlb area is used.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Makes sense, but I think at least a dmesg warning here</span>
<span class="quote">&gt; &gt; might be a good idea.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Good idea.  Should it be a warning when it is first being set up or</span>
<span class="quote">&gt; a warning the first time the bounce buffers need to be used.  Or maybe</span>
<span class="quote">&gt; both?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; A boot flag that says &quot;don&#39;t enable devices that don&#39;t support</span>
<span class="quote">&gt; &gt; encryption&quot; might be a good idea, too, since most people</span>
<span class="quote">&gt; &gt; don&#39;t read dmesg output and won&#39;t notice the message.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I&#39;ll look into this. It might be something that can be checked as</span>
<span class="quote">&gt; part of the device setting its DMA mask or the first time a DMA</span>
<span class="quote">&gt; API is used if the device doesn&#39;t explicitly set its mask.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Thanks,</span>
<span class="quote">&gt; Tom</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; </span>

I think setup time is nicer if it&#39;s possible.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=80801">Tom Lendacky</a> - Nov. 15, 2016, 8:33 p.m.</div>
<pre class="content">
On 11/15/2016 12:17 PM, Radim Krčmář wrote:
<span class="quote">&gt; 2016-11-15 11:02-0600, Tom Lendacky:</span>
<span class="quote">&gt;&gt; On 11/15/2016 8:39 AM, Radim Krčmář wrote:</span>
<span class="quote">&gt;&gt;&gt; 2016-11-09 18:37-0600, Tom Lendacky:</span>
<span class="quote">&gt;&gt;&gt;&gt; Since DMA addresses will effectively look like 48-bit addresses when the</span>
<span class="quote">&gt;&gt;&gt;&gt; memory encryption mask is set, SWIOTLB is needed if the DMA mask of the</span>
<span class="quote">&gt;&gt;&gt;&gt; device performing the DMA does not support 48-bits. SWIOTLB will be</span>
<span class="quote">&gt;&gt;&gt;&gt; initialized to create un-encrypted bounce buffers for use by these devices.</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; Signed-off-by: Tom Lendacky &lt;thomas.lendacky@amd.com&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; ---</span>
<span class="quote">&gt;&gt;&gt;&gt; diff --git a/arch/x86/kernel/pci-swiotlb.c b/arch/x86/kernel/pci-swiotlb.c</span>
<span class="quote">&gt;&gt;&gt;&gt; @@ -64,13 +66,15 @@ static struct dma_map_ops swiotlb_dma_ops = {</span>
<span class="quote">&gt;&gt;&gt;&gt;   * pci_swiotlb_detect_override - set swiotlb to 1 if necessary</span>
<span class="quote">&gt;&gt;&gt;&gt;   *</span>
<span class="quote">&gt;&gt;&gt;&gt;   * This returns non-zero if we are forced to use swiotlb (by the boot</span>
<span class="quote">&gt;&gt;&gt;&gt; - * option).</span>
<span class="quote">&gt;&gt;&gt;&gt; + * option). If memory encryption is enabled then swiotlb will be set</span>
<span class="quote">&gt;&gt;&gt;&gt; + * to 1 so that bounce buffers are allocated and used for devices that</span>
<span class="quote">&gt;&gt;&gt;&gt; + * do not support the addressing range required for the encryption mask.</span>
<span class="quote">&gt;&gt;&gt;&gt;   */</span>
<span class="quote">&gt;&gt;&gt;&gt;  int __init pci_swiotlb_detect_override(void)</span>
<span class="quote">&gt;&gt;&gt;&gt;  {</span>
<span class="quote">&gt;&gt;&gt;&gt;  	int use_swiotlb = swiotlb | swiotlb_force;</span>
<span class="quote">&gt;&gt;&gt;&gt;  </span>
<span class="quote">&gt;&gt;&gt;&gt; -	if (swiotlb_force)</span>
<span class="quote">&gt;&gt;&gt;&gt; +	if (swiotlb_force || sme_me_mask)</span>
<span class="quote">&gt;&gt;&gt;&gt;  		swiotlb = 1;</span>
<span class="quote">&gt;&gt;&gt;&gt;  </span>
<span class="quote">&gt;&gt;&gt;&gt;  	return use_swiotlb;</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; We want to return 1 even if only sme_me_mask is 1, because the return</span>
<span class="quote">&gt;&gt;&gt; value is used for detection.  The following would be less obscure, IMO:</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; 	if (swiotlb_force || sme_me_mask)</span>
<span class="quote">&gt;&gt;&gt; 		swiotlb = 1;</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; 	return swiotlb;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; If we do that then all DMA would go through the swiotlb bounce buffers.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; No, that is decided for example in swiotlb_map_page() and we need to</span>
<span class="quote">&gt; call pci_swiotlb_init() to register that function.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; By setting swiotlb to 1 we indicate that the bounce buffers will be</span>
<span class="quote">&gt;&gt; needed for those devices that can&#39;t support the addressing range when</span>
<span class="quote">&gt;&gt; the encryption bit is set (48 bit DMA). But if the device can support</span>
<span class="quote">&gt;&gt; the addressing range we won&#39;t use the bounce buffers.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; If we return 0 here, then pci_swiotlb_init() will not be called =&gt;</span>
<span class="quote">&gt; dma_ops won&#39;t be set to swiotlb_dma_ops =&gt; we won&#39;t use bounce buffers.</span>
<span class="quote">&gt; </span>

Ok, I see why this was working for me...  By setting swiotlb = 1 and
returning 0 it was continuing to the pci_swiotlb_detect_4gb table which
would return the current value of swiotlb, which is 1, and so the
swiotlb ops were setup.

So the change that you mentioned will work, thanks for pointing that out
and getting me to dig deeper on it.  I&#39;ll update the patch.
<span class="quote">
&gt;&gt;&gt; We setup encrypted swiotlb and then decrypt it, but sometimes set it up</span>
<span class="quote">&gt;&gt;&gt; decrypted (late_alloc) ... why isn&#39;t the swiotlb set up decrypted</span>
<span class="quote">&gt;&gt;&gt; directly?</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; When swiotlb is allocated in swiotlb_init(), it is too early to make</span>
<span class="quote">&gt;&gt; use of the api to the change the page attributes. Because of this,</span>
<span class="quote">&gt;&gt; the callback to make those changes is needed.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Thanks. (I don&#39;t know page table setup enough to see a lesser evil. :])</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt; @@ -541,7 +583,7 @@ static phys_addr_t</span>
<span class="quote">&gt;&gt;&gt;&gt;  map_single(struct device *hwdev, phys_addr_t phys, size_t size,</span>
<span class="quote">&gt;&gt;&gt;&gt;  	   enum dma_data_direction dir)</span>
<span class="quote">&gt;&gt;&gt;&gt;  {</span>
<span class="quote">&gt;&gt;&gt;&gt; -	dma_addr_t start_dma_addr = phys_to_dma(hwdev, io_tlb_start);</span>
<span class="quote">&gt;&gt;&gt;&gt; +	dma_addr_t start_dma_addr = swiotlb_phys_to_dma(hwdev, io_tlb_start);</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; We have decrypted io_tlb_start before, so shouldn&#39;t its physical address</span>
<span class="quote">&gt;&gt;&gt; be saved without the sme bit?  (Which changes a lot ...)</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; I&#39;m not sure what you mean here, can you elaborate a bit more?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The C-bit (sme bit) is a part of the physical address.</span>

The C-bit (sme_me_mask) isn&#39;t part of the physical address for
io_tlb_start, but since the original call was to phys_to_dma(), which
now will automatically &quot;or&quot; in the C-bit, I needed to adjust that by
using swiotlb_phys_to_dma() to remove the C-bit.
<span class="quote">
&gt; If we know that a certain physical page should be accessed as</span>
<span class="quote">&gt; unencrypted (the bounce buffer) then the C-bit is 0.</span>
<span class="quote">&gt; I&#39;m wondering why we save the physical address with the C-bit set when</span>
<span class="quote">&gt; we know that it can&#39;t be accessed that way (because we remove it every</span>
<span class="quote">&gt; time).</span>

It&#39;s not saved with the C-bit, but the phys_to_dma call will &quot;or&quot; in the
C-bit automatically.  And since this is common code I need to leave that
call to phys_to_dma in.
<span class="quote">
&gt; </span>
<span class="quote">&gt; The naming is a bit confusing, because physical addresses are actually</span>
<span class="quote">&gt; virtualized by SME -- maybe we should be calling them SME addresses?</span>

Interesting idea...  I&#39;ll have to look at how that plays out in the
patches and documentation.

Thanks,
Tom
<span class="quote">
&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=7500">Borislav Petkov</a> - Nov. 22, 2016, 11:38 a.m.</div>
<pre class="content">
On Tue, Nov 15, 2016 at 12:29:35PM -0600, Tom Lendacky wrote:
<span class="quote">&gt; &gt; Makes sense, but I think at least a dmesg warning here</span>
<span class="quote">&gt; &gt; might be a good idea.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Good idea.  Should it be a warning when it is first being set up or</span>
<span class="quote">&gt; a warning the first time the bounce buffers need to be used.  Or maybe</span>
<span class="quote">&gt; both?</span>

Ok, let me put my user hat on...

(... puts a felt hat ...)

so what am I supposed to do about this as a user? Go and physically
remove those devices because I want to enable SME?!

IMO, the only thing we should do is issue a *single* warning -
pr_warn_once - along the lines of:

&quot;... devices present which due to SME will use bounce buffers and will
cause their speed to diminish. Boot with sme=debug to see full info&quot;.

And then sme=debug will dump the whole gory details. I don&#39;t think
screaming for each device is going to change anything in many cases.
99% of people don&#39;t care - they just want shit to work.
<span class="quote">
&gt; &gt; A boot flag that says &quot;don&#39;t enable devices that don&#39;t support</span>
<span class="quote">&gt; &gt; encryption&quot; might be a good idea, too, since most people</span>
<span class="quote">&gt; &gt; don&#39;t read dmesg output and won&#39;t notice the message.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I&#39;ll look into this. It might be something that can be checked as</span>
<span class="quote">&gt; part of the device setting its DMA mask or the first time a DMA</span>
<span class="quote">&gt; API is used if the device doesn&#39;t explicitly set its mask.</span>

Still with my user hat on, what would be the purpose of such an option?

We already use bounce buffers so those devices do support encryption,
albeit slower.

felt hat is confused.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=1984">Michael S. Tsirkin</a> - Nov. 22, 2016, 3:22 p.m.</div>
<pre class="content">
On Tue, Nov 22, 2016 at 12:38:59PM +0100, Borislav Petkov wrote:
<span class="quote">&gt; On Tue, Nov 15, 2016 at 12:29:35PM -0600, Tom Lendacky wrote:</span>
<span class="quote">&gt; &gt; &gt; Makes sense, but I think at least a dmesg warning here</span>
<span class="quote">&gt; &gt; &gt; might be a good idea.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Good idea.  Should it be a warning when it is first being set up or</span>
<span class="quote">&gt; &gt; a warning the first time the bounce buffers need to be used.  Or maybe</span>
<span class="quote">&gt; &gt; both?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Ok, let me put my user hat on...</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; (... puts a felt hat ...)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; so what am I supposed to do about this as a user? Go and physically</span>
<span class="quote">&gt; remove those devices because I want to enable SME?!</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; IMO, the only thing we should do is issue a *single* warning -</span>
<span class="quote">&gt; pr_warn_once - along the lines of:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &quot;... devices present which due to SME will use bounce buffers and will</span>
<span class="quote">&gt; cause their speed to diminish. Boot with sme=debug to see full info&quot;.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; And then sme=debug will dump the whole gory details. I don&#39;t think</span>
<span class="quote">&gt; screaming for each device is going to change anything in many cases.</span>
<span class="quote">&gt; 99% of people don&#39;t care - they just want shit to work.</span>

The issue is it&#39;s a (potential) security hole, not a slowdown.
<span class="quote">

&gt; &gt; &gt; A boot flag that says &quot;don&#39;t enable devices that don&#39;t support</span>
<span class="quote">&gt; &gt; &gt; encryption&quot; might be a good idea, too, since most people</span>
<span class="quote">&gt; &gt; &gt; don&#39;t read dmesg output and won&#39;t notice the message.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I&#39;ll look into this. It might be something that can be checked as</span>
<span class="quote">&gt; &gt; part of the device setting its DMA mask or the first time a DMA</span>
<span class="quote">&gt; &gt; API is used if the device doesn&#39;t explicitly set its mask.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Still with my user hat on, what would be the purpose of such an option?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; We already use bounce buffers so those devices do support encryption,</span>
<span class="quote">&gt; albeit slower.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; felt hat is confused.</span>

To disable unsecure things. If someone enables SEV one might have an
expectation of security.  Might help push vendors to do the right thing
as a side effect.
<span class="quote">
&gt; -- </span>
<span class="quote">&gt; Regards/Gruss,</span>
<span class="quote">&gt;     Boris.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Good mailing practices for 400: avoid top-posting and trim the reply.</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=7500">Borislav Petkov</a> - Nov. 22, 2016, 3:41 p.m.</div>
<pre class="content">
On Tue, Nov 22, 2016 at 05:22:38PM +0200, Michael S. Tsirkin wrote:
<span class="quote">&gt; The issue is it&#39;s a (potential) security hole, not a slowdown.</span>

How? Because the bounce buffers will be unencrypted and someone might
intercept them?
<span class="quote">
&gt; To disable unsecure things. If someone enables SEV one might have an</span>
<span class="quote">&gt; expectation of security.  Might help push vendors to do the right thing</span>
<span class="quote">&gt; as a side effect.</span>

Ok, you&#39;re looking at the SEV-cloud-multiple-guests aspect. Right, that
makes sense.

I guess for SEV we should even flip the logic: disable such devices by
default and an opt-in option to enable them and issue a big fat warning.
I&#39;d even want to let the guest users know that they&#39;re on a system which
cannot give them encrypted DMA to some devices...
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=1984">Michael S. Tsirkin</a> - Nov. 22, 2016, 8:41 p.m.</div>
<pre class="content">
On Tue, Nov 22, 2016 at 04:41:37PM +0100, Borislav Petkov wrote:
<span class="quote">&gt; On Tue, Nov 22, 2016 at 05:22:38PM +0200, Michael S. Tsirkin wrote:</span>
<span class="quote">&gt; &gt; The issue is it&#39;s a (potential) security hole, not a slowdown.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; How? Because the bounce buffers will be unencrypted and someone might</span>
<span class="quote">&gt; intercept them?</span>

Or even modify them. Guests generally trust devices since they
assume they are under their control.
<span class="quote">
&gt; &gt; To disable unsecure things. If someone enables SEV one might have an</span>
<span class="quote">&gt; &gt; expectation of security.  Might help push vendors to do the right thing</span>
<span class="quote">&gt; &gt; as a side effect.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Ok, you&#39;re looking at the SEV-cloud-multiple-guests aspect. Right, that</span>
<span class="quote">&gt; makes sense.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I guess for SEV we should even flip the logic: disable such devices by</span>
<span class="quote">&gt; default and an opt-in option to enable them and issue a big fat warning.</span>
<span class="quote">&gt; I&#39;d even want to let the guest users know that they&#39;re on a system which</span>
<span class="quote">&gt; cannot give them encrypted DMA to some devices...</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -- </span>
<span class="quote">&gt; Regards/Gruss,</span>
<span class="quote">&gt;     Boris.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Good mailing practices for 400: avoid top-posting and trim the reply.</span>
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/include/asm/dma-mapping.h b/arch/x86/include/asm/dma-mapping.h</span>
<span class="p_header">index 4446162..c9cdcae 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/dma-mapping.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/dma-mapping.h</span>
<span class="p_chunk">@@ -12,6 +12,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/io.h&gt;
 #include &lt;asm/swiotlb.h&gt;
 #include &lt;linux/dma-contiguous.h&gt;
<span class="p_add">+#include &lt;asm/mem_encrypt.h&gt;</span>
 
 #ifdef CONFIG_ISA
 # define ISA_DMA_BIT_MASK DMA_BIT_MASK(24)
<span class="p_chunk">@@ -69,12 +70,12 @@</span> <span class="p_context"> static inline bool dma_capable(struct device *dev, dma_addr_t addr, size_t size)</span>
 
 static inline dma_addr_t phys_to_dma(struct device *dev, phys_addr_t paddr)
 {
<span class="p_del">-	return paddr;</span>
<span class="p_add">+	return paddr | sme_me_mask;</span>
 }
 
 static inline phys_addr_t dma_to_phys(struct device *dev, dma_addr_t daddr)
 {
<span class="p_del">-	return daddr;</span>
<span class="p_add">+	return daddr &amp; ~sme_me_mask;</span>
 }
 #endif /* CONFIG_X86_DMA_REMAP */
 
<span class="p_header">diff --git a/arch/x86/include/asm/mem_encrypt.h b/arch/x86/include/asm/mem_encrypt.h</span>
<span class="p_header">index d544481..a024451 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/mem_encrypt.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/mem_encrypt.h</span>
<span class="p_chunk">@@ -35,6 +35,11 @@</span> <span class="p_context"> void __init sme_encrypt_ramdisk(resource_size_t paddr,</span>
 
 void __init sme_early_init(void);
 
<span class="p_add">+/* Architecture __weak replacement functions */</span>
<span class="p_add">+void __init mem_encrypt_init(void);</span>
<span class="p_add">+</span>
<span class="p_add">+void swiotlb_set_mem_unenc(void *vaddr, unsigned long size);</span>
<span class="p_add">+</span>
 #define __sme_pa(x)		(__pa((x)) | sme_me_mask)
 #define __sme_pa_nodebug(x)	(__pa_nodebug((x)) | sme_me_mask)
 
<span class="p_header">diff --git a/arch/x86/kernel/pci-dma.c b/arch/x86/kernel/pci-dma.c</span>
<span class="p_header">index d30c377..0ce28df 100644</span>
<span class="p_header">--- a/arch/x86/kernel/pci-dma.c</span>
<span class="p_header">+++ b/arch/x86/kernel/pci-dma.c</span>
<span class="p_chunk">@@ -92,9 +92,12 @@</span> <span class="p_context"> again:</span>
 	/* CMA can be used only in the context which permits sleeping */
 	if (gfpflags_allow_blocking(flag)) {
 		page = dma_alloc_from_contiguous(dev, count, get_order(size));
<span class="p_del">-		if (page &amp;&amp; page_to_phys(page) + size &gt; dma_mask) {</span>
<span class="p_del">-			dma_release_from_contiguous(dev, page, count);</span>
<span class="p_del">-			page = NULL;</span>
<span class="p_add">+		if (page) {</span>
<span class="p_add">+			addr = phys_to_dma(dev, page_to_phys(page));</span>
<span class="p_add">+			if (addr + size &gt; dma_mask) {</span>
<span class="p_add">+				dma_release_from_contiguous(dev, page, count);</span>
<span class="p_add">+				page = NULL;</span>
<span class="p_add">+			}</span>
 		}
 	}
 	/* fallback */
<span class="p_chunk">@@ -103,7 +106,7 @@</span> <span class="p_context"> again:</span>
 	if (!page)
 		return NULL;
 
<span class="p_del">-	addr = page_to_phys(page);</span>
<span class="p_add">+	addr = phys_to_dma(dev, page_to_phys(page));</span>
 	if (addr + size &gt; dma_mask) {
 		__free_pages(page, get_order(size));
 
<span class="p_header">diff --git a/arch/x86/kernel/pci-nommu.c b/arch/x86/kernel/pci-nommu.c</span>
<span class="p_header">index 00e71ce..922c10d 100644</span>
<span class="p_header">--- a/arch/x86/kernel/pci-nommu.c</span>
<span class="p_header">+++ b/arch/x86/kernel/pci-nommu.c</span>
<span class="p_chunk">@@ -30,7 +30,7 @@</span> <span class="p_context"> static dma_addr_t nommu_map_page(struct device *dev, struct page *page,</span>
 				 enum dma_data_direction dir,
 				 unsigned long attrs)
 {
<span class="p_del">-	dma_addr_t bus = page_to_phys(page) + offset;</span>
<span class="p_add">+	dma_addr_t bus = phys_to_dma(dev, page_to_phys(page)) + offset;</span>
 	WARN_ON(size == 0);
 	if (!check_addr(&quot;map_single&quot;, dev, bus, size))
 		return DMA_ERROR_CODE;
<span class="p_header">diff --git a/arch/x86/kernel/pci-swiotlb.c b/arch/x86/kernel/pci-swiotlb.c</span>
<span class="p_header">index b47edb8..34a9e524 100644</span>
<span class="p_header">--- a/arch/x86/kernel/pci-swiotlb.c</span>
<span class="p_header">+++ b/arch/x86/kernel/pci-swiotlb.c</span>
<span class="p_chunk">@@ -12,6 +12,8 @@</span> <span class="p_context"></span>
 #include &lt;asm/dma.h&gt;
 #include &lt;asm/xen/swiotlb-xen.h&gt;
 #include &lt;asm/iommu_table.h&gt;
<span class="p_add">+#include &lt;asm/mem_encrypt.h&gt;</span>
<span class="p_add">+</span>
 int swiotlb __read_mostly;
 
 void *x86_swiotlb_alloc_coherent(struct device *hwdev, size_t size,
<span class="p_chunk">@@ -64,13 +66,15 @@</span> <span class="p_context"> static struct dma_map_ops swiotlb_dma_ops = {</span>
  * pci_swiotlb_detect_override - set swiotlb to 1 if necessary
  *
  * This returns non-zero if we are forced to use swiotlb (by the boot
<span class="p_del">- * option).</span>
<span class="p_add">+ * option). If memory encryption is enabled then swiotlb will be set</span>
<span class="p_add">+ * to 1 so that bounce buffers are allocated and used for devices that</span>
<span class="p_add">+ * do not support the addressing range required for the encryption mask.</span>
  */
 int __init pci_swiotlb_detect_override(void)
 {
 	int use_swiotlb = swiotlb | swiotlb_force;
 
<span class="p_del">-	if (swiotlb_force)</span>
<span class="p_add">+	if (swiotlb_force || sme_me_mask)</span>
 		swiotlb = 1;
 
 	return use_swiotlb;
<span class="p_header">diff --git a/arch/x86/mm/mem_encrypt.c b/arch/x86/mm/mem_encrypt.c</span>
<span class="p_header">index 41cfdf9..e351003 100644</span>
<span class="p_header">--- a/arch/x86/mm/mem_encrypt.c</span>
<span class="p_header">+++ b/arch/x86/mm/mem_encrypt.c</span>
<span class="p_chunk">@@ -13,6 +13,8 @@</span> <span class="p_context"></span>
 #include &lt;linux/linkage.h&gt;
 #include &lt;linux/init.h&gt;
 #include &lt;linux/mm.h&gt;
<span class="p_add">+#include &lt;linux/dma-mapping.h&gt;</span>
<span class="p_add">+#include &lt;linux/swiotlb.h&gt;</span>
 
 #include &lt;asm/tlbflush.h&gt;
 #include &lt;asm/fixmap.h&gt;
<span class="p_chunk">@@ -240,3 +242,18 @@</span> <span class="p_context"> void __init sme_early_init(void)</span>
 	for (i = 0; i &lt; ARRAY_SIZE(protection_map); i++)
 		protection_map[i] = __pgprot(pgprot_val(protection_map[i]) | sme_me_mask);
 }
<span class="p_add">+</span>
<span class="p_add">+/* Architecture __weak replacement functions */</span>
<span class="p_add">+void __init mem_encrypt_init(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!sme_me_mask)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Make SWIOTLB use an unencrypted DMA area */</span>
<span class="p_add">+	swiotlb_clear_encryption();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void swiotlb_set_mem_unenc(void *vaddr, unsigned long size)</span>
<span class="p_add">+{</span>
<span class="p_add">+	sme_set_mem_unenc(vaddr, size);</span>
<span class="p_add">+}</span>
<span class="p_header">diff --git a/include/linux/swiotlb.h b/include/linux/swiotlb.h</span>
<span class="p_header">index 5f81f8a..5c909fc 100644</span>
<span class="p_header">--- a/include/linux/swiotlb.h</span>
<span class="p_header">+++ b/include/linux/swiotlb.h</span>
<span class="p_chunk">@@ -29,6 +29,7 @@</span> <span class="p_context"> int swiotlb_init_with_tbl(char *tlb, unsigned long nslabs, int verbose);</span>
 extern unsigned long swiotlb_nr_tbl(void);
 unsigned long swiotlb_size_or_default(void);
 extern int swiotlb_late_init_with_tbl(char *tlb, unsigned long nslabs);
<span class="p_add">+extern void __init swiotlb_clear_encryption(void);</span>
 
 /*
  * Enumeration for sync targets
<span class="p_header">diff --git a/init/main.c b/init/main.c</span>
<span class="p_header">index a8a58e2..ae37f0d 100644</span>
<span class="p_header">--- a/init/main.c</span>
<span class="p_header">+++ b/init/main.c</span>
<span class="p_chunk">@@ -458,6 +458,10 @@</span> <span class="p_context"> void __init __weak thread_stack_cache_init(void)</span>
 }
 #endif
 
<span class="p_add">+void __init __weak mem_encrypt_init(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /*
  * Set up kernel memory allocators
  */
<span class="p_chunk">@@ -598,6 +602,15 @@</span> <span class="p_context"> asmlinkage __visible void __init start_kernel(void)</span>
 	 */
 	locking_selftest();
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * This needs to be called before any devices perform DMA</span>
<span class="p_add">+	 * operations that might use the swiotlb bounce buffers.</span>
<span class="p_add">+	 * This call will mark the bounce buffers as un-encrypted so</span>
<span class="p_add">+	 * that their usage will not cause &quot;plain-text&quot; data to be</span>
<span class="p_add">+	 * decrypted when accessed.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	mem_encrypt_init();</span>
<span class="p_add">+</span>
 #ifdef CONFIG_BLK_DEV_INITRD
 	if (initrd_start &amp;&amp; !initrd_below_start_ok &amp;&amp;
 	    page_to_pfn(virt_to_page((void *)initrd_start)) &lt; min_low_pfn) {
<span class="p_header">diff --git a/lib/swiotlb.c b/lib/swiotlb.c</span>
<span class="p_header">index 22e13a0..638e99c 100644</span>
<span class="p_header">--- a/lib/swiotlb.c</span>
<span class="p_header">+++ b/lib/swiotlb.c</span>
<span class="p_chunk">@@ -30,6 +30,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/highmem.h&gt;
 #include &lt;linux/gfp.h&gt;
 #include &lt;linux/scatterlist.h&gt;
<span class="p_add">+#include &lt;linux/mem_encrypt.h&gt;</span>
 
 #include &lt;asm/io.h&gt;
 #include &lt;asm/dma.h&gt;
<span class="p_chunk">@@ -131,6 +132,17 @@</span> <span class="p_context"> unsigned long swiotlb_size_or_default(void)</span>
 	return size ? size : (IO_TLB_DEFAULT_SIZE);
 }
 
<span class="p_add">+void __weak swiotlb_set_mem_unenc(void *vaddr, unsigned long size)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* For swiotlb, clear memory encryption mask from dma addresses */</span>
<span class="p_add">+static dma_addr_t swiotlb_phys_to_dma(struct device *hwdev,</span>
<span class="p_add">+				      phys_addr_t address)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return phys_to_dma(hwdev, address) &amp; ~sme_me_mask;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /* Note that this doesn&#39;t work with highmem page */
 static dma_addr_t swiotlb_virt_to_bus(struct device *hwdev,
 				      volatile void *address)
<span class="p_chunk">@@ -159,6 +171,31 @@</span> <span class="p_context"> void swiotlb_print_info(void)</span>
 	       bytes &gt;&gt; 20, vstart, vend - 1);
 }
 
<span class="p_add">+/*</span>
<span class="p_add">+ * If memory encryption is active, the DMA address for an encrypted page may</span>
<span class="p_add">+ * be beyond the range of the device. If bounce buffers are required be sure</span>
<span class="p_add">+ * that they are not on an encrypted page. This should be called before the</span>
<span class="p_add">+ * iotlb area is used.</span>
<span class="p_add">+ */</span>
<span class="p_add">+void __init swiotlb_clear_encryption(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	void *vaddr;</span>
<span class="p_add">+	unsigned long bytes;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (no_iotlb_memory || !io_tlb_start || late_alloc)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	vaddr = phys_to_virt(io_tlb_start);</span>
<span class="p_add">+	bytes = PAGE_ALIGN(io_tlb_nslabs &lt;&lt; IO_TLB_SHIFT);</span>
<span class="p_add">+	swiotlb_set_mem_unenc(vaddr, bytes);</span>
<span class="p_add">+	memset(vaddr, 0, bytes);</span>
<span class="p_add">+</span>
<span class="p_add">+	vaddr = phys_to_virt(io_tlb_overflow_buffer);</span>
<span class="p_add">+	bytes = PAGE_ALIGN(io_tlb_overflow);</span>
<span class="p_add">+	swiotlb_set_mem_unenc(vaddr, bytes);</span>
<span class="p_add">+	memset(vaddr, 0, bytes);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 int __init swiotlb_init_with_tbl(char *tlb, unsigned long nslabs, int verbose)
 {
 	void *v_overflow_buffer;
<span class="p_chunk">@@ -294,6 +331,8 @@</span> <span class="p_context"> swiotlb_late_init_with_tbl(char *tlb, unsigned long nslabs)</span>
 	io_tlb_start = virt_to_phys(tlb);
 	io_tlb_end = io_tlb_start + bytes;
 
<span class="p_add">+	/* Keep TLB in unencrypted memory if memory encryption is active */</span>
<span class="p_add">+	swiotlb_set_mem_unenc(tlb, bytes);</span>
 	memset(tlb, 0, bytes);
 
 	/*
<span class="p_chunk">@@ -304,6 +343,9 @@</span> <span class="p_context"> swiotlb_late_init_with_tbl(char *tlb, unsigned long nslabs)</span>
 	if (!v_overflow_buffer)
 		goto cleanup2;
 
<span class="p_add">+	/* Keep overflow in unencrypted memory if memory encryption is active */</span>
<span class="p_add">+	swiotlb_set_mem_unenc(v_overflow_buffer, io_tlb_overflow);</span>
<span class="p_add">+	memset(v_overflow_buffer, 0, io_tlb_overflow);</span>
 	io_tlb_overflow_buffer = virt_to_phys(v_overflow_buffer);
 
 	/*
<span class="p_chunk">@@ -541,7 +583,7 @@</span> <span class="p_context"> static phys_addr_t</span>
 map_single(struct device *hwdev, phys_addr_t phys, size_t size,
 	   enum dma_data_direction dir)
 {
<span class="p_del">-	dma_addr_t start_dma_addr = phys_to_dma(hwdev, io_tlb_start);</span>
<span class="p_add">+	dma_addr_t start_dma_addr = swiotlb_phys_to_dma(hwdev, io_tlb_start);</span>
 
 	return swiotlb_tbl_map_single(hwdev, start_dma_addr, phys, size, dir);
 }
<span class="p_chunk">@@ -659,7 +701,7 @@</span> <span class="p_context"> swiotlb_alloc_coherent(struct device *hwdev, size_t size,</span>
 			goto err_warn;
 
 		ret = phys_to_virt(paddr);
<span class="p_del">-		dev_addr = phys_to_dma(hwdev, paddr);</span>
<span class="p_add">+		dev_addr = swiotlb_phys_to_dma(hwdev, paddr);</span>
 
 		/* Confirm address can be DMA&#39;d by device */
 		if (dev_addr + size - 1 &gt; dma_mask) {
<span class="p_chunk">@@ -758,15 +800,15 @@</span> <span class="p_context"> dma_addr_t swiotlb_map_page(struct device *dev, struct page *page,</span>
 	map = map_single(dev, phys, size, dir);
 	if (map == SWIOTLB_MAP_ERROR) {
 		swiotlb_full(dev, size, dir, 1);
<span class="p_del">-		return phys_to_dma(dev, io_tlb_overflow_buffer);</span>
<span class="p_add">+		return swiotlb_phys_to_dma(dev, io_tlb_overflow_buffer);</span>
 	}
 
<span class="p_del">-	dev_addr = phys_to_dma(dev, map);</span>
<span class="p_add">+	dev_addr = swiotlb_phys_to_dma(dev, map);</span>
 
 	/* Ensure that the address returned is DMA&#39;ble */
 	if (!dma_capable(dev, dev_addr, size)) {
 		swiotlb_tbl_unmap_single(dev, map, size, dir);
<span class="p_del">-		return phys_to_dma(dev, io_tlb_overflow_buffer);</span>
<span class="p_add">+		return swiotlb_phys_to_dma(dev, io_tlb_overflow_buffer);</span>
 	}
 
 	return dev_addr;
<span class="p_chunk">@@ -901,7 +943,7 @@</span> <span class="p_context"> swiotlb_map_sg_attrs(struct device *hwdev, struct scatterlist *sgl, int nelems,</span>
 				sg_dma_len(sgl) = 0;
 				return 0;
 			}
<span class="p_del">-			sg-&gt;dma_address = phys_to_dma(hwdev, map);</span>
<span class="p_add">+			sg-&gt;dma_address = swiotlb_phys_to_dma(hwdev, map);</span>
 		} else
 			sg-&gt;dma_address = dev_addr;
 		sg_dma_len(sg) = sg-&gt;length;
<span class="p_chunk">@@ -985,7 +1027,7 @@</span> <span class="p_context"> EXPORT_SYMBOL(swiotlb_sync_sg_for_device);</span>
 int
 swiotlb_dma_mapping_error(struct device *hwdev, dma_addr_t dma_addr)
 {
<span class="p_del">-	return (dma_addr == phys_to_dma(hwdev, io_tlb_overflow_buffer));</span>
<span class="p_add">+	return (dma_addr == swiotlb_phys_to_dma(hwdev, io_tlb_overflow_buffer));</span>
 }
 EXPORT_SYMBOL(swiotlb_dma_mapping_error);
 
<span class="p_chunk">@@ -998,6 +1040,6 @@</span> <span class="p_context"> EXPORT_SYMBOL(swiotlb_dma_mapping_error);</span>
 int
 swiotlb_dma_supported(struct device *hwdev, u64 mask)
 {
<span class="p_del">-	return phys_to_dma(hwdev, io_tlb_end - 1) &lt;= mask;</span>
<span class="p_add">+	return swiotlb_phys_to_dma(hwdev, io_tlb_end - 1) &lt;= mask;</span>
 }
 EXPORT_SYMBOL(swiotlb_dma_supported);

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



