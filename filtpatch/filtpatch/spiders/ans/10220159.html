
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[GIT,PULL] x86 PTI and Spectre related fixes and updates - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [GIT,PULL] x86 PTI and Spectre related fixes and updates</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=35552">Ingo Molnar</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Feb. 15, 2018, 12:38 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20180215003832.pzi7zbze5vlovp6l@gmail.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10220159/mbox/"
   >mbox</a>
|
   <a href="/patch/10220159/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10220159/">/patch/10220159/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	BC803602C2 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 15 Feb 2018 00:38:53 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 90A8E29044
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 15 Feb 2018 00:38:53 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 84C9829079; Thu, 15 Feb 2018 00:38:53 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-3.5 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	FSL_HELO_FAKE, RCVD_IN_DNSWL_HI,
	T_DKIM_INVALID autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 13E3D29044
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 15 Feb 2018 00:38:49 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1032410AbeBOAip (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Wed, 14 Feb 2018 19:38:45 -0500
Received: from mail-wr0-f193.google.com ([209.85.128.193]:38358 &quot;EHLO
	mail-wr0-f193.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1032245AbeBOAii (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Wed, 14 Feb 2018 19:38:38 -0500
Received: by mail-wr0-f193.google.com with SMTP id n7so1711574wrn.5
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Wed, 14 Feb 2018 16:38:37 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=gmail.com; s=20161025;
	h=sender:date:from:to:cc:subject:message-id:mime-version
	:content-disposition:user-agent;
	bh=MwKk41AnPv/dPMlq9dHbxnmq8FjAuSjI5JXma7wbLw4=;
	b=a7jXHl8YM6jIXE80ymvSLyKZ4rpO/S1Bdc/VO9T/lfuBY4wSHO+Z8mDW4JV2c86hHZ
	MT74Ix4BQqIY4rFCes4HxULJK5n5gdJGjlUhnZ+jjABXsCSvzsv9QkOj4m5b0W4zEcTQ
	yV8SdglRyorfvAdi4GPjGjCWEv79zt33XUy32UGvTyCNtMc7saLDc8fxd6boBntSYfJK
	uOT4TuvbU0B6R7Nsgx27zLzOMTYJipDr7B5WT9uNsUYVYR3P40GHWlydxXNfzG8igPWk
	DNLLhVxwTjo74jhg1dikXYHkmIG0htmsBLW4jhmslG0VqLG5ar8ZM61q6n400PFjefMS
	OhVg==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20161025;
	h=x-gm-message-state:sender:date:from:to:cc:subject:message-id
	:mime-version:content-disposition:user-agent;
	bh=MwKk41AnPv/dPMlq9dHbxnmq8FjAuSjI5JXma7wbLw4=;
	b=UxwdPFlFZdDmq8xYZTmuzeQruMh+dQIvEil2hv2x2pfdCyHI1WwKvUtaJZPiKnDEus
	9Qo1NA/6w84kTLkVzStEPg/BNM3t6eBee6EjiVZhKwPe89s7/4BqrRBdeBkw7BBtibhu
	uDouKJTtpjAnmWKDcCWt7yc4KMBPkIQvR8T9KEmVfB6VMFrtGpFck5rPMi2Xj86lyvmk
	H7aZ15upbeAUe104vJOBphmB05Y+3S9d1IfY7uCCpaTz3GIFa9hfPM97JP2pjA9wBRs5
	PJTm1PgNYzSYgUhu20UlwoymrFfPa8J7aYG1pBGCax8hBueTr+EwRZvZF/txZS38RLNT
	rbtg==
X-Gm-Message-State: APf1xPARVde9bmvC40fcT19c5QozZR4qJDNbs3jB22pmWVV4ZTqq16zA
	GL/gaU7QUlgR2dPOPlk9Sk0=
X-Google-Smtp-Source: AH8x226TORQlfbYtA7z0gr9A8EApuqQmUltpQBrOhXNEn3tz5rwWMeTmlCERhC3PuPcJSi3ftjovLg==
X-Received: by 10.223.201.147 with SMTP id f19mr853167wrh.61.1518655115487; 
	Wed, 14 Feb 2018 16:38:35 -0800 (PST)
Received: from gmail.com (2E8B0CD5.catv.pool.telekom.hu. [46.139.12.213])
	by smtp.gmail.com with ESMTPSA id
	k47sm4306586wrf.96.2018.02.14.16.38.33
	(version=TLS1_2 cipher=ECDHE-RSA-CHACHA20-POLY1305 bits=256/256);
	Wed, 14 Feb 2018 16:38:34 -0800 (PST)
Date: Thu, 15 Feb 2018 01:38:32 +0100
From: Ingo Molnar &lt;mingo@kernel.org&gt;
To: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;
Cc: linux-kernel@vger.kernel.org, Thomas Gleixner &lt;tglx@linutronix.de&gt;,
	Peter Zijlstra &lt;a.p.zijlstra@chello.nl&gt;,
	Andy Lutomirski &lt;luto@kernel.org&gt;,
	&quot;H. Peter Anvin&quot; &lt;hpa@zytor.com&gt;, Borislav Petkov &lt;bp@alien8.de&gt;,
	Dave Hansen &lt;dave.hansen@linux.intel.com&gt;,
	Josh Poimboeuf &lt;jpoimboe@redhat.com&gt;,
	Greg Kroah-Hartman &lt;gregkh@linuxfoundation.org&gt;,
	Dan Williams &lt;dan.j.williams@intel.com&gt;,
	Arjan van de Ven &lt;arjan@linux.intel.com&gt;
Subject: [GIT PULL] x86 PTI and Spectre related fixes and updates
Message-ID: &lt;20180215003832.pzi7zbze5vlovp6l@gmail.com&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Disposition: inline
User-Agent: NeoMutt/20170609 (1.8.3)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=35552">Ingo Molnar</a> - Feb. 15, 2018, 12:38 a.m.</div>
<pre class="content">
Linus,

Please consider pulling the latest x86-pti-for-linus git tree from:

   git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip.git x86-pti-for-linus

   # HEAD: 48753793350974b7afe9598fd1dc46b2f1f47c2d x86/entry/64: Fix CR3 restore in paranoid_exit()

Here&#39;s the latest set of Spectre and PTI related fixes and updates:

 - spectre: Add entry code register clearing to reduce the Spectre attack surface
 - spectre: Update the Spectre microcode blacklist
 - spectre: Inline the KVM Spectre helpers to get close to v4.14 performance again.
 - spectre: Fix indirect_branch_prediction_barrier()
 - spectre: Fix/improve Spectre related kernel messages
 - spectre: Fix array_index_nospec_mask() asm constraint
 - spectre: KVM: fix two MSR handling bugs

 -     PTI: Fix a paranoid entry PTI CR3 handling bug
 -     PTI: Fix comments

 - objtool: Fix paranoid_entry() frame pointer warning
 - objtool: Annotate WARN()-related UD2 as reachable
 - objtool: Various fixes
 - objtool: Add Add Peter Zijlstra as objtool co-maintainer

 - generic: Various x86 entry code self-test fixes
 - generic: Improve/simplify entry code stack frame generation and handling after
            recent heavy-handed PTI and Spectre changes.
            (There&#39;s two more WIP improvements expected here.)
 - generic: Type fix for cache entries

There&#39;s also some low risk non-fix changes I&#39;ve included in this branch to reduce 
backporting conflicts:

 - generic: rename a confusing x86_cpu field name
 -  x86/mm: de-obfuscate the naming of single-TLB flushing primitives

(Let me know if these are not acceptable and I&#39;ll re-do the branch.)

NOTE #1:

   The last ~10 commits were freshly rebased today, because there was an
   authorship misattribution problem in two commits which I noticed only 
   when preparing the pull request, so I did an exact-same-content NOP rebase.

NOTE #2:

   This tree generates two relatively simple conflicts with your tree:

          Conflicts:
              arch/x86/kvm/vmx.c
              tools/testing/selftests/x86/Makefile

   If you want to double check it then both tip:master and linux-next have 
   the same resolutions for these conflicts.

Thanks,

	Ingo

------------------&gt;

Andy Lutomirski (1):
      x86/mm: Rename flush_tlb_single() and flush_tlb_one() to __flush_tlb_one_[user|kernel]()

Borislav Petkov (1):
      x86/entry/64: Remove the unused &#39;icebp&#39; macro

Dan Carpenter (1):
      x86/spectre: Fix an error message

Dan Williams (4):
      x86/entry/64: Clear extra registers beyond syscall arguments, to reduce speculation attack surface
      x86/entry/64: Clear registers for exceptions/interrupts, to reduce speculation attack surface
      x86/entry/64/compat: Clear registers for compat syscalls, to reduce speculation attack surface
      x86/speculation: Fix up array_index_nospec_mask() asm constraint

David Woodhouse (4):
      x86/speculation: Update Speculation Control microcode blacklist
      x86/speculation: Correct Speculation Control microcode blacklist again
      Revert &quot;x86/speculation: Simplify indirect_branch_prediction_barrier()&quot;
      KVM/x86: Reduce retpoline performance impact in slot_handle_level_range(), by always inlining iterator helper methods

Dominik Brodowski (13):
      x86/entry/64: Merge SAVE_C_REGS and SAVE_EXTRA_REGS, remove unused extensions
      x86/entry/64: Merge the POP_C_REGS and POP_EXTRA_REGS macros into a single POP_REGS macro
      x86/entry/64: Interleave XOR register clearing with PUSH instructions
      x86/entry/64: Introduce the PUSH_AND_CLEAN_REGS macro
      x86/entry/64: Use PUSH_AND_CLEAN_REGS in more cases
      x86/entry/64: Get rid of the ALLOC_PT_GPREGS_ON_STACK and SAVE_AND_CLEAR_REGS macros
      x86/entry/64: Indent PUSH_AND_CLEAR_REGS and POP_REGS properly
      selftests/x86: Fix vDSO selftest segfault for vsyscall=none
      selftests/x86: Clean up and document sscanf() usage
      selftests/x86: Fix build bug caused by the 5lvl test which has been moved to the VM directory
      selftests/x86: Do not rely on &quot;int $0x80&quot; in test_mremap_vdso.c
      selftests/x86: Do not rely on &quot;int $0x80&quot; in single_step_syscall.c
      selftests/x86: Disable tests requiring 32-bit support on pure 64-bit systems

Gustavo A. R. Silva (1):
      x86/cpu: Change type of x86_cache_size variable to unsigned int

Ingo Molnar (3):
      x86/speculation: Clean up various Spectre related details
      selftests/x86/pkeys: Remove unused functions
      x86/entry/64: Fix CR3 restore in paranoid_exit()

Jia Zhang (1):
      x86/cpu: Rename cpu_data.x86_mask to cpu_data.x86_stepping

Josh Poimboeuf (3):
      x86/entry/64: Fix paranoid_entry() frame pointer warning
      objtool: Fix segfault in ignore_unreachable_insn()
      x86/debug, objtool: Annotate WARN()-related UD2 as reachable

KarimAllah Ahmed (2):
      X86/nVMX: Properly set spec_ctrl and pred_cmd before merging MSRs
      KVM/nVMX: Set the CPU_BASED_USE_MSR_BITMAPS if we have a valid L02 MSR bitmap

Nadav Amit (1):
      x86/mm/pti: Fix PTI comment in entry_SYSCALL_64()

Peter Zijlstra (4):
      objtool: Fix switch-table detection
      MAINTAINERS: Add Peter Zijlstra as objtool co-maintainer
      x86/debug: Use UD2 for WARN()
      x86/speculation: Add &lt;asm/msr-index.h&gt; dependency

Rui Wang (1):
      selftests/x86/mpx: Fix incorrect bounds with old _sigfault

Will Deacon (1):
      nospec: Move array_index_nospec() parameter checking into separate macro


 MAINTAINERS                                       |   1 +
 arch/x86/entry/calling.h                          | 107 ++++++++++------------
 arch/x86/entry/entry_64.S                         |  92 ++++---------------
 arch/x86/entry/entry_64_compat.S                  |  30 ++++++
 arch/x86/events/intel/core.c                      |   2 +-
 arch/x86/events/intel/lbr.c                       |   2 +-
 arch/x86/events/intel/p6.c                        |   2 +-
 arch/x86/include/asm/acpi.h                       |   2 +-
 arch/x86/include/asm/barrier.h                    |   2 +-
 arch/x86/include/asm/bug.h                        |  19 ++--
 arch/x86/include/asm/nospec-branch.h              |  14 ++-
 arch/x86/include/asm/paravirt.h                   |   4 +-
 arch/x86/include/asm/paravirt_types.h             |   2 +-
 arch/x86/include/asm/pgtable_32.h                 |   2 +-
 arch/x86/include/asm/processor.h                  |   7 +-
 arch/x86/include/asm/tlbflush.h                   |  27 ++++--
 arch/x86/kernel/amd_nb.c                          |   2 +-
 arch/x86/kernel/apic/apic.c                       |   6 +-
 arch/x86/kernel/asm-offsets_32.c                  |   2 +-
 arch/x86/kernel/cpu/amd.c                         |  28 +++---
 arch/x86/kernel/cpu/bugs.c                        |  34 +++----
 arch/x86/kernel/cpu/centaur.c                     |   4 +-
 arch/x86/kernel/cpu/common.c                      |  10 +-
 arch/x86/kernel/cpu/cyrix.c                       |   2 +-
 arch/x86/kernel/cpu/intel.c                       |  31 +++----
 arch/x86/kernel/cpu/intel_rdt.c                   |   2 +-
 arch/x86/kernel/cpu/microcode/intel.c             |   6 +-
 arch/x86/kernel/cpu/mtrr/generic.c                |   2 +-
 arch/x86/kernel/cpu/mtrr/main.c                   |   4 +-
 arch/x86/kernel/cpu/proc.c                        |   8 +-
 arch/x86/kernel/head_32.S                         |   4 +-
 arch/x86/kernel/mpparse.c                         |   2 +-
 arch/x86/kernel/paravirt.c                        |   6 +-
 arch/x86/kernel/traps.c                           |   2 +-
 arch/x86/kvm/mmu.c                                |  10 +-
 arch/x86/kvm/vmx.c                                |   7 +-
 arch/x86/lib/cpu.c                                |   2 +-
 arch/x86/mm/init_64.c                             |   2 +-
 arch/x86/mm/ioremap.c                             |   2 +-
 arch/x86/mm/kmmio.c                               |   2 +-
 arch/x86/mm/pgtable_32.c                          |   2 +-
 arch/x86/mm/tlb.c                                 |   6 +-
 arch/x86/platform/uv/tlb_uv.c                     |   2 +-
 arch/x86/xen/mmu_pv.c                             |   6 +-
 drivers/char/hw_random/via-rng.c                  |   2 +-
 drivers/cpufreq/acpi-cpufreq.c                    |   2 +-
 drivers/cpufreq/longhaul.c                        |   6 +-
 drivers/cpufreq/p4-clockmod.c                     |   2 +-
 drivers/cpufreq/powernow-k7.c                     |   2 +-
 drivers/cpufreq/speedstep-centrino.c              |   4 +-
 drivers/cpufreq/speedstep-lib.c                   |   6 +-
 drivers/crypto/padlock-aes.c                      |   2 +-
 drivers/edac/amd64_edac.c                         |   2 +-
 drivers/hwmon/coretemp.c                          |   6 +-
 drivers/hwmon/hwmon-vid.c                         |   2 +-
 drivers/hwmon/k10temp.c                           |   2 +-
 drivers/hwmon/k8temp.c                            |   2 +-
 drivers/video/fbdev/geode/video_gx.c              |   2 +-
 include/linux/nospec.h                            |  36 +++++---
 include/trace/events/xen.h                        |   2 +-
 tools/objtool/check.c                             |  53 ++++++++++-
 tools/objtool/check.h                             |   1 +
 tools/testing/selftests/x86/Makefile              |  24 +++--
 tools/testing/selftests/x86/mpx-mini-test.c       |  32 ++++++-
 tools/testing/selftests/x86/protection_keys.c     |  28 ------
 tools/testing/selftests/x86/single_step_syscall.c |   5 +-
 tools/testing/selftests/x86/test_mremap_vdso.c    |   4 +
 tools/testing/selftests/x86/test_vdso.c           |  55 +++++++++--
 tools/testing/selftests/x86/test_vsyscall.c       |  11 ++-
 69 files changed, 442 insertions(+), 362 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=77">Linus Torvalds</a> - Feb. 15, 2018, 1:17 a.m.</div>
<pre class="content">
On Wed, Feb 14, 2018 at 4:38 PM, Ingo Molnar &lt;mingo@kernel.org&gt; wrote:
<span class="quote">&gt;</span>
<span class="quote">&gt;    This tree generates two relatively simple conflicts with your tree:</span>

So what annoys me about these conflicts is that I&#39;m not convinced that
the stable tree actually *uses* your fancy x86/pti branch?

I think stable ends up working like a patch-queue anyway due to how
Greg works and all his helper scripts, so the whole &quot;let&#39;s keep a
branch for pti&quot; ends up being of dubious advantage when it results in
conflicts on merging, and it&#39;s not the same commits in the end anyway.

This is not a complaint so much as a &quot;is it worth it?&quot; question..

              Linus
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=37061">gregkh@linuxfoundation.org</a> - Feb. 15, 2018, 5:48 a.m.</div>
<pre class="content">
On Wed, Feb 14, 2018 at 05:17:25PM -0800, Linus Torvalds wrote:
<span class="quote">&gt; On Wed, Feb 14, 2018 at 4:38 PM, Ingo Molnar &lt;mingo@kernel.org&gt; wrote:</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;    This tree generates two relatively simple conflicts with your tree:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So what annoys me about these conflicts is that I&#39;m not convinced that</span>
<span class="quote">&gt; the stable tree actually *uses* your fancy x86/pti branch?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I think stable ends up working like a patch-queue anyway due to how</span>
<span class="quote">&gt; Greg works and all his helper scripts, so the whole &quot;let&#39;s keep a</span>
<span class="quote">&gt; branch for pti&quot; ends up being of dubious advantage when it results in</span>
<span class="quote">&gt; conflicts on merging, and it&#39;s not the same commits in the end anyway.</span>

I do use it, I take the commits from there and then queue them up as
individual patches for the stable releases.

And if it wasn&#39;t there, the conflict resolution would have to be on my
side, making them &quot;not the same commits in the end&quot;, so either I have to
do that, or you do :)
<span class="quote">
&gt; This is not a complaint so much as a &quot;is it worth it?&quot; question..</span>

So far, I think this is the first conflict it&#39;s generated in a long
time, so previously it was worth it from my point of view.  As long as
it doesn&#39;t cause more work for the TIP maintainers, or for you, I
appreciate it.  But if it does cause more work, don&#39;t worry about it, I
can handle backporting things as needed.

thanks,

greg k-h
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=35552">Ingo Molnar</a> - Feb. 15, 2018, 8:03 a.m.</div>
<pre class="content">
* Greg Kroah-Hartman &lt;gregkh@linuxfoundation.org&gt; wrote:
<span class="quote">
&gt; On Wed, Feb 14, 2018 at 05:17:25PM -0800, Linus Torvalds wrote:</span>
<span class="quote">&gt; &gt; On Wed, Feb 14, 2018 at 4:38 PM, Ingo Molnar &lt;mingo@kernel.org&gt; wrote:</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt;    This tree generates two relatively simple conflicts with your tree:</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; So what annoys me about these conflicts is that I&#39;m not convinced that</span>
<span class="quote">&gt; &gt; the stable tree actually *uses* your fancy x86/pti branch?</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I think stable ends up working like a patch-queue anyway due to how</span>
<span class="quote">&gt; &gt; Greg works and all his helper scripts, so the whole &quot;let&#39;s keep a</span>
<span class="quote">&gt; &gt; branch for pti&quot; ends up being of dubious advantage when it results in</span>
<span class="quote">&gt; &gt; conflicts on merging, and it&#39;s not the same commits in the end anyway.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I do use it, I take the commits from there and then queue them up as</span>
<span class="quote">&gt; individual patches for the stable releases.</span>

Also note that these two conflicts came in recently - and today I eliminated them 
by switching x86/pti over to a clean v4.16 base (d4667ca14261). I&#39;d have done this 
v4.16 backmerge earlier, had I realised that it&#39;s a problem to Linus!

Also:
<span class="quote">
&gt; And if it wasn&#39;t there, the conflict resolution would have to be on my</span>
<span class="quote">&gt; side, making them &quot;not the same commits in the end&quot;, so either I have to</span>
<span class="quote">&gt; do that, or you do :)</span>

If tip:x86/pti wasn&#39;t there, I do claim that your -stable conflict resolution and 
stabilization work would have been a _LOT_ harder:
<span class="quote">
&gt; &gt; This is not a complaint so much as a &quot;is it worth it?&quot; question..</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So far, I think this is the first conflict it&#39;s generated in a long</span>
<span class="quote">&gt; time, so previously it was worth it from my point of view.  As long as</span>
<span class="quote">&gt; it doesn&#39;t cause more work for the TIP maintainers, or for you, I</span>
<span class="quote">&gt; appreciate it.  But if it does cause more work, don&#39;t worry about it, I</span>
<span class="quote">&gt; can handle backporting things as needed.</span>

Note that the role of x86/pti is not just to identify backporting commits for 
-stable, but to allow us on the x86 tree side to see how current upstream work 
interacts, and proactively allow us to group commits in a low-friction fashion.

So even if you didn&#39;t follow the x86/pti branch at all, -stable would _still_ 
benefit from the tip:x86/pti approach and the inherent backportability of all the 
PTI and Spectre commits.

There is no way we could have done this via the classic -stable tagging approach 
alone: the sheer scale of this effort (over 300 backported non-driver commits to 
core code) is almost two magnitudes higher than the Cc: stable commit tagging 
approach is usually applied to!

Also note that while x86/pti was v4.15 based yesterday and is pure v4.16+ based 
today, it&#39;s still fundamentally two (mostly) linear sets of commits:

   # v4.14 base
   v4.14..e5d77a73f364          # 71 commits: The original &#39;cherry-picked/merged&#39; PTI base

   e5d77a73f364..64e16720ea08   # 167 linear commits: PTI and Spectre v4.15 work

   7e86548e2cc8                 # merge v4.15-final into x86/pti

   7e86548e2cc8..e48657573481   # 78 linear commits + 1 limited KVM merge of related changes

   d4667ca14261                 # today I switched x86/pti to this v4.16-rc1 
                                # upstream commit that includes all of x86/pti

   d4667ca14261..x86/pti        # third backporting range starts today, empty now

The &quot;backporting value-add&quot; here is that this short description compactly and 
unambiguously identifies a set of _316_ commits to very fragile pieces of low 
level x86 code (75% of which are must-have &#39;fixes&#39; in the PTI/Spectre sense) that 
need to be applied to get v4.14 to the latest PTI and Spectre code,

Had we done this the usual way I do claim that it would have generated about a 
dozen nasty conflicts in v4.14 already by today, due to the sheer number and scope 
of PTI and Spectre commits, and the fragility and complexity of the underlying x86 
code.

Note that via the x86/pti approach the relevant v4.14.y x86 arch code is still 
almost 100% shared with Linus&#39;s latest tree, where testing results flow both 
downstream and upstream, even though it&#39;s cherry-picked and technically not the 
same tree in the Git space.

We&#39;ll see how this evolves in the v4.17 development window: PTI has already calmed 
down and things are calming down now for Spectre as well, so maybe we can switch 
back to the -stable tagging method and rely on -stable side conflict resolution.

OTOH exactly because things are calming down we can still keep x86/pti and 
robustly group, test and document the list of commits to backport. But we won&#39;t 
overdo it.

Thanks,

	Ingo
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=35552">Ingo Molnar</a> - Feb. 15, 2018, 8:16 a.m.</div>
<pre class="content">
* Ingo Molnar &lt;mingo@kernel.org&gt; wrote:
<span class="quote">
&gt; &gt; &gt; This is not a complaint so much as a &quot;is it worth it?&quot; question..</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; So far, I think this is the first conflict it&#39;s generated in a long</span>
<span class="quote">&gt; &gt; time, so previously it was worth it from my point of view.  As long as</span>
<span class="quote">&gt; &gt; it doesn&#39;t cause more work for the TIP maintainers, or for you, I</span>
<span class="quote">&gt; &gt; appreciate it.  But if it does cause more work, don&#39;t worry about it, I</span>
<span class="quote">&gt; &gt; can handle backporting things as needed.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Note that the role of x86/pti is not just to identify backporting commits for </span>
<span class="quote">&gt; -stable, but to allow us on the x86 tree side to see how current upstream work </span>
<span class="quote">&gt; interacts, and proactively allow us to group commits in a low-friction fashion.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So even if you didn&#39;t follow the x86/pti branch at all, -stable would _still_ </span>
<span class="quote">&gt; benefit from the tip:x86/pti approach and the inherent backportability of all the </span>
<span class="quote">&gt; PTI and Spectre commits.</span>

Put differently: this approach isn&#39;t a zero-sum game of &#39;upstream conflicts versus 
-stable conflicts&#39;, where if we don&#39;t resolve a conflict upstream then -stable has 
to do it and vice versa.

The tip:x86/pti approach actively avoided literally _dozens_ of nasty conflicts in 
the -stable space, at an (IMHO) much lower cost to upstream. It also IMHO 
successfully avoided the destabilizing effect that otherwise the backporting of 
most of ~300 these commits would have caused on the widely deployed v4.14 and 
v4.15 base kernels ....

Had I done this latest pull request a bit smarter Linus would not have seen these 
two conflicts either, so I still think this is the right approach and the cost to 
upstream is very low.

The x86 tree maintenance overhead was obviously higher due to all this, but right 
now I&#39;m reasonably happy about the backporting aspect/overhead, because the v4.14 
and v4.15 stable kernels are now essentially using and testing our latest upstream 
code, which allows it to stabilize a lot faster.

Thanks,

	Ingo
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=77">Linus Torvalds</a> - Feb. 15, 2018, 5:04 p.m.</div>
<pre class="content">
On Wed, Feb 14, 2018 at 9:48 PM, Greg Kroah-Hartman
&lt;gregkh@linuxfoundation.org&gt; wrote:
<span class="quote">&gt;</span>
<span class="quote">&gt; I do use it, I take the commits from there and then queue them up as</span>
<span class="quote">&gt; individual patches for the stable releases.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; And if it wasn&#39;t there, the conflict resolution would have to be on my</span>
<span class="quote">&gt; side, making them &quot;not the same commits in the end&quot;, so either I have to</span>
<span class="quote">&gt; do that, or you do :)</span>

Ok, as long as it helps something, I&#39;m ok with it. I was just
questioning whether it was just extra work.

                   Linus
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/MAINTAINERS b/MAINTAINERS</span>
<span class="p_header">index 845fc25812f1..98a22cb60773 100644</span>
<span class="p_header">--- a/MAINTAINERS</span>
<span class="p_header">+++ b/MAINTAINERS</span>
<span class="p_chunk">@@ -9813,6 +9813,7 @@</span> <span class="p_context"> F:	drivers/nfc/nxp-nci</span>
 
 OBJTOOL
 M:	Josh Poimboeuf &lt;jpoimboe@redhat.com&gt;
<span class="p_add">+M:	Peter Zijlstra &lt;peterz@infradead.org&gt;</span>
 S:	Supported
 F:	tools/objtool/
 
<span class="p_header">diff --git a/arch/x86/entry/calling.h b/arch/x86/entry/calling.h</span>
<span class="p_header">index 3f48f695d5e6..dce7092ab24a 100644</span>
<span class="p_header">--- a/arch/x86/entry/calling.h</span>
<span class="p_header">+++ b/arch/x86/entry/calling.h</span>
<span class="p_chunk">@@ -97,80 +97,69 @@</span> <span class="p_context"> For 32-bit we have the following conventions - kernel is built with</span>
 
 #define SIZEOF_PTREGS	21*8
 
<span class="p_del">-	.macro ALLOC_PT_GPREGS_ON_STACK</span>
<span class="p_del">-	addq	$-(15*8), %rsp</span>
<span class="p_del">-	.endm</span>
<span class="p_add">+.macro PUSH_AND_CLEAR_REGS rdx=%rdx rax=%rax</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Push registers and sanitize registers of values that a</span>
<span class="p_add">+	 * speculation attack might otherwise want to exploit. The</span>
<span class="p_add">+	 * lower registers are likely clobbered well before they</span>
<span class="p_add">+	 * could be put to use in a speculative execution gadget.</span>
<span class="p_add">+	 * Interleave XOR with PUSH for better uop scheduling:</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	pushq   %rdi		/* pt_regs-&gt;di */</span>
<span class="p_add">+	pushq   %rsi		/* pt_regs-&gt;si */</span>
<span class="p_add">+	pushq	\rdx		/* pt_regs-&gt;dx */</span>
<span class="p_add">+	pushq   %rcx		/* pt_regs-&gt;cx */</span>
<span class="p_add">+	pushq   \rax		/* pt_regs-&gt;ax */</span>
<span class="p_add">+	pushq   %r8		/* pt_regs-&gt;r8 */</span>
<span class="p_add">+	xorq    %r8, %r8	/* nospec   r8 */</span>
<span class="p_add">+	pushq   %r9		/* pt_regs-&gt;r9 */</span>
<span class="p_add">+	xorq    %r9, %r9	/* nospec   r9 */</span>
<span class="p_add">+	pushq   %r10		/* pt_regs-&gt;r10 */</span>
<span class="p_add">+	xorq    %r10, %r10	/* nospec   r10 */</span>
<span class="p_add">+	pushq   %r11		/* pt_regs-&gt;r11 */</span>
<span class="p_add">+	xorq    %r11, %r11	/* nospec   r11*/</span>
<span class="p_add">+	pushq	%rbx		/* pt_regs-&gt;rbx */</span>
<span class="p_add">+	xorl    %ebx, %ebx	/* nospec   rbx*/</span>
<span class="p_add">+	pushq	%rbp		/* pt_regs-&gt;rbp */</span>
<span class="p_add">+	xorl    %ebp, %ebp	/* nospec   rbp*/</span>
<span class="p_add">+	pushq	%r12		/* pt_regs-&gt;r12 */</span>
<span class="p_add">+	xorq    %r12, %r12	/* nospec   r12*/</span>
<span class="p_add">+	pushq	%r13		/* pt_regs-&gt;r13 */</span>
<span class="p_add">+	xorq    %r13, %r13	/* nospec   r13*/</span>
<span class="p_add">+	pushq	%r14		/* pt_regs-&gt;r14 */</span>
<span class="p_add">+	xorq    %r14, %r14	/* nospec   r14*/</span>
<span class="p_add">+	pushq	%r15		/* pt_regs-&gt;r15 */</span>
<span class="p_add">+	xorq    %r15, %r15	/* nospec   r15*/</span>
<span class="p_add">+	UNWIND_HINT_REGS</span>
<span class="p_add">+.endm</span>
 
<span class="p_del">-	.macro SAVE_C_REGS_HELPER offset=0 rax=1 rcx=1 r8910=1 r11=1</span>
<span class="p_del">-	.if \r11</span>
<span class="p_del">-	movq %r11, 6*8+\offset(%rsp)</span>
<span class="p_del">-	.endif</span>
<span class="p_del">-	.if \r8910</span>
<span class="p_del">-	movq %r10, 7*8+\offset(%rsp)</span>
<span class="p_del">-	movq %r9,  8*8+\offset(%rsp)</span>
<span class="p_del">-	movq %r8,  9*8+\offset(%rsp)</span>
<span class="p_del">-	.endif</span>
<span class="p_del">-	.if \rax</span>
<span class="p_del">-	movq %rax, 10*8+\offset(%rsp)</span>
<span class="p_del">-	.endif</span>
<span class="p_del">-	.if \rcx</span>
<span class="p_del">-	movq %rcx, 11*8+\offset(%rsp)</span>
<span class="p_del">-	.endif</span>
<span class="p_del">-	movq %rdx, 12*8+\offset(%rsp)</span>
<span class="p_del">-	movq %rsi, 13*8+\offset(%rsp)</span>
<span class="p_del">-	movq %rdi, 14*8+\offset(%rsp)</span>
<span class="p_del">-	UNWIND_HINT_REGS offset=\offset extra=0</span>
<span class="p_del">-	.endm</span>
<span class="p_del">-	.macro SAVE_C_REGS offset=0</span>
<span class="p_del">-	SAVE_C_REGS_HELPER \offset, 1, 1, 1, 1</span>
<span class="p_del">-	.endm</span>
<span class="p_del">-	.macro SAVE_C_REGS_EXCEPT_RAX_RCX offset=0</span>
<span class="p_del">-	SAVE_C_REGS_HELPER \offset, 0, 0, 1, 1</span>
<span class="p_del">-	.endm</span>
<span class="p_del">-	.macro SAVE_C_REGS_EXCEPT_R891011</span>
<span class="p_del">-	SAVE_C_REGS_HELPER 0, 1, 1, 0, 0</span>
<span class="p_del">-	.endm</span>
<span class="p_del">-	.macro SAVE_C_REGS_EXCEPT_RCX_R891011</span>
<span class="p_del">-	SAVE_C_REGS_HELPER 0, 1, 0, 0, 0</span>
<span class="p_del">-	.endm</span>
<span class="p_del">-	.macro SAVE_C_REGS_EXCEPT_RAX_RCX_R11</span>
<span class="p_del">-	SAVE_C_REGS_HELPER 0, 0, 0, 1, 0</span>
<span class="p_del">-	.endm</span>
<span class="p_del">-</span>
<span class="p_del">-	.macro SAVE_EXTRA_REGS offset=0</span>
<span class="p_del">-	movq %r15, 0*8+\offset(%rsp)</span>
<span class="p_del">-	movq %r14, 1*8+\offset(%rsp)</span>
<span class="p_del">-	movq %r13, 2*8+\offset(%rsp)</span>
<span class="p_del">-	movq %r12, 3*8+\offset(%rsp)</span>
<span class="p_del">-	movq %rbp, 4*8+\offset(%rsp)</span>
<span class="p_del">-	movq %rbx, 5*8+\offset(%rsp)</span>
<span class="p_del">-	UNWIND_HINT_REGS offset=\offset</span>
<span class="p_del">-	.endm</span>
<span class="p_del">-</span>
<span class="p_del">-	.macro POP_EXTRA_REGS</span>
<span class="p_add">+.macro POP_REGS pop_rdi=1 skip_r11rcx=0</span>
 	popq %r15
 	popq %r14
 	popq %r13
 	popq %r12
 	popq %rbp
 	popq %rbx
<span class="p_del">-	.endm</span>
<span class="p_del">-</span>
<span class="p_del">-	.macro POP_C_REGS</span>
<span class="p_add">+	.if \skip_r11rcx</span>
<span class="p_add">+	popq %rsi</span>
<span class="p_add">+	.else</span>
 	popq %r11
<span class="p_add">+	.endif</span>
 	popq %r10
 	popq %r9
 	popq %r8
 	popq %rax
<span class="p_add">+	.if \skip_r11rcx</span>
<span class="p_add">+	popq %rsi</span>
<span class="p_add">+	.else</span>
 	popq %rcx
<span class="p_add">+	.endif</span>
 	popq %rdx
 	popq %rsi
<span class="p_add">+	.if \pop_rdi</span>
 	popq %rdi
<span class="p_del">-	.endm</span>
<span class="p_del">-</span>
<span class="p_del">-	.macro icebp</span>
<span class="p_del">-	.byte 0xf1</span>
<span class="p_del">-	.endm</span>
<span class="p_add">+	.endif</span>
<span class="p_add">+.endm</span>
 
 /*
  * This is a sneaky trick to help the unwinder find pt_regs on the stack.  The
<span class="p_chunk">@@ -178,7 +167,7 @@</span> <span class="p_context"> For 32-bit we have the following conventions - kernel is built with</span>
  * is just setting the LSB, which makes it an invalid stack address and is also
  * a signal to the unwinder that it&#39;s a pt_regs pointer in disguise.
  *
<span class="p_del">- * NOTE: This macro must be used *after* SAVE_EXTRA_REGS because it corrupts</span>
<span class="p_add">+ * NOTE: This macro must be used *after* PUSH_AND_CLEAR_REGS because it corrupts</span>
  * the original rbp.
  */
 .macro ENCODE_FRAME_POINTER ptregs_offset=0
<span class="p_header">diff --git a/arch/x86/entry/entry_64.S b/arch/x86/entry/entry_64.S</span>
<span class="p_header">index c752abe89d80..4fd9044e72e7 100644</span>
<span class="p_header">--- a/arch/x86/entry/entry_64.S</span>
<span class="p_header">+++ b/arch/x86/entry/entry_64.S</span>
<span class="p_chunk">@@ -213,7 +213,7 @@</span> <span class="p_context"> ENTRY(entry_SYSCALL_64)</span>
 
 	swapgs
 	/*
<span class="p_del">-	 * This path is not taken when PAGE_TABLE_ISOLATION is disabled so it</span>
<span class="p_add">+	 * This path is only taken when PAGE_TABLE_ISOLATION is disabled so it</span>
 	 * is not required to switch CR3.
 	 */
 	movq	%rsp, PER_CPU_VAR(rsp_scratch)
<span class="p_chunk">@@ -227,22 +227,8 @@</span> <span class="p_context"> ENTRY(entry_SYSCALL_64)</span>
 	pushq	%rcx				/* pt_regs-&gt;ip */
 GLOBAL(entry_SYSCALL_64_after_hwframe)
 	pushq	%rax				/* pt_regs-&gt;orig_ax */
<span class="p_del">-	pushq	%rdi				/* pt_regs-&gt;di */</span>
<span class="p_del">-	pushq	%rsi				/* pt_regs-&gt;si */</span>
<span class="p_del">-	pushq	%rdx				/* pt_regs-&gt;dx */</span>
<span class="p_del">-	pushq	%rcx				/* pt_regs-&gt;cx */</span>
<span class="p_del">-	pushq	$-ENOSYS			/* pt_regs-&gt;ax */</span>
<span class="p_del">-	pushq	%r8				/* pt_regs-&gt;r8 */</span>
<span class="p_del">-	pushq	%r9				/* pt_regs-&gt;r9 */</span>
<span class="p_del">-	pushq	%r10				/* pt_regs-&gt;r10 */</span>
<span class="p_del">-	pushq	%r11				/* pt_regs-&gt;r11 */</span>
<span class="p_del">-	pushq	%rbx				/* pt_regs-&gt;rbx */</span>
<span class="p_del">-	pushq	%rbp				/* pt_regs-&gt;rbp */</span>
<span class="p_del">-	pushq	%r12				/* pt_regs-&gt;r12 */</span>
<span class="p_del">-	pushq	%r13				/* pt_regs-&gt;r13 */</span>
<span class="p_del">-	pushq	%r14				/* pt_regs-&gt;r14 */</span>
<span class="p_del">-	pushq	%r15				/* pt_regs-&gt;r15 */</span>
<span class="p_del">-	UNWIND_HINT_REGS</span>
<span class="p_add">+</span>
<span class="p_add">+	PUSH_AND_CLEAR_REGS rax=$-ENOSYS</span>
 
 	TRACE_IRQS_OFF
 
<span class="p_chunk">@@ -321,15 +307,7 @@</span> <span class="p_context"> GLOBAL(entry_SYSCALL_64_after_hwframe)</span>
 syscall_return_via_sysret:
 	/* rcx and r11 are already restored (see code above) */
 	UNWIND_HINT_EMPTY
<span class="p_del">-	POP_EXTRA_REGS</span>
<span class="p_del">-	popq	%rsi	/* skip r11 */</span>
<span class="p_del">-	popq	%r10</span>
<span class="p_del">-	popq	%r9</span>
<span class="p_del">-	popq	%r8</span>
<span class="p_del">-	popq	%rax</span>
<span class="p_del">-	popq	%rsi	/* skip rcx */</span>
<span class="p_del">-	popq	%rdx</span>
<span class="p_del">-	popq	%rsi</span>
<span class="p_add">+	POP_REGS pop_rdi=0 skip_r11rcx=1</span>
 
 	/*
 	 * Now all regs are restored except RSP and RDI.
<span class="p_chunk">@@ -559,9 +537,7 @@</span> <span class="p_context"> END(irq_entries_start)</span>
 	call	switch_to_thread_stack
 1:
 
<span class="p_del">-	ALLOC_PT_GPREGS_ON_STACK</span>
<span class="p_del">-	SAVE_C_REGS</span>
<span class="p_del">-	SAVE_EXTRA_REGS</span>
<span class="p_add">+	PUSH_AND_CLEAR_REGS</span>
 	ENCODE_FRAME_POINTER
 
 	testb	$3, CS(%rsp)
<span class="p_chunk">@@ -622,15 +598,7 @@</span> <span class="p_context"> GLOBAL(swapgs_restore_regs_and_return_to_usermode)</span>
 	ud2
 1:
 #endif
<span class="p_del">-	POP_EXTRA_REGS</span>
<span class="p_del">-	popq	%r11</span>
<span class="p_del">-	popq	%r10</span>
<span class="p_del">-	popq	%r9</span>
<span class="p_del">-	popq	%r8</span>
<span class="p_del">-	popq	%rax</span>
<span class="p_del">-	popq	%rcx</span>
<span class="p_del">-	popq	%rdx</span>
<span class="p_del">-	popq	%rsi</span>
<span class="p_add">+	POP_REGS pop_rdi=0</span>
 
 	/*
 	 * The stack is now user RDI, orig_ax, RIP, CS, EFLAGS, RSP, SS.
<span class="p_chunk">@@ -688,8 +656,7 @@</span> <span class="p_context"> GLOBAL(restore_regs_and_return_to_kernel)</span>
 	ud2
 1:
 #endif
<span class="p_del">-	POP_EXTRA_REGS</span>
<span class="p_del">-	POP_C_REGS</span>
<span class="p_add">+	POP_REGS</span>
 	addq	$8, %rsp	/* skip regs-&gt;orig_ax */
 	INTERRUPT_RETURN
 
<span class="p_chunk">@@ -904,7 +871,9 @@</span> <span class="p_context"> ENTRY(\sym)</span>
 	pushq	$-1				/* ORIG_RAX: no syscall to restart */
 	.endif
 
<span class="p_del">-	ALLOC_PT_GPREGS_ON_STACK</span>
<span class="p_add">+	/* Save all registers in pt_regs */</span>
<span class="p_add">+	PUSH_AND_CLEAR_REGS</span>
<span class="p_add">+	ENCODE_FRAME_POINTER</span>
 
 	.if \paranoid &lt; 2
 	testb	$3, CS(%rsp)			/* If coming from userspace, switch stacks */
<span class="p_chunk">@@ -1117,9 +1086,7 @@</span> <span class="p_context"> ENTRY(xen_failsafe_callback)</span>
 	addq	$0x30, %rsp
 	UNWIND_HINT_IRET_REGS
 	pushq	$-1 /* orig_ax = -1 =&gt; not a system call */
<span class="p_del">-	ALLOC_PT_GPREGS_ON_STACK</span>
<span class="p_del">-	SAVE_C_REGS</span>
<span class="p_del">-	SAVE_EXTRA_REGS</span>
<span class="p_add">+	PUSH_AND_CLEAR_REGS</span>
 	ENCODE_FRAME_POINTER
 	jmp	error_exit
 END(xen_failsafe_callback)
<span class="p_chunk">@@ -1156,16 +1123,13 @@</span> <span class="p_context"> idtentry machine_check		do_mce			has_error_code=0	paranoid=1</span>
 #endif
 
 /*
<span class="p_del">- * Save all registers in pt_regs, and switch gs if needed.</span>
<span class="p_add">+ * Switch gs if needed.</span>
  * Use slow, but surefire &quot;are we in kernel?&quot; check.
  * Return: ebx=0: need swapgs on exit, ebx=1: otherwise
  */
 ENTRY(paranoid_entry)
 	UNWIND_HINT_FUNC
 	cld
<span class="p_del">-	SAVE_C_REGS 8</span>
<span class="p_del">-	SAVE_EXTRA_REGS 8</span>
<span class="p_del">-	ENCODE_FRAME_POINTER 8</span>
 	movl	$1, %ebx
 	movl	$MSR_GS_BASE, %ecx
 	rdmsr
<span class="p_chunk">@@ -1204,21 +1168,18 @@</span> <span class="p_context"> ENTRY(paranoid_exit)</span>
 	jmp	.Lparanoid_exit_restore
 .Lparanoid_exit_no_swapgs:
 	TRACE_IRQS_IRETQ_DEBUG
<span class="p_add">+	RESTORE_CR3	scratch_reg=%rbx save_reg=%r14</span>
 .Lparanoid_exit_restore:
 	jmp restore_regs_and_return_to_kernel
 END(paranoid_exit)
 
 /*
<span class="p_del">- * Save all registers in pt_regs, and switch gs if needed.</span>
<span class="p_add">+ * Switch gs if needed.</span>
  * Return: EBX=0: came from user mode; EBX=1: otherwise
  */
 ENTRY(error_entry)
<span class="p_del">-	UNWIND_HINT_FUNC</span>
<span class="p_add">+	UNWIND_HINT_REGS offset=8</span>
 	cld
<span class="p_del">-	SAVE_C_REGS 8</span>
<span class="p_del">-	SAVE_EXTRA_REGS 8</span>
<span class="p_del">-	ENCODE_FRAME_POINTER 8</span>
<span class="p_del">-	xorl	%ebx, %ebx</span>
 	testb	$3, CS+8(%rsp)
 	jz	.Lerror_kernelspace
 
<span class="p_chunk">@@ -1399,22 +1360,7 @@</span> <span class="p_context"> ENTRY(nmi)</span>
 	pushq	1*8(%rdx)	/* pt_regs-&gt;rip */
 	UNWIND_HINT_IRET_REGS
 	pushq   $-1		/* pt_regs-&gt;orig_ax */
<span class="p_del">-	pushq   %rdi		/* pt_regs-&gt;di */</span>
<span class="p_del">-	pushq   %rsi		/* pt_regs-&gt;si */</span>
<span class="p_del">-	pushq   (%rdx)		/* pt_regs-&gt;dx */</span>
<span class="p_del">-	pushq   %rcx		/* pt_regs-&gt;cx */</span>
<span class="p_del">-	pushq   %rax		/* pt_regs-&gt;ax */</span>
<span class="p_del">-	pushq   %r8		/* pt_regs-&gt;r8 */</span>
<span class="p_del">-	pushq   %r9		/* pt_regs-&gt;r9 */</span>
<span class="p_del">-	pushq   %r10		/* pt_regs-&gt;r10 */</span>
<span class="p_del">-	pushq   %r11		/* pt_regs-&gt;r11 */</span>
<span class="p_del">-	pushq	%rbx		/* pt_regs-&gt;rbx */</span>
<span class="p_del">-	pushq	%rbp		/* pt_regs-&gt;rbp */</span>
<span class="p_del">-	pushq	%r12		/* pt_regs-&gt;r12 */</span>
<span class="p_del">-	pushq	%r13		/* pt_regs-&gt;r13 */</span>
<span class="p_del">-	pushq	%r14		/* pt_regs-&gt;r14 */</span>
<span class="p_del">-	pushq	%r15		/* pt_regs-&gt;r15 */</span>
<span class="p_del">-	UNWIND_HINT_REGS</span>
<span class="p_add">+	PUSH_AND_CLEAR_REGS rdx=(%rdx)</span>
 	ENCODE_FRAME_POINTER
 
 	/*
<span class="p_chunk">@@ -1624,7 +1570,8 @@</span> <span class="p_context"> ENTRY(nmi)</span>
 	 * frame to point back to repeat_nmi.
 	 */
 	pushq	$-1				/* ORIG_RAX: no syscall to restart */
<span class="p_del">-	ALLOC_PT_GPREGS_ON_STACK</span>
<span class="p_add">+	PUSH_AND_CLEAR_REGS</span>
<span class="p_add">+	ENCODE_FRAME_POINTER</span>
 
 	/*
 	 * Use paranoid_entry to handle SWAPGS, but no need to use paranoid_exit
<span class="p_chunk">@@ -1648,8 +1595,7 @@</span> <span class="p_context"> ENTRY(nmi)</span>
 nmi_swapgs:
 	SWAPGS_UNSAFE_STACK
 nmi_restore:
<span class="p_del">-	POP_EXTRA_REGS</span>
<span class="p_del">-	POP_C_REGS</span>
<span class="p_add">+	POP_REGS</span>
 
 	/*
 	 * Skip orig_ax and the &quot;outermost&quot; frame to point RSP at the &quot;iret&quot;
<span class="p_header">diff --git a/arch/x86/entry/entry_64_compat.S b/arch/x86/entry/entry_64_compat.S</span>
<span class="p_header">index 98d5358e4041..fd65e016e413 100644</span>
<span class="p_header">--- a/arch/x86/entry/entry_64_compat.S</span>
<span class="p_header">+++ b/arch/x86/entry/entry_64_compat.S</span>
<span class="p_chunk">@@ -85,15 +85,25 @@</span> <span class="p_context"> ENTRY(entry_SYSENTER_compat)</span>
 	pushq	%rcx			/* pt_regs-&gt;cx */
 	pushq	$-ENOSYS		/* pt_regs-&gt;ax */
 	pushq   $0			/* pt_regs-&gt;r8  = 0 */
<span class="p_add">+	xorq	%r8, %r8		/* nospec   r8 */</span>
 	pushq   $0			/* pt_regs-&gt;r9  = 0 */
<span class="p_add">+	xorq	%r9, %r9		/* nospec   r9 */</span>
 	pushq   $0			/* pt_regs-&gt;r10 = 0 */
<span class="p_add">+	xorq	%r10, %r10		/* nospec   r10 */</span>
 	pushq   $0			/* pt_regs-&gt;r11 = 0 */
<span class="p_add">+	xorq	%r11, %r11		/* nospec   r11 */</span>
 	pushq   %rbx                    /* pt_regs-&gt;rbx */
<span class="p_add">+	xorl	%ebx, %ebx		/* nospec   rbx */</span>
 	pushq   %rbp                    /* pt_regs-&gt;rbp (will be overwritten) */
<span class="p_add">+	xorl	%ebp, %ebp		/* nospec   rbp */</span>
 	pushq   $0			/* pt_regs-&gt;r12 = 0 */
<span class="p_add">+	xorq	%r12, %r12		/* nospec   r12 */</span>
 	pushq   $0			/* pt_regs-&gt;r13 = 0 */
<span class="p_add">+	xorq	%r13, %r13		/* nospec   r13 */</span>
 	pushq   $0			/* pt_regs-&gt;r14 = 0 */
<span class="p_add">+	xorq	%r14, %r14		/* nospec   r14 */</span>
 	pushq   $0			/* pt_regs-&gt;r15 = 0 */
<span class="p_add">+	xorq	%r15, %r15		/* nospec   r15 */</span>
 	cld
 
 	/*
<span class="p_chunk">@@ -214,15 +224,25 @@</span> <span class="p_context"> GLOBAL(entry_SYSCALL_compat_after_hwframe)</span>
 	pushq	%rbp			/* pt_regs-&gt;cx (stashed in bp) */
 	pushq	$-ENOSYS		/* pt_regs-&gt;ax */
 	pushq   $0			/* pt_regs-&gt;r8  = 0 */
<span class="p_add">+	xorq	%r8, %r8		/* nospec   r8 */</span>
 	pushq   $0			/* pt_regs-&gt;r9  = 0 */
<span class="p_add">+	xorq	%r9, %r9		/* nospec   r9 */</span>
 	pushq   $0			/* pt_regs-&gt;r10 = 0 */
<span class="p_add">+	xorq	%r10, %r10		/* nospec   r10 */</span>
 	pushq   $0			/* pt_regs-&gt;r11 = 0 */
<span class="p_add">+	xorq	%r11, %r11		/* nospec   r11 */</span>
 	pushq   %rbx                    /* pt_regs-&gt;rbx */
<span class="p_add">+	xorl	%ebx, %ebx		/* nospec   rbx */</span>
 	pushq   %rbp                    /* pt_regs-&gt;rbp (will be overwritten) */
<span class="p_add">+	xorl	%ebp, %ebp		/* nospec   rbp */</span>
 	pushq   $0			/* pt_regs-&gt;r12 = 0 */
<span class="p_add">+	xorq	%r12, %r12		/* nospec   r12 */</span>
 	pushq   $0			/* pt_regs-&gt;r13 = 0 */
<span class="p_add">+	xorq	%r13, %r13		/* nospec   r13 */</span>
 	pushq   $0			/* pt_regs-&gt;r14 = 0 */
<span class="p_add">+	xorq	%r14, %r14		/* nospec   r14 */</span>
 	pushq   $0			/* pt_regs-&gt;r15 = 0 */
<span class="p_add">+	xorq	%r15, %r15		/* nospec   r15 */</span>
 
 	/*
 	 * User mode is traced as though IRQs are on, and SYSENTER
<span class="p_chunk">@@ -338,15 +358,25 @@</span> <span class="p_context"> ENTRY(entry_INT80_compat)</span>
 	pushq	%rcx			/* pt_regs-&gt;cx */
 	pushq	$-ENOSYS		/* pt_regs-&gt;ax */
 	pushq   $0			/* pt_regs-&gt;r8  = 0 */
<span class="p_add">+	xorq	%r8, %r8		/* nospec   r8 */</span>
 	pushq   $0			/* pt_regs-&gt;r9  = 0 */
<span class="p_add">+	xorq	%r9, %r9		/* nospec   r9 */</span>
 	pushq   $0			/* pt_regs-&gt;r10 = 0 */
<span class="p_add">+	xorq	%r10, %r10		/* nospec   r10 */</span>
 	pushq   $0			/* pt_regs-&gt;r11 = 0 */
<span class="p_add">+	xorq	%r11, %r11		/* nospec   r11 */</span>
 	pushq   %rbx                    /* pt_regs-&gt;rbx */
<span class="p_add">+	xorl	%ebx, %ebx		/* nospec   rbx */</span>
 	pushq   %rbp                    /* pt_regs-&gt;rbp */
<span class="p_add">+	xorl	%ebp, %ebp		/* nospec   rbp */</span>
 	pushq   %r12                    /* pt_regs-&gt;r12 */
<span class="p_add">+	xorq	%r12, %r12		/* nospec   r12 */</span>
 	pushq   %r13                    /* pt_regs-&gt;r13 */
<span class="p_add">+	xorq	%r13, %r13		/* nospec   r13 */</span>
 	pushq   %r14                    /* pt_regs-&gt;r14 */
<span class="p_add">+	xorq	%r14, %r14		/* nospec   r14 */</span>
 	pushq   %r15                    /* pt_regs-&gt;r15 */
<span class="p_add">+	xorq	%r15, %r15		/* nospec   r15 */</span>
 	cld
 
 	/*
<span class="p_header">diff --git a/arch/x86/events/intel/core.c b/arch/x86/events/intel/core.c</span>
<span class="p_header">index 731153a4681e..56457cb73448 100644</span>
<span class="p_header">--- a/arch/x86/events/intel/core.c</span>
<span class="p_header">+++ b/arch/x86/events/intel/core.c</span>
<span class="p_chunk">@@ -3559,7 +3559,7 @@</span> <span class="p_context"> static int intel_snb_pebs_broken(int cpu)</span>
 		break;
 
 	case INTEL_FAM6_SANDYBRIDGE_X:
<span class="p_del">-		switch (cpu_data(cpu).x86_mask) {</span>
<span class="p_add">+		switch (cpu_data(cpu).x86_stepping) {</span>
 		case 6: rev = 0x618; break;
 		case 7: rev = 0x70c; break;
 		}
<span class="p_header">diff --git a/arch/x86/events/intel/lbr.c b/arch/x86/events/intel/lbr.c</span>
<span class="p_header">index ae64d0b69729..cf372b90557e 100644</span>
<span class="p_header">--- a/arch/x86/events/intel/lbr.c</span>
<span class="p_header">+++ b/arch/x86/events/intel/lbr.c</span>
<span class="p_chunk">@@ -1186,7 +1186,7 @@</span> <span class="p_context"> void __init intel_pmu_lbr_init_atom(void)</span>
 	 * on PMU interrupt
 	 */
 	if (boot_cpu_data.x86_model == 28
<span class="p_del">-	    &amp;&amp; boot_cpu_data.x86_mask &lt; 10) {</span>
<span class="p_add">+	    &amp;&amp; boot_cpu_data.x86_stepping &lt; 10) {</span>
 		pr_cont(&quot;LBR disabled due to erratum&quot;);
 		return;
 	}
<span class="p_header">diff --git a/arch/x86/events/intel/p6.c b/arch/x86/events/intel/p6.c</span>
<span class="p_header">index a5604c352930..408879b0c0d4 100644</span>
<span class="p_header">--- a/arch/x86/events/intel/p6.c</span>
<span class="p_header">+++ b/arch/x86/events/intel/p6.c</span>
<span class="p_chunk">@@ -234,7 +234,7 @@</span> <span class="p_context"> static __initconst const struct x86_pmu p6_pmu = {</span>
 
 static __init void p6_pmu_rdpmc_quirk(void)
 {
<span class="p_del">-	if (boot_cpu_data.x86_mask &lt; 9) {</span>
<span class="p_add">+	if (boot_cpu_data.x86_stepping &lt; 9) {</span>
 		/*
 		 * PPro erratum 26; fixed in stepping 9 and above.
 		 */
<span class="p_header">diff --git a/arch/x86/include/asm/acpi.h b/arch/x86/include/asm/acpi.h</span>
<span class="p_header">index 8d0ec9df1cbe..f077401869ee 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/acpi.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/acpi.h</span>
<span class="p_chunk">@@ -94,7 +94,7 @@</span> <span class="p_context"> static inline unsigned int acpi_processor_cstate_check(unsigned int max_cstate)</span>
 	if (boot_cpu_data.x86 == 0x0F &amp;&amp;
 	    boot_cpu_data.x86_vendor == X86_VENDOR_AMD &amp;&amp;
 	    boot_cpu_data.x86_model &lt;= 0x05 &amp;&amp;
<span class="p_del">-	    boot_cpu_data.x86_mask &lt; 0x0A)</span>
<span class="p_add">+	    boot_cpu_data.x86_stepping &lt; 0x0A)</span>
 		return 1;
 	else if (boot_cpu_has(X86_BUG_AMD_APIC_C1E))
 		return 1;
<span class="p_header">diff --git a/arch/x86/include/asm/barrier.h b/arch/x86/include/asm/barrier.h</span>
<span class="p_header">index 30d406146016..e1259f043ae9 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/barrier.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/barrier.h</span>
<span class="p_chunk">@@ -40,7 +40,7 @@</span> <span class="p_context"> static inline unsigned long array_index_mask_nospec(unsigned long index,</span>
 
 	asm (&quot;cmp %1,%2; sbb %0,%0;&quot;
 			:&quot;=r&quot; (mask)
<span class="p_del">-			:&quot;r&quot;(size),&quot;r&quot; (index)</span>
<span class="p_add">+			:&quot;g&quot;(size),&quot;r&quot; (index)</span>
 			:&quot;cc&quot;);
 	return mask;
 }
<span class="p_header">diff --git a/arch/x86/include/asm/bug.h b/arch/x86/include/asm/bug.h</span>
<span class="p_header">index 34d99af43994..6804d6642767 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/bug.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/bug.h</span>
<span class="p_chunk">@@ -5,23 +5,20 @@</span> <span class="p_context"></span>
 #include &lt;linux/stringify.h&gt;
 
 /*
<span class="p_del">- * Since some emulators terminate on UD2, we cannot use it for WARN.</span>
<span class="p_del">- * Since various instruction decoders disagree on the length of UD1,</span>
<span class="p_del">- * we cannot use it either. So use UD0 for WARN.</span>
<span class="p_add">+ * Despite that some emulators terminate on UD2, we use it for WARN().</span>
  *
<span class="p_del">- * (binutils knows about &quot;ud1&quot; but {en,de}codes it as 2 bytes, whereas</span>
<span class="p_del">- *  our kernel decoder thinks it takes a ModRM byte, which seems consistent</span>
<span class="p_del">- *  with various things like the Intel SDM instruction encoding rules)</span>
<span class="p_add">+ * Since various instruction decoders/specs disagree on the encoding of</span>
<span class="p_add">+ * UD0/UD1.</span>
  */
 
<span class="p_del">-#define ASM_UD0		&quot;.byte 0x0f, 0xff&quot;</span>
<span class="p_add">+#define ASM_UD0		&quot;.byte 0x0f, 0xff&quot; /* + ModRM (for Intel) */</span>
 #define ASM_UD1		&quot;.byte 0x0f, 0xb9&quot; /* + ModRM */
 #define ASM_UD2		&quot;.byte 0x0f, 0x0b&quot;
 
 #define INSN_UD0	0xff0f
 #define INSN_UD2	0x0b0f
 
<span class="p_del">-#define LEN_UD0		2</span>
<span class="p_add">+#define LEN_UD2		2</span>
 
 #ifdef CONFIG_GENERIC_BUG
 
<span class="p_chunk">@@ -77,7 +74,11 @@</span> <span class="p_context"> do {								\</span>
 	unreachable();						\
 } while (0)
 
<span class="p_del">-#define __WARN_FLAGS(flags)	_BUG_FLAGS(ASM_UD0, BUGFLAG_WARNING|(flags))</span>
<span class="p_add">+#define __WARN_FLAGS(flags)					\</span>
<span class="p_add">+do {								\</span>
<span class="p_add">+	_BUG_FLAGS(ASM_UD2, BUGFLAG_WARNING|(flags));		\</span>
<span class="p_add">+	annotate_reachable();					\</span>
<span class="p_add">+} while (0)</span>
 
 #include &lt;asm-generic/bug.h&gt;
 
<span class="p_header">diff --git a/arch/x86/include/asm/nospec-branch.h b/arch/x86/include/asm/nospec-branch.h</span>
<span class="p_header">index 4d57894635f2..76b058533e47 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/nospec-branch.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/nospec-branch.h</span>
<span class="p_chunk">@@ -6,6 +6,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/alternative.h&gt;
 #include &lt;asm/alternative-asm.h&gt;
 #include &lt;asm/cpufeatures.h&gt;
<span class="p_add">+#include &lt;asm/msr-index.h&gt;</span>
 
 #ifdef __ASSEMBLY__
 
<span class="p_chunk">@@ -164,10 +165,15 @@</span> <span class="p_context"> static inline void vmexit_fill_RSB(void)</span>
 
 static inline void indirect_branch_prediction_barrier(void)
 {
<span class="p_del">-	alternative_input(&quot;&quot;,</span>
<span class="p_del">-			  &quot;call __ibp_barrier&quot;,</span>
<span class="p_del">-			  X86_FEATURE_USE_IBPB,</span>
<span class="p_del">-			  ASM_NO_INPUT_CLOBBER(&quot;eax&quot;, &quot;ecx&quot;, &quot;edx&quot;, &quot;memory&quot;));</span>
<span class="p_add">+	asm volatile(ALTERNATIVE(&quot;&quot;,</span>
<span class="p_add">+				 &quot;movl %[msr], %%ecx\n\t&quot;</span>
<span class="p_add">+				 &quot;movl %[val], %%eax\n\t&quot;</span>
<span class="p_add">+				 &quot;movl $0, %%edx\n\t&quot;</span>
<span class="p_add">+				 &quot;wrmsr&quot;,</span>
<span class="p_add">+				 X86_FEATURE_USE_IBPB)</span>
<span class="p_add">+		     : : [msr] &quot;i&quot; (MSR_IA32_PRED_CMD),</span>
<span class="p_add">+			 [val] &quot;i&quot; (PRED_CMD_IBPB)</span>
<span class="p_add">+		     : &quot;eax&quot;, &quot;ecx&quot;, &quot;edx&quot;, &quot;memory&quot;);</span>
 }
 
 #endif /* __ASSEMBLY__ */
<span class="p_header">diff --git a/arch/x86/include/asm/paravirt.h b/arch/x86/include/asm/paravirt.h</span>
<span class="p_header">index 892df375b615..554841fab717 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/paravirt.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/paravirt.h</span>
<span class="p_chunk">@@ -297,9 +297,9 @@</span> <span class="p_context"> static inline void __flush_tlb_global(void)</span>
 {
 	PVOP_VCALL0(pv_mmu_ops.flush_tlb_kernel);
 }
<span class="p_del">-static inline void __flush_tlb_single(unsigned long addr)</span>
<span class="p_add">+static inline void __flush_tlb_one_user(unsigned long addr)</span>
 {
<span class="p_del">-	PVOP_VCALL1(pv_mmu_ops.flush_tlb_single, addr);</span>
<span class="p_add">+	PVOP_VCALL1(pv_mmu_ops.flush_tlb_one_user, addr);</span>
 }
 
 static inline void flush_tlb_others(const struct cpumask *cpumask,
<span class="p_header">diff --git a/arch/x86/include/asm/paravirt_types.h b/arch/x86/include/asm/paravirt_types.h</span>
<span class="p_header">index 6ec54d01972d..f624f1f10316 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/paravirt_types.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/paravirt_types.h</span>
<span class="p_chunk">@@ -217,7 +217,7 @@</span> <span class="p_context"> struct pv_mmu_ops {</span>
 	/* TLB operations */
 	void (*flush_tlb_user)(void);
 	void (*flush_tlb_kernel)(void);
<span class="p_del">-	void (*flush_tlb_single)(unsigned long addr);</span>
<span class="p_add">+	void (*flush_tlb_one_user)(unsigned long addr);</span>
 	void (*flush_tlb_others)(const struct cpumask *cpus,
 				 const struct flush_tlb_info *info);
 
<span class="p_header">diff --git a/arch/x86/include/asm/pgtable_32.h b/arch/x86/include/asm/pgtable_32.h</span>
<span class="p_header">index e67c0620aec2..e55466760ff8 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/pgtable_32.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/pgtable_32.h</span>
<span class="p_chunk">@@ -61,7 +61,7 @@</span> <span class="p_context"> void paging_init(void);</span>
 #define kpte_clear_flush(ptep, vaddr)		\
 do {						\
 	pte_clear(&amp;init_mm, (vaddr), (ptep));	\
<span class="p_del">-	__flush_tlb_one((vaddr));		\</span>
<span class="p_add">+	__flush_tlb_one_kernel((vaddr));		\</span>
 } while (0)
 
 #endif /* !__ASSEMBLY__ */
<span class="p_header">diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h</span>
<span class="p_header">index 513f9604c192..44c2c4ec6d60 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/processor.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/processor.h</span>
<span class="p_chunk">@@ -91,7 +91,7 @@</span> <span class="p_context"> struct cpuinfo_x86 {</span>
 	__u8			x86;		/* CPU family */
 	__u8			x86_vendor;	/* CPU vendor */
 	__u8			x86_model;
<span class="p_del">-	__u8			x86_mask;</span>
<span class="p_add">+	__u8			x86_stepping;</span>
 #ifdef CONFIG_X86_64
 	/* Number of 4K pages in DTLB/ITLB combined(in pages): */
 	int			x86_tlbsize;
<span class="p_chunk">@@ -109,7 +109,7 @@</span> <span class="p_context"> struct cpuinfo_x86 {</span>
 	char			x86_vendor_id[16];
 	char			x86_model_id[64];
 	/* in KB - valid for CPUS which support this call: */
<span class="p_del">-	int			x86_cache_size;</span>
<span class="p_add">+	unsigned int		x86_cache_size;</span>
 	int			x86_cache_alignment;	/* In bytes */
 	/* Cache QoS architectural values: */
 	int			x86_cache_max_rmid;	/* max index */
<span class="p_chunk">@@ -969,7 +969,4 @@</span> <span class="p_context"> bool xen_set_default_idle(void);</span>
 
 void stop_this_cpu(void *dummy);
 void df_debug(struct pt_regs *regs, long error_code);
<span class="p_del">-</span>
<span class="p_del">-void __ibp_barrier(void);</span>
<span class="p_del">-</span>
 #endif /* _ASM_X86_PROCESSOR_H */
<span class="p_header">diff --git a/arch/x86/include/asm/tlbflush.h b/arch/x86/include/asm/tlbflush.h</span>
<span class="p_header">index 2b8f18ca5874..84137c22fdfa 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/tlbflush.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/tlbflush.h</span>
<span class="p_chunk">@@ -140,7 +140,7 @@</span> <span class="p_context"> static inline unsigned long build_cr3_noflush(pgd_t *pgd, u16 asid)</span>
 #else
 #define __flush_tlb() __native_flush_tlb()
 #define __flush_tlb_global() __native_flush_tlb_global()
<span class="p_del">-#define __flush_tlb_single(addr) __native_flush_tlb_single(addr)</span>
<span class="p_add">+#define __flush_tlb_one_user(addr) __native_flush_tlb_one_user(addr)</span>
 #endif
 
 static inline bool tlb_defer_switch_to_init_mm(void)
<span class="p_chunk">@@ -400,7 +400,7 @@</span> <span class="p_context"> static inline void __native_flush_tlb_global(void)</span>
 /*
  * flush one page in the user mapping
  */
<span class="p_del">-static inline void __native_flush_tlb_single(unsigned long addr)</span>
<span class="p_add">+static inline void __native_flush_tlb_one_user(unsigned long addr)</span>
 {
 	u32 loaded_mm_asid = this_cpu_read(cpu_tlbstate.loaded_mm_asid);
 
<span class="p_chunk">@@ -437,18 +437,31 @@</span> <span class="p_context"> static inline void __flush_tlb_all(void)</span>
 /*
  * flush one page in the kernel mapping
  */
<span class="p_del">-static inline void __flush_tlb_one(unsigned long addr)</span>
<span class="p_add">+static inline void __flush_tlb_one_kernel(unsigned long addr)</span>
 {
 	count_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ONE);
<span class="p_del">-	__flush_tlb_single(addr);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If PTI is off, then __flush_tlb_one_user() is just INVLPG or its</span>
<span class="p_add">+	 * paravirt equivalent.  Even with PCID, this is sufficient: we only</span>
<span class="p_add">+	 * use PCID if we also use global PTEs for the kernel mapping, and</span>
<span class="p_add">+	 * INVLPG flushes global translations across all address spaces.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * If PTI is on, then the kernel is mapped with non-global PTEs, and</span>
<span class="p_add">+	 * __flush_tlb_one_user() will flush the given address for the current</span>
<span class="p_add">+	 * kernel address space and for its usermode counterpart, but it does</span>
<span class="p_add">+	 * not flush it for other address spaces.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	__flush_tlb_one_user(addr);</span>
 
 	if (!static_cpu_has(X86_FEATURE_PTI))
 		return;
 
 	/*
<span class="p_del">-	 * __flush_tlb_single() will have cleared the TLB entry for this ASID,</span>
<span class="p_del">-	 * but since kernel space is replicated across all, we must also</span>
<span class="p_del">-	 * invalidate all others.</span>
<span class="p_add">+	 * See above.  We need to propagate the flush to all other address</span>
<span class="p_add">+	 * spaces.  In principle, we only need to propagate it to kernelmode</span>
<span class="p_add">+	 * address spaces, but the extra bookkeeping we would need is not</span>
<span class="p_add">+	 * worth it.</span>
 	 */
 	invalidate_other_asid();
 }
<span class="p_header">diff --git a/arch/x86/kernel/amd_nb.c b/arch/x86/kernel/amd_nb.c</span>
<span class="p_header">index 6db28f17ff28..c88e0b127810 100644</span>
<span class="p_header">--- a/arch/x86/kernel/amd_nb.c</span>
<span class="p_header">+++ b/arch/x86/kernel/amd_nb.c</span>
<span class="p_chunk">@@ -235,7 +235,7 @@</span> <span class="p_context"> int amd_cache_northbridges(void)</span>
 	if (boot_cpu_data.x86 == 0x10 &amp;&amp;
 	    boot_cpu_data.x86_model &gt;= 0x8 &amp;&amp;
 	    (boot_cpu_data.x86_model &gt; 0x9 ||
<span class="p_del">-	     boot_cpu_data.x86_mask &gt;= 0x1))</span>
<span class="p_add">+	     boot_cpu_data.x86_stepping &gt;= 0x1))</span>
 		amd_northbridges.flags |= AMD_NB_L3_INDEX_DISABLE;
 
 	if (boot_cpu_data.x86 == 0x15)
<span class="p_header">diff --git a/arch/x86/kernel/apic/apic.c b/arch/x86/kernel/apic/apic.c</span>
<span class="p_header">index 25ddf02598d2..b203af0855b5 100644</span>
<span class="p_header">--- a/arch/x86/kernel/apic/apic.c</span>
<span class="p_header">+++ b/arch/x86/kernel/apic/apic.c</span>
<span class="p_chunk">@@ -546,7 +546,7 @@</span> <span class="p_context"> static DEFINE_PER_CPU(struct clock_event_device, lapic_events);</span>
 
 static u32 hsx_deadline_rev(void)
 {
<span class="p_del">-	switch (boot_cpu_data.x86_mask) {</span>
<span class="p_add">+	switch (boot_cpu_data.x86_stepping) {</span>
 	case 0x02: return 0x3a; /* EP */
 	case 0x04: return 0x0f; /* EX */
 	}
<span class="p_chunk">@@ -556,7 +556,7 @@</span> <span class="p_context"> static u32 hsx_deadline_rev(void)</span>
 
 static u32 bdx_deadline_rev(void)
 {
<span class="p_del">-	switch (boot_cpu_data.x86_mask) {</span>
<span class="p_add">+	switch (boot_cpu_data.x86_stepping) {</span>
 	case 0x02: return 0x00000011;
 	case 0x03: return 0x0700000e;
 	case 0x04: return 0x0f00000c;
<span class="p_chunk">@@ -568,7 +568,7 @@</span> <span class="p_context"> static u32 bdx_deadline_rev(void)</span>
 
 static u32 skx_deadline_rev(void)
 {
<span class="p_del">-	switch (boot_cpu_data.x86_mask) {</span>
<span class="p_add">+	switch (boot_cpu_data.x86_stepping) {</span>
 	case 0x03: return 0x01000136;
 	case 0x04: return 0x02000014;
 	}
<span class="p_header">diff --git a/arch/x86/kernel/asm-offsets_32.c b/arch/x86/kernel/asm-offsets_32.c</span>
<span class="p_header">index fa1261eefa16..f91ba53e06c8 100644</span>
<span class="p_header">--- a/arch/x86/kernel/asm-offsets_32.c</span>
<span class="p_header">+++ b/arch/x86/kernel/asm-offsets_32.c</span>
<span class="p_chunk">@@ -18,7 +18,7 @@</span> <span class="p_context"> void foo(void)</span>
 	OFFSET(CPUINFO_x86, cpuinfo_x86, x86);
 	OFFSET(CPUINFO_x86_vendor, cpuinfo_x86, x86_vendor);
 	OFFSET(CPUINFO_x86_model, cpuinfo_x86, x86_model);
<span class="p_del">-	OFFSET(CPUINFO_x86_mask, cpuinfo_x86, x86_mask);</span>
<span class="p_add">+	OFFSET(CPUINFO_x86_stepping, cpuinfo_x86, x86_stepping);</span>
 	OFFSET(CPUINFO_cpuid_level, cpuinfo_x86, cpuid_level);
 	OFFSET(CPUINFO_x86_capability, cpuinfo_x86, x86_capability);
 	OFFSET(CPUINFO_x86_vendor_id, cpuinfo_x86, x86_vendor_id);
<span class="p_header">diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c</span>
<span class="p_header">index ea831c858195..e7d5a7883632 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/amd.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/amd.c</span>
<span class="p_chunk">@@ -119,7 +119,7 @@</span> <span class="p_context"> static void init_amd_k6(struct cpuinfo_x86 *c)</span>
 		return;
 	}
 
<span class="p_del">-	if (c-&gt;x86_model == 6 &amp;&amp; c-&gt;x86_mask == 1) {</span>
<span class="p_add">+	if (c-&gt;x86_model == 6 &amp;&amp; c-&gt;x86_stepping == 1) {</span>
 		const int K6_BUG_LOOP = 1000000;
 		int n;
 		void (*f_vide)(void);
<span class="p_chunk">@@ -149,7 +149,7 @@</span> <span class="p_context"> static void init_amd_k6(struct cpuinfo_x86 *c)</span>
 
 	/* K6 with old style WHCR */
 	if (c-&gt;x86_model &lt; 8 ||
<span class="p_del">-	   (c-&gt;x86_model == 8 &amp;&amp; c-&gt;x86_mask &lt; 8)) {</span>
<span class="p_add">+	   (c-&gt;x86_model == 8 &amp;&amp; c-&gt;x86_stepping &lt; 8)) {</span>
 		/* We can only write allocate on the low 508Mb */
 		if (mbytes &gt; 508)
 			mbytes = 508;
<span class="p_chunk">@@ -168,7 +168,7 @@</span> <span class="p_context"> static void init_amd_k6(struct cpuinfo_x86 *c)</span>
 		return;
 	}
 
<span class="p_del">-	if ((c-&gt;x86_model == 8 &amp;&amp; c-&gt;x86_mask &gt; 7) ||</span>
<span class="p_add">+	if ((c-&gt;x86_model == 8 &amp;&amp; c-&gt;x86_stepping &gt; 7) ||</span>
 	     c-&gt;x86_model == 9 || c-&gt;x86_model == 13) {
 		/* The more serious chips .. */
 
<span class="p_chunk">@@ -221,7 +221,7 @@</span> <span class="p_context"> static void init_amd_k7(struct cpuinfo_x86 *c)</span>
 	 * are more robust with CLK_CTL set to 200xxxxx instead of 600xxxxx
 	 * As per AMD technical note 27212 0.2
 	 */
<span class="p_del">-	if ((c-&gt;x86_model == 8 &amp;&amp; c-&gt;x86_mask &gt;= 1) || (c-&gt;x86_model &gt; 8)) {</span>
<span class="p_add">+	if ((c-&gt;x86_model == 8 &amp;&amp; c-&gt;x86_stepping &gt;= 1) || (c-&gt;x86_model &gt; 8)) {</span>
 		rdmsr(MSR_K7_CLK_CTL, l, h);
 		if ((l &amp; 0xfff00000) != 0x20000000) {
 			pr_info(&quot;CPU: CLK_CTL MSR was %x. Reprogramming to %x\n&quot;,
<span class="p_chunk">@@ -241,12 +241,12 @@</span> <span class="p_context"> static void init_amd_k7(struct cpuinfo_x86 *c)</span>
 	 * but they are not certified as MP capable.
 	 */
 	/* Athlon 660/661 is valid. */
<span class="p_del">-	if ((c-&gt;x86_model == 6) &amp;&amp; ((c-&gt;x86_mask == 0) ||</span>
<span class="p_del">-	    (c-&gt;x86_mask == 1)))</span>
<span class="p_add">+	if ((c-&gt;x86_model == 6) &amp;&amp; ((c-&gt;x86_stepping == 0) ||</span>
<span class="p_add">+	    (c-&gt;x86_stepping == 1)))</span>
 		return;
 
 	/* Duron 670 is valid */
<span class="p_del">-	if ((c-&gt;x86_model == 7) &amp;&amp; (c-&gt;x86_mask == 0))</span>
<span class="p_add">+	if ((c-&gt;x86_model == 7) &amp;&amp; (c-&gt;x86_stepping == 0))</span>
 		return;
 
 	/*
<span class="p_chunk">@@ -256,8 +256,8 @@</span> <span class="p_context"> static void init_amd_k7(struct cpuinfo_x86 *c)</span>
 	 * See http://www.heise.de/newsticker/data/jow-18.10.01-000 for
 	 * more.
 	 */
<span class="p_del">-	if (((c-&gt;x86_model == 6) &amp;&amp; (c-&gt;x86_mask &gt;= 2)) ||</span>
<span class="p_del">-	    ((c-&gt;x86_model == 7) &amp;&amp; (c-&gt;x86_mask &gt;= 1)) ||</span>
<span class="p_add">+	if (((c-&gt;x86_model == 6) &amp;&amp; (c-&gt;x86_stepping &gt;= 2)) ||</span>
<span class="p_add">+	    ((c-&gt;x86_model == 7) &amp;&amp; (c-&gt;x86_stepping &gt;= 1)) ||</span>
 	     (c-&gt;x86_model &gt; 7))
 		if (cpu_has(c, X86_FEATURE_MP))
 			return;
<span class="p_chunk">@@ -583,7 +583,7 @@</span> <span class="p_context"> static void early_init_amd(struct cpuinfo_x86 *c)</span>
 	/*  Set MTRR capability flag if appropriate */
 	if (c-&gt;x86 == 5)
 		if (c-&gt;x86_model == 13 || c-&gt;x86_model == 9 ||
<span class="p_del">-		    (c-&gt;x86_model == 8 &amp;&amp; c-&gt;x86_mask &gt;= 8))</span>
<span class="p_add">+		    (c-&gt;x86_model == 8 &amp;&amp; c-&gt;x86_stepping &gt;= 8))</span>
 			set_cpu_cap(c, X86_FEATURE_K6_MTRR);
 #endif
 #if defined(CONFIG_X86_LOCAL_APIC) &amp;&amp; defined(CONFIG_PCI)
<span class="p_chunk">@@ -769,7 +769,7 @@</span> <span class="p_context"> static void init_amd_zn(struct cpuinfo_x86 *c)</span>
 	 * Fix erratum 1076: CPB feature bit not being set in CPUID. It affects
 	 * all up to and including B1.
 	 */
<span class="p_del">-	if (c-&gt;x86_model &lt;= 1 &amp;&amp; c-&gt;x86_mask &lt;= 1)</span>
<span class="p_add">+	if (c-&gt;x86_model &lt;= 1 &amp;&amp; c-&gt;x86_stepping &lt;= 1)</span>
 		set_cpu_cap(c, X86_FEATURE_CPB);
 }
 
<span class="p_chunk">@@ -880,11 +880,11 @@</span> <span class="p_context"> static unsigned int amd_size_cache(struct cpuinfo_x86 *c, unsigned int size)</span>
 	/* AMD errata T13 (order #21922) */
 	if ((c-&gt;x86 == 6)) {
 		/* Duron Rev A0 */
<span class="p_del">-		if (c-&gt;x86_model == 3 &amp;&amp; c-&gt;x86_mask == 0)</span>
<span class="p_add">+		if (c-&gt;x86_model == 3 &amp;&amp; c-&gt;x86_stepping == 0)</span>
 			size = 64;
 		/* Tbird rev A1/A2 */
 		if (c-&gt;x86_model == 4 &amp;&amp;
<span class="p_del">-			(c-&gt;x86_mask == 0 || c-&gt;x86_mask == 1))</span>
<span class="p_add">+			(c-&gt;x86_stepping == 0 || c-&gt;x86_stepping == 1))</span>
 			size = 256;
 	}
 	return size;
<span class="p_chunk">@@ -1021,7 +1021,7 @@</span> <span class="p_context"> static bool cpu_has_amd_erratum(struct cpuinfo_x86 *cpu, const int *erratum)</span>
 	}
 
 	/* OSVW unavailable or ID unknown, match family-model-stepping range */
<span class="p_del">-	ms = (cpu-&gt;x86_model &lt;&lt; 4) | cpu-&gt;x86_mask;</span>
<span class="p_add">+	ms = (cpu-&gt;x86_model &lt;&lt; 4) | cpu-&gt;x86_stepping;</span>
 	while ((range = *erratum++))
 		if ((cpu-&gt;x86 == AMD_MODEL_RANGE_FAMILY(range)) &amp;&amp;
 		    (ms &gt;= AMD_MODEL_RANGE_START(range)) &amp;&amp;
<span class="p_header">diff --git a/arch/x86/kernel/cpu/bugs.c b/arch/x86/kernel/cpu/bugs.c</span>
<span class="p_header">index 71949bf2de5a..d71c8b54b696 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/bugs.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/bugs.c</span>
<span class="p_chunk">@@ -162,8 +162,7 @@</span> <span class="p_context"> static enum spectre_v2_mitigation_cmd __init spectre_v2_parse_cmdline(void)</span>
 	if (cmdline_find_option_bool(boot_command_line, &quot;nospectre_v2&quot;))
 		return SPECTRE_V2_CMD_NONE;
 	else {
<span class="p_del">-		ret = cmdline_find_option(boot_command_line, &quot;spectre_v2&quot;, arg,</span>
<span class="p_del">-					  sizeof(arg));</span>
<span class="p_add">+		ret = cmdline_find_option(boot_command_line, &quot;spectre_v2&quot;, arg, sizeof(arg));</span>
 		if (ret &lt; 0)
 			return SPECTRE_V2_CMD_AUTO;
 
<span class="p_chunk">@@ -175,8 +174,7 @@</span> <span class="p_context"> static enum spectre_v2_mitigation_cmd __init spectre_v2_parse_cmdline(void)</span>
 		}
 
 		if (i &gt;= ARRAY_SIZE(mitigation_options)) {
<span class="p_del">-			pr_err(&quot;unknown option (%s). Switching to AUTO select\n&quot;,</span>
<span class="p_del">-			       mitigation_options[i].option);</span>
<span class="p_add">+			pr_err(&quot;unknown option (%s). Switching to AUTO select\n&quot;, arg);</span>
 			return SPECTRE_V2_CMD_AUTO;
 		}
 	}
<span class="p_chunk">@@ -185,8 +183,7 @@</span> <span class="p_context"> static enum spectre_v2_mitigation_cmd __init spectre_v2_parse_cmdline(void)</span>
 	     cmd == SPECTRE_V2_CMD_RETPOLINE_AMD ||
 	     cmd == SPECTRE_V2_CMD_RETPOLINE_GENERIC) &amp;&amp;
 	    !IS_ENABLED(CONFIG_RETPOLINE)) {
<span class="p_del">-		pr_err(&quot;%s selected but not compiled in. Switching to AUTO select\n&quot;,</span>
<span class="p_del">-		       mitigation_options[i].option);</span>
<span class="p_add">+		pr_err(&quot;%s selected but not compiled in. Switching to AUTO select\n&quot;, mitigation_options[i].option);</span>
 		return SPECTRE_V2_CMD_AUTO;
 	}
 
<span class="p_chunk">@@ -256,14 +253,14 @@</span> <span class="p_context"> static void __init spectre_v2_select_mitigation(void)</span>
 			goto retpoline_auto;
 		break;
 	}
<span class="p_del">-	pr_err(&quot;kernel not compiled with retpoline; no mitigation available!&quot;);</span>
<span class="p_add">+	pr_err(&quot;Spectre mitigation: kernel not compiled with retpoline; no mitigation available!&quot;);</span>
 	return;
 
 retpoline_auto:
 	if (boot_cpu_data.x86_vendor == X86_VENDOR_AMD) {
 	retpoline_amd:
 		if (!boot_cpu_has(X86_FEATURE_LFENCE_RDTSC)) {
<span class="p_del">-			pr_err(&quot;LFENCE not serializing. Switching to generic retpoline\n&quot;);</span>
<span class="p_add">+			pr_err(&quot;Spectre mitigation: LFENCE not serializing, switching to generic retpoline\n&quot;);</span>
 			goto retpoline_generic;
 		}
 		mode = retp_compiler() ? SPECTRE_V2_RETPOLINE_AMD :
<span class="p_chunk">@@ -281,7 +278,7 @@</span> <span class="p_context"> static void __init spectre_v2_select_mitigation(void)</span>
 	pr_info(&quot;%s\n&quot;, spectre_v2_strings[mode]);
 
 	/*
<span class="p_del">-	 * If neither SMEP or KPTI are available, there is a risk of</span>
<span class="p_add">+	 * If neither SMEP nor PTI are available, there is a risk of</span>
 	 * hitting userspace addresses in the RSB after a context switch
 	 * from a shallow call stack to a deeper one. To prevent this fill
 	 * the entire RSB, even when using IBRS.
<span class="p_chunk">@@ -295,21 +292,20 @@</span> <span class="p_context"> static void __init spectre_v2_select_mitigation(void)</span>
 	if ((!boot_cpu_has(X86_FEATURE_PTI) &amp;&amp;
 	     !boot_cpu_has(X86_FEATURE_SMEP)) || is_skylake_era()) {
 		setup_force_cpu_cap(X86_FEATURE_RSB_CTXSW);
<span class="p_del">-		pr_info(&quot;Filling RSB on context switch\n&quot;);</span>
<span class="p_add">+		pr_info(&quot;Spectre v2 mitigation: Filling RSB on context switch\n&quot;);</span>
 	}
 
 	/* Initialize Indirect Branch Prediction Barrier if supported */
 	if (boot_cpu_has(X86_FEATURE_IBPB)) {
 		setup_force_cpu_cap(X86_FEATURE_USE_IBPB);
<span class="p_del">-		pr_info(&quot;Enabling Indirect Branch Prediction Barrier\n&quot;);</span>
<span class="p_add">+		pr_info(&quot;Spectre v2 mitigation: Enabling Indirect Branch Prediction Barrier\n&quot;);</span>
 	}
 }
 
 #undef pr_fmt
 
 #ifdef CONFIG_SYSFS
<span class="p_del">-ssize_t cpu_show_meltdown(struct device *dev,</span>
<span class="p_del">-			  struct device_attribute *attr, char *buf)</span>
<span class="p_add">+ssize_t cpu_show_meltdown(struct device *dev, struct device_attribute *attr, char *buf)</span>
 {
 	if (!boot_cpu_has_bug(X86_BUG_CPU_MELTDOWN))
 		return sprintf(buf, &quot;Not affected\n&quot;);
<span class="p_chunk">@@ -318,16 +314,14 @@</span> <span class="p_context"> ssize_t cpu_show_meltdown(struct device *dev,</span>
 	return sprintf(buf, &quot;Vulnerable\n&quot;);
 }
 
<span class="p_del">-ssize_t cpu_show_spectre_v1(struct device *dev,</span>
<span class="p_del">-			    struct device_attribute *attr, char *buf)</span>
<span class="p_add">+ssize_t cpu_show_spectre_v1(struct device *dev, struct device_attribute *attr, char *buf)</span>
 {
 	if (!boot_cpu_has_bug(X86_BUG_SPECTRE_V1))
 		return sprintf(buf, &quot;Not affected\n&quot;);
 	return sprintf(buf, &quot;Mitigation: __user pointer sanitization\n&quot;);
 }
 
<span class="p_del">-ssize_t cpu_show_spectre_v2(struct device *dev,</span>
<span class="p_del">-			    struct device_attribute *attr, char *buf)</span>
<span class="p_add">+ssize_t cpu_show_spectre_v2(struct device *dev, struct device_attribute *attr, char *buf)</span>
 {
 	if (!boot_cpu_has_bug(X86_BUG_SPECTRE_V2))
 		return sprintf(buf, &quot;Not affected\n&quot;);
<span class="p_chunk">@@ -337,9 +331,3 @@</span> <span class="p_context"> ssize_t cpu_show_spectre_v2(struct device *dev,</span>
 		       spectre_v2_module_string());
 }
 #endif
<span class="p_del">-</span>
<span class="p_del">-void __ibp_barrier(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	__wrmsr(MSR_IA32_PRED_CMD, PRED_CMD_IBPB, 0);</span>
<span class="p_del">-}</span>
<span class="p_del">-EXPORT_SYMBOL_GPL(__ibp_barrier);</span>
<span class="p_header">diff --git a/arch/x86/kernel/cpu/centaur.c b/arch/x86/kernel/cpu/centaur.c</span>
<span class="p_header">index 68bc6d9b3132..595be776727d 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/centaur.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/centaur.c</span>
<span class="p_chunk">@@ -136,7 +136,7 @@</span> <span class="p_context"> static void init_centaur(struct cpuinfo_x86 *c)</span>
 			clear_cpu_cap(c, X86_FEATURE_TSC);
 			break;
 		case 8:
<span class="p_del">-			switch (c-&gt;x86_mask) {</span>
<span class="p_add">+			switch (c-&gt;x86_stepping) {</span>
 			default:
 			name = &quot;2&quot;;
 				break;
<span class="p_chunk">@@ -211,7 +211,7 @@</span> <span class="p_context"> centaur_size_cache(struct cpuinfo_x86 *c, unsigned int size)</span>
 	 *  - Note, it seems this may only be in engineering samples.
 	 */
 	if ((c-&gt;x86 == 6) &amp;&amp; (c-&gt;x86_model == 9) &amp;&amp;
<span class="p_del">-				(c-&gt;x86_mask == 1) &amp;&amp; (size == 65))</span>
<span class="p_add">+				(c-&gt;x86_stepping == 1) &amp;&amp; (size == 65))</span>
 		size -= 1;
 	return size;
 }
<span class="p_header">diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c</span>
<span class="p_header">index d63f4b5706e4..824aee0117bb 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/common.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/common.c</span>
<span class="p_chunk">@@ -731,7 +731,7 @@</span> <span class="p_context"> void cpu_detect(struct cpuinfo_x86 *c)</span>
 		cpuid(0x00000001, &amp;tfms, &amp;misc, &amp;junk, &amp;cap0);
 		c-&gt;x86		= x86_family(tfms);
 		c-&gt;x86_model	= x86_model(tfms);
<span class="p_del">-		c-&gt;x86_mask	= x86_stepping(tfms);</span>
<span class="p_add">+		c-&gt;x86_stepping	= x86_stepping(tfms);</span>
 
 		if (cap0 &amp; (1&lt;&lt;19)) {
 			c-&gt;x86_clflush_size = ((misc &gt;&gt; 8) &amp; 0xff) * 8;
<span class="p_chunk">@@ -1184,9 +1184,9 @@</span> <span class="p_context"> static void identify_cpu(struct cpuinfo_x86 *c)</span>
 	int i;
 
 	c-&gt;loops_per_jiffy = loops_per_jiffy;
<span class="p_del">-	c-&gt;x86_cache_size = -1;</span>
<span class="p_add">+	c-&gt;x86_cache_size = 0;</span>
 	c-&gt;x86_vendor = X86_VENDOR_UNKNOWN;
<span class="p_del">-	c-&gt;x86_model = c-&gt;x86_mask = 0;	/* So far unknown... */</span>
<span class="p_add">+	c-&gt;x86_model = c-&gt;x86_stepping = 0;	/* So far unknown... */</span>
 	c-&gt;x86_vendor_id[0] = &#39;\0&#39;; /* Unset */
 	c-&gt;x86_model_id[0] = &#39;\0&#39;;  /* Unset */
 	c-&gt;x86_max_cores = 1;
<span class="p_chunk">@@ -1378,8 +1378,8 @@</span> <span class="p_context"> void print_cpu_info(struct cpuinfo_x86 *c)</span>
 
 	pr_cont(&quot; (family: 0x%x, model: 0x%x&quot;, c-&gt;x86, c-&gt;x86_model);
 
<span class="p_del">-	if (c-&gt;x86_mask || c-&gt;cpuid_level &gt;= 0)</span>
<span class="p_del">-		pr_cont(&quot;, stepping: 0x%x)\n&quot;, c-&gt;x86_mask);</span>
<span class="p_add">+	if (c-&gt;x86_stepping || c-&gt;cpuid_level &gt;= 0)</span>
<span class="p_add">+		pr_cont(&quot;, stepping: 0x%x)\n&quot;, c-&gt;x86_stepping);</span>
 	else
 		pr_cont(&quot;)\n&quot;);
 }
<span class="p_header">diff --git a/arch/x86/kernel/cpu/cyrix.c b/arch/x86/kernel/cpu/cyrix.c</span>
<span class="p_header">index 6b4bb335641f..8949b7ae6d92 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/cyrix.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/cyrix.c</span>
<span class="p_chunk">@@ -215,7 +215,7 @@</span> <span class="p_context"> static void init_cyrix(struct cpuinfo_x86 *c)</span>
 
 	/* common case step number/rev -- exceptions handled below */
 	c-&gt;x86_model = (dir1 &gt;&gt; 4) + 1;
<span class="p_del">-	c-&gt;x86_mask = dir1 &amp; 0xf;</span>
<span class="p_add">+	c-&gt;x86_stepping = dir1 &amp; 0xf;</span>
 
 	/* Now cook; the original recipe is by Channing Corn, from Cyrix.
 	 * We do the same thing for each generation: we work out
<span class="p_header">diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c</span>
<span class="p_header">index 319bf989fad1..d19e903214b4 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/intel.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/intel.c</span>
<span class="p_chunk">@@ -116,14 +116,13 @@</span> <span class="p_context"> struct sku_microcode {</span>
 	u32 microcode;
 };
 static const struct sku_microcode spectre_bad_microcodes[] = {
<span class="p_del">-	{ INTEL_FAM6_KABYLAKE_DESKTOP,	0x0B,	0x84 },</span>
<span class="p_del">-	{ INTEL_FAM6_KABYLAKE_DESKTOP,	0x0A,	0x84 },</span>
<span class="p_del">-	{ INTEL_FAM6_KABYLAKE_DESKTOP,	0x09,	0x84 },</span>
<span class="p_del">-	{ INTEL_FAM6_KABYLAKE_MOBILE,	0x0A,	0x84 },</span>
<span class="p_del">-	{ INTEL_FAM6_KABYLAKE_MOBILE,	0x09,	0x84 },</span>
<span class="p_add">+	{ INTEL_FAM6_KABYLAKE_DESKTOP,	0x0B,	0x80 },</span>
<span class="p_add">+	{ INTEL_FAM6_KABYLAKE_DESKTOP,	0x0A,	0x80 },</span>
<span class="p_add">+	{ INTEL_FAM6_KABYLAKE_DESKTOP,	0x09,	0x80 },</span>
<span class="p_add">+	{ INTEL_FAM6_KABYLAKE_MOBILE,	0x0A,	0x80 },</span>
<span class="p_add">+	{ INTEL_FAM6_KABYLAKE_MOBILE,	0x09,	0x80 },</span>
 	{ INTEL_FAM6_SKYLAKE_X,		0x03,	0x0100013e },
 	{ INTEL_FAM6_SKYLAKE_X,		0x04,	0x0200003c },
<span class="p_del">-	{ INTEL_FAM6_SKYLAKE_MOBILE,	0x03,	0xc2 },</span>
 	{ INTEL_FAM6_SKYLAKE_DESKTOP,	0x03,	0xc2 },
 	{ INTEL_FAM6_BROADWELL_CORE,	0x04,	0x28 },
 	{ INTEL_FAM6_BROADWELL_GT3E,	0x01,	0x1b },
<span class="p_chunk">@@ -136,8 +135,6 @@</span> <span class="p_context"> static const struct sku_microcode spectre_bad_microcodes[] = {</span>
 	{ INTEL_FAM6_HASWELL_X,		0x02,	0x3b },
 	{ INTEL_FAM6_HASWELL_X,		0x04,	0x10 },
 	{ INTEL_FAM6_IVYBRIDGE_X,	0x04,	0x42a },
<span class="p_del">-	/* Updated in the 20180108 release; blacklist until we know otherwise */</span>
<span class="p_del">-	{ INTEL_FAM6_ATOM_GEMINI_LAKE,	0x01,	0x22 },</span>
 	/* Observed in the wild */
 	{ INTEL_FAM6_SANDYBRIDGE_X,	0x06,	0x61b },
 	{ INTEL_FAM6_SANDYBRIDGE_X,	0x07,	0x712 },
<span class="p_chunk">@@ -149,7 +146,7 @@</span> <span class="p_context"> static bool bad_spectre_microcode(struct cpuinfo_x86 *c)</span>
 
 	for (i = 0; i &lt; ARRAY_SIZE(spectre_bad_microcodes); i++) {
 		if (c-&gt;x86_model == spectre_bad_microcodes[i].model &amp;&amp;
<span class="p_del">-		    c-&gt;x86_mask == spectre_bad_microcodes[i].stepping)</span>
<span class="p_add">+		    c-&gt;x86_stepping == spectre_bad_microcodes[i].stepping)</span>
 			return (c-&gt;microcode &lt;= spectre_bad_microcodes[i].microcode);
 	}
 	return false;
<span class="p_chunk">@@ -196,7 +193,7 @@</span> <span class="p_context"> static void early_init_intel(struct cpuinfo_x86 *c)</span>
 	 * need the microcode to have already been loaded... so if it is
 	 * not, recommend a BIOS update and disable large pages.
 	 */
<span class="p_del">-	if (c-&gt;x86 == 6 &amp;&amp; c-&gt;x86_model == 0x1c &amp;&amp; c-&gt;x86_mask &lt;= 2 &amp;&amp;</span>
<span class="p_add">+	if (c-&gt;x86 == 6 &amp;&amp; c-&gt;x86_model == 0x1c &amp;&amp; c-&gt;x86_stepping &lt;= 2 &amp;&amp;</span>
 	    c-&gt;microcode &lt; 0x20e) {
 		pr_warn(&quot;Atom PSE erratum detected, BIOS microcode update recommended\n&quot;);
 		clear_cpu_cap(c, X86_FEATURE_PSE);
<span class="p_chunk">@@ -212,7 +209,7 @@</span> <span class="p_context"> static void early_init_intel(struct cpuinfo_x86 *c)</span>
 
 	/* CPUID workaround for 0F33/0F34 CPU */
 	if (c-&gt;x86 == 0xF &amp;&amp; c-&gt;x86_model == 0x3
<span class="p_del">-	    &amp;&amp; (c-&gt;x86_mask == 0x3 || c-&gt;x86_mask == 0x4))</span>
<span class="p_add">+	    &amp;&amp; (c-&gt;x86_stepping == 0x3 || c-&gt;x86_stepping == 0x4))</span>
 		c-&gt;x86_phys_bits = 36;
 
 	/*
<span class="p_chunk">@@ -310,7 +307,7 @@</span> <span class="p_context"> int ppro_with_ram_bug(void)</span>
 	if (boot_cpu_data.x86_vendor == X86_VENDOR_INTEL &amp;&amp;
 	    boot_cpu_data.x86 == 6 &amp;&amp;
 	    boot_cpu_data.x86_model == 1 &amp;&amp;
<span class="p_del">-	    boot_cpu_data.x86_mask &lt; 8) {</span>
<span class="p_add">+	    boot_cpu_data.x86_stepping &lt; 8) {</span>
 		pr_info(&quot;Pentium Pro with Errata#50 detected. Taking evasive action.\n&quot;);
 		return 1;
 	}
<span class="p_chunk">@@ -327,7 +324,7 @@</span> <span class="p_context"> static void intel_smp_check(struct cpuinfo_x86 *c)</span>
 	 * Mask B, Pentium, but not Pentium MMX
 	 */
 	if (c-&gt;x86 == 5 &amp;&amp;
<span class="p_del">-	    c-&gt;x86_mask &gt;= 1 &amp;&amp; c-&gt;x86_mask &lt;= 4 &amp;&amp;</span>
<span class="p_add">+	    c-&gt;x86_stepping &gt;= 1 &amp;&amp; c-&gt;x86_stepping &lt;= 4 &amp;&amp;</span>
 	    c-&gt;x86_model &lt;= 3) {
 		/*
 		 * Remember we have B step Pentia with bugs
<span class="p_chunk">@@ -370,7 +367,7 @@</span> <span class="p_context"> static void intel_workarounds(struct cpuinfo_x86 *c)</span>
 	 * SEP CPUID bug: Pentium Pro reports SEP but doesn&#39;t have it until
 	 * model 3 mask 3
 	 */
<span class="p_del">-	if ((c-&gt;x86&lt;&lt;8 | c-&gt;x86_model&lt;&lt;4 | c-&gt;x86_mask) &lt; 0x633)</span>
<span class="p_add">+	if ((c-&gt;x86&lt;&lt;8 | c-&gt;x86_model&lt;&lt;4 | c-&gt;x86_stepping) &lt; 0x633)</span>
 		clear_cpu_cap(c, X86_FEATURE_SEP);
 
 	/*
<span class="p_chunk">@@ -388,7 +385,7 @@</span> <span class="p_context"> static void intel_workarounds(struct cpuinfo_x86 *c)</span>
 	 * P4 Xeon erratum 037 workaround.
 	 * Hardware prefetcher may cause stale data to be loaded into the cache.
 	 */
<span class="p_del">-	if ((c-&gt;x86 == 15) &amp;&amp; (c-&gt;x86_model == 1) &amp;&amp; (c-&gt;x86_mask == 1)) {</span>
<span class="p_add">+	if ((c-&gt;x86 == 15) &amp;&amp; (c-&gt;x86_model == 1) &amp;&amp; (c-&gt;x86_stepping == 1)) {</span>
 		if (msr_set_bit(MSR_IA32_MISC_ENABLE,
 				MSR_IA32_MISC_ENABLE_PREFETCH_DISABLE_BIT) &gt; 0) {
 			pr_info(&quot;CPU: C0 stepping P4 Xeon detected.\n&quot;);
<span class="p_chunk">@@ -403,7 +400,7 @@</span> <span class="p_context"> static void intel_workarounds(struct cpuinfo_x86 *c)</span>
 	 * Specification Update&quot;).
 	 */
 	if (boot_cpu_has(X86_FEATURE_APIC) &amp;&amp; (c-&gt;x86&lt;&lt;8 | c-&gt;x86_model&lt;&lt;4) == 0x520 &amp;&amp;
<span class="p_del">-	    (c-&gt;x86_mask &lt; 0x6 || c-&gt;x86_mask == 0xb))</span>
<span class="p_add">+	    (c-&gt;x86_stepping &lt; 0x6 || c-&gt;x86_stepping == 0xb))</span>
 		set_cpu_bug(c, X86_BUG_11AP);
 
 
<span class="p_chunk">@@ -650,7 +647,7 @@</span> <span class="p_context"> static void init_intel(struct cpuinfo_x86 *c)</span>
 		case 6:
 			if (l2 == 128)
 				p = &quot;Celeron (Mendocino)&quot;;
<span class="p_del">-			else if (c-&gt;x86_mask == 0 || c-&gt;x86_mask == 5)</span>
<span class="p_add">+			else if (c-&gt;x86_stepping == 0 || c-&gt;x86_stepping == 5)</span>
 				p = &quot;Celeron-A&quot;;
 			break;
 
<span class="p_header">diff --git a/arch/x86/kernel/cpu/intel_rdt.c b/arch/x86/kernel/cpu/intel_rdt.c</span>
<span class="p_header">index 99442370de40..18dd8f22e353 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/intel_rdt.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/intel_rdt.c</span>
<span class="p_chunk">@@ -771,7 +771,7 @@</span> <span class="p_context"> static __init void rdt_quirks(void)</span>
 			cache_alloc_hsw_probe();
 		break;
 	case INTEL_FAM6_SKYLAKE_X:
<span class="p_del">-		if (boot_cpu_data.x86_mask &lt;= 4)</span>
<span class="p_add">+		if (boot_cpu_data.x86_stepping &lt;= 4)</span>
 			set_rdt_options(&quot;!cmt,!mbmtotal,!mbmlocal,!l3cat&quot;);
 	}
 }
<span class="p_header">diff --git a/arch/x86/kernel/cpu/microcode/intel.c b/arch/x86/kernel/cpu/microcode/intel.c</span>
<span class="p_header">index f7c55b0e753a..a15db2b4e0d6 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/microcode/intel.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/microcode/intel.c</span>
<span class="p_chunk">@@ -921,7 +921,7 @@</span> <span class="p_context"> static bool is_blacklisted(unsigned int cpu)</span>
 	 */
 	if (c-&gt;x86 == 6 &amp;&amp;
 	    c-&gt;x86_model == INTEL_FAM6_BROADWELL_X &amp;&amp;
<span class="p_del">-	    c-&gt;x86_mask == 0x01 &amp;&amp;</span>
<span class="p_add">+	    c-&gt;x86_stepping == 0x01 &amp;&amp;</span>
 	    llc_size_per_core &gt; 2621440 &amp;&amp;
 	    c-&gt;microcode &lt; 0x0b000021) {
 		pr_err_once(&quot;Erratum BDF90: late loading with revision &lt; 0x0b000021 (0x%x) disabled.\n&quot;, c-&gt;microcode);
<span class="p_chunk">@@ -944,7 +944,7 @@</span> <span class="p_context"> static enum ucode_state request_microcode_fw(int cpu, struct device *device,</span>
 		return UCODE_NFOUND;
 
 	sprintf(name, &quot;intel-ucode/%02x-%02x-%02x&quot;,
<span class="p_del">-		c-&gt;x86, c-&gt;x86_model, c-&gt;x86_mask);</span>
<span class="p_add">+		c-&gt;x86, c-&gt;x86_model, c-&gt;x86_stepping);</span>
 
 	if (request_firmware_direct(&amp;firmware, name, device)) {
 		pr_debug(&quot;data file %s load failed\n&quot;, name);
<span class="p_chunk">@@ -982,7 +982,7 @@</span> <span class="p_context"> static struct microcode_ops microcode_intel_ops = {</span>
 
 static int __init calc_llc_size_per_core(struct cpuinfo_x86 *c)
 {
<span class="p_del">-	u64 llc_size = c-&gt;x86_cache_size * 1024;</span>
<span class="p_add">+	u64 llc_size = c-&gt;x86_cache_size * 1024ULL;</span>
 
 	do_div(llc_size, c-&gt;x86_max_cores);
 
<span class="p_header">diff --git a/arch/x86/kernel/cpu/mtrr/generic.c b/arch/x86/kernel/cpu/mtrr/generic.c</span>
<span class="p_header">index fdc55215d44d..e12ee86906c6 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/mtrr/generic.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/mtrr/generic.c</span>
<span class="p_chunk">@@ -859,7 +859,7 @@</span> <span class="p_context"> int generic_validate_add_page(unsigned long base, unsigned long size,</span>
 	 */
 	if (is_cpu(INTEL) &amp;&amp; boot_cpu_data.x86 == 6 &amp;&amp;
 	    boot_cpu_data.x86_model == 1 &amp;&amp;
<span class="p_del">-	    boot_cpu_data.x86_mask &lt;= 7) {</span>
<span class="p_add">+	    boot_cpu_data.x86_stepping &lt;= 7) {</span>
 		if (base &amp; ((1 &lt;&lt; (22 - PAGE_SHIFT)) - 1)) {
 			pr_warn(&quot;mtrr: base(0x%lx000) is not 4 MiB aligned\n&quot;, base);
 			return -EINVAL;
<span class="p_header">diff --git a/arch/x86/kernel/cpu/mtrr/main.c b/arch/x86/kernel/cpu/mtrr/main.c</span>
<span class="p_header">index 40d5a8a75212..7468de429087 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/mtrr/main.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/mtrr/main.c</span>
<span class="p_chunk">@@ -711,8 +711,8 @@</span> <span class="p_context"> void __init mtrr_bp_init(void)</span>
 			if (boot_cpu_data.x86_vendor == X86_VENDOR_INTEL &amp;&amp;
 			    boot_cpu_data.x86 == 0xF &amp;&amp;
 			    boot_cpu_data.x86_model == 0x3 &amp;&amp;
<span class="p_del">-			    (boot_cpu_data.x86_mask == 0x3 ||</span>
<span class="p_del">-			     boot_cpu_data.x86_mask == 0x4))</span>
<span class="p_add">+			    (boot_cpu_data.x86_stepping == 0x3 ||</span>
<span class="p_add">+			     boot_cpu_data.x86_stepping == 0x4))</span>
 				phys_addr = 36;
 
 			size_or_mask = SIZE_OR_MASK_BITS(phys_addr);
<span class="p_header">diff --git a/arch/x86/kernel/cpu/proc.c b/arch/x86/kernel/cpu/proc.c</span>
<span class="p_header">index e7ecedafa1c8..2c8522a39ed5 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/proc.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/proc.c</span>
<span class="p_chunk">@@ -72,8 +72,8 @@</span> <span class="p_context"> static int show_cpuinfo(struct seq_file *m, void *v)</span>
 		   c-&gt;x86_model,
 		   c-&gt;x86_model_id[0] ? c-&gt;x86_model_id : &quot;unknown&quot;);
 
<span class="p_del">-	if (c-&gt;x86_mask || c-&gt;cpuid_level &gt;= 0)</span>
<span class="p_del">-		seq_printf(m, &quot;stepping\t: %d\n&quot;, c-&gt;x86_mask);</span>
<span class="p_add">+	if (c-&gt;x86_stepping || c-&gt;cpuid_level &gt;= 0)</span>
<span class="p_add">+		seq_printf(m, &quot;stepping\t: %d\n&quot;, c-&gt;x86_stepping);</span>
 	else
 		seq_puts(m, &quot;stepping\t: unknown\n&quot;);
 	if (c-&gt;microcode)
<span class="p_chunk">@@ -91,8 +91,8 @@</span> <span class="p_context"> static int show_cpuinfo(struct seq_file *m, void *v)</span>
 	}
 
 	/* Cache size */
<span class="p_del">-	if (c-&gt;x86_cache_size &gt;= 0)</span>
<span class="p_del">-		seq_printf(m, &quot;cache size\t: %d KB\n&quot;, c-&gt;x86_cache_size);</span>
<span class="p_add">+	if (c-&gt;x86_cache_size)</span>
<span class="p_add">+		seq_printf(m, &quot;cache size\t: %u KB\n&quot;, c-&gt;x86_cache_size);</span>
 
 	show_cpuinfo_core(m, c, cpu);
 	show_cpuinfo_misc(m, c);
<span class="p_header">diff --git a/arch/x86/kernel/head_32.S b/arch/x86/kernel/head_32.S</span>
<span class="p_header">index c29020907886..b59e4fb40fd9 100644</span>
<span class="p_header">--- a/arch/x86/kernel/head_32.S</span>
<span class="p_header">+++ b/arch/x86/kernel/head_32.S</span>
<span class="p_chunk">@@ -37,7 +37,7 @@</span> <span class="p_context"></span>
 #define X86		new_cpu_data+CPUINFO_x86
 #define X86_VENDOR	new_cpu_data+CPUINFO_x86_vendor
 #define X86_MODEL	new_cpu_data+CPUINFO_x86_model
<span class="p_del">-#define X86_MASK	new_cpu_data+CPUINFO_x86_mask</span>
<span class="p_add">+#define X86_STEPPING	new_cpu_data+CPUINFO_x86_stepping</span>
 #define X86_HARD_MATH	new_cpu_data+CPUINFO_hard_math
 #define X86_CPUID	new_cpu_data+CPUINFO_cpuid_level
 #define X86_CAPABILITY	new_cpu_data+CPUINFO_x86_capability
<span class="p_chunk">@@ -332,7 +332,7 @@</span> <span class="p_context"> ENTRY(startup_32_smp)</span>
 	shrb $4,%al
 	movb %al,X86_MODEL
 	andb $0x0f,%cl		# mask mask revision
<span class="p_del">-	movb %cl,X86_MASK</span>
<span class="p_add">+	movb %cl,X86_STEPPING</span>
 	movl %edx,X86_CAPABILITY
 
 .Lis486:
<span class="p_header">diff --git a/arch/x86/kernel/mpparse.c b/arch/x86/kernel/mpparse.c</span>
<span class="p_header">index 3a4b12809ab5..bc6bc6689e68 100644</span>
<span class="p_header">--- a/arch/x86/kernel/mpparse.c</span>
<span class="p_header">+++ b/arch/x86/kernel/mpparse.c</span>
<span class="p_chunk">@@ -407,7 +407,7 @@</span> <span class="p_context"> static inline void __init construct_default_ISA_mptable(int mpc_default_type)</span>
 	processor.apicver = mpc_default_type &gt; 4 ? 0x10 : 0x01;
 	processor.cpuflag = CPU_ENABLED;
 	processor.cpufeature = (boot_cpu_data.x86 &lt;&lt; 8) |
<span class="p_del">-	    (boot_cpu_data.x86_model &lt;&lt; 4) | boot_cpu_data.x86_mask;</span>
<span class="p_add">+	    (boot_cpu_data.x86_model &lt;&lt; 4) | boot_cpu_data.x86_stepping;</span>
 	processor.featureflag = boot_cpu_data.x86_capability[CPUID_1_EDX];
 	processor.reserved[0] = 0;
 	processor.reserved[1] = 0;
<span class="p_header">diff --git a/arch/x86/kernel/paravirt.c b/arch/x86/kernel/paravirt.c</span>
<span class="p_header">index 041096bdef86..99dc79e76bdc 100644</span>
<span class="p_header">--- a/arch/x86/kernel/paravirt.c</span>
<span class="p_header">+++ b/arch/x86/kernel/paravirt.c</span>
<span class="p_chunk">@@ -200,9 +200,9 @@</span> <span class="p_context"> static void native_flush_tlb_global(void)</span>
 	__native_flush_tlb_global();
 }
 
<span class="p_del">-static void native_flush_tlb_single(unsigned long addr)</span>
<span class="p_add">+static void native_flush_tlb_one_user(unsigned long addr)</span>
 {
<span class="p_del">-	__native_flush_tlb_single(addr);</span>
<span class="p_add">+	__native_flush_tlb_one_user(addr);</span>
 }
 
 struct static_key paravirt_steal_enabled;
<span class="p_chunk">@@ -401,7 +401,7 @@</span> <span class="p_context"> struct pv_mmu_ops pv_mmu_ops __ro_after_init = {</span>
 
 	.flush_tlb_user = native_flush_tlb,
 	.flush_tlb_kernel = native_flush_tlb_global,
<span class="p_del">-	.flush_tlb_single = native_flush_tlb_single,</span>
<span class="p_add">+	.flush_tlb_one_user = native_flush_tlb_one_user,</span>
 	.flush_tlb_others = native_flush_tlb_others,
 
 	.pgd_alloc = __paravirt_pgd_alloc,
<span class="p_header">diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c</span>
<span class="p_header">index 446c9ef8cfc3..3d9b2308e7fa 100644</span>
<span class="p_header">--- a/arch/x86/kernel/traps.c</span>
<span class="p_header">+++ b/arch/x86/kernel/traps.c</span>
<span class="p_chunk">@@ -181,7 +181,7 @@</span> <span class="p_context"> int fixup_bug(struct pt_regs *regs, int trapnr)</span>
 		break;
 
 	case BUG_TRAP_TYPE_WARN:
<span class="p_del">-		regs-&gt;ip += LEN_UD0;</span>
<span class="p_add">+		regs-&gt;ip += LEN_UD2;</span>
 		return 1;
 	}
 
<span class="p_header">diff --git a/arch/x86/kvm/mmu.c b/arch/x86/kvm/mmu.c</span>
<span class="p_header">index 2b8eb4da4d08..cc83bdcb65d1 100644</span>
<span class="p_header">--- a/arch/x86/kvm/mmu.c</span>
<span class="p_header">+++ b/arch/x86/kvm/mmu.c</span>
<span class="p_chunk">@@ -5058,7 +5058,7 @@</span> <span class="p_context"> void kvm_mmu_uninit_vm(struct kvm *kvm)</span>
 typedef bool (*slot_level_handler) (struct kvm *kvm, struct kvm_rmap_head *rmap_head);
 
 /* The caller should hold mmu-lock before calling this function. */
<span class="p_del">-static bool</span>
<span class="p_add">+static __always_inline bool</span>
 slot_handle_level_range(struct kvm *kvm, struct kvm_memory_slot *memslot,
 			slot_level_handler fn, int start_level, int end_level,
 			gfn_t start_gfn, gfn_t end_gfn, bool lock_flush_tlb)
<span class="p_chunk">@@ -5088,7 +5088,7 @@</span> <span class="p_context"> slot_handle_level_range(struct kvm *kvm, struct kvm_memory_slot *memslot,</span>
 	return flush;
 }
 
<span class="p_del">-static bool</span>
<span class="p_add">+static __always_inline bool</span>
 slot_handle_level(struct kvm *kvm, struct kvm_memory_slot *memslot,
 		  slot_level_handler fn, int start_level, int end_level,
 		  bool lock_flush_tlb)
<span class="p_chunk">@@ -5099,7 +5099,7 @@</span> <span class="p_context"> slot_handle_level(struct kvm *kvm, struct kvm_memory_slot *memslot,</span>
 			lock_flush_tlb);
 }
 
<span class="p_del">-static bool</span>
<span class="p_add">+static __always_inline bool</span>
 slot_handle_all_level(struct kvm *kvm, struct kvm_memory_slot *memslot,
 		      slot_level_handler fn, bool lock_flush_tlb)
 {
<span class="p_chunk">@@ -5107,7 +5107,7 @@</span> <span class="p_context"> slot_handle_all_level(struct kvm *kvm, struct kvm_memory_slot *memslot,</span>
 				 PT_MAX_HUGEPAGE_LEVEL, lock_flush_tlb);
 }
 
<span class="p_del">-static bool</span>
<span class="p_add">+static __always_inline bool</span>
 slot_handle_large_level(struct kvm *kvm, struct kvm_memory_slot *memslot,
 			slot_level_handler fn, bool lock_flush_tlb)
 {
<span class="p_chunk">@@ -5115,7 +5115,7 @@</span> <span class="p_context"> slot_handle_large_level(struct kvm *kvm, struct kvm_memory_slot *memslot,</span>
 				 PT_MAX_HUGEPAGE_LEVEL, lock_flush_tlb);
 }
 
<span class="p_del">-static bool</span>
<span class="p_add">+static __always_inline bool</span>
 slot_handle_leaf(struct kvm *kvm, struct kvm_memory_slot *memslot,
 		 slot_level_handler fn, bool lock_flush_tlb)
 {
<span class="p_header">diff --git a/arch/x86/kvm/vmx.c b/arch/x86/kvm/vmx.c</span>
<span class="p_header">index bee4c49f6dd0..91e3539cba02 100644</span>
<span class="p_header">--- a/arch/x86/kvm/vmx.c</span>
<span class="p_header">+++ b/arch/x86/kvm/vmx.c</span>
<span class="p_chunk">@@ -10130,7 +10130,8 @@</span> <span class="p_context"> static void nested_get_vmcs12_pages(struct kvm_vcpu *vcpu,</span>
 	if (cpu_has_vmx_msr_bitmap() &amp;&amp;
 	    nested_cpu_has(vmcs12, CPU_BASED_USE_MSR_BITMAPS) &amp;&amp;
 	    nested_vmx_merge_msr_bitmap(vcpu, vmcs12))
<span class="p_del">-		;</span>
<span class="p_add">+		vmcs_set_bits(CPU_BASED_VM_EXEC_CONTROL,</span>
<span class="p_add">+			      CPU_BASED_USE_MSR_BITMAPS);</span>
 	else
 		vmcs_clear_bits(CPU_BASED_VM_EXEC_CONTROL,
 				CPU_BASED_USE_MSR_BITMAPS);
<span class="p_chunk">@@ -10219,8 +10220,8 @@</span> <span class="p_context"> static inline bool nested_vmx_merge_msr_bitmap(struct kvm_vcpu *vcpu,</span>
 	 *    updated to reflect this when L1 (or its L2s) actually write to
 	 *    the MSR.
 	 */
<span class="p_del">-	bool pred_cmd = msr_write_intercepted_l01(vcpu, MSR_IA32_PRED_CMD);</span>
<span class="p_del">-	bool spec_ctrl = msr_write_intercepted_l01(vcpu, MSR_IA32_SPEC_CTRL);</span>
<span class="p_add">+	bool pred_cmd = !msr_write_intercepted_l01(vcpu, MSR_IA32_PRED_CMD);</span>
<span class="p_add">+	bool spec_ctrl = !msr_write_intercepted_l01(vcpu, MSR_IA32_SPEC_CTRL);</span>
 
 	if (!nested_cpu_has_virt_x2apic_mode(vmcs12) &amp;&amp;
 	    !pred_cmd &amp;&amp; !spec_ctrl)
<span class="p_header">diff --git a/arch/x86/lib/cpu.c b/arch/x86/lib/cpu.c</span>
<span class="p_header">index d6f848d1211d..2dd1fe13a37b 100644</span>
<span class="p_header">--- a/arch/x86/lib/cpu.c</span>
<span class="p_header">+++ b/arch/x86/lib/cpu.c</span>
<span class="p_chunk">@@ -18,7 +18,7 @@</span> <span class="p_context"> unsigned int x86_model(unsigned int sig)</span>
 {
 	unsigned int fam, model;
 
<span class="p_del">-	 fam = x86_family(sig);</span>
<span class="p_add">+	fam = x86_family(sig);</span>
 
 	model = (sig &gt;&gt; 4) &amp; 0xf;
 
<span class="p_header">diff --git a/arch/x86/mm/init_64.c b/arch/x86/mm/init_64.c</span>
<span class="p_header">index 4a837289f2ad..60ae1fe3609f 100644</span>
<span class="p_header">--- a/arch/x86/mm/init_64.c</span>
<span class="p_header">+++ b/arch/x86/mm/init_64.c</span>
<span class="p_chunk">@@ -256,7 +256,7 @@</span> <span class="p_context"> static void __set_pte_vaddr(pud_t *pud, unsigned long vaddr, pte_t new_pte)</span>
 	 * It&#39;s enough to flush this one mapping.
 	 * (PGE mappings get flushed as well)
 	 */
<span class="p_del">-	__flush_tlb_one(vaddr);</span>
<span class="p_add">+	__flush_tlb_one_kernel(vaddr);</span>
 }
 
 void set_pte_vaddr_p4d(p4d_t *p4d_page, unsigned long vaddr, pte_t new_pte)
<span class="p_header">diff --git a/arch/x86/mm/ioremap.c b/arch/x86/mm/ioremap.c</span>
<span class="p_header">index c45b6ec5357b..e2db83bebc3b 100644</span>
<span class="p_header">--- a/arch/x86/mm/ioremap.c</span>
<span class="p_header">+++ b/arch/x86/mm/ioremap.c</span>
<span class="p_chunk">@@ -820,5 +820,5 @@</span> <span class="p_context"> void __init __early_set_fixmap(enum fixed_addresses idx,</span>
 		set_pte(pte, pfn_pte(phys &gt;&gt; PAGE_SHIFT, flags));
 	else
 		pte_clear(&amp;init_mm, addr, pte);
<span class="p_del">-	__flush_tlb_one(addr);</span>
<span class="p_add">+	__flush_tlb_one_kernel(addr);</span>
 }
<span class="p_header">diff --git a/arch/x86/mm/kmmio.c b/arch/x86/mm/kmmio.c</span>
<span class="p_header">index 58477ec3d66d..7c8686709636 100644</span>
<span class="p_header">--- a/arch/x86/mm/kmmio.c</span>
<span class="p_header">+++ b/arch/x86/mm/kmmio.c</span>
<span class="p_chunk">@@ -168,7 +168,7 @@</span> <span class="p_context"> static int clear_page_presence(struct kmmio_fault_page *f, bool clear)</span>
 		return -1;
 	}
 
<span class="p_del">-	__flush_tlb_one(f-&gt;addr);</span>
<span class="p_add">+	__flush_tlb_one_kernel(f-&gt;addr);</span>
 	return 0;
 }
 
<span class="p_header">diff --git a/arch/x86/mm/pgtable_32.c b/arch/x86/mm/pgtable_32.c</span>
<span class="p_header">index c3c5274410a9..9bb7f0ab9fe6 100644</span>
<span class="p_header">--- a/arch/x86/mm/pgtable_32.c</span>
<span class="p_header">+++ b/arch/x86/mm/pgtable_32.c</span>
<span class="p_chunk">@@ -63,7 +63,7 @@</span> <span class="p_context"> void set_pte_vaddr(unsigned long vaddr, pte_t pteval)</span>
 	 * It&#39;s enough to flush this one mapping.
 	 * (PGE mappings get flushed as well)
 	 */
<span class="p_del">-	__flush_tlb_one(vaddr);</span>
<span class="p_add">+	__flush_tlb_one_kernel(vaddr);</span>
 }
 
 unsigned long __FIXADDR_TOP = 0xfffff000;
<span class="p_header">diff --git a/arch/x86/mm/tlb.c b/arch/x86/mm/tlb.c</span>
<span class="p_header">index 012d02624848..0c936435ea93 100644</span>
<span class="p_header">--- a/arch/x86/mm/tlb.c</span>
<span class="p_header">+++ b/arch/x86/mm/tlb.c</span>
<span class="p_chunk">@@ -492,7 +492,7 @@</span> <span class="p_context"> static void flush_tlb_func_common(const struct flush_tlb_info *f,</span>
 	 *    flush that changes context.tlb_gen from 2 to 3.  If they get
 	 *    processed on this CPU in reverse order, we&#39;ll see
 	 *     local_tlb_gen == 1, mm_tlb_gen == 3, and end != TLB_FLUSH_ALL.
<span class="p_del">-	 *    If we were to use __flush_tlb_single() and set local_tlb_gen to</span>
<span class="p_add">+	 *    If we were to use __flush_tlb_one_user() and set local_tlb_gen to</span>
 	 *    3, we&#39;d be break the invariant: we&#39;d update local_tlb_gen above
 	 *    1 without the full flush that&#39;s needed for tlb_gen 2.
 	 *
<span class="p_chunk">@@ -513,7 +513,7 @@</span> <span class="p_context"> static void flush_tlb_func_common(const struct flush_tlb_info *f,</span>
 
 		addr = f-&gt;start;
 		while (addr &lt; f-&gt;end) {
<span class="p_del">-			__flush_tlb_single(addr);</span>
<span class="p_add">+			__flush_tlb_one_user(addr);</span>
 			addr += PAGE_SIZE;
 		}
 		if (local)
<span class="p_chunk">@@ -660,7 +660,7 @@</span> <span class="p_context"> static void do_kernel_range_flush(void *info)</span>
 
 	/* flush range by one by one &#39;invlpg&#39; */
 	for (addr = f-&gt;start; addr &lt; f-&gt;end; addr += PAGE_SIZE)
<span class="p_del">-		__flush_tlb_one(addr);</span>
<span class="p_add">+		__flush_tlb_one_kernel(addr);</span>
 }
 
 void flush_tlb_kernel_range(unsigned long start, unsigned long end)
<span class="p_header">diff --git a/arch/x86/platform/uv/tlb_uv.c b/arch/x86/platform/uv/tlb_uv.c</span>
<span class="p_header">index 8538a6723171..7d5d53f36a7a 100644</span>
<span class="p_header">--- a/arch/x86/platform/uv/tlb_uv.c</span>
<span class="p_header">+++ b/arch/x86/platform/uv/tlb_uv.c</span>
<span class="p_chunk">@@ -299,7 +299,7 @@</span> <span class="p_context"> static void bau_process_message(struct msg_desc *mdp, struct bau_control *bcp,</span>
 		local_flush_tlb();
 		stat-&gt;d_alltlb++;
 	} else {
<span class="p_del">-		__flush_tlb_single(msg-&gt;address);</span>
<span class="p_add">+		__flush_tlb_one_user(msg-&gt;address);</span>
 		stat-&gt;d_onetlb++;
 	}
 	stat-&gt;d_requestee++;
<span class="p_header">diff --git a/arch/x86/xen/mmu_pv.c b/arch/x86/xen/mmu_pv.c</span>
<span class="p_header">index d85076223a69..aae88fec9941 100644</span>
<span class="p_header">--- a/arch/x86/xen/mmu_pv.c</span>
<span class="p_header">+++ b/arch/x86/xen/mmu_pv.c</span>
<span class="p_chunk">@@ -1300,12 +1300,12 @@</span> <span class="p_context"> static void xen_flush_tlb(void)</span>
 	preempt_enable();
 }
 
<span class="p_del">-static void xen_flush_tlb_single(unsigned long addr)</span>
<span class="p_add">+static void xen_flush_tlb_one_user(unsigned long addr)</span>
 {
 	struct mmuext_op *op;
 	struct multicall_space mcs;
 
<span class="p_del">-	trace_xen_mmu_flush_tlb_single(addr);</span>
<span class="p_add">+	trace_xen_mmu_flush_tlb_one_user(addr);</span>
 
 	preempt_disable();
 
<span class="p_chunk">@@ -2370,7 +2370,7 @@</span> <span class="p_context"> static const struct pv_mmu_ops xen_mmu_ops __initconst = {</span>
 
 	.flush_tlb_user = xen_flush_tlb,
 	.flush_tlb_kernel = xen_flush_tlb,
<span class="p_del">-	.flush_tlb_single = xen_flush_tlb_single,</span>
<span class="p_add">+	.flush_tlb_one_user = xen_flush_tlb_one_user,</span>
 	.flush_tlb_others = xen_flush_tlb_others,
 
 	.pgd_alloc = xen_pgd_alloc,
<span class="p_header">diff --git a/drivers/char/hw_random/via-rng.c b/drivers/char/hw_random/via-rng.c</span>
<span class="p_header">index d1f5bb534e0e..6e9df558325b 100644</span>
<span class="p_header">--- a/drivers/char/hw_random/via-rng.c</span>
<span class="p_header">+++ b/drivers/char/hw_random/via-rng.c</span>
<span class="p_chunk">@@ -162,7 +162,7 @@</span> <span class="p_context"> static int via_rng_init(struct hwrng *rng)</span>
 	/* Enable secondary noise source on CPUs where it is present. */
 
 	/* Nehemiah stepping 8 and higher */
<span class="p_del">-	if ((c-&gt;x86_model == 9) &amp;&amp; (c-&gt;x86_mask &gt; 7))</span>
<span class="p_add">+	if ((c-&gt;x86_model == 9) &amp;&amp; (c-&gt;x86_stepping &gt; 7))</span>
 		lo |= VIA_NOISESRC2;
 
 	/* Esther */
<span class="p_header">diff --git a/drivers/cpufreq/acpi-cpufreq.c b/drivers/cpufreq/acpi-cpufreq.c</span>
<span class="p_header">index 3a2ca0f79daf..d0c34df0529c 100644</span>
<span class="p_header">--- a/drivers/cpufreq/acpi-cpufreq.c</span>
<span class="p_header">+++ b/drivers/cpufreq/acpi-cpufreq.c</span>
<span class="p_chunk">@@ -629,7 +629,7 @@</span> <span class="p_context"> static int acpi_cpufreq_blacklist(struct cpuinfo_x86 *c)</span>
 	if (c-&gt;x86_vendor == X86_VENDOR_INTEL) {
 		if ((c-&gt;x86 == 15) &amp;&amp;
 		    (c-&gt;x86_model == 6) &amp;&amp;
<span class="p_del">-		    (c-&gt;x86_mask == 8)) {</span>
<span class="p_add">+		    (c-&gt;x86_stepping == 8)) {</span>
 			pr_info(&quot;Intel(R) Xeon(R) 7100 Errata AL30, processors may lock up on frequency changes: disabling acpi-cpufreq\n&quot;);
 			return -ENODEV;
 		    }
<span class="p_header">diff --git a/drivers/cpufreq/longhaul.c b/drivers/cpufreq/longhaul.c</span>
<span class="p_header">index c46a12df40dd..d5e27bc7585a 100644</span>
<span class="p_header">--- a/drivers/cpufreq/longhaul.c</span>
<span class="p_header">+++ b/drivers/cpufreq/longhaul.c</span>
<span class="p_chunk">@@ -775,7 +775,7 @@</span> <span class="p_context"> static int longhaul_cpu_init(struct cpufreq_policy *policy)</span>
 		break;
 
 	case 7:
<span class="p_del">-		switch (c-&gt;x86_mask) {</span>
<span class="p_add">+		switch (c-&gt;x86_stepping) {</span>
 		case 0:
 			longhaul_version = TYPE_LONGHAUL_V1;
 			cpu_model = CPU_SAMUEL2;
<span class="p_chunk">@@ -787,7 +787,7 @@</span> <span class="p_context"> static int longhaul_cpu_init(struct cpufreq_policy *policy)</span>
 			break;
 		case 1 ... 15:
 			longhaul_version = TYPE_LONGHAUL_V2;
<span class="p_del">-			if (c-&gt;x86_mask &lt; 8) {</span>
<span class="p_add">+			if (c-&gt;x86_stepping &lt; 8) {</span>
 				cpu_model = CPU_SAMUEL2;
 				cpuname = &quot;C3 &#39;Samuel 2&#39; [C5B]&quot;;
 			} else {
<span class="p_chunk">@@ -814,7 +814,7 @@</span> <span class="p_context"> static int longhaul_cpu_init(struct cpufreq_policy *policy)</span>
 		numscales = 32;
 		memcpy(mults, nehemiah_mults, sizeof(nehemiah_mults));
 		memcpy(eblcr, nehemiah_eblcr, sizeof(nehemiah_eblcr));
<span class="p_del">-		switch (c-&gt;x86_mask) {</span>
<span class="p_add">+		switch (c-&gt;x86_stepping) {</span>
 		case 0 ... 1:
 			cpu_model = CPU_NEHEMIAH;
 			cpuname = &quot;C3 &#39;Nehemiah A&#39; [C5XLOE]&quot;;
<span class="p_header">diff --git a/drivers/cpufreq/p4-clockmod.c b/drivers/cpufreq/p4-clockmod.c</span>
<span class="p_header">index fd77812313f3..a25741b1281b 100644</span>
<span class="p_header">--- a/drivers/cpufreq/p4-clockmod.c</span>
<span class="p_header">+++ b/drivers/cpufreq/p4-clockmod.c</span>
<span class="p_chunk">@@ -168,7 +168,7 @@</span> <span class="p_context"> static int cpufreq_p4_cpu_init(struct cpufreq_policy *policy)</span>
 #endif
 
 	/* Errata workaround */
<span class="p_del">-	cpuid = (c-&gt;x86 &lt;&lt; 8) | (c-&gt;x86_model &lt;&lt; 4) | c-&gt;x86_mask;</span>
<span class="p_add">+	cpuid = (c-&gt;x86 &lt;&lt; 8) | (c-&gt;x86_model &lt;&lt; 4) | c-&gt;x86_stepping;</span>
 	switch (cpuid) {
 	case 0x0f07:
 	case 0x0f0a:
<span class="p_header">diff --git a/drivers/cpufreq/powernow-k7.c b/drivers/cpufreq/powernow-k7.c</span>
<span class="p_header">index 80ac313e6c59..302e9ce793a0 100644</span>
<span class="p_header">--- a/drivers/cpufreq/powernow-k7.c</span>
<span class="p_header">+++ b/drivers/cpufreq/powernow-k7.c</span>
<span class="p_chunk">@@ -131,7 +131,7 @@</span> <span class="p_context"> static int check_powernow(void)</span>
 		return 0;
 	}
 
<span class="p_del">-	if ((c-&gt;x86_model == 6) &amp;&amp; (c-&gt;x86_mask == 0)) {</span>
<span class="p_add">+	if ((c-&gt;x86_model == 6) &amp;&amp; (c-&gt;x86_stepping == 0)) {</span>
 		pr_info(&quot;K7 660[A0] core detected, enabling errata workarounds\n&quot;);
 		have_a0 = 1;
 	}
<span class="p_header">diff --git a/drivers/cpufreq/speedstep-centrino.c b/drivers/cpufreq/speedstep-centrino.c</span>
<span class="p_header">index 41bc5397f4bb..4fa5adf16c70 100644</span>
<span class="p_header">--- a/drivers/cpufreq/speedstep-centrino.c</span>
<span class="p_header">+++ b/drivers/cpufreq/speedstep-centrino.c</span>
<span class="p_chunk">@@ -37,7 +37,7 @@</span> <span class="p_context"> struct cpu_id</span>
 {
 	__u8	x86;            /* CPU family */
 	__u8	x86_model;	/* model */
<span class="p_del">-	__u8	x86_mask;	/* stepping */</span>
<span class="p_add">+	__u8	x86_stepping;	/* stepping */</span>
 };
 
 enum {
<span class="p_chunk">@@ -277,7 +277,7 @@</span> <span class="p_context"> static int centrino_verify_cpu_id(const struct cpuinfo_x86 *c,</span>
 {
 	if ((c-&gt;x86 == x-&gt;x86) &amp;&amp;
 	    (c-&gt;x86_model == x-&gt;x86_model) &amp;&amp;
<span class="p_del">-	    (c-&gt;x86_mask == x-&gt;x86_mask))</span>
<span class="p_add">+	    (c-&gt;x86_stepping == x-&gt;x86_stepping))</span>
 		return 1;
 	return 0;
 }
<span class="p_header">diff --git a/drivers/cpufreq/speedstep-lib.c b/drivers/cpufreq/speedstep-lib.c</span>
<span class="p_header">index 8085ec9000d1..e3a9962ee410 100644</span>
<span class="p_header">--- a/drivers/cpufreq/speedstep-lib.c</span>
<span class="p_header">+++ b/drivers/cpufreq/speedstep-lib.c</span>
<span class="p_chunk">@@ -272,9 +272,9 @@</span> <span class="p_context"> unsigned int speedstep_detect_processor(void)</span>
 		ebx = cpuid_ebx(0x00000001);
 		ebx &amp;= 0x000000FF;
 
<span class="p_del">-		pr_debug(&quot;ebx value is %x, x86_mask is %x\n&quot;, ebx, c-&gt;x86_mask);</span>
<span class="p_add">+		pr_debug(&quot;ebx value is %x, x86_stepping is %x\n&quot;, ebx, c-&gt;x86_stepping);</span>
 
<span class="p_del">-		switch (c-&gt;x86_mask) {</span>
<span class="p_add">+		switch (c-&gt;x86_stepping) {</span>
 		case 4:
 			/*
 			 * B-stepping [M-P4-M]
<span class="p_chunk">@@ -361,7 +361,7 @@</span> <span class="p_context"> unsigned int speedstep_detect_processor(void)</span>
 				msr_lo, msr_hi);
 		if ((msr_hi &amp; (1&lt;&lt;18)) &amp;&amp;
 		    (relaxed_check ? 1 : (msr_hi &amp; (3&lt;&lt;24)))) {
<span class="p_del">-			if (c-&gt;x86_mask == 0x01) {</span>
<span class="p_add">+			if (c-&gt;x86_stepping == 0x01) {</span>
 				pr_debug(&quot;early PIII version\n&quot;);
 				return SPEEDSTEP_CPU_PIII_C_EARLY;
 			} else
<span class="p_header">diff --git a/drivers/crypto/padlock-aes.c b/drivers/crypto/padlock-aes.c</span>
<span class="p_header">index 4b6642a25df5..1c6cbda56afe 100644</span>
<span class="p_header">--- a/drivers/crypto/padlock-aes.c</span>
<span class="p_header">+++ b/drivers/crypto/padlock-aes.c</span>
<span class="p_chunk">@@ -512,7 +512,7 @@</span> <span class="p_context"> static int __init padlock_init(void)</span>
 
 	printk(KERN_NOTICE PFX &quot;Using VIA PadLock ACE for AES algorithm.\n&quot;);
 
<span class="p_del">-	if (c-&gt;x86 == 6 &amp;&amp; c-&gt;x86_model == 15 &amp;&amp; c-&gt;x86_mask == 2) {</span>
<span class="p_add">+	if (c-&gt;x86 == 6 &amp;&amp; c-&gt;x86_model == 15 &amp;&amp; c-&gt;x86_stepping == 2) {</span>
 		ecb_fetch_blocks = MAX_ECB_FETCH_BLOCKS;
 		cbc_fetch_blocks = MAX_CBC_FETCH_BLOCKS;
 		printk(KERN_NOTICE PFX &quot;VIA Nano stepping 2 detected: enabling workaround.\n&quot;);
<span class="p_header">diff --git a/drivers/edac/amd64_edac.c b/drivers/edac/amd64_edac.c</span>
<span class="p_header">index 8b16ec595fa7..329cb96f886f 100644</span>
<span class="p_header">--- a/drivers/edac/amd64_edac.c</span>
<span class="p_header">+++ b/drivers/edac/amd64_edac.c</span>
<span class="p_chunk">@@ -3147,7 +3147,7 @@</span> <span class="p_context"> static struct amd64_family_type *per_family_init(struct amd64_pvt *pvt)</span>
 	struct amd64_family_type *fam_type = NULL;
 
 	pvt-&gt;ext_model  = boot_cpu_data.x86_model &gt;&gt; 4;
<span class="p_del">-	pvt-&gt;stepping	= boot_cpu_data.x86_mask;</span>
<span class="p_add">+	pvt-&gt;stepping	= boot_cpu_data.x86_stepping;</span>
 	pvt-&gt;model	= boot_cpu_data.x86_model;
 	pvt-&gt;fam	= boot_cpu_data.x86;
 
<span class="p_header">diff --git a/drivers/hwmon/coretemp.c b/drivers/hwmon/coretemp.c</span>
<span class="p_header">index c13a4fd86b3c..a42744c7665b 100644</span>
<span class="p_header">--- a/drivers/hwmon/coretemp.c</span>
<span class="p_header">+++ b/drivers/hwmon/coretemp.c</span>
<span class="p_chunk">@@ -268,13 +268,13 @@</span> <span class="p_context"> static int adjust_tjmax(struct cpuinfo_x86 *c, u32 id, struct device *dev)</span>
 	for (i = 0; i &lt; ARRAY_SIZE(tjmax_model_table); i++) {
 		const struct tjmax_model *tm = &amp;tjmax_model_table[i];
 		if (c-&gt;x86_model == tm-&gt;model &amp;&amp;
<span class="p_del">-		    (tm-&gt;mask == ANY || c-&gt;x86_mask == tm-&gt;mask))</span>
<span class="p_add">+		    (tm-&gt;mask == ANY || c-&gt;x86_stepping == tm-&gt;mask))</span>
 			return tm-&gt;tjmax;
 	}
 
 	/* Early chips have no MSR for TjMax */
 
<span class="p_del">-	if (c-&gt;x86_model == 0xf &amp;&amp; c-&gt;x86_mask &lt; 4)</span>
<span class="p_add">+	if (c-&gt;x86_model == 0xf &amp;&amp; c-&gt;x86_stepping &lt; 4)</span>
 		usemsr_ee = 0;
 
 	if (c-&gt;x86_model &gt; 0xe &amp;&amp; usemsr_ee) {
<span class="p_chunk">@@ -425,7 +425,7 @@</span> <span class="p_context"> static int chk_ucode_version(unsigned int cpu)</span>
 	 * Readings might stop update when processor visited too deep sleep,
 	 * fixed for stepping D0 (6EC).
 	 */
<span class="p_del">-	if (c-&gt;x86_model == 0xe &amp;&amp; c-&gt;x86_mask &lt; 0xc &amp;&amp; c-&gt;microcode &lt; 0x39) {</span>
<span class="p_add">+	if (c-&gt;x86_model == 0xe &amp;&amp; c-&gt;x86_stepping &lt; 0xc &amp;&amp; c-&gt;microcode &lt; 0x39) {</span>
 		pr_err(&quot;Errata AE18 not fixed, update BIOS or microcode of the CPU!\n&quot;);
 		return -ENODEV;
 	}
<span class="p_header">diff --git a/drivers/hwmon/hwmon-vid.c b/drivers/hwmon/hwmon-vid.c</span>
<span class="p_header">index ef91b8a67549..84e91286fc4f 100644</span>
<span class="p_header">--- a/drivers/hwmon/hwmon-vid.c</span>
<span class="p_header">+++ b/drivers/hwmon/hwmon-vid.c</span>
<span class="p_chunk">@@ -293,7 +293,7 @@</span> <span class="p_context"> u8 vid_which_vrm(void)</span>
 	if (c-&gt;x86 &lt; 6)		/* Any CPU with family lower than 6 */
 		return 0;	/* doesn&#39;t have VID */
 
<span class="p_del">-	vrm_ret = find_vrm(c-&gt;x86, c-&gt;x86_model, c-&gt;x86_mask, c-&gt;x86_vendor);</span>
<span class="p_add">+	vrm_ret = find_vrm(c-&gt;x86, c-&gt;x86_model, c-&gt;x86_stepping, c-&gt;x86_vendor);</span>
 	if (vrm_ret == 134)
 		vrm_ret = get_via_model_d_vrm();
 	if (vrm_ret == 0)
<span class="p_header">diff --git a/drivers/hwmon/k10temp.c b/drivers/hwmon/k10temp.c</span>
<span class="p_header">index 0721e175664a..b960015cb073 100644</span>
<span class="p_header">--- a/drivers/hwmon/k10temp.c</span>
<span class="p_header">+++ b/drivers/hwmon/k10temp.c</span>
<span class="p_chunk">@@ -226,7 +226,7 @@</span> <span class="p_context"> static bool has_erratum_319(struct pci_dev *pdev)</span>
 	 * and AM3 formats, but that&#39;s the best we can do.
 	 */
 	return boot_cpu_data.x86_model &lt; 4 ||
<span class="p_del">-	       (boot_cpu_data.x86_model == 4 &amp;&amp; boot_cpu_data.x86_mask &lt;= 2);</span>
<span class="p_add">+	       (boot_cpu_data.x86_model == 4 &amp;&amp; boot_cpu_data.x86_stepping &lt;= 2);</span>
 }
 
 static int k10temp_probe(struct pci_dev *pdev,
<span class="p_header">diff --git a/drivers/hwmon/k8temp.c b/drivers/hwmon/k8temp.c</span>
<span class="p_header">index 5a632bcf869b..e59f9113fb93 100644</span>
<span class="p_header">--- a/drivers/hwmon/k8temp.c</span>
<span class="p_header">+++ b/drivers/hwmon/k8temp.c</span>
<span class="p_chunk">@@ -187,7 +187,7 @@</span> <span class="p_context"> static int k8temp_probe(struct pci_dev *pdev,</span>
 		return -ENOMEM;
 
 	model = boot_cpu_data.x86_model;
<span class="p_del">-	stepping = boot_cpu_data.x86_mask;</span>
<span class="p_add">+	stepping = boot_cpu_data.x86_stepping;</span>
 
 	/* feature available since SH-C0, exclude older revisions */
 	if ((model == 4 &amp;&amp; stepping == 0) ||
<span class="p_header">diff --git a/drivers/video/fbdev/geode/video_gx.c b/drivers/video/fbdev/geode/video_gx.c</span>
<span class="p_header">index 6082f653c68a..67773e8bbb95 100644</span>
<span class="p_header">--- a/drivers/video/fbdev/geode/video_gx.c</span>
<span class="p_header">+++ b/drivers/video/fbdev/geode/video_gx.c</span>
<span class="p_chunk">@@ -127,7 +127,7 @@</span> <span class="p_context"> void gx_set_dclk_frequency(struct fb_info *info)</span>
 	int timeout = 1000;
 
 	/* Rev. 1 Geode GXs use a 14 MHz reference clock instead of 48 MHz. */
<span class="p_del">-	if (cpu_data(0).x86_mask == 1) {</span>
<span class="p_add">+	if (cpu_data(0).x86_stepping == 1) {</span>
 		pll_table = gx_pll_table_14MHz;
 		pll_table_len = ARRAY_SIZE(gx_pll_table_14MHz);
 	} else {
<span class="p_header">diff --git a/include/linux/nospec.h b/include/linux/nospec.h</span>
<span class="p_header">index b99bced39ac2..fbc98e2c8228 100644</span>
<span class="p_header">--- a/include/linux/nospec.h</span>
<span class="p_header">+++ b/include/linux/nospec.h</span>
<span class="p_chunk">@@ -19,20 +19,6 @@</span> <span class="p_context"></span>
 static inline unsigned long array_index_mask_nospec(unsigned long index,
 						    unsigned long size)
 {
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Warn developers about inappropriate array_index_nospec() usage.</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 * Even if the CPU speculates past the WARN_ONCE branch, the</span>
<span class="p_del">-	 * sign bit of @index is taken into account when generating the</span>
<span class="p_del">-	 * mask.</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 * This warning is compiled out when the compiler can infer that</span>
<span class="p_del">-	 * @index and @size are less than LONG_MAX.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	if (WARN_ONCE(index &gt; LONG_MAX || size &gt; LONG_MAX,</span>
<span class="p_del">-			&quot;array_index_nospec() limited to range of [0, LONG_MAX]\n&quot;))</span>
<span class="p_del">-		return 0;</span>
<span class="p_del">-</span>
 	/*
 	 * Always calculate and emit the mask even if the compiler
 	 * thinks the mask is not needed. The compiler does not take
<span class="p_chunk">@@ -43,6 +29,26 @@</span> <span class="p_context"> static inline unsigned long array_index_mask_nospec(unsigned long index,</span>
 }
 #endif
 
<span class="p_add">+/*</span>
<span class="p_add">+ * Warn developers about inappropriate array_index_nospec() usage.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Even if the CPU speculates past the WARN_ONCE branch, the</span>
<span class="p_add">+ * sign bit of @index is taken into account when generating the</span>
<span class="p_add">+ * mask.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This warning is compiled out when the compiler can infer that</span>
<span class="p_add">+ * @index and @size are less than LONG_MAX.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define array_index_mask_nospec_check(index, size)				\</span>
<span class="p_add">+({										\</span>
<span class="p_add">+	if (WARN_ONCE(index &gt; LONG_MAX || size &gt; LONG_MAX,			\</span>
<span class="p_add">+	    &quot;array_index_nospec() limited to range of [0, LONG_MAX]\n&quot;))	\</span>
<span class="p_add">+		_mask = 0;							\</span>
<span class="p_add">+	else									\</span>
<span class="p_add">+		_mask = array_index_mask_nospec(index, size);			\</span>
<span class="p_add">+	_mask;									\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
 /*
  * array_index_nospec - sanitize an array index after a bounds check
  *
<span class="p_chunk">@@ -61,7 +67,7 @@</span> <span class="p_context"> static inline unsigned long array_index_mask_nospec(unsigned long index,</span>
 ({									\
 	typeof(index) _i = (index);					\
 	typeof(size) _s = (size);					\
<span class="p_del">-	unsigned long _mask = array_index_mask_nospec(_i, _s);		\</span>
<span class="p_add">+	unsigned long _mask = array_index_mask_nospec_check(_i, _s);	\</span>
 									\
 	BUILD_BUG_ON(sizeof(_i) &gt; sizeof(long));			\
 	BUILD_BUG_ON(sizeof(_s) &gt; sizeof(long));			\
<span class="p_header">diff --git a/include/trace/events/xen.h b/include/trace/events/xen.h</span>
<span class="p_header">index b8adf05c534e..7dd8f34c37df 100644</span>
<span class="p_header">--- a/include/trace/events/xen.h</span>
<span class="p_header">+++ b/include/trace/events/xen.h</span>
<span class="p_chunk">@@ -368,7 +368,7 @@</span> <span class="p_context"> TRACE_EVENT(xen_mmu_flush_tlb,</span>
 	    TP_printk(&quot;%s&quot;, &quot;&quot;)
 	);
 
<span class="p_del">-TRACE_EVENT(xen_mmu_flush_tlb_single,</span>
<span class="p_add">+TRACE_EVENT(xen_mmu_flush_tlb_one_user,</span>
 	    TP_PROTO(unsigned long addr),
 	    TP_ARGS(addr),
 	    TP_STRUCT__entry(
<span class="p_header">diff --git a/tools/objtool/check.c b/tools/objtool/check.c</span>
<span class="p_header">index 9cd028aa1509..c7fb5c2392ee 100644</span>
<span class="p_header">--- a/tools/objtool/check.c</span>
<span class="p_header">+++ b/tools/objtool/check.c</span>
<span class="p_chunk">@@ -851,8 +851,14 @@</span> <span class="p_context"> static int add_switch_table(struct objtool_file *file, struct symbol *func,</span>
  *    This is a fairly uncommon pattern which is new for GCC 6.  As of this
  *    writing, there are 11 occurrences of it in the allmodconfig kernel.
  *
<span class="p_add">+ *    As of GCC 7 there are quite a few more of these and the &#39;in between&#39; code</span>
<span class="p_add">+ *    is significant. Esp. with KASAN enabled some of the code between the mov</span>
<span class="p_add">+ *    and jmpq uses .rodata itself, which can confuse things.</span>
<span class="p_add">+ *</span>
  *    TODO: Once we have DWARF CFI and smarter instruction decoding logic,
  *    ensure the same register is used in the mov and jump instructions.
<span class="p_add">+ *</span>
<span class="p_add">+ *    NOTE: RETPOLINE made it harder still to decode dynamic jumps.</span>
  */
 static struct rela *find_switch_table(struct objtool_file *file,
 				      struct symbol *func,
<span class="p_chunk">@@ -874,12 +880,25 @@</span> <span class="p_context"> static struct rela *find_switch_table(struct objtool_file *file,</span>
 						text_rela-&gt;addend + 4);
 		if (!rodata_rela)
 			return NULL;
<span class="p_add">+</span>
 		file-&gt;ignore_unreachables = true;
 		return rodata_rela;
 	}
 
 	/* case 3 */
<span class="p_del">-	func_for_each_insn_continue_reverse(file, func, insn) {</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Backward search using the @first_jump_src links, these help avoid</span>
<span class="p_add">+	 * much of the &#39;in between&#39; code. Which avoids us getting confused by</span>
<span class="p_add">+	 * it.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	for (insn = list_prev_entry(insn, list);</span>
<span class="p_add">+</span>
<span class="p_add">+	     &amp;insn-&gt;list != &amp;file-&gt;insn_list &amp;&amp;</span>
<span class="p_add">+	     insn-&gt;sec == func-&gt;sec &amp;&amp;</span>
<span class="p_add">+	     insn-&gt;offset &gt;= func-&gt;offset;</span>
<span class="p_add">+</span>
<span class="p_add">+	     insn = insn-&gt;first_jump_src ?: list_prev_entry(insn, list)) {</span>
<span class="p_add">+</span>
 		if (insn-&gt;type == INSN_JUMP_DYNAMIC)
 			break;
 
<span class="p_chunk">@@ -909,14 +928,32 @@</span> <span class="p_context"> static struct rela *find_switch_table(struct objtool_file *file,</span>
 	return NULL;
 }
 
<span class="p_add">+</span>
 static int add_func_switch_tables(struct objtool_file *file,
 				  struct symbol *func)
 {
<span class="p_del">-	struct instruction *insn, *prev_jump = NULL;</span>
<span class="p_add">+	struct instruction *insn, *last = NULL, *prev_jump = NULL;</span>
 	struct rela *rela, *prev_rela = NULL;
 	int ret;
 
 	func_for_each_insn(file, func, insn) {
<span class="p_add">+		if (!last)</span>
<span class="p_add">+			last = insn;</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Store back-pointers for unconditional forward jumps such</span>
<span class="p_add">+		 * that find_switch_table() can back-track using those and</span>
<span class="p_add">+		 * avoid some potentially confusing code.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (insn-&gt;type == INSN_JUMP_UNCONDITIONAL &amp;&amp; insn-&gt;jump_dest &amp;&amp;</span>
<span class="p_add">+		    insn-&gt;offset &gt; last-&gt;offset &amp;&amp;</span>
<span class="p_add">+		    insn-&gt;jump_dest-&gt;offset &gt; insn-&gt;offset &amp;&amp;</span>
<span class="p_add">+		    !insn-&gt;jump_dest-&gt;first_jump_src) {</span>
<span class="p_add">+</span>
<span class="p_add">+			insn-&gt;jump_dest-&gt;first_jump_src = insn;</span>
<span class="p_add">+			last = insn-&gt;jump_dest;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
 		if (insn-&gt;type != INSN_JUMP_DYNAMIC)
 			continue;
 
<span class="p_chunk">@@ -1898,13 +1935,19 @@</span> <span class="p_context"> static bool ignore_unreachable_insn(struct instruction *insn)</span>
 		if (is_kasan_insn(insn) || is_ubsan_insn(insn))
 			return true;
 
<span class="p_del">-		if (insn-&gt;type == INSN_JUMP_UNCONDITIONAL &amp;&amp; insn-&gt;jump_dest) {</span>
<span class="p_del">-			insn = insn-&gt;jump_dest;</span>
<span class="p_del">-			continue;</span>
<span class="p_add">+		if (insn-&gt;type == INSN_JUMP_UNCONDITIONAL) {</span>
<span class="p_add">+			if (insn-&gt;jump_dest &amp;&amp;</span>
<span class="p_add">+			    insn-&gt;jump_dest-&gt;func == insn-&gt;func) {</span>
<span class="p_add">+				insn = insn-&gt;jump_dest;</span>
<span class="p_add">+				continue;</span>
<span class="p_add">+			}</span>
<span class="p_add">+</span>
<span class="p_add">+			break;</span>
 		}
 
 		if (insn-&gt;offset + insn-&gt;len &gt;= insn-&gt;func-&gt;offset + insn-&gt;func-&gt;len)
 			break;
<span class="p_add">+</span>
 		insn = list_next_entry(insn, list);
 	}
 
<span class="p_header">diff --git a/tools/objtool/check.h b/tools/objtool/check.h</span>
<span class="p_header">index dbadb304a410..23a1d065cae1 100644</span>
<span class="p_header">--- a/tools/objtool/check.h</span>
<span class="p_header">+++ b/tools/objtool/check.h</span>
<span class="p_chunk">@@ -47,6 +47,7 @@</span> <span class="p_context"> struct instruction {</span>
 	bool alt_group, visited, dead_end, ignore, hint, save, restore, ignore_alts;
 	struct symbol *call_dest;
 	struct instruction *jump_dest;
<span class="p_add">+	struct instruction *first_jump_src;</span>
 	struct list_head alts;
 	struct symbol *func;
 	struct stack_op stack_op;
<span class="p_header">diff --git a/tools/testing/selftests/x86/Makefile b/tools/testing/selftests/x86/Makefile</span>
<span class="p_header">index 5d4f10ac2af2..aa6e2d7f6a1f 100644</span>
<span class="p_header">--- a/tools/testing/selftests/x86/Makefile</span>
<span class="p_header">+++ b/tools/testing/selftests/x86/Makefile</span>
<span class="p_chunk">@@ -5,16 +5,26 @@</span> <span class="p_context"> include ../lib.mk</span>
 
 .PHONY: all all_32 all_64 warn_32bit_failure clean
 
<span class="p_del">-TARGETS_C_BOTHBITS := single_step_syscall sysret_ss_attrs syscall_nt ptrace_syscall test_mremap_vdso \</span>
<span class="p_del">-			check_initial_reg_state sigreturn ldt_gdt iopl mpx-mini-test ioperm \</span>
<span class="p_add">+UNAME_M := $(shell uname -m)</span>
<span class="p_add">+CAN_BUILD_I386 := $(shell ./check_cc.sh $(CC) trivial_32bit_program.c -m32)</span>
<span class="p_add">+CAN_BUILD_X86_64 := $(shell ./check_cc.sh $(CC) trivial_64bit_program.c)</span>
<span class="p_add">+</span>
<span class="p_add">+TARGETS_C_BOTHBITS := single_step_syscall sysret_ss_attrs syscall_nt test_mremap_vdso \</span>
<span class="p_add">+			check_initial_reg_state sigreturn iopl mpx-mini-test ioperm \</span>
 			protection_keys test_vdso test_vsyscall
 TARGETS_C_32BIT_ONLY := entry_from_vm86 syscall_arg_fault test_syscall_vdso unwind_vdso \
 			test_FCMOV test_FCOMI test_FISTTP \
 			vdso_restorer
<span class="p_del">-TARGETS_C_64BIT_ONLY := fsgsbase sysret_rip 5lvl</span>
<span class="p_add">+TARGETS_C_64BIT_ONLY := fsgsbase sysret_rip</span>
<span class="p_add">+# Some selftests require 32bit support enabled also on 64bit systems</span>
<span class="p_add">+TARGETS_C_32BIT_NEEDED := ldt_gdt ptrace_syscall</span>
 
<span class="p_del">-TARGETS_C_32BIT_ALL := $(TARGETS_C_BOTHBITS) $(TARGETS_C_32BIT_ONLY)</span>
<span class="p_add">+TARGETS_C_32BIT_ALL := $(TARGETS_C_BOTHBITS) $(TARGETS_C_32BIT_ONLY) $(TARGETS_C_32BIT_NEEDED)</span>
 TARGETS_C_64BIT_ALL := $(TARGETS_C_BOTHBITS) $(TARGETS_C_64BIT_ONLY)
<span class="p_add">+ifeq ($(CAN_BUILD_I386)$(CAN_BUILD_X86_64),11)</span>
<span class="p_add">+TARGETS_C_64BIT_ALL += $(TARGETS_C_32BIT_NEEDED)</span>
<span class="p_add">+endif</span>
<span class="p_add">+</span>
 BINARIES_32 := $(TARGETS_C_32BIT_ALL:%=%_32)
 BINARIES_64 := $(TARGETS_C_64BIT_ALL:%=%_64)
 
<span class="p_chunk">@@ -23,18 +33,16 @@</span> <span class="p_context"> BINARIES_64 := $(patsubst %,$(OUTPUT)/%,$(BINARIES_64))</span>
 
 CFLAGS := -O2 -g -std=gnu99 -pthread -Wall -no-pie
 
<span class="p_del">-UNAME_M := $(shell uname -m)</span>
<span class="p_del">-CAN_BUILD_I386 := $(shell ./check_cc.sh $(CC) trivial_32bit_program.c -m32)</span>
<span class="p_del">-CAN_BUILD_X86_64 := $(shell ./check_cc.sh $(CC) trivial_64bit_program.c)</span>
<span class="p_del">-</span>
 ifeq ($(CAN_BUILD_I386),1)
 all: all_32
 TEST_PROGS += $(BINARIES_32)
<span class="p_add">+EXTRA_CFLAGS += -DCAN_BUILD_32</span>
 endif
 
 ifeq ($(CAN_BUILD_X86_64),1)
 all: all_64
 TEST_PROGS += $(BINARIES_64)
<span class="p_add">+EXTRA_CFLAGS += -DCAN_BUILD_64</span>
 endif
 
 all_32: $(BINARIES_32)
<span class="p_header">diff --git a/tools/testing/selftests/x86/mpx-mini-test.c b/tools/testing/selftests/x86/mpx-mini-test.c</span>
<span class="p_header">index ec0f6b45ce8b..9c0325e1ea68 100644</span>
<span class="p_header">--- a/tools/testing/selftests/x86/mpx-mini-test.c</span>
<span class="p_header">+++ b/tools/testing/selftests/x86/mpx-mini-test.c</span>
<span class="p_chunk">@@ -315,11 +315,39 @@</span> <span class="p_context"> static inline void *__si_bounds_upper(siginfo_t *si)</span>
 	return si-&gt;si_upper;
 }
 #else
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * This deals with old version of _sigfault in some distros:</span>
<span class="p_add">+ *</span>
<span class="p_add">+</span>
<span class="p_add">+old _sigfault:</span>
<span class="p_add">+        struct {</span>
<span class="p_add">+            void *si_addr;</span>
<span class="p_add">+	} _sigfault;</span>
<span class="p_add">+</span>
<span class="p_add">+new _sigfault:</span>
<span class="p_add">+	struct {</span>
<span class="p_add">+		void __user *_addr;</span>
<span class="p_add">+		int _trapno;</span>
<span class="p_add">+		short _addr_lsb;</span>
<span class="p_add">+		union {</span>
<span class="p_add">+			struct {</span>
<span class="p_add">+				void __user *_lower;</span>
<span class="p_add">+				void __user *_upper;</span>
<span class="p_add">+			} _addr_bnd;</span>
<span class="p_add">+			__u32 _pkey;</span>
<span class="p_add">+		};</span>
<span class="p_add">+	} _sigfault;</span>
<span class="p_add">+ *</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
 static inline void **__si_bounds_hack(siginfo_t *si)
 {
 	void *sigfault = &amp;si-&gt;_sifields._sigfault;
 	void *end_sigfault = sigfault + sizeof(si-&gt;_sifields._sigfault);
<span class="p_del">-	void **__si_lower = end_sigfault;</span>
<span class="p_add">+	int *trapno = (int*)end_sigfault;</span>
<span class="p_add">+	/* skip _trapno and _addr_lsb */</span>
<span class="p_add">+	void **__si_lower = (void**)(trapno + 2);</span>
 
 	return __si_lower;
 }
<span class="p_chunk">@@ -331,7 +359,7 @@</span> <span class="p_context"> static inline void *__si_bounds_lower(siginfo_t *si)</span>
 
 static inline void *__si_bounds_upper(siginfo_t *si)
 {
<span class="p_del">-	return (*__si_bounds_hack(si)) + sizeof(void *);</span>
<span class="p_add">+	return *(__si_bounds_hack(si) + 1);</span>
 }
 #endif
 
<span class="p_header">diff --git a/tools/testing/selftests/x86/protection_keys.c b/tools/testing/selftests/x86/protection_keys.c</span>
<span class="p_header">index bc1b0735bb50..f15aa5a76fe3 100644</span>
<span class="p_header">--- a/tools/testing/selftests/x86/protection_keys.c</span>
<span class="p_header">+++ b/tools/testing/selftests/x86/protection_keys.c</span>
<span class="p_chunk">@@ -393,34 +393,6 @@</span> <span class="p_context"> pid_t fork_lazy_child(void)</span>
 	return forkret;
 }
 
<span class="p_del">-void davecmp(void *_a, void *_b, int len)</span>
<span class="p_del">-{</span>
<span class="p_del">-	int i;</span>
<span class="p_del">-	unsigned long *a = _a;</span>
<span class="p_del">-	unsigned long *b = _b;</span>
<span class="p_del">-</span>
<span class="p_del">-	for (i = 0; i &lt; len / sizeof(*a); i++) {</span>
<span class="p_del">-		if (a[i] == b[i])</span>
<span class="p_del">-			continue;</span>
<span class="p_del">-</span>
<span class="p_del">-		dprintf3(&quot;[%3d]: a: %016lx b: %016lx\n&quot;, i, a[i], b[i]);</span>
<span class="p_del">-	}</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-void dumpit(char *f)</span>
<span class="p_del">-{</span>
<span class="p_del">-	int fd = open(f, O_RDONLY);</span>
<span class="p_del">-	char buf[100];</span>
<span class="p_del">-	int nr_read;</span>
<span class="p_del">-</span>
<span class="p_del">-	dprintf2(&quot;maps fd: %d\n&quot;, fd);</span>
<span class="p_del">-	do {</span>
<span class="p_del">-		nr_read = read(fd, &amp;buf[0], sizeof(buf));</span>
<span class="p_del">-		write(1, buf, nr_read);</span>
<span class="p_del">-	} while (nr_read &gt; 0);</span>
<span class="p_del">-	close(fd);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 #define PKEY_DISABLE_ACCESS    0x1
 #define PKEY_DISABLE_WRITE     0x2
 
<span class="p_header">diff --git a/tools/testing/selftests/x86/single_step_syscall.c b/tools/testing/selftests/x86/single_step_syscall.c</span>
<span class="p_header">index a48da95c18fd..ddfdd635de16 100644</span>
<span class="p_header">--- a/tools/testing/selftests/x86/single_step_syscall.c</span>
<span class="p_header">+++ b/tools/testing/selftests/x86/single_step_syscall.c</span>
<span class="p_chunk">@@ -119,7 +119,9 @@</span> <span class="p_context"> static void check_result(void)</span>
 
 int main()
 {
<span class="p_add">+#ifdef CAN_BUILD_32</span>
 	int tmp;
<span class="p_add">+#endif</span>
 
 	sethandler(SIGTRAP, sigtrap, 0);
 
<span class="p_chunk">@@ -139,12 +141,13 @@</span> <span class="p_context"> int main()</span>
 		      : : &quot;c&quot; (post_nop) : &quot;r11&quot;);
 	check_result();
 #endif
<span class="p_del">-</span>
<span class="p_add">+#ifdef CAN_BUILD_32</span>
 	printf(&quot;[RUN]\tSet TF and check int80\n&quot;);
 	set_eflags(get_eflags() | X86_EFLAGS_TF);
 	asm volatile (&quot;int $0x80&quot; : &quot;=a&quot; (tmp) : &quot;a&quot; (SYS_getpid)
 			: INT80_CLOBBERS);
 	check_result();
<span class="p_add">+#endif</span>
 
 	/*
 	 * This test is particularly interesting if fast syscalls use
<span class="p_header">diff --git a/tools/testing/selftests/x86/test_mremap_vdso.c b/tools/testing/selftests/x86/test_mremap_vdso.c</span>
<span class="p_header">index bf0d687c7db7..64f11c8d9b76 100644</span>
<span class="p_header">--- a/tools/testing/selftests/x86/test_mremap_vdso.c</span>
<span class="p_header">+++ b/tools/testing/selftests/x86/test_mremap_vdso.c</span>
<span class="p_chunk">@@ -90,8 +90,12 @@</span> <span class="p_context"> int main(int argc, char **argv, char **envp)</span>
 			vdso_size += PAGE_SIZE;
 		}
 
<span class="p_add">+#ifdef __i386__</span>
 		/* Glibc is likely to explode now - exit with raw syscall */
 		asm volatile (&quot;int $0x80&quot; : : &quot;a&quot; (__NR_exit), &quot;b&quot; (!!ret));
<span class="p_add">+#else /* __x86_64__ */</span>
<span class="p_add">+		syscall(SYS_exit, ret);</span>
<span class="p_add">+#endif</span>
 	} else {
 		int status;
 
<span class="p_header">diff --git a/tools/testing/selftests/x86/test_vdso.c b/tools/testing/selftests/x86/test_vdso.c</span>
<span class="p_header">index 29973cde06d3..235259011704 100644</span>
<span class="p_header">--- a/tools/testing/selftests/x86/test_vdso.c</span>
<span class="p_header">+++ b/tools/testing/selftests/x86/test_vdso.c</span>
<span class="p_chunk">@@ -26,20 +26,59 @@</span> <span class="p_context"></span>
 # endif
 #endif
 
<span class="p_add">+/* max length of lines in /proc/self/maps - anything longer is skipped here */</span>
<span class="p_add">+#define MAPS_LINE_LEN 128</span>
<span class="p_add">+</span>
 int nerrs = 0;
 
<span class="p_add">+typedef long (*getcpu_t)(unsigned *, unsigned *, void *);</span>
<span class="p_add">+</span>
<span class="p_add">+getcpu_t vgetcpu;</span>
<span class="p_add">+getcpu_t vdso_getcpu;</span>
<span class="p_add">+</span>
<span class="p_add">+static void *vsyscall_getcpu(void)</span>
<span class="p_add">+{</span>
 #ifdef __x86_64__
<span class="p_del">-# define VSYS(x) (x)</span>
<span class="p_add">+	FILE *maps;</span>
<span class="p_add">+	char line[MAPS_LINE_LEN];</span>
<span class="p_add">+	bool found = false;</span>
<span class="p_add">+</span>
<span class="p_add">+	maps = fopen(&quot;/proc/self/maps&quot;, &quot;r&quot;);</span>
<span class="p_add">+	if (!maps) /* might still be present, but ignore it here, as we test vDSO not vsyscall */</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+	while (fgets(line, MAPS_LINE_LEN, maps)) {</span>
<span class="p_add">+		char r, x;</span>
<span class="p_add">+		void *start, *end;</span>
<span class="p_add">+		char name[MAPS_LINE_LEN];</span>
<span class="p_add">+</span>
<span class="p_add">+		/* sscanf() is safe here as strlen(name) &gt;= strlen(line) */</span>
<span class="p_add">+		if (sscanf(line, &quot;%p-%p %c-%cp %*x %*x:%*x %*u %s&quot;,</span>
<span class="p_add">+			   &amp;start, &amp;end, &amp;r, &amp;x, name) != 5)</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (strcmp(name, &quot;[vsyscall]&quot;))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		/* assume entries are OK, as we test vDSO here not vsyscall */</span>
<span class="p_add">+		found = true;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	fclose(maps);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!found) {</span>
<span class="p_add">+		printf(&quot;Warning: failed to find vsyscall getcpu\n&quot;);</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return (void *) (0xffffffffff600800);</span>
 #else
<span class="p_del">-# define VSYS(x) 0</span>
<span class="p_add">+	return NULL;</span>
 #endif
<span class="p_add">+}</span>
 
<span class="p_del">-typedef long (*getcpu_t)(unsigned *, unsigned *, void *);</span>
<span class="p_del">-</span>
<span class="p_del">-const getcpu_t vgetcpu = (getcpu_t)VSYS(0xffffffffff600800);</span>
<span class="p_del">-getcpu_t vdso_getcpu;</span>
 
<span class="p_del">-void fill_function_pointers()</span>
<span class="p_add">+static void fill_function_pointers()</span>
 {
 	void *vdso = dlopen(&quot;linux-vdso.so.1&quot;,
 			    RTLD_LAZY | RTLD_LOCAL | RTLD_NOLOAD);
<span class="p_chunk">@@ -54,6 +93,8 @@</span> <span class="p_context"> void fill_function_pointers()</span>
 	vdso_getcpu = (getcpu_t)dlsym(vdso, &quot;__vdso_getcpu&quot;);
 	if (!vdso_getcpu)
 		printf(&quot;Warning: failed to find getcpu in vDSO\n&quot;);
<span class="p_add">+</span>
<span class="p_add">+	vgetcpu = (getcpu_t) vsyscall_getcpu();</span>
 }
 
 static long sys_getcpu(unsigned * cpu, unsigned * node,
<span class="p_header">diff --git a/tools/testing/selftests/x86/test_vsyscall.c b/tools/testing/selftests/x86/test_vsyscall.c</span>
<span class="p_header">index 7a744fa7b786..be81621446f0 100644</span>
<span class="p_header">--- a/tools/testing/selftests/x86/test_vsyscall.c</span>
<span class="p_header">+++ b/tools/testing/selftests/x86/test_vsyscall.c</span>
<span class="p_chunk">@@ -33,6 +33,9 @@</span> <span class="p_context"></span>
 # endif
 #endif
 
<span class="p_add">+/* max length of lines in /proc/self/maps - anything longer is skipped here */</span>
<span class="p_add">+#define MAPS_LINE_LEN 128</span>
<span class="p_add">+</span>
 static void sethandler(int sig, void (*handler)(int, siginfo_t *, void *),
 		       int flags)
 {
<span class="p_chunk">@@ -98,7 +101,7 @@</span> <span class="p_context"> static int init_vsys(void)</span>
 #ifdef __x86_64__
 	int nerrs = 0;
 	FILE *maps;
<span class="p_del">-	char line[128];</span>
<span class="p_add">+	char line[MAPS_LINE_LEN];</span>
 	bool found = false;
 
 	maps = fopen(&quot;/proc/self/maps&quot;, &quot;r&quot;);
<span class="p_chunk">@@ -108,10 +111,12 @@</span> <span class="p_context"> static int init_vsys(void)</span>
 		return 0;
 	}
 
<span class="p_del">-	while (fgets(line, sizeof(line), maps)) {</span>
<span class="p_add">+	while (fgets(line, MAPS_LINE_LEN, maps)) {</span>
 		char r, x;
 		void *start, *end;
<span class="p_del">-		char name[128];</span>
<span class="p_add">+		char name[MAPS_LINE_LEN];</span>
<span class="p_add">+</span>
<span class="p_add">+		/* sscanf() is safe here as strlen(name) &gt;= strlen(line) */</span>
 		if (sscanf(line, &quot;%p-%p %c-%cp %*x %*x:%*x %*u %s&quot;,
 			   &amp;start, &amp;end, &amp;r, &amp;x, name) != 5)
 			continue;

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



