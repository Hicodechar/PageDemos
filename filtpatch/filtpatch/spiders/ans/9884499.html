
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[-mm] mm: Clear to access sub-page last when clearing huge page - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [-mm] mm: Clear to access sub-page last when clearing huge page</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=294">Huang Ying</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Aug. 7, 2017, 7:21 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20170807072131.8343-1-ying.huang@intel.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9884499/mbox/"
   >mbox</a>
|
   <a href="/patch/9884499/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9884499/">/patch/9884499/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	5BB1160363 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  7 Aug 2017 07:22:12 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 505DA285A8
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  7 Aug 2017 07:22:12 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 438FB285AD; Mon,  7 Aug 2017 07:22:12 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 5BB37285A8
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  7 Aug 2017 07:22:11 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752130AbdHGHWH (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 7 Aug 2017 03:22:07 -0400
Received: from mga01.intel.com ([192.55.52.88]:16464 &quot;EHLO mga01.intel.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1751377AbdHGHWG (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 7 Aug 2017 03:22:06 -0400
Received: from fmsmga003.fm.intel.com ([10.253.24.29])
	by fmsmga101.fm.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384;
	07 Aug 2017 00:22:05 -0700
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i=&quot;5.41,336,1498546800&quot;; d=&quot;scan&#39;208&quot;;a=&quot;887304111&quot;
Received: from gzhang22-mobl1.ccr.corp.intel.com (HELO
	yhuang-mobile.ccr.corp.intel.com) ([10.255.31.217])
	by FMSMGA003.fm.intel.com with ESMTP; 07 Aug 2017 00:21:47 -0700
From: &quot;Huang, Ying&quot; &lt;ying.huang@intel.com&gt;
To: Andrew Morton &lt;akpm@linux-foundation.org&gt;
Cc: linux-mm@kvack.org, linux-kernel@vger.kernel.org,
	Huang Ying &lt;ying.huang@intel.com&gt;,
	Andrea Arcangeli &lt;aarcange@redhat.com&gt;,
	&quot;Kirill A. Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt;,
	Nadia Yvette Chambers &lt;nyc@holomorphy.com&gt;,
	Michal Hocko &lt;mhocko@suse.com&gt;, Jan Kara &lt;jack@suse.cz&gt;,
	Matthew Wilcox &lt;willy@linux.intel.com&gt;, Hugh Dickins &lt;hughd@google.com&gt;,
	Minchan Kim &lt;minchan@kernel.org&gt;, Shaohua Li &lt;shli@fb.com&gt;
Subject: [PATCH -mm] mm: Clear to access sub-page last when clearing huge
	page
Date: Mon,  7 Aug 2017 15:21:31 +0800
Message-Id: &lt;20170807072131.8343-1-ying.huang@intel.com&gt;
X-Mailer: git-send-email 2.11.0
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=294">Huang Ying</a> - Aug. 7, 2017, 7:21 a.m.</div>
<pre class="content">
<span class="from">From: Huang Ying &lt;ying.huang@intel.com&gt;</span>

Huge page helps to reduce TLB miss rate, but it has higher cache
footprint, sometimes this may cause some issue.  For example, when
clearing huge page on x86_64 platform, the cache footprint is 2M.  But
on a Xeon E5 v3 2699 CPU, there are 18 cores, 36 threads, and only 45M
LLC (last level cache).  That is, in average, there are 2.5M LLC for
each core and 1.25M LLC for each thread.  If the cache pressure is
heavy when clearing the huge page, and we clear the huge page from the
begin to the end, it is possible that the begin of huge page is
evicted from the cache after we finishing clearing the end of the huge
page.  And it is possible for the application to access the begin of
the huge page after clearing the huge page.

To help the above situation, in this patch, when we clear a huge page,
the order to clear sub-pages is changed.  In quite some situation, we
can get the address that the application will access after we clear
the huge page, for example, in a page fault handler.  Instead of
clearing the huge page from begin to end, we will clear the sub-pages
farthest from the the sub-page to access firstly, and clear the
sub-page to access last.  This will make the sub-page to access most
cache-hot and sub-pages around it more cache-hot too.  If we cannot
know the address the application will access, the begin of the huge
page is assumed to be the the address the application will access.

With this patch, the throughput increases ~28.3% in vm-scalability
anon-w-seq test case with 72 processes on a 2 socket Xeon E5 v3 2699
system (36 cores, 72 threads).  The test case creates 72 processes,
each process mmap a big anonymous memory area and writes to it from
the begin to the end.  For each process, other processes could be seen
as other workload which generates heavy cache pressure.  At the same
time, the cache miss rate reduced from ~33.4% to ~31.7%, the
IPC (instruction per cycle) increased from 0.56 to 0.74, and the time
spent in user space is reduced ~7.9%

Thanks Andi Kleen to propose to use address to access to determine the
order of sub-pages to clear.

The hugetlbfs access address could be improved, will do that in
another patch.

[Use address to access information]
Suggested-by: Andi Kleen &lt;andi.kleen@intel.com&gt;
<span class="signed-off-by">Signed-off-by: &quot;Huang, Ying&quot; &lt;ying.huang@intel.com&gt;</span>
Cc: Andrea Arcangeli &lt;aarcange@redhat.com&gt;
Cc: &quot;Kirill A. Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt;
Cc: Nadia Yvette Chambers &lt;nyc@holomorphy.com&gt;
Cc: Michal Hocko &lt;mhocko@suse.com&gt;
Cc: Jan Kara &lt;jack@suse.cz&gt;
Cc: Matthew Wilcox &lt;willy@linux.intel.com&gt;
Cc: Hugh Dickins &lt;hughd@google.com&gt;
Cc: Minchan Kim &lt;minchan@kernel.org&gt;
Cc: Shaohua Li &lt;shli@fb.com&gt;
---
 fs/hugetlbfs/inode.c |  2 +-
 include/linux/mm.h   |  3 ++-
 mm/huge_memory.c     | 10 ++++++----
 mm/hugetlb.c         |  2 +-
 mm/memory.c          | 32 +++++++++++++++++++++++++++-----
 5 files changed, 37 insertions(+), 12 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=305">Jan Kara</a> - Aug. 7, 2017, 9:55 a.m.</div>
<pre class="content">
On Mon 07-08-17 15:21:31, Huang, Ying wrote:
<span class="quote">&gt; From: Huang Ying &lt;ying.huang@intel.com&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Huge page helps to reduce TLB miss rate, but it has higher cache</span>
<span class="quote">&gt; footprint, sometimes this may cause some issue.  For example, when</span>
<span class="quote">&gt; clearing huge page on x86_64 platform, the cache footprint is 2M.  But</span>
<span class="quote">&gt; on a Xeon E5 v3 2699 CPU, there are 18 cores, 36 threads, and only 45M</span>
<span class="quote">&gt; LLC (last level cache).  That is, in average, there are 2.5M LLC for</span>
<span class="quote">&gt; each core and 1.25M LLC for each thread.  If the cache pressure is</span>
<span class="quote">&gt; heavy when clearing the huge page, and we clear the huge page from the</span>
<span class="quote">&gt; begin to the end, it is possible that the begin of huge page is</span>
<span class="quote">&gt; evicted from the cache after we finishing clearing the end of the huge</span>
<span class="quote">&gt; page.  And it is possible for the application to access the begin of</span>
<span class="quote">&gt; the huge page after clearing the huge page.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; To help the above situation, in this patch, when we clear a huge page,</span>
<span class="quote">&gt; the order to clear sub-pages is changed.  In quite some situation, we</span>
<span class="quote">&gt; can get the address that the application will access after we clear</span>
<span class="quote">&gt; the huge page, for example, in a page fault handler.  Instead of</span>
<span class="quote">&gt; clearing the huge page from begin to end, we will clear the sub-pages</span>
<span class="quote">&gt; farthest from the the sub-page to access firstly, and clear the</span>
<span class="quote">&gt; sub-page to access last.  This will make the sub-page to access most</span>
<span class="quote">&gt; cache-hot and sub-pages around it more cache-hot too.  If we cannot</span>
<span class="quote">&gt; know the address the application will access, the begin of the huge</span>
<span class="quote">&gt; page is assumed to be the the address the application will access.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; With this patch, the throughput increases ~28.3% in vm-scalability</span>
<span class="quote">&gt; anon-w-seq test case with 72 processes on a 2 socket Xeon E5 v3 2699</span>
<span class="quote">&gt; system (36 cores, 72 threads).  The test case creates 72 processes,</span>
<span class="quote">&gt; each process mmap a big anonymous memory area and writes to it from</span>
<span class="quote">&gt; the begin to the end.  For each process, other processes could be seen</span>
<span class="quote">&gt; as other workload which generates heavy cache pressure.  At the same</span>
<span class="quote">&gt; time, the cache miss rate reduced from ~33.4% to ~31.7%, the</span>
<span class="quote">&gt; IPC (instruction per cycle) increased from 0.56 to 0.74, and the time</span>
<span class="quote">&gt; spent in user space is reduced ~7.9%</span>

Hum, the improvement looks impressive enough that it is probably worth the
bother. But please add at least a brief explanation why you do stuff in
this more complicated way to a comment in clear_huge_page() so that people
don&#39;t have to look it up in the changelog. Otherwise the patch looks good
to me so feel free to add:
<span class="acked-by">
Acked-by: Jan Kara &lt;jack@suse.cz&gt;</span>

								Honza
<span class="quote">
&gt; @@ -4374,9 +4374,31 @@ void clear_huge_page(struct page *page,</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	might_sleep();</span>
<span class="quote">&gt; -	for (i = 0; i &lt; pages_per_huge_page; i++) {</span>
<span class="quote">&gt; +	VM_BUG_ON(clamp(addr_hint, addr, addr +</span>
<span class="quote">&gt; +			(pages_per_huge_page &lt;&lt; PAGE_SHIFT)) != addr_hint);</span>
<span class="quote">&gt; +	n = (addr_hint - addr) / PAGE_SIZE;</span>
<span class="quote">&gt; +	if (2 * n &lt;= pages_per_huge_page) {</span>
<span class="quote">&gt; +		base = 0;</span>
<span class="quote">&gt; +		l = n;</span>
<span class="quote">&gt; +		for (i = pages_per_huge_page - 1; i &gt;= 2 * n; i--) {</span>
<span class="quote">&gt; +			cond_resched();</span>
<span class="quote">&gt; +			clear_user_highpage(page + i, addr + i * PAGE_SIZE);</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +	} else {</span>
<span class="quote">&gt; +		base = 2 * n - pages_per_huge_page;</span>
<span class="quote">&gt; +		l = pages_per_huge_page - n;</span>
<span class="quote">&gt; +		for (i = 0; i &lt; base; i++) {</span>
<span class="quote">&gt; +			cond_resched();</span>
<span class="quote">&gt; +			clear_user_highpage(page + i, addr + i * PAGE_SIZE);</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +	for (i = 0; i &lt; l; i++) {</span>
<span class="quote">&gt; +		cond_resched();</span>
<span class="quote">&gt; +		clear_user_highpage(page + base + i,</span>
<span class="quote">&gt; +				    addr + (base + i) * PAGE_SIZE);</span>
<span class="quote">&gt;  		cond_resched();</span>
<span class="quote">&gt; -		clear_user_highpage(page + i, addr + i * PAGE_SIZE);</span>
<span class="quote">&gt; +		clear_user_highpage(page + base + 2 * l - 1 - i,</span>
<span class="quote">&gt; +				    addr + (base + 2 * l - 1 - i) * PAGE_SIZE);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -- </span>
<span class="quote">&gt; 2.11.0</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=294">Huang Ying</a> - Aug. 7, 2017, 10 a.m.</div>
<pre class="content">
Jan Kara &lt;jack@suse.cz&gt; writes:
<span class="quote">
&gt; On Mon 07-08-17 15:21:31, Huang, Ying wrote:</span>
<span class="quote">&gt;&gt; From: Huang Ying &lt;ying.huang@intel.com&gt;</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; Huge page helps to reduce TLB miss rate, but it has higher cache</span>
<span class="quote">&gt;&gt; footprint, sometimes this may cause some issue.  For example, when</span>
<span class="quote">&gt;&gt; clearing huge page on x86_64 platform, the cache footprint is 2M.  But</span>
<span class="quote">&gt;&gt; on a Xeon E5 v3 2699 CPU, there are 18 cores, 36 threads, and only 45M</span>
<span class="quote">&gt;&gt; LLC (last level cache).  That is, in average, there are 2.5M LLC for</span>
<span class="quote">&gt;&gt; each core and 1.25M LLC for each thread.  If the cache pressure is</span>
<span class="quote">&gt;&gt; heavy when clearing the huge page, and we clear the huge page from the</span>
<span class="quote">&gt;&gt; begin to the end, it is possible that the begin of huge page is</span>
<span class="quote">&gt;&gt; evicted from the cache after we finishing clearing the end of the huge</span>
<span class="quote">&gt;&gt; page.  And it is possible for the application to access the begin of</span>
<span class="quote">&gt;&gt; the huge page after clearing the huge page.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; To help the above situation, in this patch, when we clear a huge page,</span>
<span class="quote">&gt;&gt; the order to clear sub-pages is changed.  In quite some situation, we</span>
<span class="quote">&gt;&gt; can get the address that the application will access after we clear</span>
<span class="quote">&gt;&gt; the huge page, for example, in a page fault handler.  Instead of</span>
<span class="quote">&gt;&gt; clearing the huge page from begin to end, we will clear the sub-pages</span>
<span class="quote">&gt;&gt; farthest from the the sub-page to access firstly, and clear the</span>
<span class="quote">&gt;&gt; sub-page to access last.  This will make the sub-page to access most</span>
<span class="quote">&gt;&gt; cache-hot and sub-pages around it more cache-hot too.  If we cannot</span>
<span class="quote">&gt;&gt; know the address the application will access, the begin of the huge</span>
<span class="quote">&gt;&gt; page is assumed to be the the address the application will access.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; With this patch, the throughput increases ~28.3% in vm-scalability</span>
<span class="quote">&gt;&gt; anon-w-seq test case with 72 processes on a 2 socket Xeon E5 v3 2699</span>
<span class="quote">&gt;&gt; system (36 cores, 72 threads).  The test case creates 72 processes,</span>
<span class="quote">&gt;&gt; each process mmap a big anonymous memory area and writes to it from</span>
<span class="quote">&gt;&gt; the begin to the end.  For each process, other processes could be seen</span>
<span class="quote">&gt;&gt; as other workload which generates heavy cache pressure.  At the same</span>
<span class="quote">&gt;&gt; time, the cache miss rate reduced from ~33.4% to ~31.7%, the</span>
<span class="quote">&gt;&gt; IPC (instruction per cycle) increased from 0.56 to 0.74, and the time</span>
<span class="quote">&gt;&gt; spent in user space is reduced ~7.9%</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Hum, the improvement looks impressive enough that it is probably worth the</span>
<span class="quote">&gt; bother. But please add at least a brief explanation why you do stuff in</span>
<span class="quote">&gt; this more complicated way to a comment in clear_huge_page() so that people</span>
<span class="quote">&gt; don&#39;t have to look it up in the changelog.</span>

Good suggestion!  I will do that in the next version.
<span class="quote">
&gt; Otherwise the patch looks good</span>
<span class="quote">&gt; to me so feel free to add:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Acked-by: Jan Kara &lt;jack@suse.cz&gt;</span>

Thanks!

Best Regards,
Huang, Ying
<span class="quote">
&gt; 								Honza</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; @@ -4374,9 +4374,31 @@ void clear_huge_page(struct page *page,</span>
<span class="quote">&gt;&gt;  	}</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  	might_sleep();</span>
<span class="quote">&gt;&gt; -	for (i = 0; i &lt; pages_per_huge_page; i++) {</span>
<span class="quote">&gt;&gt; +	VM_BUG_ON(clamp(addr_hint, addr, addr +</span>
<span class="quote">&gt;&gt; +			(pages_per_huge_page &lt;&lt; PAGE_SHIFT)) != addr_hint);</span>
<span class="quote">&gt;&gt; +	n = (addr_hint - addr) / PAGE_SIZE;</span>
<span class="quote">&gt;&gt; +	if (2 * n &lt;= pages_per_huge_page) {</span>
<span class="quote">&gt;&gt; +		base = 0;</span>
<span class="quote">&gt;&gt; +		l = n;</span>
<span class="quote">&gt;&gt; +		for (i = pages_per_huge_page - 1; i &gt;= 2 * n; i--) {</span>
<span class="quote">&gt;&gt; +			cond_resched();</span>
<span class="quote">&gt;&gt; +			clear_user_highpage(page + i, addr + i * PAGE_SIZE);</span>
<span class="quote">&gt;&gt; +		}</span>
<span class="quote">&gt;&gt; +	} else {</span>
<span class="quote">&gt;&gt; +		base = 2 * n - pages_per_huge_page;</span>
<span class="quote">&gt;&gt; +		l = pages_per_huge_page - n;</span>
<span class="quote">&gt;&gt; +		for (i = 0; i &lt; base; i++) {</span>
<span class="quote">&gt;&gt; +			cond_resched();</span>
<span class="quote">&gt;&gt; +			clear_user_highpage(page + i, addr + i * PAGE_SIZE);</span>
<span class="quote">&gt;&gt; +		}</span>
<span class="quote">&gt;&gt; +	}</span>
<span class="quote">&gt;&gt; +	for (i = 0; i &lt; l; i++) {</span>
<span class="quote">&gt;&gt; +		cond_resched();</span>
<span class="quote">&gt;&gt; +		clear_user_highpage(page + base + i,</span>
<span class="quote">&gt;&gt; +				    addr + (base + i) * PAGE_SIZE);</span>
<span class="quote">&gt;&gt;  		cond_resched();</span>
<span class="quote">&gt;&gt; -		clear_user_highpage(page + i, addr + i * PAGE_SIZE);</span>
<span class="quote">&gt;&gt; +		clear_user_highpage(page + base + 2 * l - 1 - i,</span>
<span class="quote">&gt;&gt; +				    addr + (base + 2 * l - 1 - i) * PAGE_SIZE);</span>
<span class="quote">&gt;&gt;  	}</span>
<span class="quote">&gt;&gt;  }</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; -- </span>
<span class="quote">&gt;&gt; 2.11.0</span>
<span class="quote">&gt;&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=874">Kirill A. Shutemov</a> - Aug. 7, 2017, 10:16 a.m.</div>
<pre class="content">
On Mon, Aug 07, 2017 at 03:21:31PM +0800, Huang, Ying wrote:
<span class="quote">&gt; From: Huang Ying &lt;ying.huang@intel.com&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Huge page helps to reduce TLB miss rate, but it has higher cache</span>
<span class="quote">&gt; footprint, sometimes this may cause some issue.  For example, when</span>
<span class="quote">&gt; clearing huge page on x86_64 platform, the cache footprint is 2M.  But</span>
<span class="quote">&gt; on a Xeon E5 v3 2699 CPU, there are 18 cores, 36 threads, and only 45M</span>
<span class="quote">&gt; LLC (last level cache).  That is, in average, there are 2.5M LLC for</span>
<span class="quote">&gt; each core and 1.25M LLC for each thread.  If the cache pressure is</span>
<span class="quote">&gt; heavy when clearing the huge page, and we clear the huge page from the</span>
<span class="quote">&gt; begin to the end, it is possible that the begin of huge page is</span>
<span class="quote">&gt; evicted from the cache after we finishing clearing the end of the huge</span>
<span class="quote">&gt; page.  And it is possible for the application to access the begin of</span>
<span class="quote">&gt; the huge page after clearing the huge page.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; To help the above situation, in this patch, when we clear a huge page,</span>
<span class="quote">&gt; the order to clear sub-pages is changed.  In quite some situation, we</span>
<span class="quote">&gt; can get the address that the application will access after we clear</span>
<span class="quote">&gt; the huge page, for example, in a page fault handler.  Instead of</span>
<span class="quote">&gt; clearing the huge page from begin to end, we will clear the sub-pages</span>
<span class="quote">&gt; farthest from the the sub-page to access firstly, and clear the</span>
<span class="quote">&gt; sub-page to access last.  This will make the sub-page to access most</span>
<span class="quote">&gt; cache-hot and sub-pages around it more cache-hot too.  If we cannot</span>
<span class="quote">&gt; know the address the application will access, the begin of the huge</span>
<span class="quote">&gt; page is assumed to be the the address the application will access.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; With this patch, the throughput increases ~28.3% in vm-scalability</span>
<span class="quote">&gt; anon-w-seq test case with 72 processes on a 2 socket Xeon E5 v3 2699</span>
<span class="quote">&gt; system (36 cores, 72 threads).  The test case creates 72 processes,</span>
<span class="quote">&gt; each process mmap a big anonymous memory area and writes to it from</span>
<span class="quote">&gt; the begin to the end.  For each process, other processes could be seen</span>
<span class="quote">&gt; as other workload which generates heavy cache pressure.  At the same</span>
<span class="quote">&gt; time, the cache miss rate reduced from ~33.4% to ~31.7%, the</span>
<span class="quote">&gt; IPC (instruction per cycle) increased from 0.56 to 0.74, and the time</span>
<span class="quote">&gt; spent in user space is reduced ~7.9%</span>

That&#39;s impressive.

But what about the case when we are not bounded that much by the size of
LLC? What about running the same test on the same hardware, but with 4
processes instead of 72.

I just want to make sure we don&#39;t regress on more realistic tast case.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=1731">Christoph Lameter</a> - Aug. 7, 2017, 6:46 p.m.</div>
<pre class="content">
On Mon, 7 Aug 2017, Huang, Ying wrote:
<span class="quote">
&gt; --- a/mm/memory.c</span>
<span class="quote">&gt; +++ b/mm/memory.c</span>
<span class="quote">&gt; @@ -4374,9 +4374,31 @@ void clear_huge_page(struct page *page,</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  	might_sleep();</span>
<span class="quote">&gt; -	for (i = 0; i &lt; pages_per_huge_page; i++) {</span>
<span class="quote">&gt; +	VM_BUG_ON(clamp(addr_hint, addr, addr +</span>
<span class="quote">&gt; +			(pages_per_huge_page &lt;&lt; PAGE_SHIFT)) != addr_hint);</span>
<span class="quote">&gt; +	n = (addr_hint - addr) / PAGE_SIZE;</span>
<span class="quote">&gt; +	if (2 * n &lt;= pages_per_huge_page) {</span>
<span class="quote">&gt; +		base = 0;</span>
<span class="quote">&gt; +		l = n;</span>
<span class="quote">&gt; +		for (i = pages_per_huge_page - 1; i &gt;= 2 * n; i--) {</span>
<span class="quote">&gt; +			cond_resched();</span>
<span class="quote">&gt; +			clear_user_highpage(page + i, addr + i * PAGE_SIZE);</span>
<span class="quote">&gt; +		}</span>

I really like the idea behind the patch but this is not clearing from last
to first byte of the huge page.

What seems to be happening here is clearing from the last page to the
first page and I would think that within each page the clearing is from
first byte to last byte. Maybe more gains can be had by really clearing
from last to first byte of the huge page instead of this jumping over 4k
addresses?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=294">Huang Ying</a> - Aug. 7, 2017, 10:51 p.m.</div>
<pre class="content">
&quot;Kirill A. Shutemov&quot; &lt;kirill@shutemov.name&gt; writes:
<span class="quote">
&gt; On Mon, Aug 07, 2017 at 03:21:31PM +0800, Huang, Ying wrote:</span>
<span class="quote">&gt;&gt; From: Huang Ying &lt;ying.huang@intel.com&gt;</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; Huge page helps to reduce TLB miss rate, but it has higher cache</span>
<span class="quote">&gt;&gt; footprint, sometimes this may cause some issue.  For example, when</span>
<span class="quote">&gt;&gt; clearing huge page on x86_64 platform, the cache footprint is 2M.  But</span>
<span class="quote">&gt;&gt; on a Xeon E5 v3 2699 CPU, there are 18 cores, 36 threads, and only 45M</span>
<span class="quote">&gt;&gt; LLC (last level cache).  That is, in average, there are 2.5M LLC for</span>
<span class="quote">&gt;&gt; each core and 1.25M LLC for each thread.  If the cache pressure is</span>
<span class="quote">&gt;&gt; heavy when clearing the huge page, and we clear the huge page from the</span>
<span class="quote">&gt;&gt; begin to the end, it is possible that the begin of huge page is</span>
<span class="quote">&gt;&gt; evicted from the cache after we finishing clearing the end of the huge</span>
<span class="quote">&gt;&gt; page.  And it is possible for the application to access the begin of</span>
<span class="quote">&gt;&gt; the huge page after clearing the huge page.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; To help the above situation, in this patch, when we clear a huge page,</span>
<span class="quote">&gt;&gt; the order to clear sub-pages is changed.  In quite some situation, we</span>
<span class="quote">&gt;&gt; can get the address that the application will access after we clear</span>
<span class="quote">&gt;&gt; the huge page, for example, in a page fault handler.  Instead of</span>
<span class="quote">&gt;&gt; clearing the huge page from begin to end, we will clear the sub-pages</span>
<span class="quote">&gt;&gt; farthest from the the sub-page to access firstly, and clear the</span>
<span class="quote">&gt;&gt; sub-page to access last.  This will make the sub-page to access most</span>
<span class="quote">&gt;&gt; cache-hot and sub-pages around it more cache-hot too.  If we cannot</span>
<span class="quote">&gt;&gt; know the address the application will access, the begin of the huge</span>
<span class="quote">&gt;&gt; page is assumed to be the the address the application will access.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; With this patch, the throughput increases ~28.3% in vm-scalability</span>
<span class="quote">&gt;&gt; anon-w-seq test case with 72 processes on a 2 socket Xeon E5 v3 2699</span>
<span class="quote">&gt;&gt; system (36 cores, 72 threads).  The test case creates 72 processes,</span>
<span class="quote">&gt;&gt; each process mmap a big anonymous memory area and writes to it from</span>
<span class="quote">&gt;&gt; the begin to the end.  For each process, other processes could be seen</span>
<span class="quote">&gt;&gt; as other workload which generates heavy cache pressure.  At the same</span>
<span class="quote">&gt;&gt; time, the cache miss rate reduced from ~33.4% to ~31.7%, the</span>
<span class="quote">&gt;&gt; IPC (instruction per cycle) increased from 0.56 to 0.74, and the time</span>
<span class="quote">&gt;&gt; spent in user space is reduced ~7.9%</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; That&#39;s impressive.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; But what about the case when we are not bounded that much by the size of</span>
<span class="quote">&gt; LLC? What about running the same test on the same hardware, but with 4</span>
<span class="quote">&gt; processes instead of 72.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I just want to make sure we don&#39;t regress on more realistic tast case.</span>

Sure.  I will test it.

Best Regards,
Huang, Ying
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=294">Huang Ying</a> - Aug. 7, 2017, 11:05 p.m.</div>
<pre class="content">
Christopher Lameter &lt;cl@linux.com&gt; writes:
<span class="quote">
&gt; On Mon, 7 Aug 2017, Huang, Ying wrote:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; --- a/mm/memory.c</span>
<span class="quote">&gt;&gt; +++ b/mm/memory.c</span>
<span class="quote">&gt;&gt; @@ -4374,9 +4374,31 @@ void clear_huge_page(struct page *page,</span>
<span class="quote">&gt;&gt;  	}</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;  	might_sleep();</span>
<span class="quote">&gt;&gt; -	for (i = 0; i &lt; pages_per_huge_page; i++) {</span>
<span class="quote">&gt;&gt; +	VM_BUG_ON(clamp(addr_hint, addr, addr +</span>
<span class="quote">&gt;&gt; +			(pages_per_huge_page &lt;&lt; PAGE_SHIFT)) != addr_hint);</span>
<span class="quote">&gt;&gt; +	n = (addr_hint - addr) / PAGE_SIZE;</span>
<span class="quote">&gt;&gt; +	if (2 * n &lt;= pages_per_huge_page) {</span>
<span class="quote">&gt;&gt; +		base = 0;</span>
<span class="quote">&gt;&gt; +		l = n;</span>
<span class="quote">&gt;&gt; +		for (i = pages_per_huge_page - 1; i &gt;= 2 * n; i--) {</span>
<span class="quote">&gt;&gt; +			cond_resched();</span>
<span class="quote">&gt;&gt; +			clear_user_highpage(page + i, addr + i * PAGE_SIZE);</span>
<span class="quote">&gt;&gt; +		}</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I really like the idea behind the patch but this is not clearing from last</span>
<span class="quote">&gt; to first byte of the huge page.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; What seems to be happening here is clearing from the last page to the</span>
<span class="quote">&gt; first page and I would think that within each page the clearing is from</span>
<span class="quote">&gt; first byte to last byte. Maybe more gains can be had by really clearing</span>
<span class="quote">&gt; from last to first byte of the huge page instead of this jumping over 4k</span>
<span class="quote">&gt; addresses?</span>

Yes.  That is a good idea.  I will experiment it via changing the
direction to clear in clear_user_highpage().

Best Regards,
Huang, Ying
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=124511">Mike Kravetz</a> - Aug. 8, 2017, 4:07 a.m.</div>
<pre class="content">
On 08/07/2017 12:21 AM, Huang, Ying wrote:
<span class="quote">&gt; From: Huang Ying &lt;ying.huang@intel.com&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Huge page helps to reduce TLB miss rate, but it has higher cache</span>
<span class="quote">&gt; footprint, sometimes this may cause some issue.  For example, when</span>
<span class="quote">&gt; clearing huge page on x86_64 platform, the cache footprint is 2M.  But</span>
<span class="quote">&gt; on a Xeon E5 v3 2699 CPU, there are 18 cores, 36 threads, and only 45M</span>
<span class="quote">&gt; LLC (last level cache).  That is, in average, there are 2.5M LLC for</span>
<span class="quote">&gt; each core and 1.25M LLC for each thread.  If the cache pressure is</span>
<span class="quote">&gt; heavy when clearing the huge page, and we clear the huge page from the</span>
<span class="quote">&gt; begin to the end, it is possible that the begin of huge page is</span>
<span class="quote">&gt; evicted from the cache after we finishing clearing the end of the huge</span>
<span class="quote">&gt; page.  And it is possible for the application to access the begin of</span>
<span class="quote">&gt; the huge page after clearing the huge page.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; To help the above situation, in this patch, when we clear a huge page,</span>
<span class="quote">&gt; the order to clear sub-pages is changed.  In quite some situation, we</span>
<span class="quote">&gt; can get the address that the application will access after we clear</span>
<span class="quote">&gt; the huge page, for example, in a page fault handler.  Instead of</span>
<span class="quote">&gt; clearing the huge page from begin to end, we will clear the sub-pages</span>
<span class="quote">&gt; farthest from the the sub-page to access firstly, and clear the</span>
<span class="quote">&gt; sub-page to access last.  This will make the sub-page to access most</span>
<span class="quote">&gt; cache-hot and sub-pages around it more cache-hot too.  If we cannot</span>
<span class="quote">&gt; know the address the application will access, the begin of the huge</span>
<span class="quote">&gt; page is assumed to be the the address the application will access.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; With this patch, the throughput increases ~28.3% in vm-scalability</span>
<span class="quote">&gt; anon-w-seq test case with 72 processes on a 2 socket Xeon E5 v3 2699</span>
<span class="quote">&gt; system (36 cores, 72 threads).  The test case creates 72 processes,</span>
<span class="quote">&gt; each process mmap a big anonymous memory area and writes to it from</span>
<span class="quote">&gt; the begin to the end.  For each process, other processes could be seen</span>
<span class="quote">&gt; as other workload which generates heavy cache pressure.  At the same</span>
<span class="quote">&gt; time, the cache miss rate reduced from ~33.4% to ~31.7%, the</span>
<span class="quote">&gt; IPC (instruction per cycle) increased from 0.56 to 0.74, and the time</span>
<span class="quote">&gt; spent in user space is reduced ~7.9%</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Thanks Andi Kleen to propose to use address to access to determine the</span>
<span class="quote">&gt; order of sub-pages to clear.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The hugetlbfs access address could be improved, will do that in</span>
<span class="quote">&gt; another patch.</span>

hugetlb_fault masks off the actual faulting address with,
        address &amp;= huge_page_mask(h);
before calling hugetlb_no_page.

But, we could pass down the actual (unmasked) address to take advantage
of this optimization for hugetlb faults as well.  hugetlb_fault is the
only caller of hugetlb_no_page, so this should be pretty straight forward.

Were you thinking of additional improvements?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=294">Huang Ying</a> - Aug. 8, 2017, 4:24 a.m.</div>
<pre class="content">
Mike Kravetz &lt;mike.kravetz@oracle.com&gt; writes:
<span class="quote">
&gt; On 08/07/2017 12:21 AM, Huang, Ying wrote:</span>
<span class="quote">&gt;&gt; From: Huang Ying &lt;ying.huang@intel.com&gt;</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; Huge page helps to reduce TLB miss rate, but it has higher cache</span>
<span class="quote">&gt;&gt; footprint, sometimes this may cause some issue.  For example, when</span>
<span class="quote">&gt;&gt; clearing huge page on x86_64 platform, the cache footprint is 2M.  But</span>
<span class="quote">&gt;&gt; on a Xeon E5 v3 2699 CPU, there are 18 cores, 36 threads, and only 45M</span>
<span class="quote">&gt;&gt; LLC (last level cache).  That is, in average, there are 2.5M LLC for</span>
<span class="quote">&gt;&gt; each core and 1.25M LLC for each thread.  If the cache pressure is</span>
<span class="quote">&gt;&gt; heavy when clearing the huge page, and we clear the huge page from the</span>
<span class="quote">&gt;&gt; begin to the end, it is possible that the begin of huge page is</span>
<span class="quote">&gt;&gt; evicted from the cache after we finishing clearing the end of the huge</span>
<span class="quote">&gt;&gt; page.  And it is possible for the application to access the begin of</span>
<span class="quote">&gt;&gt; the huge page after clearing the huge page.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; To help the above situation, in this patch, when we clear a huge page,</span>
<span class="quote">&gt;&gt; the order to clear sub-pages is changed.  In quite some situation, we</span>
<span class="quote">&gt;&gt; can get the address that the application will access after we clear</span>
<span class="quote">&gt;&gt; the huge page, for example, in a page fault handler.  Instead of</span>
<span class="quote">&gt;&gt; clearing the huge page from begin to end, we will clear the sub-pages</span>
<span class="quote">&gt;&gt; farthest from the the sub-page to access firstly, and clear the</span>
<span class="quote">&gt;&gt; sub-page to access last.  This will make the sub-page to access most</span>
<span class="quote">&gt;&gt; cache-hot and sub-pages around it more cache-hot too.  If we cannot</span>
<span class="quote">&gt;&gt; know the address the application will access, the begin of the huge</span>
<span class="quote">&gt;&gt; page is assumed to be the the address the application will access.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; With this patch, the throughput increases ~28.3% in vm-scalability</span>
<span class="quote">&gt;&gt; anon-w-seq test case with 72 processes on a 2 socket Xeon E5 v3 2699</span>
<span class="quote">&gt;&gt; system (36 cores, 72 threads).  The test case creates 72 processes,</span>
<span class="quote">&gt;&gt; each process mmap a big anonymous memory area and writes to it from</span>
<span class="quote">&gt;&gt; the begin to the end.  For each process, other processes could be seen</span>
<span class="quote">&gt;&gt; as other workload which generates heavy cache pressure.  At the same</span>
<span class="quote">&gt;&gt; time, the cache miss rate reduced from ~33.4% to ~31.7%, the</span>
<span class="quote">&gt;&gt; IPC (instruction per cycle) increased from 0.56 to 0.74, and the time</span>
<span class="quote">&gt;&gt; spent in user space is reduced ~7.9%</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; Thanks Andi Kleen to propose to use address to access to determine the</span>
<span class="quote">&gt;&gt; order of sub-pages to clear.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; The hugetlbfs access address could be improved, will do that in</span>
<span class="quote">&gt;&gt; another patch.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; hugetlb_fault masks off the actual faulting address with,</span>
<span class="quote">&gt;         address &amp;= huge_page_mask(h);</span>
<span class="quote">&gt; before calling hugetlb_no_page.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; But, we could pass down the actual (unmasked) address to take advantage</span>
<span class="quote">&gt; of this optimization for hugetlb faults as well.  hugetlb_fault is the</span>
<span class="quote">&gt; only caller of hugetlb_no_page, so this should be pretty straight forward.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Were you thinking of additional improvements?</span>

No.  I am thinking of something like this.  If the basic idea is
accepted, I plan to add better support like this for hugetlbfs in
another patch.

Best Regards,
Huang, Ying
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=294">Huang Ying</a> - Aug. 8, 2017, 7:40 a.m.</div>
<pre class="content">
&quot;Huang, Ying&quot; &lt;ying.huang@intel.com&gt; writes:
<span class="quote">
&gt; &quot;Kirill A. Shutemov&quot; &lt;kirill@shutemov.name&gt; writes:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; On Mon, Aug 07, 2017 at 03:21:31PM +0800, Huang, Ying wrote:</span>
<span class="quote">&gt;&gt;&gt; From: Huang Ying &lt;ying.huang@intel.com&gt;</span>
<span class="quote">&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt; Huge page helps to reduce TLB miss rate, but it has higher cache</span>
<span class="quote">&gt;&gt;&gt; footprint, sometimes this may cause some issue.  For example, when</span>
<span class="quote">&gt;&gt;&gt; clearing huge page on x86_64 platform, the cache footprint is 2M.  But</span>
<span class="quote">&gt;&gt;&gt; on a Xeon E5 v3 2699 CPU, there are 18 cores, 36 threads, and only 45M</span>
<span class="quote">&gt;&gt;&gt; LLC (last level cache).  That is, in average, there are 2.5M LLC for</span>
<span class="quote">&gt;&gt;&gt; each core and 1.25M LLC for each thread.  If the cache pressure is</span>
<span class="quote">&gt;&gt;&gt; heavy when clearing the huge page, and we clear the huge page from the</span>
<span class="quote">&gt;&gt;&gt; begin to the end, it is possible that the begin of huge page is</span>
<span class="quote">&gt;&gt;&gt; evicted from the cache after we finishing clearing the end of the huge</span>
<span class="quote">&gt;&gt;&gt; page.  And it is possible for the application to access the begin of</span>
<span class="quote">&gt;&gt;&gt; the huge page after clearing the huge page.</span>
<span class="quote">&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt; To help the above situation, in this patch, when we clear a huge page,</span>
<span class="quote">&gt;&gt;&gt; the order to clear sub-pages is changed.  In quite some situation, we</span>
<span class="quote">&gt;&gt;&gt; can get the address that the application will access after we clear</span>
<span class="quote">&gt;&gt;&gt; the huge page, for example, in a page fault handler.  Instead of</span>
<span class="quote">&gt;&gt;&gt; clearing the huge page from begin to end, we will clear the sub-pages</span>
<span class="quote">&gt;&gt;&gt; farthest from the the sub-page to access firstly, and clear the</span>
<span class="quote">&gt;&gt;&gt; sub-page to access last.  This will make the sub-page to access most</span>
<span class="quote">&gt;&gt;&gt; cache-hot and sub-pages around it more cache-hot too.  If we cannot</span>
<span class="quote">&gt;&gt;&gt; know the address the application will access, the begin of the huge</span>
<span class="quote">&gt;&gt;&gt; page is assumed to be the the address the application will access.</span>
<span class="quote">&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt; With this patch, the throughput increases ~28.3% in vm-scalability</span>
<span class="quote">&gt;&gt;&gt; anon-w-seq test case with 72 processes on a 2 socket Xeon E5 v3 2699</span>
<span class="quote">&gt;&gt;&gt; system (36 cores, 72 threads).  The test case creates 72 processes,</span>
<span class="quote">&gt;&gt;&gt; each process mmap a big anonymous memory area and writes to it from</span>
<span class="quote">&gt;&gt;&gt; the begin to the end.  For each process, other processes could be seen</span>
<span class="quote">&gt;&gt;&gt; as other workload which generates heavy cache pressure.  At the same</span>
<span class="quote">&gt;&gt;&gt; time, the cache miss rate reduced from ~33.4% to ~31.7%, the</span>
<span class="quote">&gt;&gt;&gt; IPC (instruction per cycle) increased from 0.56 to 0.74, and the time</span>
<span class="quote">&gt;&gt;&gt; spent in user space is reduced ~7.9%</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; That&#39;s impressive.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; But what about the case when we are not bounded that much by the size of</span>
<span class="quote">&gt;&gt; LLC? What about running the same test on the same hardware, but with 4</span>
<span class="quote">&gt;&gt; processes instead of 72.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; I just want to make sure we don&#39;t regress on more realistic tast case.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Sure.  I will test it.</span>

Tested with 4 processes, there is no visible changes for benchmark result.

Best Regards,
Huang, Ying
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=172187">willy@infradead.org</a> - Aug. 8, 2017, 12:12 p.m.</div>
<pre class="content">
On Mon, Aug 07, 2017 at 03:21:31PM +0800, Huang, Ying wrote:
<span class="quote">&gt; @@ -2509,7 +2509,8 @@ enum mf_action_page_type {</span>
<span class="quote">&gt;  #if defined(CONFIG_TRANSPARENT_HUGEPAGE) || defined(CONFIG_HUGETLBFS)</span>
<span class="quote">&gt;  extern void clear_huge_page(struct page *page,</span>
<span class="quote">&gt;  			    unsigned long addr,</span>
<span class="quote">&gt; -			    unsigned int pages_per_huge_page);</span>
<span class="quote">&gt; +			    unsigned int pages_per_huge_page,</span>
<span class="quote">&gt; +			    unsigned long addr_hint);</span>

I don&#39;t really like adding the extra argument to this function ...
<span class="quote">
&gt; +++ b/mm/huge_memory.c</span>
<span class="quote">&gt; @@ -549,7 +549,8 @@ static int __do_huge_pmd_anonymous_page(struct vm_fault *vmf, struct page *page,</span>
<span class="quote">&gt;  	struct vm_area_struct *vma = vmf-&gt;vma;</span>
<span class="quote">&gt;  	struct mem_cgroup *memcg;</span>
<span class="quote">&gt;  	pgtable_t pgtable;</span>
<span class="quote">&gt; -	unsigned long haddr = vmf-&gt;address &amp; HPAGE_PMD_MASK;</span>
<span class="quote">&gt; +	unsigned long address = vmf-&gt;address;</span>
<span class="quote">&gt; +	unsigned long haddr = address &amp; HPAGE_PMD_MASK;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	VM_BUG_ON_PAGE(!PageCompound(page), page);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -566,7 +567,7 @@ static int __do_huge_pmd_anonymous_page(struct vm_fault *vmf, struct page *page,</span>
<span class="quote">&gt;  		return VM_FAULT_OOM;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	clear_huge_page(page, haddr, HPAGE_PMD_NR);</span>
<span class="quote">&gt; +	clear_huge_page(page, haddr, HPAGE_PMD_NR, address);</span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt;  	 * The memory barrier inside __SetPageUptodate makes sure that</span>
<span class="quote">&gt;  	 * clear_huge_page writes become visible before the set_pmd_at()</span>

How about calling:

-	clear_huge_page(page, haddr, HPAGE_PMD_NR);
+	clear_huge_page(page, address, HPAGE_PMD_NR);
<span class="quote">
&gt; +++ b/mm/memory.c</span>
<span class="quote">&gt; @@ -4363,10 +4363,10 @@ static void clear_gigantic_page(struct page *page,</span>
<span class="quote">&gt;  		clear_user_highpage(p, addr + i * PAGE_SIZE);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; -void clear_huge_page(struct page *page,</span>
<span class="quote">&gt; -		     unsigned long addr, unsigned int pages_per_huge_page)</span>
<span class="quote">&gt; +void clear_huge_page(struct page *page, unsigned long addr,</span>
<span class="quote">&gt; +		     unsigned int pages_per_huge_page, unsigned long addr_hint)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	int i;</span>
<span class="quote">&gt; +	int i, n, base, l;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (unlikely(pages_per_huge_page &gt; MAX_ORDER_NR_PAGES)) {</span>
<span class="quote">&gt;  		clear_gigantic_page(page, addr, pages_per_huge_page);</span>

... and doing this:

 void clear_huge_page(struct page *page,
-		     unsigned long addr, unsigned int pages_per_huge_page)
+		     unsigned long addr_hint, unsigned int pages_per_huge_page)
 {
-	int i;
+	int i, n, base, l;
+	unsigned long addr = addr_hint &amp;
+				(1UL &lt;&lt; (pages_per_huge_page + PAGE_SHIFT));
<span class="quote">
&gt; @@ -4374,9 +4374,31 @@ void clear_huge_page(struct page *page,</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	might_sleep();</span>
<span class="quote">&gt; -	for (i = 0; i &lt; pages_per_huge_page; i++) {</span>
<span class="quote">&gt; +	VM_BUG_ON(clamp(addr_hint, addr, addr +</span>
<span class="quote">&gt; +			(pages_per_huge_page &lt;&lt; PAGE_SHIFT)) != addr_hint);</span>

... then you can ditch this check
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=294">Huang Ying</a> - Aug. 8, 2017, 11:06 p.m.</div>
<pre class="content">
Matthew Wilcox &lt;willy@infradead.org&gt; writes:
<span class="quote">
&gt; On Mon, Aug 07, 2017 at 03:21:31PM +0800, Huang, Ying wrote:</span>
<span class="quote">&gt;&gt; @@ -2509,7 +2509,8 @@ enum mf_action_page_type {</span>
<span class="quote">&gt;&gt;  #if defined(CONFIG_TRANSPARENT_HUGEPAGE) || defined(CONFIG_HUGETLBFS)</span>
<span class="quote">&gt;&gt;  extern void clear_huge_page(struct page *page,</span>
<span class="quote">&gt;&gt;  			    unsigned long addr,</span>
<span class="quote">&gt;&gt; -			    unsigned int pages_per_huge_page);</span>
<span class="quote">&gt;&gt; +			    unsigned int pages_per_huge_page,</span>
<span class="quote">&gt;&gt; +			    unsigned long addr_hint);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I don&#39;t really like adding the extra argument to this function ...</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; +++ b/mm/huge_memory.c</span>
<span class="quote">&gt;&gt; @@ -549,7 +549,8 @@ static int __do_huge_pmd_anonymous_page(struct vm_fault *vmf, struct page *page,</span>
<span class="quote">&gt;&gt;  	struct vm_area_struct *vma = vmf-&gt;vma;</span>
<span class="quote">&gt;&gt;  	struct mem_cgroup *memcg;</span>
<span class="quote">&gt;&gt;  	pgtable_t pgtable;</span>
<span class="quote">&gt;&gt; -	unsigned long haddr = vmf-&gt;address &amp; HPAGE_PMD_MASK;</span>
<span class="quote">&gt;&gt; +	unsigned long address = vmf-&gt;address;</span>
<span class="quote">&gt;&gt; +	unsigned long haddr = address &amp; HPAGE_PMD_MASK;</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  	VM_BUG_ON_PAGE(!PageCompound(page), page);</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; @@ -566,7 +567,7 @@ static int __do_huge_pmd_anonymous_page(struct vm_fault *vmf, struct page *page,</span>
<span class="quote">&gt;&gt;  		return VM_FAULT_OOM;</span>
<span class="quote">&gt;&gt;  	}</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; -	clear_huge_page(page, haddr, HPAGE_PMD_NR);</span>
<span class="quote">&gt;&gt; +	clear_huge_page(page, haddr, HPAGE_PMD_NR, address);</span>
<span class="quote">&gt;&gt;  	/*</span>
<span class="quote">&gt;&gt;  	 * The memory barrier inside __SetPageUptodate makes sure that</span>
<span class="quote">&gt;&gt;  	 * clear_huge_page writes become visible before the set_pmd_at()</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; How about calling:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -	clear_huge_page(page, haddr, HPAGE_PMD_NR);</span>
<span class="quote">&gt; +	clear_huge_page(page, address, HPAGE_PMD_NR);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; +++ b/mm/memory.c</span>
<span class="quote">&gt;&gt; @@ -4363,10 +4363,10 @@ static void clear_gigantic_page(struct page *page,</span>
<span class="quote">&gt;&gt;  		clear_user_highpage(p, addr + i * PAGE_SIZE);</span>
<span class="quote">&gt;&gt;  	}</span>
<span class="quote">&gt;&gt;  }</span>
<span class="quote">&gt;&gt; -void clear_huge_page(struct page *page,</span>
<span class="quote">&gt;&gt; -		     unsigned long addr, unsigned int pages_per_huge_page)</span>
<span class="quote">&gt;&gt; +void clear_huge_page(struct page *page, unsigned long addr,</span>
<span class="quote">&gt;&gt; +		     unsigned int pages_per_huge_page, unsigned long addr_hint)</span>
<span class="quote">&gt;&gt;  {</span>
<span class="quote">&gt;&gt; -	int i;</span>
<span class="quote">&gt;&gt; +	int i, n, base, l;</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  	if (unlikely(pages_per_huge_page &gt; MAX_ORDER_NR_PAGES)) {</span>
<span class="quote">&gt;&gt;  		clear_gigantic_page(page, addr, pages_per_huge_page);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; ... and doing this:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  void clear_huge_page(struct page *page,</span>
<span class="quote">&gt; -		     unsigned long addr, unsigned int pages_per_huge_page)</span>
<span class="quote">&gt; +		     unsigned long addr_hint, unsigned int pages_per_huge_page)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	int i;</span>
<span class="quote">&gt; +	int i, n, base, l;</span>
<span class="quote">&gt; +	unsigned long addr = addr_hint &amp;</span>
<span class="quote">&gt; +				(1UL &lt;&lt; (pages_per_huge_page + PAGE_SHIFT));</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; @@ -4374,9 +4374,31 @@ void clear_huge_page(struct page *page,</span>
<span class="quote">&gt;&gt;  	}</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  	might_sleep();</span>
<span class="quote">&gt;&gt; -	for (i = 0; i &lt; pages_per_huge_page; i++) {</span>
<span class="quote">&gt;&gt; +	VM_BUG_ON(clamp(addr_hint, addr, addr +</span>
<span class="quote">&gt;&gt; +			(pages_per_huge_page &lt;&lt; PAGE_SHIFT)) != addr_hint);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; ... then you can ditch this check</span>

Yes.  This looks good for me.  If there is no objection, I will go this
way in the next version.

Best Regards,
Huang, Ying
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=41">Andrew Morton</a> - Aug. 9, 2017, 9:25 p.m.</div>
<pre class="content">
On Mon,  7 Aug 2017 15:21:31 +0800 &quot;Huang, Ying&quot; &lt;ying.huang@intel.com&gt; wrote:
<span class="quote">
&gt; From: Huang Ying &lt;ying.huang@intel.com&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Huge page helps to reduce TLB miss rate, but it has higher cache</span>
<span class="quote">&gt; footprint, sometimes this may cause some issue.  For example, when</span>
<span class="quote">&gt; clearing huge page on x86_64 platform, the cache footprint is 2M.  But</span>
<span class="quote">&gt; on a Xeon E5 v3 2699 CPU, there are 18 cores, 36 threads, and only 45M</span>
<span class="quote">&gt; LLC (last level cache).  That is, in average, there are 2.5M LLC for</span>
<span class="quote">&gt; each core and 1.25M LLC for each thread.  If the cache pressure is</span>
<span class="quote">&gt; heavy when clearing the huge page, and we clear the huge page from the</span>
<span class="quote">&gt; begin to the end, it is possible that the begin of huge page is</span>
<span class="quote">&gt; evicted from the cache after we finishing clearing the end of the huge</span>
<span class="quote">&gt; page.  And it is possible for the application to access the begin of</span>
<span class="quote">&gt; the huge page after clearing the huge page.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; To help the above situation, in this patch, when we clear a huge page,</span>
<span class="quote">&gt; the order to clear sub-pages is changed.  In quite some situation, we</span>
<span class="quote">&gt; can get the address that the application will access after we clear</span>
<span class="quote">&gt; the huge page, for example, in a page fault handler.  Instead of</span>
<span class="quote">&gt; clearing the huge page from begin to end, we will clear the sub-pages</span>
<span class="quote">&gt; farthest from the the sub-page to access firstly, and clear the</span>
<span class="quote">&gt; sub-page to access last.  This will make the sub-page to access most</span>
<span class="quote">&gt; cache-hot and sub-pages around it more cache-hot too.  If we cannot</span>
<span class="quote">&gt; know the address the application will access, the begin of the huge</span>
<span class="quote">&gt; page is assumed to be the the address the application will access.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; With this patch, the throughput increases ~28.3% in vm-scalability</span>
<span class="quote">&gt; anon-w-seq test case with 72 processes on a 2 socket Xeon E5 v3 2699</span>
<span class="quote">&gt; system (36 cores, 72 threads).  The test case creates 72 processes,</span>
<span class="quote">&gt; each process mmap a big anonymous memory area and writes to it from</span>
<span class="quote">&gt; the begin to the end.  For each process, other processes could be seen</span>
<span class="quote">&gt; as other workload which generates heavy cache pressure.  At the same</span>
<span class="quote">&gt; time, the cache miss rate reduced from ~33.4% to ~31.7%, the</span>
<span class="quote">&gt; IPC (instruction per cycle) increased from 0.56 to 0.74, and the time</span>
<span class="quote">&gt; spent in user space is reduced ~7.9%</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Thanks Andi Kleen to propose to use address to access to determine the</span>
<span class="quote">&gt; order of sub-pages to clear.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The hugetlbfs access address could be improved, will do that in</span>
<span class="quote">&gt; another patch.</span>

I agree with what others said, plus...
<span class="quote">
&gt; @@ -4374,9 +4374,31 @@ void clear_huge_page(struct page *page,</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	might_sleep();</span>
<span class="quote">&gt; -	for (i = 0; i &lt; pages_per_huge_page; i++) {</span>
<span class="quote">&gt; +	VM_BUG_ON(clamp(addr_hint, addr, addr +</span>
<span class="quote">&gt; +			(pages_per_huge_page &lt;&lt; PAGE_SHIFT)) != addr_hint);</span>
<span class="quote">&gt; +	n = (addr_hint - addr) / PAGE_SIZE;</span>
<span class="quote">&gt; +	if (2 * n &lt;= pages_per_huge_page) {</span>
<span class="quote">&gt; +		base = 0;</span>
<span class="quote">&gt; +		l = n;</span>
<span class="quote">&gt; +		for (i = pages_per_huge_page - 1; i &gt;= 2 * n; i--) {</span>
<span class="quote">&gt; +			cond_resched();</span>
<span class="quote">&gt; +			clear_user_highpage(page + i, addr + i * PAGE_SIZE);</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +	} else {</span>
<span class="quote">&gt; +		base = 2 * n - pages_per_huge_page;</span>
<span class="quote">&gt; +		l = pages_per_huge_page - n;</span>
<span class="quote">&gt; +		for (i = 0; i &lt; base; i++) {</span>
<span class="quote">&gt; +			cond_resched();</span>
<span class="quote">&gt; +			clear_user_highpage(page + i, addr + i * PAGE_SIZE);</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +	for (i = 0; i &lt; l; i++) {</span>
<span class="quote">&gt; +		cond_resched();</span>
<span class="quote">&gt; +		clear_user_highpage(page + base + i,</span>
<span class="quote">&gt; +				    addr + (base + i) * PAGE_SIZE);</span>
<span class="quote">&gt;  		cond_resched();</span>
<span class="quote">&gt; -		clear_user_highpage(page + i, addr + i * PAGE_SIZE);</span>
<span class="quote">&gt; +		clear_user_highpage(page + base + 2 * l - 1 - i,</span>
<span class="quote">&gt; +				    addr + (base + 2 * l - 1 - i) * PAGE_SIZE);</span>

Please document this design with a carefully written code comment.
For example, why was &quot;2 * n&quot; chosen?  What is it trying to achieve?

Also, the final clearing loop &quot;for (i = 0; i &lt; l; i++)&quot; might cause
eviction of data which was cached in the previous loop.  Perhaps some
additional gains will be made by clearing the hugepage in a
left-right-left-right &quot;start from the ends and work inwards&quot; manner, if
you see what I mean.  So the 4k pages immediately surrounding addr_hint
are the most-recently-cleared.  Although accesses to the data at lower
addresses than addr_hint are probably somewhat rare (and may be
nonexistent in your synthetic test case).
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=294">Huang Ying</a> - Aug. 10, 2017, 12:58 a.m.</div>
<pre class="content">
Hi, Andrew,

Andrew Morton &lt;akpm@linux-foundation.org&gt; writes:
<span class="quote">
&gt; On Mon,  7 Aug 2017 15:21:31 +0800 &quot;Huang, Ying&quot; &lt;ying.huang@intel.com&gt; wrote:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; From: Huang Ying &lt;ying.huang@intel.com&gt;</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; Huge page helps to reduce TLB miss rate, but it has higher cache</span>
<span class="quote">&gt;&gt; footprint, sometimes this may cause some issue.  For example, when</span>
<span class="quote">&gt;&gt; clearing huge page on x86_64 platform, the cache footprint is 2M.  But</span>
<span class="quote">&gt;&gt; on a Xeon E5 v3 2699 CPU, there are 18 cores, 36 threads, and only 45M</span>
<span class="quote">&gt;&gt; LLC (last level cache).  That is, in average, there are 2.5M LLC for</span>
<span class="quote">&gt;&gt; each core and 1.25M LLC for each thread.  If the cache pressure is</span>
<span class="quote">&gt;&gt; heavy when clearing the huge page, and we clear the huge page from the</span>
<span class="quote">&gt;&gt; begin to the end, it is possible that the begin of huge page is</span>
<span class="quote">&gt;&gt; evicted from the cache after we finishing clearing the end of the huge</span>
<span class="quote">&gt;&gt; page.  And it is possible for the application to access the begin of</span>
<span class="quote">&gt;&gt; the huge page after clearing the huge page.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; To help the above situation, in this patch, when we clear a huge page,</span>
<span class="quote">&gt;&gt; the order to clear sub-pages is changed.  In quite some situation, we</span>
<span class="quote">&gt;&gt; can get the address that the application will access after we clear</span>
<span class="quote">&gt;&gt; the huge page, for example, in a page fault handler.  Instead of</span>
<span class="quote">&gt;&gt; clearing the huge page from begin to end, we will clear the sub-pages</span>
<span class="quote">&gt;&gt; farthest from the the sub-page to access firstly, and clear the</span>
<span class="quote">&gt;&gt; sub-page to access last.  This will make the sub-page to access most</span>
<span class="quote">&gt;&gt; cache-hot and sub-pages around it more cache-hot too.  If we cannot</span>
<span class="quote">&gt;&gt; know the address the application will access, the begin of the huge</span>
<span class="quote">&gt;&gt; page is assumed to be the the address the application will access.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; With this patch, the throughput increases ~28.3% in vm-scalability</span>
<span class="quote">&gt;&gt; anon-w-seq test case with 72 processes on a 2 socket Xeon E5 v3 2699</span>
<span class="quote">&gt;&gt; system (36 cores, 72 threads).  The test case creates 72 processes,</span>
<span class="quote">&gt;&gt; each process mmap a big anonymous memory area and writes to it from</span>
<span class="quote">&gt;&gt; the begin to the end.  For each process, other processes could be seen</span>
<span class="quote">&gt;&gt; as other workload which generates heavy cache pressure.  At the same</span>
<span class="quote">&gt;&gt; time, the cache miss rate reduced from ~33.4% to ~31.7%, the</span>
<span class="quote">&gt;&gt; IPC (instruction per cycle) increased from 0.56 to 0.74, and the time</span>
<span class="quote">&gt;&gt; spent in user space is reduced ~7.9%</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; Thanks Andi Kleen to propose to use address to access to determine the</span>
<span class="quote">&gt;&gt; order of sub-pages to clear.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; The hugetlbfs access address could be improved, will do that in</span>
<span class="quote">&gt;&gt; another patch.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I agree with what others said, plus...</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; @@ -4374,9 +4374,31 @@ void clear_huge_page(struct page *page,</span>
<span class="quote">&gt;&gt;  	}</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  	might_sleep();</span>
<span class="quote">&gt;&gt; -	for (i = 0; i &lt; pages_per_huge_page; i++) {</span>
<span class="quote">&gt;&gt; +	VM_BUG_ON(clamp(addr_hint, addr, addr +</span>
<span class="quote">&gt;&gt; +			(pages_per_huge_page &lt;&lt; PAGE_SHIFT)) != addr_hint);</span>
<span class="quote">&gt;&gt; +	n = (addr_hint - addr) / PAGE_SIZE;</span>
<span class="quote">&gt;&gt; +	if (2 * n &lt;= pages_per_huge_page) {</span>
<span class="quote">&gt;&gt; +		base = 0;</span>
<span class="quote">&gt;&gt; +		l = n;</span>
<span class="quote">&gt;&gt; +		for (i = pages_per_huge_page - 1; i &gt;= 2 * n; i--) {</span>
<span class="quote">&gt;&gt; +			cond_resched();</span>
<span class="quote">&gt;&gt; +			clear_user_highpage(page + i, addr + i * PAGE_SIZE);</span>
<span class="quote">&gt;&gt; +		}</span>
<span class="quote">&gt;&gt; +	} else {</span>
<span class="quote">&gt;&gt; +		base = 2 * n - pages_per_huge_page;</span>
<span class="quote">&gt;&gt; +		l = pages_per_huge_page - n;</span>
<span class="quote">&gt;&gt; +		for (i = 0; i &lt; base; i++) {</span>
<span class="quote">&gt;&gt; +			cond_resched();</span>
<span class="quote">&gt;&gt; +			clear_user_highpage(page + i, addr + i * PAGE_SIZE);</span>
<span class="quote">&gt;&gt; +		}</span>
<span class="quote">&gt;&gt; +	}</span>
<span class="quote">&gt;&gt; +	for (i = 0; i &lt; l; i++) {</span>
<span class="quote">&gt;&gt; +		cond_resched();</span>
<span class="quote">&gt;&gt; +		clear_user_highpage(page + base + i,</span>
<span class="quote">&gt;&gt; +				    addr + (base + i) * PAGE_SIZE);</span>
<span class="quote">&gt;&gt;  		cond_resched();</span>
<span class="quote">&gt;&gt; -		clear_user_highpage(page + i, addr + i * PAGE_SIZE);</span>
<span class="quote">&gt;&gt; +		clear_user_highpage(page + base + 2 * l - 1 - i,</span>
<span class="quote">&gt;&gt; +				    addr + (base + 2 * l - 1 - i) * PAGE_SIZE);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Please document this design with a carefully written code comment.</span>
<span class="quote">&gt; For example, why was &quot;2 * n&quot; chosen?  What is it trying to achieve?</span>

Sure.

&quot;2 * n&quot; here is to determine whether addr_hint is in the first half (2 *
n &lt;= pages_per_huge_page) or the second half (2 * n &gt;
pages_per_huge_page) of the huge page.
<span class="quote">
&gt; Also, the final clearing loop &quot;for (i = 0; i &lt; l; i++)&quot; might cause</span>
<span class="quote">&gt; eviction of data which was cached in the previous loop.  Perhaps some</span>
<span class="quote">&gt; additional gains will be made by clearing the hugepage in a</span>
<span class="quote">&gt; left-right-left-right &quot;start from the ends and work inwards&quot; manner, if</span>
<span class="quote">&gt; you see what I mean.  So the 4k pages immediately surrounding addr_hint</span>
<span class="quote">&gt; are the most-recently-cleared.  Although accesses to the data at lower</span>
<span class="quote">&gt; addresses than addr_hint are probably somewhat rare (and may be</span>
<span class="quote">&gt; nonexistent in your synthetic test case).</span>

Yes.  I think I have done exactly this in the patch.  For each iteration
of the loop, two sub-pages will be cleared: base + i, and base + 2 * l -
1 - i, that is, the left and right of the fault sub-page, and finally
reach the fault sub-page as the last sub-page to clear.

Best Regards,
Huang, Ying
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/fs/hugetlbfs/inode.c b/fs/hugetlbfs/inode.c</span>
<span class="p_header">index 33961b35007b..1bbb38fcaa11 100644</span>
<span class="p_header">--- a/fs/hugetlbfs/inode.c</span>
<span class="p_header">+++ b/fs/hugetlbfs/inode.c</span>
<span class="p_chunk">@@ -627,7 +627,7 @@</span> <span class="p_context"> static long hugetlbfs_fallocate(struct file *file, int mode, loff_t offset,</span>
 			error = PTR_ERR(page);
 			goto out;
 		}
<span class="p_del">-		clear_huge_page(page, addr, pages_per_huge_page(h));</span>
<span class="p_add">+		clear_huge_page(page, addr, pages_per_huge_page(h), addr);</span>
 		__SetPageUptodate(page);
 		error = huge_add_to_page_cache(page, mapping, index);
 		if (unlikely(error)) {
<span class="p_header">diff --git a/include/linux/mm.h b/include/linux/mm.h</span>
<span class="p_header">index 9fee3213a75e..a954f63a13c9 100644</span>
<span class="p_header">--- a/include/linux/mm.h</span>
<span class="p_header">+++ b/include/linux/mm.h</span>
<span class="p_chunk">@@ -2509,7 +2509,8 @@</span> <span class="p_context"> enum mf_action_page_type {</span>
 #if defined(CONFIG_TRANSPARENT_HUGEPAGE) || defined(CONFIG_HUGETLBFS)
 extern void clear_huge_page(struct page *page,
 			    unsigned long addr,
<span class="p_del">-			    unsigned int pages_per_huge_page);</span>
<span class="p_add">+			    unsigned int pages_per_huge_page,</span>
<span class="p_add">+			    unsigned long addr_hint);</span>
 extern void copy_user_huge_page(struct page *dst, struct page *src,
 				unsigned long addr, struct vm_area_struct *vma,
 				unsigned int pages_per_huge_page);
<span class="p_header">diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="p_header">index fd3ad6c88c8a..b1e66df38661 100644</span>
<span class="p_header">--- a/mm/huge_memory.c</span>
<span class="p_header">+++ b/mm/huge_memory.c</span>
<span class="p_chunk">@@ -549,7 +549,8 @@</span> <span class="p_context"> static int __do_huge_pmd_anonymous_page(struct vm_fault *vmf, struct page *page,</span>
 	struct vm_area_struct *vma = vmf-&gt;vma;
 	struct mem_cgroup *memcg;
 	pgtable_t pgtable;
<span class="p_del">-	unsigned long haddr = vmf-&gt;address &amp; HPAGE_PMD_MASK;</span>
<span class="p_add">+	unsigned long address = vmf-&gt;address;</span>
<span class="p_add">+	unsigned long haddr = address &amp; HPAGE_PMD_MASK;</span>
 
 	VM_BUG_ON_PAGE(!PageCompound(page), page);
 
<span class="p_chunk">@@ -566,7 +567,7 @@</span> <span class="p_context"> static int __do_huge_pmd_anonymous_page(struct vm_fault *vmf, struct page *page,</span>
 		return VM_FAULT_OOM;
 	}
 
<span class="p_del">-	clear_huge_page(page, haddr, HPAGE_PMD_NR);</span>
<span class="p_add">+	clear_huge_page(page, haddr, HPAGE_PMD_NR, address);</span>
 	/*
 	 * The memory barrier inside __SetPageUptodate makes sure that
 	 * clear_huge_page writes become visible before the set_pmd_at()
<span class="p_chunk">@@ -1225,7 +1226,8 @@</span> <span class="p_context"> int do_huge_pmd_wp_page(struct vm_fault *vmf, pmd_t orig_pmd)</span>
 	struct vm_area_struct *vma = vmf-&gt;vma;
 	struct page *page = NULL, *new_page;
 	struct mem_cgroup *memcg;
<span class="p_del">-	unsigned long haddr = vmf-&gt;address &amp; HPAGE_PMD_MASK;</span>
<span class="p_add">+	unsigned long address = vmf-&gt;address;</span>
<span class="p_add">+	unsigned long haddr = address &amp; HPAGE_PMD_MASK;</span>
 	unsigned long mmun_start;	/* For mmu_notifiers */
 	unsigned long mmun_end;		/* For mmu_notifiers */
 	gfp_t huge_gfp;			/* for allocation and charge */
<span class="p_chunk">@@ -1310,7 +1312,7 @@</span> <span class="p_context"> int do_huge_pmd_wp_page(struct vm_fault *vmf, pmd_t orig_pmd)</span>
 	count_vm_event(THP_FAULT_ALLOC);
 
 	if (!page)
<span class="p_del">-		clear_huge_page(new_page, haddr, HPAGE_PMD_NR);</span>
<span class="p_add">+		clear_huge_page(new_page, haddr, HPAGE_PMD_NR, address);</span>
 	else
 		copy_user_huge_page(new_page, page, haddr, vma, HPAGE_PMD_NR);
 	__SetPageUptodate(new_page);
<span class="p_header">diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="p_header">index 5dae4fff368d..fb2ff230236a 100644</span>
<span class="p_header">--- a/mm/hugetlb.c</span>
<span class="p_header">+++ b/mm/hugetlb.c</span>
<span class="p_chunk">@@ -3707,7 +3707,7 @@</span> <span class="p_context"> static int hugetlb_no_page(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 				ret = VM_FAULT_SIGBUS;
 			goto out;
 		}
<span class="p_del">-		clear_huge_page(page, address, pages_per_huge_page(h));</span>
<span class="p_add">+		clear_huge_page(page, address, pages_per_huge_page(h), address);</span>
 		__SetPageUptodate(page);
 		set_page_huge_active(page);
 
<span class="p_header">diff --git a/mm/memory.c b/mm/memory.c</span>
<span class="p_header">index edabf6f03447..d5bd7633a443 100644</span>
<span class="p_header">--- a/mm/memory.c</span>
<span class="p_header">+++ b/mm/memory.c</span>
<span class="p_chunk">@@ -4363,10 +4363,10 @@</span> <span class="p_context"> static void clear_gigantic_page(struct page *page,</span>
 		clear_user_highpage(p, addr + i * PAGE_SIZE);
 	}
 }
<span class="p_del">-void clear_huge_page(struct page *page,</span>
<span class="p_del">-		     unsigned long addr, unsigned int pages_per_huge_page)</span>
<span class="p_add">+void clear_huge_page(struct page *page, unsigned long addr,</span>
<span class="p_add">+		     unsigned int pages_per_huge_page, unsigned long addr_hint)</span>
 {
<span class="p_del">-	int i;</span>
<span class="p_add">+	int i, n, base, l;</span>
 
 	if (unlikely(pages_per_huge_page &gt; MAX_ORDER_NR_PAGES)) {
 		clear_gigantic_page(page, addr, pages_per_huge_page);
<span class="p_chunk">@@ -4374,9 +4374,31 @@</span> <span class="p_context"> void clear_huge_page(struct page *page,</span>
 	}
 
 	might_sleep();
<span class="p_del">-	for (i = 0; i &lt; pages_per_huge_page; i++) {</span>
<span class="p_add">+	VM_BUG_ON(clamp(addr_hint, addr, addr +</span>
<span class="p_add">+			(pages_per_huge_page &lt;&lt; PAGE_SHIFT)) != addr_hint);</span>
<span class="p_add">+	n = (addr_hint - addr) / PAGE_SIZE;</span>
<span class="p_add">+	if (2 * n &lt;= pages_per_huge_page) {</span>
<span class="p_add">+		base = 0;</span>
<span class="p_add">+		l = n;</span>
<span class="p_add">+		for (i = pages_per_huge_page - 1; i &gt;= 2 * n; i--) {</span>
<span class="p_add">+			cond_resched();</span>
<span class="p_add">+			clear_user_highpage(page + i, addr + i * PAGE_SIZE);</span>
<span class="p_add">+		}</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		base = 2 * n - pages_per_huge_page;</span>
<span class="p_add">+		l = pages_per_huge_page - n;</span>
<span class="p_add">+		for (i = 0; i &lt; base; i++) {</span>
<span class="p_add">+			cond_resched();</span>
<span class="p_add">+			clear_user_highpage(page + i, addr + i * PAGE_SIZE);</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+	for (i = 0; i &lt; l; i++) {</span>
<span class="p_add">+		cond_resched();</span>
<span class="p_add">+		clear_user_highpage(page + base + i,</span>
<span class="p_add">+				    addr + (base + i) * PAGE_SIZE);</span>
 		cond_resched();
<span class="p_del">-		clear_user_highpage(page + i, addr + i * PAGE_SIZE);</span>
<span class="p_add">+		clear_user_highpage(page + base + 2 * l - 1 - i,</span>
<span class="p_add">+				    addr + (base + 2 * l - 1 - i) * PAGE_SIZE);</span>
 	}
 }
 

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



