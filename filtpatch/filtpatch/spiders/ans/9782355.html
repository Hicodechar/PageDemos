
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[4.9,020/119] sparc64: new context wrap - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [4.9,020/119] sparc64: new context wrap</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=37061">gregkh@linuxfoundation.org</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>June 12, 2017, 3:24 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20170612152557.681582670@linuxfoundation.org&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9782355/mbox/"
   >mbox</a>
|
   <a href="/patch/9782355/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9782355/">/patch/9782355/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	1DC6C60244 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 12 Jun 2017 17:01:13 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 0C775283F2
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 12 Jun 2017 17:01:13 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 0125128508; Mon, 12 Jun 2017 17:01:12 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 49FE7283F2
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 12 Jun 2017 17:01:12 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1754093AbdFLRBC (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 12 Jun 2017 13:01:02 -0400
Received: from mail.linuxfoundation.org ([140.211.169.12]:40848 &quot;EHLO
	mail.linuxfoundation.org&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1753347AbdFLPdc (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 12 Jun 2017 11:33:32 -0400
Received: from localhost (LFbn-1-12060-104.w90-92.abo.wanadoo.fr
	[90.92.122.104])
	by mail.linuxfoundation.org (Postfix) with ESMTPSA id 76F92B7A;
	Mon, 12 Jun 2017 15:33:31 +0000 (UTC)
From: Greg Kroah-Hartman &lt;gregkh@linuxfoundation.org&gt;
To: linux-kernel@vger.kernel.org
Cc: Greg Kroah-Hartman &lt;gregkh@linuxfoundation.org&gt;,
	stable@vger.kernel.org, Pavel Tatashin &lt;pasha.tatashin@oracle.com&gt;,
	Bob Picco &lt;bob.picco@oracle.com&gt;,
	Steven Sistare &lt;steven.sistare@oracle.com&gt;,
	&quot;David S. Miller&quot; &lt;davem@davemloft.net&gt;
Subject: [PATCH 4.9 020/119] sparc64: new context wrap
Date: Mon, 12 Jun 2017 17:24:42 +0200
Message-Id: &lt;20170612152557.681582670@linuxfoundation.org&gt;
X-Mailer: git-send-email 2.13.1
In-Reply-To: &lt;20170612152556.601664278@linuxfoundation.org&gt;
References: &lt;20170612152556.601664278@linuxfoundation.org&gt;
User-Agent: quilt/0.65
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=37061">gregkh@linuxfoundation.org</a> - June 12, 2017, 3:24 p.m.</div>
<pre class="content">
4.9-stable review patch.  If anyone has any objections, please let me know.

------------------
<span class="from">
From: Pavel Tatashin &lt;pasha.tatashin@oracle.com&gt;</span>


[ Upstream commit a0582f26ec9dfd5360ea2f35dd9a1b026f8adda0 ]

The current wrap implementation has a race issue: it is called outside of
the ctx_alloc_lock, and also does not wait for all CPUs to complete the
wrap.  This means that a thread can get a new context with a new version
and another thread might still be running with the same context. The
problem is especially severe on CPUs with shared TLBs, like sun4v. I used
the following test to very quickly reproduce the problem:
- start over 8K processes (must be more than context IDs)
- write and read values at a  memory location in every process.

Very quickly memory corruptions start happening, and what we read back
does not equal what we wrote.

Several approaches were explored before settling on this one:

Approach 1:
Move smp_new_mmu_context_version() inside ctx_alloc_lock, and wait for
every process to complete the wrap. (Note: every CPU must WAIT before
leaving smp_new_mmu_context_version_client() until every one arrives).

This approach ends up with deadlocks, as some threads own locks which other
threads are waiting for, and they never receive softint until these threads
exit smp_new_mmu_context_version_client(). Since we do not allow the exit,
deadlock happens.

Approach 2:
Handle wrap right during mondo interrupt. Use etrap/rtrap to enter into
into C code, and issue new versions to every CPU.
This approach adds some overhead to runtime: in switch_mm() we must add
some checks to make sure that versions have not changed due to wrap while
we were loading the new secondary context. (could be protected by PSTATE_IE
but that degrades performance as on M7 and older CPUs as it takes 50 cycles
for each access). Also, we still need a global per-cpu array of MMs to know
where we need to load new contexts, otherwise we can change context to a
thread that is going way (if we received mondo between switch_mm() and
switch_to() time). Finally, there are some issues with window registers in
rtrap() when context IDs are changed during CPU mondo time.

The approach in this patch is the simplest and has almost no impact on
runtime.  We use the array with mm&#39;s where last secondary contexts were
loaded onto CPUs and bump their versions to the new generation without
changing context IDs. If a new process comes in to get a context ID, it
will go through get_new_mmu_context() because of version mismatch. But the
running processes do not need to be interrupted. And wrap is quicker as we
do not need to xcall and wait for everyone to receive and complete wrap.
<span class="signed-off-by">
Signed-off-by: Pavel Tatashin &lt;pasha.tatashin@oracle.com&gt;</span>
<span class="reviewed-by">Reviewed-by: Bob Picco &lt;bob.picco@oracle.com&gt;</span>
<span class="reviewed-by">Reviewed-by: Steven Sistare &lt;steven.sistare@oracle.com&gt;</span>
<span class="signed-off-by">Signed-off-by: David S. Miller &lt;davem@davemloft.net&gt;</span>
<span class="signed-off-by">Signed-off-by: Greg Kroah-Hartman &lt;gregkh@linuxfoundation.org&gt;</span>
---
 arch/sparc/mm/init_64.c |   81 ++++++++++++++++++++++++++++++++----------------
 1 file changed, 54 insertions(+), 27 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">--- a/arch/sparc/mm/init_64.c</span>
<span class="p_header">+++ b/arch/sparc/mm/init_64.c</span>
<span class="p_chunk">@@ -664,6 +664,53 @@</span> <span class="p_context"> unsigned long tlb_context_cache = CTX_FI</span>
 DECLARE_BITMAP(mmu_context_bmap, MAX_CTX_NR);
 DEFINE_PER_CPU(struct mm_struct *, per_cpu_secondary_mm) = {0};
 
<span class="p_add">+static void mmu_context_wrap(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long old_ver = tlb_context_cache &amp; CTX_VERSION_MASK;</span>
<span class="p_add">+	unsigned long new_ver, new_ctx, old_ctx;</span>
<span class="p_add">+	struct mm_struct *mm;</span>
<span class="p_add">+	int cpu;</span>
<span class="p_add">+</span>
<span class="p_add">+	bitmap_zero(mmu_context_bmap, 1 &lt;&lt; CTX_NR_BITS);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Reserve kernel context */</span>
<span class="p_add">+	set_bit(0, mmu_context_bmap);</span>
<span class="p_add">+</span>
<span class="p_add">+	new_ver = (tlb_context_cache &amp; CTX_VERSION_MASK) + CTX_FIRST_VERSION;</span>
<span class="p_add">+	if (unlikely(new_ver == 0))</span>
<span class="p_add">+		new_ver = CTX_FIRST_VERSION;</span>
<span class="p_add">+	tlb_context_cache = new_ver;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Make sure that any new mm that are added into per_cpu_secondary_mm,</span>
<span class="p_add">+	 * are going to go through get_new_mmu_context() path.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	mb();</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Updated versions to current on those CPUs that had valid secondary</span>
<span class="p_add">+	 * contexts</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	for_each_online_cpu(cpu) {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * If a new mm is stored after we took this mm from the array,</span>
<span class="p_add">+		 * it will go into get_new_mmu_context() path, because we</span>
<span class="p_add">+		 * already bumped the version in tlb_context_cache.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		mm = per_cpu(per_cpu_secondary_mm, cpu);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (unlikely(!mm || mm == &amp;init_mm))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		old_ctx = mm-&gt;context.sparc64_ctx_val;</span>
<span class="p_add">+		if (likely((old_ctx &amp; CTX_VERSION_MASK) == old_ver)) {</span>
<span class="p_add">+			new_ctx = (old_ctx &amp; ~CTX_VERSION_MASK) | new_ver;</span>
<span class="p_add">+			set_bit(new_ctx &amp; CTX_NR_MASK, mmu_context_bmap);</span>
<span class="p_add">+			mm-&gt;context.sparc64_ctx_val = new_ctx;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /* Caller does TLB context flushing on local CPU if necessary.
  * The caller also ensures that CTX_VALID(mm-&gt;context) is false.
  *
<span class="p_chunk">@@ -678,50 +725,30 @@</span> <span class="p_context"> void get_new_mmu_context(struct mm_struc</span>
 {
 	unsigned long ctx, new_ctx;
 	unsigned long orig_pgsz_bits;
<span class="p_del">-	int new_version;</span>
 
 	spin_lock(&amp;ctx_alloc_lock);
<span class="p_add">+retry:</span>
<span class="p_add">+	/* wrap might have happened, test again if our context became valid */</span>
<span class="p_add">+	if (unlikely(CTX_VALID(mm-&gt;context)))</span>
<span class="p_add">+		goto out;</span>
 	orig_pgsz_bits = (mm-&gt;context.sparc64_ctx_val &amp; CTX_PGSZ_MASK);
 	ctx = (tlb_context_cache + 1) &amp; CTX_NR_MASK;
 	new_ctx = find_next_zero_bit(mmu_context_bmap, 1 &lt;&lt; CTX_NR_BITS, ctx);
<span class="p_del">-	new_version = 0;</span>
 	if (new_ctx &gt;= (1 &lt;&lt; CTX_NR_BITS)) {
 		new_ctx = find_next_zero_bit(mmu_context_bmap, ctx, 1);
 		if (new_ctx &gt;= ctx) {
<span class="p_del">-			int i;</span>
<span class="p_del">-			new_ctx = (tlb_context_cache &amp; CTX_VERSION_MASK) +</span>
<span class="p_del">-				CTX_FIRST_VERSION + 1;</span>
<span class="p_del">-			if (new_ctx == 1)</span>
<span class="p_del">-				new_ctx = CTX_FIRST_VERSION + 1;</span>
<span class="p_del">-</span>
<span class="p_del">-			/* Don&#39;t call memset, for 16 entries that&#39;s just</span>
<span class="p_del">-			 * plain silly...</span>
<span class="p_del">-			 */</span>
<span class="p_del">-			mmu_context_bmap[0] = 3;</span>
<span class="p_del">-			mmu_context_bmap[1] = 0;</span>
<span class="p_del">-			mmu_context_bmap[2] = 0;</span>
<span class="p_del">-			mmu_context_bmap[3] = 0;</span>
<span class="p_del">-			for (i = 4; i &lt; CTX_BMAP_SLOTS; i += 4) {</span>
<span class="p_del">-				mmu_context_bmap[i + 0] = 0;</span>
<span class="p_del">-				mmu_context_bmap[i + 1] = 0;</span>
<span class="p_del">-				mmu_context_bmap[i + 2] = 0;</span>
<span class="p_del">-				mmu_context_bmap[i + 3] = 0;</span>
<span class="p_del">-			}</span>
<span class="p_del">-			new_version = 1;</span>
<span class="p_del">-			goto out;</span>
<span class="p_add">+			mmu_context_wrap();</span>
<span class="p_add">+			goto retry;</span>
 		}
 	}
 	if (mm-&gt;context.sparc64_ctx_val)
 		cpumask_clear(mm_cpumask(mm));
 	mmu_context_bmap[new_ctx&gt;&gt;6] |= (1UL &lt;&lt; (new_ctx &amp; 63));
 	new_ctx |= (tlb_context_cache &amp; CTX_VERSION_MASK);
<span class="p_del">-out:</span>
 	tlb_context_cache = new_ctx;
 	mm-&gt;context.sparc64_ctx_val = new_ctx | orig_pgsz_bits;
<span class="p_add">+out:</span>
 	spin_unlock(&amp;ctx_alloc_lock);
<span class="p_del">-</span>
<span class="p_del">-	if (unlikely(new_version))</span>
<span class="p_del">-		smp_new_mmu_context_version();</span>
 }
 
 static int numa_enabled = 1;

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



