
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>linux-next: manual merge of the akpm-current tree with the tip tree - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    linux-next: manual merge of the akpm-current tree with the tip tree</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Aug. 11, 2017, 2:04 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20170811140450.irhxa2bhdpmmhhpv@hirez.programming.kicks-ass.net&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9896011/mbox/"
   >mbox</a>
|
   <a href="/patch/9896011/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9896011/">/patch/9896011/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	D72B860236 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 11 Aug 2017 14:05:09 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id C83BB28C32
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 11 Aug 2017 14:05:09 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id BCBE628C34; Fri, 11 Aug 2017 14:05:09 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.8 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	RCVD_IN_DNSWL_HI,T_DKIM_INVALID autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id D0EC728C32
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 11 Aug 2017 14:05:07 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1753120AbdHKOFE (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Fri, 11 Aug 2017 10:05:04 -0400
Received: from merlin.infradead.org ([205.233.59.134]:50642 &quot;EHLO
	merlin.infradead.org&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1752850AbdHKOFC (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Fri, 11 Aug 2017 10:05:02 -0400
DKIM-Signature: v=1; a=rsa-sha256; q=dns/txt; c=relaxed/relaxed;
	d=infradead.org; s=merlin.20170209;
	h=In-Reply-To:Content-Type:MIME-Version:
	References:Message-ID:Subject:Cc:To:From:Date:Sender:Reply-To:
	Content-Transfer-Encoding:Content-ID:Content-Description:Resent-Date:
	Resent-From:Resent-Sender:Resent-To:Resent-Cc:Resent-Message-ID:List-Id:
	List-Help:List-Unsubscribe:List-Subscribe:List-Post:List-Owner:List-Archive;
	bh=GxoNGQLIxLOa2vwZuclLVmMSZxdKxq23jnuUJB6rWIg=;
	b=G/gxtwascZT0946OBCpV72Rdj
	G0um0CAYzkC2+eVDKE9JXxybHFDgGAS+HFVarpbi8kwaxqDjjTjPSu3e0B6HoHwpjXJx0vtWMvd4E
	hOcMWN/gnoRJL5y+pya7QRlsCGc97zeU3XTktZS1IBGTuyBa2EOMASVwrtUViaiLZS3ab4dIJWsRb
	8gsaPi3WFKmnttO/jH/mxQVTIa21Ik53eTxXdSjG7sQtKQ7Rqm4tzLBKmIBP2fepYqlkEOavNVXmM
	9qfCjjaGjbVBmeF4HQkPJkqoc7w0HWzSM/YYn778cUl0X4SxpFaWbw8Fe6r7Z6Dobrq14M/6UUgrs
	+fJSN5sJA==;
Received: from j217100.upc-j.chello.nl ([24.132.217.100]
	helo=hirez.programming.kicks-ass.net)
	by merlin.infradead.org with esmtpsa (Exim 4.87 #1 (Red Hat Linux))
	id 1dgAYV-0000qM-ER; Fri, 11 Aug 2017 14:04:51 +0000
Received: by hirez.programming.kicks-ass.net (Postfix, from userid 1000)
	id 359FC201F52AC; Fri, 11 Aug 2017 16:04:50 +0200 (CEST)
Date: Fri, 11 Aug 2017 16:04:50 +0200
From: Peter Zijlstra &lt;peterz@infradead.org&gt;
To: Ingo Molnar &lt;mingo@kernel.org&gt;
Cc: Stephen Rothwell &lt;sfr@canb.auug.org.au&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	Thomas Gleixner &lt;tglx@linutronix.de&gt;,
	Ingo Molnar &lt;mingo@elte.hu&gt;, &quot;H. Peter Anvin&quot; &lt;hpa@zytor.com&gt;,
	Linux-Next Mailing List &lt;linux-next@vger.kernel.org&gt;,
	Linux Kernel Mailing List &lt;linux-kernel@vger.kernel.org&gt;,
	Nadav Amit &lt;namit@vmware.com&gt;,
	Linus &lt;torvalds@linux-foundation.org&gt;, minchan@kernel.org
Subject: Re: linux-next: manual merge of the akpm-current tree with the tip
	tree
Message-ID: &lt;20170811140450.irhxa2bhdpmmhhpv@hirez.programming.kicks-ass.net&gt;
References: &lt;20170811175326.36d546dc@canb.auug.org.au&gt;
	&lt;20170811093449.w5wttpulmwfykjzm@hirez.programming.kicks-ass.net&gt;
	&lt;20170811214556.322b3c4e@canb.auug.org.au&gt;
	&lt;20170811115607.p2vgqcp7w3wurhvw@gmail.com&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Disposition: inline
In-Reply-To: &lt;20170811115607.p2vgqcp7w3wurhvw@gmail.com&gt;
User-Agent: NeoMutt/20170609 (1.8.3)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - Aug. 11, 2017, 2:04 p.m.</div>
<pre class="content">
Ok, so I have the below to still go on-top.

Ideally someone would clarify the situation around
mm_tlb_flush_nested(), because ideally we&#39;d remove the
smp_mb__after_atomic() and go back to relying on PTL alone.

This also removes the pointless smp_mb__before_atomic()

---
Subject: mm: Fix barriers for the tlb_flush_pending thing
<span class="from">From: Peter Zijlstra &lt;peterz@infradead.org&gt;</span>
Date: Fri Aug 11 12:43:33 CEST 2017

I&#39;m not 100% sure we always care about the same PTL and when we have
SPLIT_PTE_PTLOCKS and have RCpc locks (PPC) the UNLOCK of one does not
in fact order against the LOCK of another lock. Therefore the
documented scheme does not work if we care about multiple PTLs

mm_tlb_flush_pending() appears to only care about a single PTL:

 - arch pte_accessible() (x86, arm64) only cares about that one PTE.
 - do_huge_pmd_numa_page() also only cares about a single (huge) page.
 - ksm write_protect_page() also only cares about a single page.

however mm_tlb_flush_nested() is a mystery, it appears to care about
anything inside the range. For now rely on it doing at least _a_ PTL
lock instead of taking  _the_ PTL lock.

Therefore add an explicit smp_mb__after_atomic() to cure things.

Also remove the smp_mb__before_atomic() on the dec side, as its
completely pointless. We must rely on flush_tlb_range() to DTRT.
<span class="signed-off-by">
Signed-off-by: Peter Zijlstra (Intel) &lt;peterz@infradead.org&gt;</span>
---
 include/linux/mm_types.h |   38 ++++++++++++++++++++++----------------
 1 file changed, 22 insertions(+), 16 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=159401">Nadav Amit</a> - Aug. 13, 2017, 6:06 a.m.</div>
<pre class="content">
Peter Zijlstra &lt;peterz@infradead.org&gt; wrote:
<span class="quote">
&gt; </span>
<span class="quote">&gt; Ok, so I have the below to still go on-top.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Ideally someone would clarify the situation around</span>
<span class="quote">&gt; mm_tlb_flush_nested(), because ideally we&#39;d remove the</span>
<span class="quote">&gt; smp_mb__after_atomic() and go back to relying on PTL alone.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This also removes the pointless smp_mb__before_atomic()</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt; Subject: mm: Fix barriers for the tlb_flush_pending thing</span>
<span class="quote">&gt; From: Peter Zijlstra &lt;peterz@infradead.org&gt;</span>
<span class="quote">&gt; Date: Fri Aug 11 12:43:33 CEST 2017</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I&#39;m not 100% sure we always care about the same PTL and when we have</span>
<span class="quote">&gt; SPLIT_PTE_PTLOCKS and have RCpc locks (PPC) the UNLOCK of one does not</span>
<span class="quote">&gt; in fact order against the LOCK of another lock. Therefore the</span>
<span class="quote">&gt; documented scheme does not work if we care about multiple PTLs</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; mm_tlb_flush_pending() appears to only care about a single PTL:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; - arch pte_accessible() (x86, arm64) only cares about that one PTE.</span>
<span class="quote">&gt; - do_huge_pmd_numa_page() also only cares about a single (huge) page.</span>
<span class="quote">&gt; - ksm write_protect_page() also only cares about a single page.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; however mm_tlb_flush_nested() is a mystery, it appears to care about</span>
<span class="quote">&gt; anything inside the range. For now rely on it doing at least _a_ PTL</span>
<span class="quote">&gt; lock instead of taking  _the_ PTL lock.</span>

It does not care about “anything” inside the range, but only on situations
in which there is at least one (same) PT that was modified by one core and
then read by the other. So, yes, it will always be _the_ same PTL, and not
_a_ PTL - in the cases that flush is really needed.

The issue that might require additional barriers is that
inc_tlb_flush_pending() and mm_tlb_flush_nested() are called when the PTL is
not held. IIUC, since the release-acquire might not behave as a full memory
barrier, this requires an explicit memory barrier.
<span class="quote">
&gt; Therefore add an explicit smp_mb__after_atomic() to cure things.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Also remove the smp_mb__before_atomic() on the dec side, as its</span>
<span class="quote">&gt; completely pointless. We must rely on flush_tlb_range() to DTRT.</span>

Good. It seemed fishy to me, but I was focused on the TLB consistency and
less on the barriers (that’s my excuse).

Nadav
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - Aug. 13, 2017, 12:50 p.m.</div>
<pre class="content">
On Sun, Aug 13, 2017 at 06:06:32AM +0000, Nadav Amit wrote:
<span class="quote">&gt; &gt; however mm_tlb_flush_nested() is a mystery, it appears to care about</span>
<span class="quote">&gt; &gt; anything inside the range. For now rely on it doing at least _a_ PTL</span>
<span class="quote">&gt; &gt; lock instead of taking  _the_ PTL lock.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It does not care about “anything” inside the range, but only on situations</span>
<span class="quote">&gt; in which there is at least one (same) PT that was modified by one core and</span>
<span class="quote">&gt; then read by the other. So, yes, it will always be _the_ same PTL, and not</span>
<span class="quote">&gt; _a_ PTL - in the cases that flush is really needed.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The issue that might require additional barriers is that</span>
<span class="quote">&gt; inc_tlb_flush_pending() and mm_tlb_flush_nested() are called when the PTL is</span>
<span class="quote">&gt; not held. IIUC, since the release-acquire might not behave as a full memory</span>
<span class="quote">&gt; barrier, this requires an explicit memory barrier.</span>

So I&#39;m not entirely clear about this yet.

How about:


	CPU0				CPU1

					tlb_gather_mmu()

					lock PTLn
					no mod
					unlock PTLn

	tlb_gather_mmu()

					lock PTLm
					mod
					include in tlb range
					unlock PTLm

	lock PTLn
	mod
	unlock PTLn

					tlb_finish_mmu()
					  force = mm_tlb_flush_nested(tlb-&gt;mm);
					  arch_tlb_finish_mmu(force);


	... more ...

	tlb_finish_mmu()



In this case you also want CPU1&#39;s mm_tlb_flush_nested() call to return
true, right?

But even with an smp_mb__after_atomic() at CPU0&#39;s tlg_bather_mmu()
you&#39;re not guaranteed CPU1 sees the increment. The only way to do that
is to make the PTL locks RCsc and that is a much more expensive
proposition.


What about:


	CPU0				CPU1

					tlb_gather_mmu()

					lock PTLn
					no mod
					unlock PTLn


					lock PTLm
					mod
					include in tlb range
					unlock PTLm

	tlb_gather_mmu()

	lock PTLn
	mod
	unlock PTLn

					tlb_finish_mmu()
					  force = mm_tlb_flush_nested(tlb-&gt;mm);
					  arch_tlb_finish_mmu(force);


	... more ...

	tlb_finish_mmu()

Do we want CPU1 to see it here? If so, where does it end?


	CPU0				CPU1

					tlb_gather_mmu()

					lock PTLn
					no mod
					unlock PTLn


					lock PTLm
					mod
					include in tlb range
					unlock PTLm

					tlb_finish_mmu()
					  force = mm_tlb_flush_nested(tlb-&gt;mm);

	tlb_gather_mmu()

	lock PTLn
	mod
	unlock PTLn

					  arch_tlb_finish_mmu(force);


	... more ...

	tlb_finish_mmu()


This?


Could you clarify under what exact condition mm_tlb_flush_nested() must
return true?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36811">Minchan Kim</a> - Aug. 14, 2017, 3:16 a.m.</div>
<pre class="content">
On Sun, Aug 13, 2017 at 02:50:19PM +0200, Peter Zijlstra wrote:
<span class="quote">&gt; On Sun, Aug 13, 2017 at 06:06:32AM +0000, Nadav Amit wrote:</span>
<span class="quote">&gt; &gt; &gt; however mm_tlb_flush_nested() is a mystery, it appears to care about</span>
<span class="quote">&gt; &gt; &gt; anything inside the range. For now rely on it doing at least _a_ PTL</span>
<span class="quote">&gt; &gt; &gt; lock instead of taking  _the_ PTL lock.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; It does not care about “anything” inside the range, but only on situations</span>
<span class="quote">&gt; &gt; in which there is at least one (same) PT that was modified by one core and</span>
<span class="quote">&gt; &gt; then read by the other. So, yes, it will always be _the_ same PTL, and not</span>
<span class="quote">&gt; &gt; _a_ PTL - in the cases that flush is really needed.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; The issue that might require additional barriers is that</span>
<span class="quote">&gt; &gt; inc_tlb_flush_pending() and mm_tlb_flush_nested() are called when the PTL is</span>
<span class="quote">&gt; &gt; not held. IIUC, since the release-acquire might not behave as a full memory</span>
<span class="quote">&gt; &gt; barrier, this requires an explicit memory barrier.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So I&#39;m not entirely clear about this yet.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; How about:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	CPU0				CPU1</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 					tlb_gather_mmu()</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 					lock PTLn</span>
<span class="quote">&gt; 					no mod</span>
<span class="quote">&gt; 					unlock PTLn</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	tlb_gather_mmu()</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 					lock PTLm</span>
<span class="quote">&gt; 					mod</span>
<span class="quote">&gt; 					include in tlb range</span>
<span class="quote">&gt; 					unlock PTLm</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	lock PTLn</span>
<span class="quote">&gt; 	mod</span>
<span class="quote">&gt; 	unlock PTLn</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 					tlb_finish_mmu()</span>
<span class="quote">&gt; 					  force = mm_tlb_flush_nested(tlb-&gt;mm);</span>
<span class="quote">&gt; 					  arch_tlb_finish_mmu(force);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	... more ...</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	tlb_finish_mmu()</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; In this case you also want CPU1&#39;s mm_tlb_flush_nested() call to return</span>
<span class="quote">&gt; true, right?</span>

No, because CPU 1 mofified pte and added it into tlb range
so regardless of nested, it will flush TLB so there is no stale
TLB problem.
<span class="quote">
&gt; </span>
<span class="quote">&gt; But even with an smp_mb__after_atomic() at CPU0&#39;s tlg_bather_mmu()</span>
<span class="quote">&gt; you&#39;re not guaranteed CPU1 sees the increment. The only way to do that</span>
<span class="quote">&gt; is to make the PTL locks RCsc and that is a much more expensive</span>
<span class="quote">&gt; proposition.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; What about:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	CPU0				CPU1</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 					tlb_gather_mmu()</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 					lock PTLn</span>
<span class="quote">&gt; 					no mod</span>
<span class="quote">&gt; 					unlock PTLn</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 					lock PTLm</span>
<span class="quote">&gt; 					mod</span>
<span class="quote">&gt; 					include in tlb range</span>
<span class="quote">&gt; 					unlock PTLm</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	tlb_gather_mmu()</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	lock PTLn</span>
<span class="quote">&gt; 	mod</span>
<span class="quote">&gt; 	unlock PTLn</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 					tlb_finish_mmu()</span>
<span class="quote">&gt; 					  force = mm_tlb_flush_nested(tlb-&gt;mm);</span>
<span class="quote">&gt; 					  arch_tlb_finish_mmu(force);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	... more ...</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	tlb_finish_mmu()</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Do we want CPU1 to see it here? If so, where does it end?</span>

Ditto. Since CPU 1 has added range, it will flush TLB regardless
of nested condition.
<span class="quote">
&gt; </span>
<span class="quote">&gt; 	CPU0				CPU1</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 					tlb_gather_mmu()</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 					lock PTLn</span>
<span class="quote">&gt; 					no mod</span>
<span class="quote">&gt; 					unlock PTLn</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 					lock PTLm</span>
<span class="quote">&gt; 					mod</span>
<span class="quote">&gt; 					include in tlb range</span>
<span class="quote">&gt; 					unlock PTLm</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 					tlb_finish_mmu()</span>
<span class="quote">&gt; 					  force = mm_tlb_flush_nested(tlb-&gt;mm);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	tlb_gather_mmu()</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	lock PTLn</span>
<span class="quote">&gt; 	mod</span>
<span class="quote">&gt; 	unlock PTLn</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 					  arch_tlb_finish_mmu(force);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	... more ...</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	tlb_finish_mmu()</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Could you clarify under what exact condition mm_tlb_flush_nested() must</span>
<span class="quote">&gt; return true?</span>

mm_tlb_flush_nested aims for the CPU side where there is no pte update
but need TLB flush.
As I wrote https://marc.info/?l=linux-mm&amp;m=150267398226529&amp;w=2,
it has stable TLB problem if we don&#39;t flush TLB although there is no
pte modification.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=159401">Nadav Amit</a> - Aug. 14, 2017, 5:07 a.m.</div>
<pre class="content">
Minchan Kim &lt;minchan@kernel.org&gt; wrote:
<span class="quote">
&gt; On Sun, Aug 13, 2017 at 02:50:19PM +0200, Peter Zijlstra wrote:</span>
<span class="quote">&gt;&gt; On Sun, Aug 13, 2017 at 06:06:32AM +0000, Nadav Amit wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; however mm_tlb_flush_nested() is a mystery, it appears to care about</span>
<span class="quote">&gt;&gt;&gt;&gt; anything inside the range. For now rely on it doing at least _a_ PTL</span>
<span class="quote">&gt;&gt;&gt;&gt; lock instead of taking  _the_ PTL lock.</span>
<span class="quote">&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt; It does not care about “anything” inside the range, but only on situations</span>
<span class="quote">&gt;&gt;&gt; in which there is at least one (same) PT that was modified by one core and</span>
<span class="quote">&gt;&gt;&gt; then read by the other. So, yes, it will always be _the_ same PTL, and not</span>
<span class="quote">&gt;&gt;&gt; _a_ PTL - in the cases that flush is really needed.</span>
<span class="quote">&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt; The issue that might require additional barriers is that</span>
<span class="quote">&gt;&gt;&gt; inc_tlb_flush_pending() and mm_tlb_flush_nested() are called when the PTL is</span>
<span class="quote">&gt;&gt;&gt; not held. IIUC, since the release-acquire might not behave as a full memory</span>
<span class="quote">&gt;&gt;&gt; barrier, this requires an explicit memory barrier.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; So I&#39;m not entirely clear about this yet.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; How about:</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; 	CPU0				CPU1</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; 					tlb_gather_mmu()</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; 					lock PTLn</span>
<span class="quote">&gt;&gt; 					no mod</span>
<span class="quote">&gt;&gt; 					unlock PTLn</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; 	tlb_gather_mmu()</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; 					lock PTLm</span>
<span class="quote">&gt;&gt; 					mod</span>
<span class="quote">&gt;&gt; 					include in tlb range</span>
<span class="quote">&gt;&gt; 					unlock PTLm</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; 	lock PTLn</span>
<span class="quote">&gt;&gt; 	mod</span>
<span class="quote">&gt;&gt; 	unlock PTLn</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; 					tlb_finish_mmu()</span>
<span class="quote">&gt;&gt; 					  force = mm_tlb_flush_nested(tlb-&gt;mm);</span>
<span class="quote">&gt;&gt; 					  arch_tlb_finish_mmu(force);</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; 	... more ...</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; 	tlb_finish_mmu()</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; In this case you also want CPU1&#39;s mm_tlb_flush_nested() call to return</span>
<span class="quote">&gt;&gt; true, right?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; No, because CPU 1 mofified pte and added it into tlb range</span>
<span class="quote">&gt; so regardless of nested, it will flush TLB so there is no stale</span>
<span class="quote">&gt; TLB problem.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; But even with an smp_mb__after_atomic() at CPU0&#39;s tlg_bather_mmu()</span>
<span class="quote">&gt;&gt; you&#39;re not guaranteed CPU1 sees the increment. The only way to do that</span>
<span class="quote">&gt;&gt; is to make the PTL locks RCsc and that is a much more expensive</span>
<span class="quote">&gt;&gt; proposition.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; What about:</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; 	CPU0				CPU1</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; 					tlb_gather_mmu()</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; 					lock PTLn</span>
<span class="quote">&gt;&gt; 					no mod</span>
<span class="quote">&gt;&gt; 					unlock PTLn</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; 					lock PTLm</span>
<span class="quote">&gt;&gt; 					mod</span>
<span class="quote">&gt;&gt; 					include in tlb range</span>
<span class="quote">&gt;&gt; 					unlock PTLm</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; 	tlb_gather_mmu()</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; 	lock PTLn</span>
<span class="quote">&gt;&gt; 	mod</span>
<span class="quote">&gt;&gt; 	unlock PTLn</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; 					tlb_finish_mmu()</span>
<span class="quote">&gt;&gt; 					  force = mm_tlb_flush_nested(tlb-&gt;mm);</span>
<span class="quote">&gt;&gt; 					  arch_tlb_finish_mmu(force);</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; 	... more ...</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; 	tlb_finish_mmu()</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; Do we want CPU1 to see it here? If so, where does it end?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Ditto. Since CPU 1 has added range, it will flush TLB regardless</span>
<span class="quote">&gt; of nested condition.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; CPU0				CPU1</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; 					tlb_gather_mmu()</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; 					lock PTLn</span>
<span class="quote">&gt;&gt; 					no mod</span>
<span class="quote">&gt;&gt; 					unlock PTLn</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; 					lock PTLm</span>
<span class="quote">&gt;&gt; 					mod</span>
<span class="quote">&gt;&gt; 					include in tlb range</span>
<span class="quote">&gt;&gt; 					unlock PTLm</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; 					tlb_finish_mmu()</span>
<span class="quote">&gt;&gt; 					  force = mm_tlb_flush_nested(tlb-&gt;mm);</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; 	tlb_gather_mmu()</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; 	lock PTLn</span>
<span class="quote">&gt;&gt; 	mod</span>
<span class="quote">&gt;&gt; 	unlock PTLn</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; 					  arch_tlb_finish_mmu(force);</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; 	... more ...</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; 	tlb_finish_mmu()</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; This?</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; Could you clarify under what exact condition mm_tlb_flush_nested() must</span>
<span class="quote">&gt;&gt; return true?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; mm_tlb_flush_nested aims for the CPU side where there is no pte update</span>
<span class="quote">&gt; but need TLB flush.</span>
<span class="quote">&gt; As I wrote https://urldefense.proofpoint.com/v2/url?u=https-3A__marc.info_-3Fl-3Dlinux-2Dmm-26m-3D150267398226529-26w-3D2&amp;d=DwIDaQ&amp;c=uilaK90D4TOVoH58JNXRgQ&amp;r=x9zhXCtCLvTDtvE65-BGSA&amp;m=v2Z7eDi7z1H9zdngcjZvlNeBudWzA9KvcXFNpU2A77s&amp;s=amaSu_gurmBHHPcl3Pxfdl0Tk_uTnmf60tMQAsNDHVU&amp;e= ,</span>
<span class="quote">&gt; it has stable TLB problem if we don&#39;t flush TLB although there is no</span>
<span class="quote">&gt; pte modification.</span>

To clarify: the main problem that these patches address is when the first
CPU updates the PTE, and second CPU sees the updated value and thinks: “the
PTE is already what I wanted - no flush is needed”.

For some reason (I would assume intentional), all the examples here first
“do not modify” the PTE, and then modify it - which is not an “interesting”
case. However, based on what I understand on the memory barriers, I think
there is indeed a missing barrier before reading it in
mm_tlb_flush_nested(). IIUC using smp_mb__after_unlock_lock() in this case,
before reading, would solve the problem with least impact on systems with
strong memory ordering.

Minchan, as for the solution you proposed, it seems to open again a race,
since the “pending” indication is removed before the actual TLB flush is
performed.

Nadav
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36811">Minchan Kim</a> - Aug. 14, 2017, 5:23 a.m.</div>
<pre class="content">
On Mon, Aug 14, 2017 at 05:07:19AM +0000, Nadav Amit wrote:
&lt; snip &gt;
<span class="quote">
&gt; Minchan, as for the solution you proposed, it seems to open again a race,</span>
<span class="quote">&gt; since the “pending” indication is removed before the actual TLB flush is</span>
<span class="quote">&gt; performed.</span>

Oops, you&#39;re right!
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36811">Minchan Kim</a> - Aug. 14, 2017, 8:38 a.m.</div>
<pre class="content">
Hi Nadav,

On Mon, Aug 14, 2017 at 05:07:19AM +0000, Nadav Amit wrote:
&lt; snip &gt;
<span class="quote">
&gt; For some reason (I would assume intentional), all the examples here first</span>
<span class="quote">&gt; “do not modify” the PTE, and then modify it - which is not an “interesting”</span>
<span class="quote">&gt; case. However, based on what I understand on the memory barriers, I think</span>
<span class="quote">&gt; there is indeed a missing barrier before reading it in</span>
<span class="quote">&gt; mm_tlb_flush_nested(). IIUC using smp_mb__after_unlock_lock() in this case,</span>

memory-barrier.txt always scares me. I have read it for a while
and IIUC, it seems semantic of spin_unlock(&amp;same_pte) would be
enough without some memory-barrier inside mm_tlb_flush_nested.

I would be missing something totally.

Could you explain what kinds of sequence you have in mind to
have such problem?

Thanks.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - Aug. 14, 2017, 7:57 p.m.</div>
<pre class="content">
On Mon, Aug 14, 2017 at 05:38:39PM +0900, Minchan Kim wrote:
<span class="quote">&gt; memory-barrier.txt always scares me. I have read it for a while</span>
<span class="quote">&gt; and IIUC, it seems semantic of spin_unlock(&amp;same_pte) would be</span>
<span class="quote">&gt; enough without some memory-barrier inside mm_tlb_flush_nested.</span>

Indeed, see the email I just send. Its both spin_lock() and
spin_unlock() that we care about.

Aside from the semi permeable barrier of these primitives, RCpc ensures
these orderings only work against the _same_ lock variable.

Let me try and explain the ordering for PPC (which is by far the worst
we have in this regard):


spin_lock(lock)
{
	while (test_and_set(lock))
		cpu_relax();
	lwsync();
}


spin_unlock(lock)
{
	lwsync();
	clear(lock);
}

Now LWSYNC has fairly &#39;simple&#39; semantics, but with fairly horrible
ramifications. Consider LWSYNC to provide _local_ TSO ordering, this
means that it allows &#39;stores reordered after loads&#39;.

For the spin_lock() that implies that all load/store&#39;s inside the lock
do indeed stay in, but the ACQUIRE is only on the LOAD of the
test_and_set(). That is, the actual _set_ can leak in. After all it can
re-order stores after load (inside the lock).

For unlock it again means all load/store&#39;s prior stay prior, and the
RELEASE is on the store clearing the lock state (nothing surprising
here).

Now the _local_ part, the main take-away is that these orderings are
strictly CPU local. What makes the spinlock work across CPUs (as we&#39;d
very much expect it to) is the address dependency on the lock variable.

In order for the spin_lock() to succeed, it must observe the clear. Its
this link that crosses between the CPUs and builds the ordering. But
only the two CPUs agree on this order. A third CPU not involved in
this transaction can disagree on the order of events.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36811">Minchan Kim</a> - Aug. 16, 2017, 4:14 a.m.</div>
<pre class="content">
On Mon, Aug 14, 2017 at 09:57:23PM +0200, Peter Zijlstra wrote:
<span class="quote">&gt; On Mon, Aug 14, 2017 at 05:38:39PM +0900, Minchan Kim wrote:</span>
<span class="quote">&gt; &gt; memory-barrier.txt always scares me. I have read it for a while</span>
<span class="quote">&gt; &gt; and IIUC, it seems semantic of spin_unlock(&amp;same_pte) would be</span>
<span class="quote">&gt; &gt; enough without some memory-barrier inside mm_tlb_flush_nested.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Indeed, see the email I just send. Its both spin_lock() and</span>
<span class="quote">&gt; spin_unlock() that we care about.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Aside from the semi permeable barrier of these primitives, RCpc ensures</span>
<span class="quote">&gt; these orderings only work against the _same_ lock variable.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Let me try and explain the ordering for PPC (which is by far the worst</span>
<span class="quote">&gt; we have in this regard):</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; spin_lock(lock)</span>
<span class="quote">&gt; {</span>
<span class="quote">&gt; 	while (test_and_set(lock))</span>
<span class="quote">&gt; 		cpu_relax();</span>
<span class="quote">&gt; 	lwsync();</span>
<span class="quote">&gt; }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; spin_unlock(lock)</span>
<span class="quote">&gt; {</span>
<span class="quote">&gt; 	lwsync();</span>
<span class="quote">&gt; 	clear(lock);</span>
<span class="quote">&gt; }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Now LWSYNC has fairly &#39;simple&#39; semantics, but with fairly horrible</span>
<span class="quote">&gt; ramifications. Consider LWSYNC to provide _local_ TSO ordering, this</span>
<span class="quote">&gt; means that it allows &#39;stores reordered after loads&#39;.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; For the spin_lock() that implies that all load/store&#39;s inside the lock</span>
<span class="quote">&gt; do indeed stay in, but the ACQUIRE is only on the LOAD of the</span>
<span class="quote">&gt; test_and_set(). That is, the actual _set_ can leak in. After all it can</span>
<span class="quote">&gt; re-order stores after load (inside the lock).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; For unlock it again means all load/store&#39;s prior stay prior, and the</span>
<span class="quote">&gt; RELEASE is on the store clearing the lock state (nothing surprising</span>
<span class="quote">&gt; here).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Now the _local_ part, the main take-away is that these orderings are</span>
<span class="quote">&gt; strictly CPU local. What makes the spinlock work across CPUs (as we&#39;d</span>
<span class="quote">&gt; very much expect it to) is the address dependency on the lock variable.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; In order for the spin_lock() to succeed, it must observe the clear. Its</span>
<span class="quote">&gt; this link that crosses between the CPUs and builds the ordering. But</span>
<span class="quote">&gt; only the two CPUs agree on this order. A third CPU not involved in</span>
<span class="quote">&gt; this transaction can disagree on the order of events.</span>

The detail explanation in your previous reply makes me comfortable
from scary memory-barrier.txt but this reply makes me scared again. ;-)

Thanks for the kind clarification, Peter!
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">--- a/include/linux/mm_types.h</span>
<span class="p_header">+++ b/include/linux/mm_types.h</span>
<span class="p_chunk">@@ -537,13 +537,13 @@</span> <span class="p_context"> static inline bool mm_tlb_flush_pending(</span>
 {
 	/*
 	 * Must be called with PTL held; such that our PTL acquire will have
<span class="p_del">-	 * observed the store from set_tlb_flush_pending().</span>
<span class="p_add">+	 * observed the increment from inc_tlb_flush_pending().</span>
 	 */
<span class="p_del">-	return atomic_read(&amp;mm-&gt;tlb_flush_pending) &gt; 0;</span>
<span class="p_add">+	return atomic_read(&amp;mm-&gt;tlb_flush_pending);</span>
 }
 
 /*
<span class="p_del">- * Returns true if there are two above TLB batching threads in parallel.</span>
<span class="p_add">+ * Returns true if there are two or more TLB batching threads in parallel.</span>
  */
 static inline bool mm_tlb_flush_nested(struct mm_struct *mm)
 {
<span class="p_chunk">@@ -558,15 +558,12 @@</span> <span class="p_context"> static inline void init_tlb_flush_pendin</span>
 static inline void inc_tlb_flush_pending(struct mm_struct *mm)
 {
 	atomic_inc(&amp;mm-&gt;tlb_flush_pending);
<span class="p_del">-</span>
 	/*
<span class="p_del">-	 * The only time this value is relevant is when there are indeed pages</span>
<span class="p_del">-	 * to flush. And we&#39;ll only flush pages after changing them, which</span>
<span class="p_del">-	 * requires the PTL.</span>
<span class="p_del">-	 *</span>
 	 * So the ordering here is:
 	 *
 	 *	atomic_inc(&amp;mm-&gt;tlb_flush_pending);
<span class="p_add">+	 *	smp_mb__after_atomic();</span>
<span class="p_add">+	 *</span>
 	 *	spin_lock(&amp;ptl);
 	 *	...
 	 *	set_pte_at();
<span class="p_chunk">@@ -580,21 +577,30 @@</span> <span class="p_context"> static inline void inc_tlb_flush_pending</span>
 	 *	flush_tlb_range();
 	 *	atomic_dec(&amp;mm-&gt;tlb_flush_pending);
 	 *
<span class="p_del">-	 * So the =true store is constrained by the PTL unlock, and the =false</span>
<span class="p_del">-	 * store is constrained by the TLB invalidate.</span>
<span class="p_add">+	 * Where we order the increment against the PTE modification with the</span>
<span class="p_add">+	 * smp_mb__after_atomic(). It would appear that the spin_unlock(&amp;ptl)</span>
<span class="p_add">+	 * is sufficient to constrain the inc, because we only care about the</span>
<span class="p_add">+	 * value if there is indeed a pending PTE modification. However with</span>
<span class="p_add">+	 * SPLIT_PTE_PTLOCKS and RCpc locks (PPC) the UNLOCK of one lock does</span>
<span class="p_add">+	 * not order against the LOCK of another lock.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * The decrement is ordered by the flush_tlb_range(), such that</span>
<span class="p_add">+	 * mm_tlb_flush_pending() will not return false unless all flushes have</span>
<span class="p_add">+	 * completed.</span>
 	 */
<span class="p_add">+	smp_mb__after_atomic();</span>
 }
 
<span class="p_del">-/* Clearing is done after a TLB flush, which also provides a barrier. */</span>
 static inline void dec_tlb_flush_pending(struct mm_struct *mm)
 {
 	/*
<span class="p_del">-	 * Guarantee that the tlb_flush_pending does not not leak into the</span>
<span class="p_del">-	 * critical section, since we must order the PTE change and changes to</span>
<span class="p_del">-	 * the pending TLB flush indication. We could have relied on TLB flush</span>
<span class="p_del">-	 * as a memory barrier, but this behavior is not clearly documented.</span>
<span class="p_add">+	 * See inc_tlb_flush_pending().</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * This cannot be smp_mb__before_atomic() because smp_mb() simply does</span>
<span class="p_add">+	 * not order against TLB invalidate completion, which is what we need.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * Therefore we must rely on tlb_flush_*() to guarantee order.</span>
 	 */
<span class="p_del">-	smp_mb__before_atomic();</span>
 	atomic_dec(&amp;mm-&gt;tlb_flush_pending);
 }
 

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



