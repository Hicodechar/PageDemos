
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v6,03/11] mm, x86: Add support for eXclusive Page Frame Ownership (XPFO) - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v6,03/11] mm, x86: Add support for eXclusive Page Frame Ownership (XPFO)</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=173139">Tycho Andersen</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Sept. 7, 2017, 5:36 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20170907173609.22696-4-tycho@docker.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9942607/mbox/"
   >mbox</a>
|
   <a href="/patch/9942607/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9942607/">/patch/9942607/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	50BE3600CB for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  7 Sep 2017 17:39:31 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 41474285C2
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  7 Sep 2017 17:39:31 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 35EFD2861A; Thu,  7 Sep 2017 17:39:31 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.5 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	DKIM_VALID, DKIM_VALID_AU, RCVD_IN_DNSWL_HI,
	RCVD_IN_SORBS_SPAM autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id C1B60285C2
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  7 Sep 2017 17:39:29 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1755820AbdIGRj1 (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 7 Sep 2017 13:39:27 -0400
Received: from mail-io0-f181.google.com ([209.85.223.181]:36545 &quot;EHLO
	mail-io0-f181.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1755484AbdIGRhH (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 7 Sep 2017 13:37:07 -0400
Received: by mail-io0-f181.google.com with SMTP id d16so831561ioj.3
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Thu, 07 Sep 2017 10:37:07 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=docker.com; s=google;
	h=from:to:cc:subject:date:message-id:in-reply-to:references;
	bh=u6wLf33YdYfsHzbX0yX7LvZEWve4b06Tzk6xErCNzeE=;
	b=WkCclUJ48RNq3ETz3F5r+OS7nMTPjLSzCQRJiT5Tjb/AGBY8izxhlbFLnPmCbPxx76
	YzrBC9VuIX0AFw4k7IuTyMPv1TWx44+21Av3/4GJr5eS0qIQSgNtkyYprfEcqzYpYUQq
	TBjQ5qpgYz+rsRtxm+Xw93/L6FfrFq1rLVlYk=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20161025;
	h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
	:references;
	bh=u6wLf33YdYfsHzbX0yX7LvZEWve4b06Tzk6xErCNzeE=;
	b=ZUiwfkARLQzc/UtTMxuMUdTORF2//p8PY8MvAAsQjLVjeBqj2c8vKpP5ZCbYN6HiUv
	uJ1Zz6uN0vsZ4++tQbH/d7JlD7gMSQN7LPhOfXYB60nIzqvhwHT049FsoaXCfuSwRVBZ
	eVSjKdy3pdzx8xEpmkAWJBwNjH2BcsWmWoKZJGQtdzIyWwsHk2toZa82v8fsUQg6NthC
	Kpl5kXCMmM4+AxlLiq4GbEASDUY7vxAFGMG+k6kJnErWFFOSaiZ3WLLhhBadtahF5uU9
	vRRk01iyCGUPWMPMWWP6JDdTMi+qitrC3EZfOLg82mTrS49Ra2kW9/qty+D/X8Qu50It
	g/8Q==
X-Gm-Message-State: AHPjjUgFdW8k3gR/AaGriV15qStAAIF+aQHt2GdWqjRYflvWnL6y7vHR
	bP1qvtSHSEUQ82/aSI5YzQ==
X-Google-Smtp-Source: AOwi7QCENs8CtkPmhu+tDiQ5md68l6JBq/3+30FSjqbAwP5aWwSaIYxuOvQgsC0Mrs9ThF+7C1aOpw==
X-Received: by 10.107.19.170 with SMTP id 42mr72927iot.315.1504805826096;
	Thu, 07 Sep 2017 10:37:06 -0700 (PDT)
Received: from localhost.localdomain ([8.24.24.129])
	by smtp.gmail.com with ESMTPSA id
	t127sm94404iod.26.2017.09.07.10.37.04
	(version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
	Thu, 07 Sep 2017 10:37:05 -0700 (PDT)
From: Tycho Andersen &lt;tycho@docker.com&gt;
To: linux-kernel@vger.kernel.org
Cc: linux-mm@kvack.org, kernel-hardening@lists.openwall.com,
	Marco Benatto &lt;marco.antonio.780@gmail.com&gt;,
	Juerg Haefliger &lt;juerg.haefliger@canonical.com&gt;,
	x86@kernel.org, Tycho Andersen &lt;tycho@docker.com&gt;
Subject: [PATCH v6 03/11] mm,
	x86: Add support for eXclusive Page Frame Ownership (XPFO)
Date: Thu,  7 Sep 2017 11:36:01 -0600
Message-Id: &lt;20170907173609.22696-4-tycho@docker.com&gt;
X-Mailer: git-send-email 2.11.0
In-Reply-To: &lt;20170907173609.22696-1-tycho@docker.com&gt;
References: &lt;20170907173609.22696-1-tycho@docker.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=173139">Tycho Andersen</a> - Sept. 7, 2017, 5:36 p.m.</div>
<pre class="content">
<span class="from">From: Juerg Haefliger &lt;juerg.haefliger@canonical.com&gt;</span>

This patch adds support for XPFO which protects against &#39;ret2dir&#39; kernel
attacks. The basic idea is to enforce exclusive ownership of page frames
by either the kernel or userspace, unless explicitly requested by the
kernel. Whenever a page destined for userspace is allocated, it is
unmapped from physmap (the kernel&#39;s page table). When such a page is
reclaimed from userspace, it is mapped back to physmap.

Additional fields in the page_ext struct are used for XPFO housekeeping,
specifically:
  - two flags to distinguish user vs. kernel pages and to tag unmapped
    pages.
  - a reference counter to balance kmap/kunmap operations.
  - a lock to serialize access to the XPFO fields.

This patch is based on the work of Vasileios P. Kemerlis et al. who
published their work in this paper:
  http://www.cs.columbia.edu/~vpk/papers/ret2dir.sec14.pdf

v6: * use flush_tlb_kernel_range() instead of __flush_tlb_one, so we flush
      the tlb entry on all CPUs when unmapping it in kunmap
    * handle lookup_page_ext()/lookup_xpfo() returning NULL
    * drop lots of BUG()s in favor of WARN()
    * don&#39;t disable irqs in xpfo_kmap/xpfo_kunmap, export
      __split_large_page so we can do our own alloc_pages(GFP_ATOMIC) to
      pass it

CC: x86@kernel.org
Suggested-by: Vasileios P. Kemerlis &lt;vpk@cs.columbia.edu&gt;
<span class="signed-off-by">Signed-off-by: Juerg Haefliger &lt;juerg.haefliger@canonical.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Tycho Andersen &lt;tycho@docker.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Marco Benatto &lt;marco.antonio.780@gmail.com&gt;</span>
---
 Documentation/admin-guide/kernel-parameters.txt |   2 +
 arch/x86/Kconfig                                |   1 +
 arch/x86/include/asm/pgtable.h                  |  25 +++
 arch/x86/mm/Makefile                            |   1 +
 arch/x86/mm/pageattr.c                          |  22 +--
 arch/x86/mm/xpfo.c                              | 114 ++++++++++++
 include/linux/highmem.h                         |  15 +-
 include/linux/xpfo.h                            |  42 +++++
 mm/Makefile                                     |   1 +
 mm/page_alloc.c                                 |   2 +
 mm/page_ext.c                                   |   4 +
 mm/xpfo.c                                       | 222 ++++++++++++++++++++++++
 security/Kconfig                                |  19 ++
 13 files changed, 449 insertions(+), 21 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=176285">Ralph Campbell</a> - Sept. 7, 2017, 6:33 p.m.</div>
<pre class="content">
<span class="quote">&gt; -----Original Message-----</span>
<span class="quote">&gt; From: owner-linux-mm@kvack.org [mailto:owner-linux-mm@kvack.org] On</span>
<span class="quote">&gt; Behalf Of Tycho Andersen</span>
<span class="quote">&gt; Sent: Thursday, September 7, 2017 10:36 AM</span>
<span class="quote">&gt; To: linux-kernel@vger.kernel.org</span>
<span class="quote">&gt; Cc: linux-mm@kvack.org; kernel-hardening@lists.openwall.com; Marco Benatto</span>
<span class="quote">&gt; &lt;marco.antonio.780@gmail.com&gt;; Juerg Haefliger</span>
<span class="quote">&gt; &lt;juerg.haefliger@canonical.com&gt;; x86@kernel.org; Tycho Andersen</span>
<span class="quote">&gt; &lt;tycho@docker.com&gt;</span>
<span class="quote">&gt; Subject: [PATCH v6 03/11] mm, x86: Add support for eXclusive Page Frame</span>
<span class="quote">&gt; Ownership (XPFO)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; From: Juerg Haefliger &lt;juerg.haefliger@canonical.com&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This patch adds support for XPFO which protects against &#39;ret2dir&#39; kernel attacks.</span>
<span class="quote">&gt; The basic idea is to enforce exclusive ownership of page frames by either the</span>
<span class="quote">&gt; kernel or userspace, unless explicitly requested by the kernel. Whenever a page</span>
<span class="quote">&gt; destined for userspace is allocated, it is unmapped from physmap (the kernel&#39;s</span>
<span class="quote">&gt; page table). When such a page is reclaimed from userspace, it is mapped back to</span>
<span class="quote">&gt; physmap.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Additional fields in the page_ext struct are used for XPFO housekeeping,</span>
<span class="quote">&gt; specifically:</span>
<span class="quote">&gt;   - two flags to distinguish user vs. kernel pages and to tag unmapped</span>
<span class="quote">&gt;     pages.</span>
<span class="quote">&gt;   - a reference counter to balance kmap/kunmap operations.</span>
<span class="quote">&gt;   - a lock to serialize access to the XPFO fields.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This patch is based on the work of Vasileios P. Kemerlis et al. who published their</span>
<span class="quote">&gt; work in this paper:</span>
<span class="quote">&gt;   http://www.cs.columbia.edu/~vpk/papers/ret2dir.sec14.pdf</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; v6: * use flush_tlb_kernel_range() instead of __flush_tlb_one, so we flush</span>
<span class="quote">&gt;       the tlb entry on all CPUs when unmapping it in kunmap</span>
<span class="quote">&gt;     * handle lookup_page_ext()/lookup_xpfo() returning NULL</span>
<span class="quote">&gt;     * drop lots of BUG()s in favor of WARN()</span>
<span class="quote">&gt;     * don&#39;t disable irqs in xpfo_kmap/xpfo_kunmap, export</span>
<span class="quote">&gt;       __split_large_page so we can do our own alloc_pages(GFP_ATOMIC) to</span>
<span class="quote">&gt;       pass it</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; CC: x86@kernel.org</span>
<span class="quote">&gt; Suggested-by: Vasileios P. Kemerlis &lt;vpk@cs.columbia.edu&gt;</span>
<span class="quote">&gt; Signed-off-by: Juerg Haefliger &lt;juerg.haefliger@canonical.com&gt;</span>
<span class="quote">&gt; Signed-off-by: Tycho Andersen &lt;tycho@docker.com&gt;</span>
<span class="quote">&gt; Signed-off-by: Marco Benatto &lt;marco.antonio.780@gmail.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  Documentation/admin-guide/kernel-parameters.txt |   2 +</span>
<span class="quote">&gt;  arch/x86/Kconfig                                |   1 +</span>
<span class="quote">&gt;  arch/x86/include/asm/pgtable.h                  |  25 +++</span>
<span class="quote">&gt;  arch/x86/mm/Makefile                            |   1 +</span>
<span class="quote">&gt;  arch/x86/mm/pageattr.c                          |  22 +--</span>
<span class="quote">&gt;  arch/x86/mm/xpfo.c                              | 114 ++++++++++++</span>
<span class="quote">&gt;  include/linux/highmem.h                         |  15 +-</span>
<span class="quote">&gt;  include/linux/xpfo.h                            |  42 +++++</span>
<span class="quote">&gt;  mm/Makefile                                     |   1 +</span>
<span class="quote">&gt;  mm/page_alloc.c                                 |   2 +</span>
<span class="quote">&gt;  mm/page_ext.c                                   |   4 +</span>
<span class="quote">&gt;  mm/xpfo.c                                       | 222 ++++++++++++++++++++++++</span>
<span class="quote">&gt;  security/Kconfig                                |  19 ++</span>
<span class="quote">&gt;  13 files changed, 449 insertions(+), 21 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/Documentation/admin-guide/kernel-parameters.txt</span>
<span class="quote">&gt; b/Documentation/admin-guide/kernel-parameters.txt</span>
<span class="quote">&gt; index d9c171ce4190..444d83183f75 100644</span>
<span class="quote">&gt; --- a/Documentation/admin-guide/kernel-parameters.txt</span>
<span class="quote">&gt; +++ b/Documentation/admin-guide/kernel-parameters.txt</span>
<span class="quote">&gt; @@ -2736,6 +2736,8 @@</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	nox2apic	[X86-64,APIC] Do not enable x2APIC mode.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; +	noxpfo		[X86-64] Disable XPFO when CONFIG_XPFO is on.</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	cpu0_hotplug	[X86] Turn on CPU0 hotplug feature when</span>
<span class="quote">&gt;  			CONFIG_BOOTPARAM_HOTPLUG_CPU0 is off.</span>
<span class="quote">&gt;  			Some features depend on CPU0. Known dependencies</span>
&lt;... snip&gt;

A bit more description for system administrators would be very useful.
Perhaps something like:

noxpfo		[XPFO,X86-64] Disable eXclusive Page Frame Ownership (XPFO)
                             Physical pages mapped into user applications will also be mapped
                             in the kernel&#39;s address space as if CONFIG_XPFO was not enabled.

Patch 05 should also update kernel-parameters.txt and add &quot;ARM64&quot; to the config option list for noxpfo.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=173139">Tycho Andersen</a> - Sept. 7, 2017, 6:50 p.m.</div>
<pre class="content">
On Thu, Sep 07, 2017 at 06:33:09PM +0000, Ralph Campbell wrote:
<span class="quote">&gt; &gt; --- a/Documentation/admin-guide/kernel-parameters.txt</span>
<span class="quote">&gt; &gt; +++ b/Documentation/admin-guide/kernel-parameters.txt</span>
<span class="quote">&gt; &gt; @@ -2736,6 +2736,8 @@</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt;  	nox2apic	[X86-64,APIC] Do not enable x2APIC mode.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; +	noxpfo		[X86-64] Disable XPFO when CONFIG_XPFO is on.</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt;  	cpu0_hotplug	[X86] Turn on CPU0 hotplug feature when</span>
<span class="quote">&gt; &gt;  			CONFIG_BOOTPARAM_HOTPLUG_CPU0 is off.</span>
<span class="quote">&gt; &gt;  			Some features depend on CPU0. Known dependencies</span>
<span class="quote">&gt; &lt;... snip&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; A bit more description for system administrators would be very useful.</span>
<span class="quote">&gt; Perhaps something like:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; noxpfo		[XPFO,X86-64] Disable eXclusive Page Frame Ownership (XPFO)</span>
<span class="quote">&gt;                              Physical pages mapped into user applications will also be mapped</span>
<span class="quote">&gt;                              in the kernel&#39;s address space as if CONFIG_XPFO was not enabled.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Patch 05 should also update kernel-parameters.txt and add &quot;ARM64&quot; to the config option list for noxpfo.</span>

Nice catch, thanks. I&#39;ll fix both.

Cheers,

Tycho
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=101">Christoph Hellwig</a> - Sept. 8, 2017, 7:51 a.m.</div>
<pre class="content">
I think this patch needs to be split into the generic mm code, and
the x86 arch code at least.
<span class="quote">
&gt; +/*</span>
<span class="quote">&gt; + * The current flushing context - we pass it instead of 5 arguments:</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +struct cpa_data {</span>
<span class="quote">&gt; +	unsigned long	*vaddr;</span>
<span class="quote">&gt; +	pgd_t		*pgd;</span>
<span class="quote">&gt; +	pgprot_t	mask_set;</span>
<span class="quote">&gt; +	pgprot_t	mask_clr;</span>
<span class="quote">&gt; +	unsigned long	numpages;</span>
<span class="quote">&gt; +	int		flags;</span>
<span class="quote">&gt; +	unsigned long	pfn;</span>
<span class="quote">&gt; +	unsigned	force_split : 1;</span>
<span class="quote">&gt; +	int		curpage;</span>
<span class="quote">&gt; +	struct page	**pages;</span>
<span class="quote">&gt; +};</span>

Fitting these 10 variables into 5 arguments would require an awesome
compression scheme anyway :)
<span class="quote">
&gt; +			if  (__split_large_page(&amp;cpa, pte, (unsigned long)kaddr, base) &lt; 0)</span>

Overly long line.
<span class="quote">
&gt; +#include &lt;linux/xpfo.h&gt;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #include &lt;asm/cacheflush.h&gt;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -55,24 +56,34 @@ static inline struct page *kmap_to_page(void *addr)</span>
<span class="quote">&gt;  #ifndef ARCH_HAS_KMAP</span>
<span class="quote">&gt;  static inline void *kmap(struct page *page)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; +	void *kaddr;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	might_sleep();</span>
<span class="quote">&gt; -	return page_address(page);</span>
<span class="quote">&gt; +	kaddr = page_address(page);</span>
<span class="quote">&gt; +	xpfo_kmap(kaddr, page);</span>
<span class="quote">&gt; +	return kaddr;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static inline void kunmap(struct page *page)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; +	xpfo_kunmap(page_address(page), page);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static inline void *kmap_atomic(struct page *page)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; +	void *kaddr;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	preempt_disable();</span>
<span class="quote">&gt;  	pagefault_disable();</span>
<span class="quote">&gt; -	return page_address(page);</span>
<span class="quote">&gt; +	kaddr = page_address(page);</span>
<span class="quote">&gt; +	xpfo_kmap(kaddr, page);</span>
<span class="quote">&gt; +	return kaddr;</span>
<span class="quote">&gt;  }</span>

It seems to me like we should simply direct to pure xpfo
implementations for the !HIGHMEM &amp;&amp; XPFO case. - that is
just have the prototypes for kmap, kunmap and co in
linux/highmem.h and implement them in xpfo under those names.

Instead of sprinkling them around.
<span class="quote">
&gt; +DEFINE_STATIC_KEY_FALSE(xpfo_inited);</span>

s/inited/initialized/g ?
<span class="quote">
&gt; +	bool &quot;Enable eXclusive Page Frame Ownership (XPFO)&quot;</span>
<span class="quote">&gt; +	default n</span>

default n is the default, so you can remove this line.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=173139">Tycho Andersen</a> - Sept. 8, 2017, 2:58 p.m.</div>
<pre class="content">
On Fri, Sep 08, 2017 at 12:51:40AM -0700, Christoph Hellwig wrote:
<span class="quote">&gt; &gt; +#include &lt;linux/xpfo.h&gt;</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  #include &lt;asm/cacheflush.h&gt;</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; @@ -55,24 +56,34 @@ static inline struct page *kmap_to_page(void *addr)</span>
<span class="quote">&gt; &gt;  #ifndef ARCH_HAS_KMAP</span>
<span class="quote">&gt; &gt;  static inline void *kmap(struct page *page)</span>
<span class="quote">&gt; &gt;  {</span>
<span class="quote">&gt; &gt; +	void *kaddr;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt;  	might_sleep();</span>
<span class="quote">&gt; &gt; -	return page_address(page);</span>
<span class="quote">&gt; &gt; +	kaddr = page_address(page);</span>
<span class="quote">&gt; &gt; +	xpfo_kmap(kaddr, page);</span>
<span class="quote">&gt; &gt; +	return kaddr;</span>
<span class="quote">&gt; &gt;  }</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  static inline void kunmap(struct page *page)</span>
<span class="quote">&gt; &gt;  {</span>
<span class="quote">&gt; &gt; +	xpfo_kunmap(page_address(page), page);</span>
<span class="quote">&gt; &gt;  }</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  static inline void *kmap_atomic(struct page *page)</span>
<span class="quote">&gt; &gt;  {</span>
<span class="quote">&gt; &gt; +	void *kaddr;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt;  	preempt_disable();</span>
<span class="quote">&gt; &gt;  	pagefault_disable();</span>
<span class="quote">&gt; &gt; -	return page_address(page);</span>
<span class="quote">&gt; &gt; +	kaddr = page_address(page);</span>
<span class="quote">&gt; &gt; +	xpfo_kmap(kaddr, page);</span>
<span class="quote">&gt; &gt; +	return kaddr;</span>
<span class="quote">&gt; &gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It seems to me like we should simply direct to pure xpfo</span>
<span class="quote">&gt; implementations for the !HIGHMEM &amp;&amp; XPFO case. - that is</span>
<span class="quote">&gt; just have the prototypes for kmap, kunmap and co in</span>
<span class="quote">&gt; linux/highmem.h and implement them in xpfo under those names.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Instead of sprinkling them around.</span>

Ok, IIUC we&#39;ll still need a #ifdef CONFIG_XPFO in this file, but at
least the implementations here won&#39;t have a diff. I&#39;ll make this
change, and all the others you&#39;ve suggested.

Thanks!

Tycho
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=130411">Laura Abbott</a> - Sept. 9, 2017, 3:35 p.m.</div>
<pre class="content">
On 09/07/2017 10:36 AM, Tycho Andersen wrote:
<span class="quote">&gt; +static inline struct xpfo *lookup_xpfo(struct page *page)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct page_ext *page_ext = lookup_page_ext(page);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (unlikely(!page_ext)) {</span>
<span class="quote">&gt; +		WARN(1, &quot;xpfo: failed to get page ext&quot;);</span>
<span class="quote">&gt; +		return NULL;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return (void *)page_ext + page_xpfo_ops.offset;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>

Just drop the WARN. On my arm64 UEFI machine this spews warnings
under most normal operation. This should be normal for some
situations but I haven&#39;t had the time to dig into why this
is so pronounced on arm64.

Thanks,
Laura
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=169781">Xie Yisheng</a> - Sept. 11, 2017, 7:24 a.m.</div>
<pre class="content">
Hi Tycho,

On 2017/9/8 1:36, Tycho Andersen wrote:
<span class="quote">&gt; From: Juerg Haefliger &lt;juerg.haefliger@canonical.com&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This patch adds support for XPFO which protects against &#39;ret2dir&#39; kernel</span>
<span class="quote">&gt; attacks. The basic idea is to enforce exclusive ownership of page frames</span>
<span class="quote">&gt; by either the kernel or userspace, unless explicitly requested by the</span>
<span class="quote">&gt; kernel. Whenever a page destined for userspace is allocated, it is</span>
<span class="quote">&gt; unmapped from physmap (the kernel&#39;s page table). When such a page is</span>
<span class="quote">&gt; reclaimed from userspace, it is mapped back to physmap.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Additional fields in the page_ext struct are used for XPFO housekeeping,</span>
<span class="quote">&gt; specifically:</span>
<span class="quote">&gt;   - two flags to distinguish user vs. kernel pages and to tag unmapped</span>
<span class="quote">&gt;     pages.</span>
<span class="quote">&gt;   - a reference counter to balance kmap/kunmap operations.</span>
<span class="quote">&gt;   - a lock to serialize access to the XPFO fields.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This patch is based on the work of Vasileios P. Kemerlis et al. who</span>
<span class="quote">&gt; published their work in this paper:</span>
<span class="quote">&gt;   http://www.cs.columbia.edu/~vpk/papers/ret2dir.sec14.pdf</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; [...]</span>
<span class="quote">&gt; +void xpfo_alloc_pages(struct page *page, int order, gfp_t gfp)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	int i, flush_tlb = 0;</span>
<span class="quote">&gt; +	struct xpfo *xpfo;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (!static_branch_unlikely(&amp;xpfo_inited))</span>
<span class="quote">&gt; +		return;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	for (i = 0; i &lt; (1 &lt;&lt; order); i++)  {</span>
<span class="quote">&gt; +		xpfo = lookup_xpfo(page + i);</span>
<span class="quote">&gt; +		if (!xpfo)</span>
<span class="quote">&gt; +			continue;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		WARN(test_bit(XPFO_PAGE_UNMAPPED, &amp;xpfo-&gt;flags),</span>
<span class="quote">&gt; +		     &quot;xpfo: unmapped page being allocated\n&quot;);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		/* Initialize the map lock and map counter */</span>
<span class="quote">&gt; +		if (unlikely(!xpfo-&gt;inited)) {</span>
<span class="quote">&gt; +			spin_lock_init(&amp;xpfo-&gt;maplock);</span>
<span class="quote">&gt; +			atomic_set(&amp;xpfo-&gt;mapcount, 0);</span>
<span class="quote">&gt; +			xpfo-&gt;inited = true;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +		WARN(atomic_read(&amp;xpfo-&gt;mapcount),</span>
<span class="quote">&gt; +		     &quot;xpfo: already mapped page being allocated\n&quot;);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		if ((gfp &amp; GFP_HIGHUSER) == GFP_HIGHUSER) {</span>
<span class="quote">&gt; +			/*</span>
<span class="quote">&gt; +			 * Tag the page as a user page and flush the TLB if it</span>
<span class="quote">&gt; +			 * was previously allocated to the kernel.</span>
<span class="quote">&gt; +			 */</span>
<span class="quote">&gt; +			if (!test_and_set_bit(XPFO_PAGE_USER, &amp;xpfo-&gt;flags))</span>
<span class="quote">&gt; +				flush_tlb = 1;</span>

I&#39;m not sure whether I am miss anything, however, when the page was previously allocated
to kernel,  should we unmap the physmap (the kernel&#39;s page table) here? For we allocate
the page to user now

Yisheng Xie
Thanks
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=173139">Tycho Andersen</a> - Sept. 11, 2017, 2:50 p.m.</div>
<pre class="content">
Hi Yisheng,

On Mon, Sep 11, 2017 at 03:24:09PM +0800, Yisheng Xie wrote:
<span class="quote">&gt; &gt; +void xpfo_alloc_pages(struct page *page, int order, gfp_t gfp)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	int i, flush_tlb = 0;</span>
<span class="quote">&gt; &gt; +	struct xpfo *xpfo;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	if (!static_branch_unlikely(&amp;xpfo_inited))</span>
<span class="quote">&gt; &gt; +		return;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	for (i = 0; i &lt; (1 &lt;&lt; order); i++)  {</span>
<span class="quote">&gt; &gt; +		xpfo = lookup_xpfo(page + i);</span>
<span class="quote">&gt; &gt; +		if (!xpfo)</span>
<span class="quote">&gt; &gt; +			continue;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		WARN(test_bit(XPFO_PAGE_UNMAPPED, &amp;xpfo-&gt;flags),</span>
<span class="quote">&gt; &gt; +		     &quot;xpfo: unmapped page being allocated\n&quot;);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		/* Initialize the map lock and map counter */</span>
<span class="quote">&gt; &gt; +		if (unlikely(!xpfo-&gt;inited)) {</span>
<span class="quote">&gt; &gt; +			spin_lock_init(&amp;xpfo-&gt;maplock);</span>
<span class="quote">&gt; &gt; +			atomic_set(&amp;xpfo-&gt;mapcount, 0);</span>
<span class="quote">&gt; &gt; +			xpfo-&gt;inited = true;</span>
<span class="quote">&gt; &gt; +		}</span>
<span class="quote">&gt; &gt; +		WARN(atomic_read(&amp;xpfo-&gt;mapcount),</span>
<span class="quote">&gt; &gt; +		     &quot;xpfo: already mapped page being allocated\n&quot;);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		if ((gfp &amp; GFP_HIGHUSER) == GFP_HIGHUSER) {</span>
<span class="quote">&gt; &gt; +			/*</span>
<span class="quote">&gt; &gt; +			 * Tag the page as a user page and flush the TLB if it</span>
<span class="quote">&gt; &gt; +			 * was previously allocated to the kernel.</span>
<span class="quote">&gt; &gt; +			 */</span>
<span class="quote">&gt; &gt; +			if (!test_and_set_bit(XPFO_PAGE_USER, &amp;xpfo-&gt;flags))</span>
<span class="quote">&gt; &gt; +				flush_tlb = 1;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I&#39;m not sure whether I am miss anything, however, when the page was previously allocated</span>
<span class="quote">&gt; to kernel,  should we unmap the physmap (the kernel&#39;s page table) here? For we allocate</span>
<span class="quote">&gt; the page to user now</span>

Yes, I think you&#39;re right. Oddly, the XPFO_READ_USER test works
correctly for me, but I think (?) should not because of this bug...

Tycho
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=173139">Tycho Andersen</a> - Sept. 11, 2017, 3:03 p.m.</div>
<pre class="content">
On Sat, Sep 09, 2017 at 08:35:17AM -0700, Laura Abbott wrote:
<span class="quote">&gt; On 09/07/2017 10:36 AM, Tycho Andersen wrote:</span>
<span class="quote">&gt; &gt; +static inline struct xpfo *lookup_xpfo(struct page *page)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	struct page_ext *page_ext = lookup_page_ext(page);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	if (unlikely(!page_ext)) {</span>
<span class="quote">&gt; &gt; +		WARN(1, &quot;xpfo: failed to get page ext&quot;);</span>
<span class="quote">&gt; &gt; +		return NULL;</span>
<span class="quote">&gt; &gt; +	}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	return (void *)page_ext + page_xpfo_ops.offset;</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Just drop the WARN. On my arm64 UEFI machine this spews warnings</span>
<span class="quote">&gt; under most normal operation. This should be normal for some</span>
<span class="quote">&gt; situations but I haven&#39;t had the time to dig into why this</span>
<span class="quote">&gt; is so pronounced on arm64.</span>

Will do, thanks! If you figure out under what conditions it&#39;s normal,
I&#39;d be curious :)

Tycho
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=176135">Juerg Haefliger</a> - Sept. 11, 2017, 4:03 p.m.</div>
<pre class="content">
On 09/11/2017 04:50 PM, Tycho Andersen wrote:
<span class="quote">&gt; Hi Yisheng,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; On Mon, Sep 11, 2017 at 03:24:09PM +0800, Yisheng Xie wrote:</span>
<span class="quote">&gt;&gt;&gt; +void xpfo_alloc_pages(struct page *page, int order, gfp_t gfp)</span>
<span class="quote">&gt;&gt;&gt; +{</span>
<span class="quote">&gt;&gt;&gt; +	int i, flush_tlb = 0;</span>
<span class="quote">&gt;&gt;&gt; +	struct xpfo *xpfo;</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; +	if (!static_branch_unlikely(&amp;xpfo_inited))</span>
<span class="quote">&gt;&gt;&gt; +		return;</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; +	for (i = 0; i &lt; (1 &lt;&lt; order); i++)  {</span>
<span class="quote">&gt;&gt;&gt; +		xpfo = lookup_xpfo(page + i);</span>
<span class="quote">&gt;&gt;&gt; +		if (!xpfo)</span>
<span class="quote">&gt;&gt;&gt; +			continue;</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; +		WARN(test_bit(XPFO_PAGE_UNMAPPED, &amp;xpfo-&gt;flags),</span>
<span class="quote">&gt;&gt;&gt; +		     &quot;xpfo: unmapped page being allocated\n&quot;);</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; +		/* Initialize the map lock and map counter */</span>
<span class="quote">&gt;&gt;&gt; +		if (unlikely(!xpfo-&gt;inited)) {</span>
<span class="quote">&gt;&gt;&gt; +			spin_lock_init(&amp;xpfo-&gt;maplock);</span>
<span class="quote">&gt;&gt;&gt; +			atomic_set(&amp;xpfo-&gt;mapcount, 0);</span>
<span class="quote">&gt;&gt;&gt; +			xpfo-&gt;inited = true;</span>
<span class="quote">&gt;&gt;&gt; +		}</span>
<span class="quote">&gt;&gt;&gt; +		WARN(atomic_read(&amp;xpfo-&gt;mapcount),</span>
<span class="quote">&gt;&gt;&gt; +		     &quot;xpfo: already mapped page being allocated\n&quot;);</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; +		if ((gfp &amp; GFP_HIGHUSER) == GFP_HIGHUSER) {</span>
<span class="quote">&gt;&gt;&gt; +			/*</span>
<span class="quote">&gt;&gt;&gt; +			 * Tag the page as a user page and flush the TLB if it</span>
<span class="quote">&gt;&gt;&gt; +			 * was previously allocated to the kernel.</span>
<span class="quote">&gt;&gt;&gt; +			 */</span>
<span class="quote">&gt;&gt;&gt; +			if (!test_and_set_bit(XPFO_PAGE_USER, &amp;xpfo-&gt;flags))</span>
<span class="quote">&gt;&gt;&gt; +				flush_tlb = 1;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; I&#39;m not sure whether I am miss anything, however, when the page was previously allocated</span>
<span class="quote">&gt;&gt; to kernel,  should we unmap the physmap (the kernel&#39;s page table) here? For we allocate</span>
<span class="quote">&gt;&gt; the page to user now</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt; Yes, I think you&#39;re right. Oddly, the XPFO_READ_USER test works</span>
<span class="quote">&gt; correctly for me, but I think (?) should not because of this bug...</span>

IIRC, this is an optimization carried forward from the initial
implementation. The assumption is that the kernel will map the user
buffer so it&#39;s not unmapped on allocation but only on the first (and
subsequent) call of kunmap. I.e.:
 - alloc  -&gt; noop
 - kmap   -&gt; noop
 - kunmap -&gt; unmapped from the kernel
 - kmap   -&gt; mapped into the kernel
 - kunmap -&gt; unmapped from the kernel
and so on until:
 - free   -&gt; mapped back into the kernel

I&#39;m not sure if that make sense though since it leaves a window.

...Juerg
<span class="quote">


&gt; Tycho</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=173139">Tycho Andersen</a> - Sept. 11, 2017, 4:59 p.m.</div>
<pre class="content">
On Mon, Sep 11, 2017 at 06:03:55PM +0200, Juerg Haefliger wrote:
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; On 09/11/2017 04:50 PM, Tycho Andersen wrote:</span>
<span class="quote">&gt; &gt; Hi Yisheng,</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; On Mon, Sep 11, 2017 at 03:24:09PM +0800, Yisheng Xie wrote:</span>
<span class="quote">&gt; &gt;&gt;&gt; +void xpfo_alloc_pages(struct page *page, int order, gfp_t gfp)</span>
<span class="quote">&gt; &gt;&gt;&gt; +{</span>
<span class="quote">&gt; &gt;&gt;&gt; +	int i, flush_tlb = 0;</span>
<span class="quote">&gt; &gt;&gt;&gt; +	struct xpfo *xpfo;</span>
<span class="quote">&gt; &gt;&gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt;&gt; +	if (!static_branch_unlikely(&amp;xpfo_inited))</span>
<span class="quote">&gt; &gt;&gt;&gt; +		return;</span>
<span class="quote">&gt; &gt;&gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt;&gt; +	for (i = 0; i &lt; (1 &lt;&lt; order); i++)  {</span>
<span class="quote">&gt; &gt;&gt;&gt; +		xpfo = lookup_xpfo(page + i);</span>
<span class="quote">&gt; &gt;&gt;&gt; +		if (!xpfo)</span>
<span class="quote">&gt; &gt;&gt;&gt; +			continue;</span>
<span class="quote">&gt; &gt;&gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt;&gt; +		WARN(test_bit(XPFO_PAGE_UNMAPPED, &amp;xpfo-&gt;flags),</span>
<span class="quote">&gt; &gt;&gt;&gt; +		     &quot;xpfo: unmapped page being allocated\n&quot;);</span>
<span class="quote">&gt; &gt;&gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt;&gt; +		/* Initialize the map lock and map counter */</span>
<span class="quote">&gt; &gt;&gt;&gt; +		if (unlikely(!xpfo-&gt;inited)) {</span>
<span class="quote">&gt; &gt;&gt;&gt; +			spin_lock_init(&amp;xpfo-&gt;maplock);</span>
<span class="quote">&gt; &gt;&gt;&gt; +			atomic_set(&amp;xpfo-&gt;mapcount, 0);</span>
<span class="quote">&gt; &gt;&gt;&gt; +			xpfo-&gt;inited = true;</span>
<span class="quote">&gt; &gt;&gt;&gt; +		}</span>
<span class="quote">&gt; &gt;&gt;&gt; +		WARN(atomic_read(&amp;xpfo-&gt;mapcount),</span>
<span class="quote">&gt; &gt;&gt;&gt; +		     &quot;xpfo: already mapped page being allocated\n&quot;);</span>
<span class="quote">&gt; &gt;&gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt;&gt; +		if ((gfp &amp; GFP_HIGHUSER) == GFP_HIGHUSER) {</span>
<span class="quote">&gt; &gt;&gt;&gt; +			/*</span>
<span class="quote">&gt; &gt;&gt;&gt; +			 * Tag the page as a user page and flush the TLB if it</span>
<span class="quote">&gt; &gt;&gt;&gt; +			 * was previously allocated to the kernel.</span>
<span class="quote">&gt; &gt;&gt;&gt; +			 */</span>
<span class="quote">&gt; &gt;&gt;&gt; +			if (!test_and_set_bit(XPFO_PAGE_USER, &amp;xpfo-&gt;flags))</span>
<span class="quote">&gt; &gt;&gt;&gt; +				flush_tlb = 1;</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; I&#39;m not sure whether I am miss anything, however, when the page was previously allocated</span>
<span class="quote">&gt; &gt;&gt; to kernel,  should we unmap the physmap (the kernel&#39;s page table) here? For we allocate</span>
<span class="quote">&gt; &gt;&gt; the page to user now</span>
<span class="quote">&gt; &gt;&gt; </span>
<span class="quote">&gt; &gt; Yes, I think you&#39;re right. Oddly, the XPFO_READ_USER test works</span>
<span class="quote">&gt; &gt; correctly for me, but I think (?) should not because of this bug...</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; IIRC, this is an optimization carried forward from the initial</span>
<span class="quote">&gt; implementation. The assumption is that the kernel will map the user</span>
<span class="quote">&gt; buffer so it&#39;s not unmapped on allocation but only on the first (and</span>

Does the kernel always map it, though? e.g. in the case of
XPFO_READ_USER, I&#39;m not sure where the kernel would do a kmap() of the
test&#39;s user buffer.

Tycho
<span class="quote">
&gt; subsequent) call of kunmap. I.e.:</span>
<span class="quote">&gt;  - alloc  -&gt; noop</span>
<span class="quote">&gt;  - kmap   -&gt; noop</span>
<span class="quote">&gt;  - kunmap -&gt; unmapped from the kernel</span>
<span class="quote">&gt;  - kmap   -&gt; mapped into the kernel</span>
<span class="quote">&gt;  - kunmap -&gt; unmapped from the kernel</span>
<span class="quote">&gt; and so on until:</span>
<span class="quote">&gt;  - free   -&gt; mapped back into the kernel</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I&#39;m not sure if that make sense though since it leaves a window.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; ...Juerg</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; Tycho</span>
<span class="quote">&gt; &gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=173139">Tycho Andersen</a> - Sept. 11, 2017, 6:32 p.m.</div>
<pre class="content">
Hi all,

On Thu, Sep 07, 2017 at 11:36:01AM -0600, Tycho Andersen wrote:
<span class="quote">&gt;</span>
<span class="quote">&gt; +inline void xpfo_flush_kernel_tlb(struct page *page, int order)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	int level;</span>
<span class="quote">&gt; +	unsigned long size, kaddr;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	kaddr = (unsigned long)page_address(page);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (unlikely(!lookup_address(kaddr, &amp;level))) {</span>
<span class="quote">&gt; +		WARN(1, &quot;xpfo: invalid address to flush %lx %d\n&quot;, kaddr, level);</span>
<span class="quote">&gt; +		return;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	switch (level) {</span>
<span class="quote">&gt; +	case PG_LEVEL_4K:</span>
<span class="quote">&gt; +		size = PAGE_SIZE;</span>
<span class="quote">&gt; +		break;</span>
<span class="quote">&gt; +	case PG_LEVEL_2M:</span>
<span class="quote">&gt; +		size = PMD_SIZE;</span>
<span class="quote">&gt; +		break;</span>
<span class="quote">&gt; +	case PG_LEVEL_1G:</span>
<span class="quote">&gt; +		size = PUD_SIZE;</span>
<span class="quote">&gt; +		break;</span>
<span class="quote">&gt; +	default:</span>
<span class="quote">&gt; +		WARN(1, &quot;xpfo: unsupported page level %x\n&quot;, level);</span>
<span class="quote">&gt; +		return;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	flush_tlb_kernel_range(kaddr, kaddr + (1 &lt;&lt; order) * size);</span>

Marco was testing and got the stack trace below. The issue is that on x86,
flush_tlb_kernel_range uses on_each_cpu, which causes the WARN() below. Since
this is called from xpfo_kmap/unmap in this interrupt handler, the WARN()
triggers.

I&#39;m not sure what to do about this -- based on the discussion in v6 we need to
flush the TLBs for all CPUs -- but we can&#39;t do that with interrupts disabled,
which basically means with this we wouldn&#39;t be able to map/unmap pages in
interrupts.

Any thoughts?

Tycho

[    2.712912] ------------[ cut here ]------------
[    2.712922] WARNING: CPU: 0 PID: 0 at kernel/smp.c:414
smp_call_function_many+0x9a/0x270
[    2.712923] Modules linked in: sd_mod ata_generic pata_acpi qxl
drm_kms_helper syscopyarea sysfillrect virtio_console sysimgblt
virtio_blk fb_sys_fops ttm drm 8139too ata_piix libata 8139cp
virtio_pci virtio_ring virtio mii crc32c_intel i2c_core serio_raw
floppy dm_mirror dm_region_hash dm_log dm_mod
[    2.712939] CPU: 0 PID: 0 Comm: swapper/0 Not tainted 4.13.0+ #8
[    2.712940] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996),
BIOS 1.9.3-1.fc25 04/01/2014
[    2.712941] task: ffffffff81c10480 task.stack: ffffffff81c00000
[    2.712943] RIP: 0010:smp_call_function_many+0x9a/0x270
[    2.712944] RSP: 0018:ffff88023fc03b38 EFLAGS: 00010046
[    2.712945] RAX: 0000000000000000 RBX: ffffffff81072a50 RCX: 0000000000000001
[    2.712946] RDX: ffff88023fc03ba8 RSI: ffffffff81072a50 RDI: ffffffff81e22320
[    2.712947] RBP: ffff88023fc03b70 R08: 0000000000000970 R09: 0000000000000063
[    2.712948] R10: ffff880000000970 R11: 0000000000000000 R12: ffff88023fc03ba8
[    2.712949] R13: 0000000000000000 R14: ffff8802332b8e18 R15: ffffffff81e22320
[    2.712950] FS:  0000000000000000(0000) GS:ffff88023fc00000(0000)
knlGS:0000000000000000
[    2.712951] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
[    2.712951] CR2: 00007fde22f6b000 CR3: 000000022727b000 CR4: 00000000003406f0
[    2.712954] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
[    2.712955] DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
[    2.712955] Call Trace:
[    2.712959]  &lt;IRQ&gt;
[    2.712964]  ? x86_configure_nx+0x50/0x50
[    2.712966]  on_each_cpu+0x2d/0x60
[    2.712967]  flush_tlb_kernel_range+0x79/0x80
[    2.712969]  xpfo_flush_kernel_tlb+0xaa/0xe0
[    2.712975]  xpfo_kunmap+0xa8/0xc0
[    2.712981]  swiotlb_bounce+0xd1/0x1c0
[    2.712982]  swiotlb_tbl_unmap_single+0x10f/0x120
[    2.712984]  unmap_single+0x20/0x30
[    2.712985]  swiotlb_unmap_sg_attrs+0x46/0x70
[    2.712991]  __ata_qc_complete+0xfa/0x150 [libata]
[    2.712994]  ata_qc_complete+0xd2/0x2e0 [libata]
[    2.712998]  ata_hsm_qc_complete+0x6f/0x90 [libata]
[    2.713004]  ata_sff_hsm_move+0xae/0x6b0 [libata]
[    2.713009]  __ata_sff_port_intr+0x8e/0x100 [libata]
[    2.713013]  ata_bmdma_port_intr+0x2f/0xd0 [libata]
[    2.713019]  ata_bmdma_interrupt+0x161/0x1b0 [libata]
[    2.713022]  __handle_irq_event_percpu+0x3c/0x190
[    2.713024]  handle_irq_event_percpu+0x32/0x80
[    2.713026]  handle_irq_event+0x3b/0x60
[    2.713027]  handle_edge_irq+0x8f/0x190
[    2.713029]  handle_irq+0xab/0x120
[    2.713032]  ? _local_bh_enable+0x21/0x30
[    2.713039]  do_IRQ+0x48/0xd0
[    2.713040]  common_interrupt+0x93/0x93
[    2.713042] RIP: 0010:native_safe_halt+0x6/0x10
[    2.713043] RSP: 0018:ffffffff81c03de0 EFLAGS: 00000246 ORIG_RAX:
ffffffffffffffc1
[    2.713044] RAX: 0000000000000000 RBX: ffffffff81c10480 RCX: 0000000000000000
[    2.713045] RDX: 0000000000000000 RSI: 0000000000000000 RDI: 0000000000000000
[    2.713046] RBP: ffffffff81c03de0 R08: 00000000b656100b R09: 0000000000000000
[    2.713047] R10: 0000000000000006 R11: 0000000000000005 R12: 0000000000000000
[    2.713047] R13: ffffffff81c10480 R14: 0000000000000000 R15: 0000000000000000
[    2.713048]  &lt;/IRQ&gt;
[    2.713050]  default_idle+0x1e/0x100
[    2.713052]  arch_cpu_idle+0xf/0x20
[    2.713053]  default_idle_call+0x2c/0x40
[    2.713055]  do_idle+0x158/0x1e0
[    2.713056]  cpu_startup_entry+0x73/0x80
[    2.713058]  rest_init+0xb8/0xc0
[    2.713070]  start_kernel+0x4a2/0x4c3
[    2.713072]  ? set_init_arg+0x5a/0x5a
[    2.713074]  ? early_idt_handler_array+0x120/0x120
[    2.713075]  x86_64_start_reservations+0x2a/0x2c
[    2.713077]  x86_64_start_kernel+0x14c/0x16f
[    2.713079]  secondary_startup_64+0x9f/0x9f
[    2.713080] Code: 44 3b 35 1e 6f d0 00 7c 26 48 83 c4 10 5b 41 5c
41 5d 41 5e 41 5f 5d c3 8b 05 63 38 fc 00 85 c0 75 be 80 3d 20 0d d0
00 00 75 b5 &lt;0f&gt; ff eb b1 48 c7 c2 20 23 e2 81 4c 89 fe 44 89 f7 e8 20
b5 62
[    2.713105] ---[ end trace 4d101d4c176c16b0 ]---
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=105821">Marco Benatto</a> - Sept. 11, 2017, 9:54 p.m.</div>
<pre class="content">
I think the point here is, as running under interrupt-context, we are
using k{map,unmap)_atomic instead of k{map,unmap}
but after the flush_tlb_kernel_range() change we don&#39;t have an XPFO
atomic version from both.

My suggestion here is, at least for x86, split this into xpfo_k{un}map
and xpfo_k{un}map_atomic which wild only flush local tlb.
This would create a window where the same page would be mapped both
for user and kernel on other cpu&#39;s due to stale remote tlb entries.

Is there any other suggestion for this scenario?

Thanks,

- Marco Benatto

On Mon, Sep 11, 2017 at 3:32 PM, Tycho Andersen &lt;tycho@docker.com&gt; wrote:
<span class="quote">&gt; Hi all,</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; On Thu, Sep 07, 2017 at 11:36:01AM -0600, Tycho Andersen wrote:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; +inline void xpfo_flush_kernel_tlb(struct page *page, int order)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +     int level;</span>
<span class="quote">&gt;&gt; +     unsigned long size, kaddr;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +     kaddr = (unsigned long)page_address(page);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +     if (unlikely(!lookup_address(kaddr, &amp;level))) {</span>
<span class="quote">&gt;&gt; +             WARN(1, &quot;xpfo: invalid address to flush %lx %d\n&quot;, kaddr, level);</span>
<span class="quote">&gt;&gt; +             return;</span>
<span class="quote">&gt;&gt; +     }</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +     switch (level) {</span>
<span class="quote">&gt;&gt; +     case PG_LEVEL_4K:</span>
<span class="quote">&gt;&gt; +             size = PAGE_SIZE;</span>
<span class="quote">&gt;&gt; +             break;</span>
<span class="quote">&gt;&gt; +     case PG_LEVEL_2M:</span>
<span class="quote">&gt;&gt; +             size = PMD_SIZE;</span>
<span class="quote">&gt;&gt; +             break;</span>
<span class="quote">&gt;&gt; +     case PG_LEVEL_1G:</span>
<span class="quote">&gt;&gt; +             size = PUD_SIZE;</span>
<span class="quote">&gt;&gt; +             break;</span>
<span class="quote">&gt;&gt; +     default:</span>
<span class="quote">&gt;&gt; +             WARN(1, &quot;xpfo: unsupported page level %x\n&quot;, level);</span>
<span class="quote">&gt;&gt; +             return;</span>
<span class="quote">&gt;&gt; +     }</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +     flush_tlb_kernel_range(kaddr, kaddr + (1 &lt;&lt; order) * size);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Marco was testing and got the stack trace below. The issue is that on x86,</span>
<span class="quote">&gt; flush_tlb_kernel_range uses on_each_cpu, which causes the WARN() below. Since</span>
<span class="quote">&gt; this is called from xpfo_kmap/unmap in this interrupt handler, the WARN()</span>
<span class="quote">&gt; triggers.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I&#39;m not sure what to do about this -- based on the discussion in v6 we need to</span>
<span class="quote">&gt; flush the TLBs for all CPUs -- but we can&#39;t do that with interrupts disabled,</span>
<span class="quote">&gt; which basically means with this we wouldn&#39;t be able to map/unmap pages in</span>
<span class="quote">&gt; interrupts.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Any thoughts?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Tycho</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; [    2.712912] ------------[ cut here ]------------</span>
<span class="quote">&gt; [    2.712922] WARNING: CPU: 0 PID: 0 at kernel/smp.c:414</span>
<span class="quote">&gt; smp_call_function_many+0x9a/0x270</span>
<span class="quote">&gt; [    2.712923] Modules linked in: sd_mod ata_generic pata_acpi qxl</span>
<span class="quote">&gt; drm_kms_helper syscopyarea sysfillrect virtio_console sysimgblt</span>
<span class="quote">&gt; virtio_blk fb_sys_fops ttm drm 8139too ata_piix libata 8139cp</span>
<span class="quote">&gt; virtio_pci virtio_ring virtio mii crc32c_intel i2c_core serio_raw</span>
<span class="quote">&gt; floppy dm_mirror dm_region_hash dm_log dm_mod</span>
<span class="quote">&gt; [    2.712939] CPU: 0 PID: 0 Comm: swapper/0 Not tainted 4.13.0+ #8</span>
<span class="quote">&gt; [    2.712940] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996),</span>
<span class="quote">&gt; BIOS 1.9.3-1.fc25 04/01/2014</span>
<span class="quote">&gt; [    2.712941] task: ffffffff81c10480 task.stack: ffffffff81c00000</span>
<span class="quote">&gt; [    2.712943] RIP: 0010:smp_call_function_many+0x9a/0x270</span>
<span class="quote">&gt; [    2.712944] RSP: 0018:ffff88023fc03b38 EFLAGS: 00010046</span>
<span class="quote">&gt; [    2.712945] RAX: 0000000000000000 RBX: ffffffff81072a50 RCX: 0000000000000001</span>
<span class="quote">&gt; [    2.712946] RDX: ffff88023fc03ba8 RSI: ffffffff81072a50 RDI: ffffffff81e22320</span>
<span class="quote">&gt; [    2.712947] RBP: ffff88023fc03b70 R08: 0000000000000970 R09: 0000000000000063</span>
<span class="quote">&gt; [    2.712948] R10: ffff880000000970 R11: 0000000000000000 R12: ffff88023fc03ba8</span>
<span class="quote">&gt; [    2.712949] R13: 0000000000000000 R14: ffff8802332b8e18 R15: ffffffff81e22320</span>
<span class="quote">&gt; [    2.712950] FS:  0000000000000000(0000) GS:ffff88023fc00000(0000)</span>
<span class="quote">&gt; knlGS:0000000000000000</span>
<span class="quote">&gt; [    2.712951] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033</span>
<span class="quote">&gt; [    2.712951] CR2: 00007fde22f6b000 CR3: 000000022727b000 CR4: 00000000003406f0</span>
<span class="quote">&gt; [    2.712954] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000</span>
<span class="quote">&gt; [    2.712955] DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400</span>
<span class="quote">&gt; [    2.712955] Call Trace:</span>
<span class="quote">&gt; [    2.712959]  &lt;IRQ&gt;</span>
<span class="quote">&gt; [    2.712964]  ? x86_configure_nx+0x50/0x50</span>
<span class="quote">&gt; [    2.712966]  on_each_cpu+0x2d/0x60</span>
<span class="quote">&gt; [    2.712967]  flush_tlb_kernel_range+0x79/0x80</span>
<span class="quote">&gt; [    2.712969]  xpfo_flush_kernel_tlb+0xaa/0xe0</span>
<span class="quote">&gt; [    2.712975]  xpfo_kunmap+0xa8/0xc0</span>
<span class="quote">&gt; [    2.712981]  swiotlb_bounce+0xd1/0x1c0</span>
<span class="quote">&gt; [    2.712982]  swiotlb_tbl_unmap_single+0x10f/0x120</span>
<span class="quote">&gt; [    2.712984]  unmap_single+0x20/0x30</span>
<span class="quote">&gt; [    2.712985]  swiotlb_unmap_sg_attrs+0x46/0x70</span>
<span class="quote">&gt; [    2.712991]  __ata_qc_complete+0xfa/0x150 [libata]</span>
<span class="quote">&gt; [    2.712994]  ata_qc_complete+0xd2/0x2e0 [libata]</span>
<span class="quote">&gt; [    2.712998]  ata_hsm_qc_complete+0x6f/0x90 [libata]</span>
<span class="quote">&gt; [    2.713004]  ata_sff_hsm_move+0xae/0x6b0 [libata]</span>
<span class="quote">&gt; [    2.713009]  __ata_sff_port_intr+0x8e/0x100 [libata]</span>
<span class="quote">&gt; [    2.713013]  ata_bmdma_port_intr+0x2f/0xd0 [libata]</span>
<span class="quote">&gt; [    2.713019]  ata_bmdma_interrupt+0x161/0x1b0 [libata]</span>
<span class="quote">&gt; [    2.713022]  __handle_irq_event_percpu+0x3c/0x190</span>
<span class="quote">&gt; [    2.713024]  handle_irq_event_percpu+0x32/0x80</span>
<span class="quote">&gt; [    2.713026]  handle_irq_event+0x3b/0x60</span>
<span class="quote">&gt; [    2.713027]  handle_edge_irq+0x8f/0x190</span>
<span class="quote">&gt; [    2.713029]  handle_irq+0xab/0x120</span>
<span class="quote">&gt; [    2.713032]  ? _local_bh_enable+0x21/0x30</span>
<span class="quote">&gt; [    2.713039]  do_IRQ+0x48/0xd0</span>
<span class="quote">&gt; [    2.713040]  common_interrupt+0x93/0x93</span>
<span class="quote">&gt; [    2.713042] RIP: 0010:native_safe_halt+0x6/0x10</span>
<span class="quote">&gt; [    2.713043] RSP: 0018:ffffffff81c03de0 EFLAGS: 00000246 ORIG_RAX:</span>
<span class="quote">&gt; ffffffffffffffc1</span>
<span class="quote">&gt; [    2.713044] RAX: 0000000000000000 RBX: ffffffff81c10480 RCX: 0000000000000000</span>
<span class="quote">&gt; [    2.713045] RDX: 0000000000000000 RSI: 0000000000000000 RDI: 0000000000000000</span>
<span class="quote">&gt; [    2.713046] RBP: ffffffff81c03de0 R08: 00000000b656100b R09: 0000000000000000</span>
<span class="quote">&gt; [    2.713047] R10: 0000000000000006 R11: 0000000000000005 R12: 0000000000000000</span>
<span class="quote">&gt; [    2.713047] R13: ffffffff81c10480 R14: 0000000000000000 R15: 0000000000000000</span>
<span class="quote">&gt; [    2.713048]  &lt;/IRQ&gt;</span>
<span class="quote">&gt; [    2.713050]  default_idle+0x1e/0x100</span>
<span class="quote">&gt; [    2.713052]  arch_cpu_idle+0xf/0x20</span>
<span class="quote">&gt; [    2.713053]  default_idle_call+0x2c/0x40</span>
<span class="quote">&gt; [    2.713055]  do_idle+0x158/0x1e0</span>
<span class="quote">&gt; [    2.713056]  cpu_startup_entry+0x73/0x80</span>
<span class="quote">&gt; [    2.713058]  rest_init+0xb8/0xc0</span>
<span class="quote">&gt; [    2.713070]  start_kernel+0x4a2/0x4c3</span>
<span class="quote">&gt; [    2.713072]  ? set_init_arg+0x5a/0x5a</span>
<span class="quote">&gt; [    2.713074]  ? early_idt_handler_array+0x120/0x120</span>
<span class="quote">&gt; [    2.713075]  x86_64_start_reservations+0x2a/0x2c</span>
<span class="quote">&gt; [    2.713077]  x86_64_start_kernel+0x14c/0x16f</span>
<span class="quote">&gt; [    2.713079]  secondary_startup_64+0x9f/0x9f</span>
<span class="quote">&gt; [    2.713080] Code: 44 3b 35 1e 6f d0 00 7c 26 48 83 c4 10 5b 41 5c</span>
<span class="quote">&gt; 41 5d 41 5e 41 5f 5d c3 8b 05 63 38 fc 00 85 c0 75 be 80 3d 20 0d d0</span>
<span class="quote">&gt; 00 00 75 b5 &lt;0f&gt; ff eb b1 48 c7 c2 20 23 e2 81 4c 89 fe 44 89 f7 e8 20</span>
<span class="quote">&gt; b5 62</span>
<span class="quote">&gt; [    2.713105] ---[ end trace 4d101d4c176c16b0 ]---</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=169781">Xie Yisheng</a> - Sept. 12, 2017, 8:05 a.m.</div>
<pre class="content">
On 2017/9/12 0:03, Juerg Haefliger wrote:
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; On 09/11/2017 04:50 PM, Tycho Andersen wrote:</span>
<span class="quote">&gt;&gt; Hi Yisheng,</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; On Mon, Sep 11, 2017 at 03:24:09PM +0800, Yisheng Xie wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; +void xpfo_alloc_pages(struct page *page, int order, gfp_t gfp)</span>
<span class="quote">&gt;&gt;&gt;&gt; +{</span>
<span class="quote">&gt;&gt;&gt;&gt; +	int i, flush_tlb = 0;</span>
<span class="quote">&gt;&gt;&gt;&gt; +	struct xpfo *xpfo;</span>
<span class="quote">&gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;&gt; +	if (!static_branch_unlikely(&amp;xpfo_inited))</span>
<span class="quote">&gt;&gt;&gt;&gt; +		return;</span>
<span class="quote">&gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;&gt; +	for (i = 0; i &lt; (1 &lt;&lt; order); i++)  {</span>
<span class="quote">&gt;&gt;&gt;&gt; +		xpfo = lookup_xpfo(page + i);</span>
<span class="quote">&gt;&gt;&gt;&gt; +		if (!xpfo)</span>
<span class="quote">&gt;&gt;&gt;&gt; +			continue;</span>
<span class="quote">&gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;&gt; +		WARN(test_bit(XPFO_PAGE_UNMAPPED, &amp;xpfo-&gt;flags),</span>
<span class="quote">&gt;&gt;&gt;&gt; +		     &quot;xpfo: unmapped page being allocated\n&quot;);</span>
<span class="quote">&gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;&gt; +		/* Initialize the map lock and map counter */</span>
<span class="quote">&gt;&gt;&gt;&gt; +		if (unlikely(!xpfo-&gt;inited)) {</span>
<span class="quote">&gt;&gt;&gt;&gt; +			spin_lock_init(&amp;xpfo-&gt;maplock);</span>
<span class="quote">&gt;&gt;&gt;&gt; +			atomic_set(&amp;xpfo-&gt;mapcount, 0);</span>
<span class="quote">&gt;&gt;&gt;&gt; +			xpfo-&gt;inited = true;</span>
<span class="quote">&gt;&gt;&gt;&gt; +		}</span>
<span class="quote">&gt;&gt;&gt;&gt; +		WARN(atomic_read(&amp;xpfo-&gt;mapcount),</span>
<span class="quote">&gt;&gt;&gt;&gt; +		     &quot;xpfo: already mapped page being allocated\n&quot;);</span>
<span class="quote">&gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;&gt; +		if ((gfp &amp; GFP_HIGHUSER) == GFP_HIGHUSER) {</span>
<span class="quote">&gt;&gt;&gt;&gt; +			/*</span>
<span class="quote">&gt;&gt;&gt;&gt; +			 * Tag the page as a user page and flush the TLB if it</span>
<span class="quote">&gt;&gt;&gt;&gt; +			 * was previously allocated to the kernel.</span>
<span class="quote">&gt;&gt;&gt;&gt; +			 */</span>
<span class="quote">&gt;&gt;&gt;&gt; +			if (!test_and_set_bit(XPFO_PAGE_USER, &amp;xpfo-&gt;flags))</span>
<span class="quote">&gt;&gt;&gt;&gt; +				flush_tlb = 1;</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; I&#39;m not sure whether I am miss anything, however, when the page was previously allocated</span>
<span class="quote">&gt;&gt;&gt; to kernel,  should we unmap the physmap (the kernel&#39;s page table) here? For we allocate</span>
<span class="quote">&gt;&gt;&gt; the page to user now</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt; Yes, I think you&#39;re right. Oddly, the XPFO_READ_USER test works</span>

Hi Tycho,
Could you share this test? I&#39;d like to know how it works.

Thanks
<span class="quote">
&gt;&gt; correctly for me, but I think (?) should not because of this bug...</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; IIRC, this is an optimization carried forward from the initial</span>
<span class="quote">&gt; implementation. </span>
Hi Juerg,

hmm.. If below is the first version, then it seems this exist from the first version:
https://patchwork.kernel.org/patch/8437451/
<span class="quote">
&gt; The assumption is that the kernel will map the user</span>
<span class="quote">&gt; buffer so it&#39;s not unmapped on allocation but only on the first (and</span>
<span class="quote">&gt; subsequent) call of kunmap.</span>

IMO, before a page is allocated, it is in buddy system, which means it is free
and no other &#39;map&#39; on the page except direct map. Then if the page is allocated
to user, XPFO should unmap the direct map. otherwise the ret2dir may works at
this window before it is freed. Or maybe I&#39;m still missing anything.

Thanks
Yisheng Xie
<span class="quote">
&gt;  I.e.:</span>
<span class="quote">&gt;  - alloc  -&gt; noop</span>
<span class="quote">&gt;  - kmap   -&gt; noop</span>
<span class="quote">&gt;  - kunmap -&gt; unmapped from the kernel</span>
<span class="quote">&gt;  - kmap   -&gt; mapped into the kernel</span>
<span class="quote">&gt;  - kunmap -&gt; unmapped from the kernel</span>
<span class="quote">&gt; and so on until:</span>
<span class="quote">&gt;  - free   -&gt; mapped back into the kernel</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I&#39;m not sure if that make sense though since it leaves a window.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; ...Juerg</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; Tycho</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; .</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=173139">Tycho Andersen</a> - Sept. 12, 2017, 2:36 p.m.</div>
<pre class="content">
On Tue, Sep 12, 2017 at 04:05:22PM +0800, Yisheng Xie wrote:
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; On 2017/9/12 0:03, Juerg Haefliger wrote:</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; On 09/11/2017 04:50 PM, Tycho Andersen wrote:</span>
<span class="quote">&gt; &gt;&gt; Hi Yisheng,</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; On Mon, Sep 11, 2017 at 03:24:09PM +0800, Yisheng Xie wrote:</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +void xpfo_alloc_pages(struct page *page, int order, gfp_t gfp)</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +{</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +	int i, flush_tlb = 0;</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +	struct xpfo *xpfo;</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +	if (!static_branch_unlikely(&amp;xpfo_inited))</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +		return;</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +	for (i = 0; i &lt; (1 &lt;&lt; order); i++)  {</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +		xpfo = lookup_xpfo(page + i);</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +		if (!xpfo)</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +			continue;</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +		WARN(test_bit(XPFO_PAGE_UNMAPPED, &amp;xpfo-&gt;flags),</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +		     &quot;xpfo: unmapped page being allocated\n&quot;);</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +		/* Initialize the map lock and map counter */</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +		if (unlikely(!xpfo-&gt;inited)) {</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +			spin_lock_init(&amp;xpfo-&gt;maplock);</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +			atomic_set(&amp;xpfo-&gt;mapcount, 0);</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +			xpfo-&gt;inited = true;</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +		}</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +		WARN(atomic_read(&amp;xpfo-&gt;mapcount),</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +		     &quot;xpfo: already mapped page being allocated\n&quot;);</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +		if ((gfp &amp; GFP_HIGHUSER) == GFP_HIGHUSER) {</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +			/*</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +			 * Tag the page as a user page and flush the TLB if it</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +			 * was previously allocated to the kernel.</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +			 */</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +			if (!test_and_set_bit(XPFO_PAGE_USER, &amp;xpfo-&gt;flags))</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +				flush_tlb = 1;</span>
<span class="quote">&gt; &gt;&gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt; I&#39;m not sure whether I am miss anything, however, when the page was previously allocated</span>
<span class="quote">&gt; &gt;&gt;&gt; to kernel,  should we unmap the physmap (the kernel&#39;s page table) here? For we allocate</span>
<span class="quote">&gt; &gt;&gt;&gt; the page to user now</span>
<span class="quote">&gt; &gt;&gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; Yes, I think you&#39;re right. Oddly, the XPFO_READ_USER test works</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Hi Tycho,</span>
<span class="quote">&gt; Could you share this test? I&#39;d like to know how it works.</span>

See the last patch in the series.
<span class="quote">
&gt; &gt;&gt; correctly for me, but I think (?) should not because of this bug...</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; IIRC, this is an optimization carried forward from the initial</span>
<span class="quote">&gt; &gt; implementation. </span>
<span class="quote">&gt; Hi Juerg,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; hmm.. If below is the first version, then it seems this exist from the first version:</span>
<span class="quote">&gt; https://patchwork.kernel.org/patch/8437451/</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; The assumption is that the kernel will map the user</span>
<span class="quote">&gt; &gt; buffer so it&#39;s not unmapped on allocation but only on the first (and</span>
<span class="quote">&gt; &gt; subsequent) call of kunmap.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; IMO, before a page is allocated, it is in buddy system, which means it is free</span>
<span class="quote">&gt; and no other &#39;map&#39; on the page except direct map. Then if the page is allocated</span>
<span class="quote">&gt; to user, XPFO should unmap the direct map. otherwise the ret2dir may works at</span>
<span class="quote">&gt; this window before it is freed. Or maybe I&#39;m still missing anything.</span>

I agree that it seems broken. I&#39;m just not sure why the test doesn&#39;t
fail. It&#39;s certainly worth understanding.

Tycho
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=65121">Dave Hansen</a> - Sept. 20, 2017, 3:48 p.m.</div>
<pre class="content">
On 09/07/2017 10:36 AM, Tycho Andersen wrote:
...
<span class="quote">&gt; Whenever a page destined for userspace is allocated, it is</span>
<span class="quote">&gt; unmapped from physmap (the kernel&#39;s page table). When such a page is</span>
<span class="quote">&gt; reclaimed from userspace, it is mapped back to physmap.</span>

I&#39;m looking for the code where it&#39;s unmapped at allocation to userspace.
 I see TLB flushing and &#39;struct xpfo&#39; manipulation, but I don&#39;t see the
unmapping.  Where is that occurring?

How badly does this hurt performance?  Since we (generally) have
different migrate types for user and kernel allocation, I can imagine
that a given page *generally* doesn&#39;t oscillate between user and
kernel-allocated, but I&#39;m curious how it works in practice.  Doesn&#39;t the
IPI load from the TLB flushes eat you alive?

It&#39;s a bit scary to have such a deep code path under the main allocator.

This all seems insanely expensive.  It will *barely* work on an
allocation-heavy workload on a desktop.  I&#39;m pretty sure the locking
will just fall over entirely on any reasonably-sized server.

I really have to wonder whether there are better ret2dir defenses than
this.  The allocator just seems like the *wrong* place to be doing this
because it&#39;s such a hot path.
<span class="quote">
&gt; +		cpa.vaddr = kaddr;</span>
<span class="quote">&gt; +		cpa.pages = &amp;page;</span>
<span class="quote">&gt; +		cpa.mask_set = prot;</span>
<span class="quote">&gt; +		cpa.mask_clr = msk_clr;</span>
<span class="quote">&gt; +		cpa.numpages = 1;</span>
<span class="quote">&gt; +		cpa.flags = 0;</span>
<span class="quote">&gt; +		cpa.curpage = 0;</span>
<span class="quote">&gt; +		cpa.force_split = 0;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		do_split = try_preserve_large_page(pte, (unsigned </span>

Is this safe to do without a TLB flush?  I thought we had plenty of bugs
in CPUs around having multiple entries for the same page in the TLB at
once.  We&#39;re *REALLY* careful when we split large pages for THP, and I&#39;m
surprised we don&#39;t do the same here.

Why do you even bother keeping large pages around?  Won&#39;t the entire
kernel just degrade to using 4k everywhere, eventually?
<span class="quote">
&gt; +		if (do_split) {</span>
<span class="quote">&gt; +			struct page *base;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			base = alloc_pages(GFP_ATOMIC | __GFP_NOTRACK, </span>

Ugh, GFP_ATOMIC.  That&#39;s nasty.  Do you really want this allocation to
fail all the time?  GFP_ATOMIC could really be called
GFP_YOU_BETTER_BE_OK_WITH_THIS_FAILING. :)

You probably want to do what the THP code does here and keep a spare
page around, then allocate it before you take the locks.
<span class="quote">
&gt; +inline void xpfo_flush_kernel_tlb(struct page *page, int order)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	int level;</span>
<span class="quote">&gt; +	unsigned long size, kaddr;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	kaddr = (unsigned long)page_address(page);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (unlikely(!lookup_address(kaddr, &amp;level))) {</span>
<span class="quote">&gt; +		WARN(1, &quot;xpfo: invalid address to flush %lx %d\n&quot;, kaddr, level);</span>
<span class="quote">&gt; +		return;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	switch (level) {</span>
<span class="quote">&gt; +	case PG_LEVEL_4K:</span>
<span class="quote">&gt; +		size = PAGE_SIZE;</span>
<span class="quote">&gt; +		break;</span>
<span class="quote">&gt; +	case PG_LEVEL_2M:</span>
<span class="quote">&gt; +		size = PMD_SIZE;</span>
<span class="quote">&gt; +		break;</span>
<span class="quote">&gt; +	case PG_LEVEL_1G:</span>
<span class="quote">&gt; +		size = PUD_SIZE;</span>
<span class="quote">&gt; +		break;</span>
<span class="quote">&gt; +	default:</span>
<span class="quote">&gt; +		WARN(1, &quot;xpfo: unsupported page level %x\n&quot;, level);</span>
<span class="quote">&gt; +		return;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	flush_tlb_kernel_range(kaddr, kaddr + (1 &lt;&lt; order) * size);</span>
<span class="quote">&gt; +}</span>

I&#39;m not sure flush_tlb_kernel_range() is the best primitive to be
calling here.

Let&#39;s say you walk the page tables and find level=PG_LEVEL_1G.  You call
flush_tlb_kernel_range(), you will be above
tlb_single_page_flush_ceiling, and you will do a full TLB flush.  But,
with a 1GB page, you could have just used a single INVLPG and skipped
the global flush.

I guess the cost of the IPI is way more than the flush itself, but it&#39;s
still a shame to toss the entire TLB when you don&#39;t have to.

I also think the TLB flush should be done closer to the page table
manipulation that it is connected to.  It&#39;s hard to figure out whether
the flush is the right one otherwise.

Also, the &quot;(1 &lt;&lt; order) * size&quot; thing looks goofy to me.  Let&#39;s say you
are flushing a order=1 (8k) page and its mapped in a 1GB mapping.  You
flush 2GB.  Is that intentional?
<span class="quote">
&gt; +</span>
<span class="quote">&gt; +void xpfo_free_pages(struct page *page, int order)</span>
<span class="quote">&gt; +{</span>
...
<span class="quote">&gt; +		/*</span>
<span class="quote">&gt; +		 * Map the page back into the kernel if it was previously</span>
<span class="quote">&gt; +		 * allocated to user space.</span>
<span class="quote">&gt; +		 */</span>
<span class="quote">&gt; +		if (test_and_clear_bit(XPFO_PAGE_USER, &amp;xpfo-&gt;flags)) {</span>
<span class="quote">&gt; +			clear_bit(XPFO_PAGE_UNMAPPED, &amp;xpfo-&gt;flags);</span>
<span class="quote">&gt; +			set_kpte(page_address(page + i), page + i,</span>
<span class="quote">&gt; +				 PAGE_KERNEL);</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +}</span>

This seems like a bad idea, performance-wise.  Kernel and userspace
pages tend to be separated by migrate types.  So, a given physical page
will tend to be used as kernel *or* for userspace.  With this nugget,
every time a userspace page is freed, we will go to the trouble of
making it *back* into a kernel page.  Then, when it is allocated again
(probably as userspace), we will re-make it into a userspace page.  That
seems horribly inefficient.

Also, this weakens the security guarantees.  Let&#39;s say you&#39;re mounting a
ret2dir attack.  You populate a page with your evil data and you know
the kernel address for the page.  All you have to do is coordinate your
attack with freeing the page.  You can control when it gets freed.  Now,
the xpfo_free_pages() helpfully just mapped your attack code back into
the kernel.

Why not *just* do these moves at allocation time?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=173139">Tycho Andersen</a> - Sept. 20, 2017, 10:34 p.m.</div>
<pre class="content">
Hi Dave,

Thanks for taking a look!

On Wed, Sep 20, 2017 at 08:48:36AM -0700, Dave Hansen wrote:
<span class="quote">&gt; On 09/07/2017 10:36 AM, Tycho Andersen wrote:</span>
<span class="quote">&gt; ...</span>
<span class="quote">&gt; &gt; Whenever a page destined for userspace is allocated, it is</span>
<span class="quote">&gt; &gt; unmapped from physmap (the kernel&#39;s page table). When such a page is</span>
<span class="quote">&gt; &gt; reclaimed from userspace, it is mapped back to physmap.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I&#39;m looking for the code where it&#39;s unmapped at allocation to userspace.</span>
<span class="quote">&gt;  I see TLB flushing and &#39;struct xpfo&#39; manipulation, but I don&#39;t see the</span>
<span class="quote">&gt; unmapping.  Where is that occurring?</span>

This is discussed here: https://lkml.org/lkml/2017/9/11/289 but,
you&#39;re right that it&#39;s wrong in some cases. I&#39;ve fixed it up for v7:
https://lkml.org/lkml/2017/9/12/512
<span class="quote">
&gt; How badly does this hurt performance?  Since we (generally) have</span>
<span class="quote">&gt; different migrate types for user and kernel allocation, I can imagine</span>
<span class="quote">&gt; that a given page *generally* doesn&#39;t oscillate between user and</span>
<span class="quote">&gt; kernel-allocated, but I&#39;m curious how it works in practice.  Doesn&#39;t the</span>
<span class="quote">&gt; IPI load from the TLB flushes eat you alive?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; It&#39;s a bit scary to have such a deep code path under the main allocator.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This all seems insanely expensive.  It will *barely* work on an</span>
<span class="quote">&gt; allocation-heavy workload on a desktop.  I&#39;m pretty sure the locking</span>
<span class="quote">&gt; will just fall over entirely on any reasonably-sized server.</span>

Basically, yes :(. I presented some numbers at LSS, but the gist was
on a 2.4x slowdown on a 24 core/48 thread Xeon E5-2650, and a 1.4x
slowdown on a 4 core/8 thread E3-1240. The story seems a little bit
better on ARM, but I&#39;m struggling to get it to boot on a box with more
than 4 cores, so I can&#39;t draw a better picture yet.
<span class="quote">
&gt; I really have to wonder whether there are better ret2dir defenses than</span>
<span class="quote">&gt; this.  The allocator just seems like the *wrong* place to be doing this</span>
<span class="quote">&gt; because it&#39;s such a hot path.</span>

This might be crazy, but what if we defer flushing of the kernel
ranges until just before we return to userspace? We&#39;d still manipulate
the prot/xpfo bits for the pages, but then just keep a list of which
ranges need to be flushed, and do the right thing before we return.
This leaves a little window between the actual allocation and the
flush, but userspace would need another thread in its threadgroup to
predict the next allocation, write the bad stuff there, and do the
exploit all in that window.

I&#39;m of course open to other suggestions. I&#39;m new :)
<span class="quote">
&gt; &gt; +		cpa.vaddr = kaddr;</span>
<span class="quote">&gt; &gt; +		cpa.pages = &amp;page;</span>
<span class="quote">&gt; &gt; +		cpa.mask_set = prot;</span>
<span class="quote">&gt; &gt; +		cpa.mask_clr = msk_clr;</span>
<span class="quote">&gt; &gt; +		cpa.numpages = 1;</span>
<span class="quote">&gt; &gt; +		cpa.flags = 0;</span>
<span class="quote">&gt; &gt; +		cpa.curpage = 0;</span>
<span class="quote">&gt; &gt; +		cpa.force_split = 0;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		do_split = try_preserve_large_page(pte, (unsigned </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Is this safe to do without a TLB flush?  I thought we had plenty of bugs</span>
<span class="quote">&gt; in CPUs around having multiple entries for the same page in the TLB at</span>
<span class="quote">&gt; once.  We&#39;re *REALLY* careful when we split large pages for THP, and I&#39;m</span>
<span class="quote">&gt; surprised we don&#39;t do the same here.</span>

It looks like on some code paths we do flush, and some we don&#39;t.
Sounds like it&#39;s not safe to do without a flush, so I&#39;ll see about
adding one.
<span class="quote">
&gt; Why do you even bother keeping large pages around?  Won&#39;t the entire</span>
<span class="quote">&gt; kernel just degrade to using 4k everywhere, eventually?</span>

Isn&#39;t that true of large pages in general? Is there something about
xpfo that makes this worse? I thought this would only split things if
they had already been split somewhere else, and the protection can&#39;t
apply to the whole huge page.
<span class="quote">
&gt; &gt; +		if (do_split) {</span>
<span class="quote">&gt; &gt; +			struct page *base;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +			base = alloc_pages(GFP_ATOMIC | __GFP_NOTRACK, </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Ugh, GFP_ATOMIC.  That&#39;s nasty.  Do you really want this allocation to</span>
<span class="quote">&gt; fail all the time?  GFP_ATOMIC could really be called</span>
<span class="quote">&gt; GFP_YOU_BETTER_BE_OK_WITH_THIS_FAILING. :)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; You probably want to do what the THP code does here and keep a spare</span>
<span class="quote">&gt; page around, then allocate it before you take the locks.</span>

Sounds like a good idea, thanks.
<span class="quote">
&gt; &gt; +inline void xpfo_flush_kernel_tlb(struct page *page, int order)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	int level;</span>
<span class="quote">&gt; &gt; +	unsigned long size, kaddr;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	kaddr = (unsigned long)page_address(page);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	if (unlikely(!lookup_address(kaddr, &amp;level))) {</span>
<span class="quote">&gt; &gt; +		WARN(1, &quot;xpfo: invalid address to flush %lx %d\n&quot;, kaddr, level);</span>
<span class="quote">&gt; &gt; +		return;</span>
<span class="quote">&gt; &gt; +	}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	switch (level) {</span>
<span class="quote">&gt; &gt; +	case PG_LEVEL_4K:</span>
<span class="quote">&gt; &gt; +		size = PAGE_SIZE;</span>
<span class="quote">&gt; &gt; +		break;</span>
<span class="quote">&gt; &gt; +	case PG_LEVEL_2M:</span>
<span class="quote">&gt; &gt; +		size = PMD_SIZE;</span>
<span class="quote">&gt; &gt; +		break;</span>
<span class="quote">&gt; &gt; +	case PG_LEVEL_1G:</span>
<span class="quote">&gt; &gt; +		size = PUD_SIZE;</span>
<span class="quote">&gt; &gt; +		break;</span>
<span class="quote">&gt; &gt; +	default:</span>
<span class="quote">&gt; &gt; +		WARN(1, &quot;xpfo: unsupported page level %x\n&quot;, level);</span>
<span class="quote">&gt; &gt; +		return;</span>
<span class="quote">&gt; &gt; +	}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	flush_tlb_kernel_range(kaddr, kaddr + (1 &lt;&lt; order) * size);</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I&#39;m not sure flush_tlb_kernel_range() is the best primitive to be</span>
<span class="quote">&gt; calling here.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Let&#39;s say you walk the page tables and find level=PG_LEVEL_1G.  You call</span>
<span class="quote">&gt; flush_tlb_kernel_range(), you will be above</span>
<span class="quote">&gt; tlb_single_page_flush_ceiling, and you will do a full TLB flush.  But,</span>
<span class="quote">&gt; with a 1GB page, you could have just used a single INVLPG and skipped</span>
<span class="quote">&gt; the global flush.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I guess the cost of the IPI is way more than the flush itself, but it&#39;s</span>
<span class="quote">&gt; still a shame to toss the entire TLB when you don&#39;t have to.</span>

Ok, do you think it&#39;s worth making a new helper for others to use? Or
should I just keep the logic in this function?
<span class="quote">
&gt; I also think the TLB flush should be done closer to the page table</span>
<span class="quote">&gt; manipulation that it is connected to.  It&#39;s hard to figure out whether</span>
<span class="quote">&gt; the flush is the right one otherwise.</span>

Yes, sounds good.
<span class="quote">
&gt; Also, the &quot;(1 &lt;&lt; order) * size&quot; thing looks goofy to me.  Let&#39;s say you</span>
<span class="quote">&gt; are flushing a order=1 (8k) page and its mapped in a 1GB mapping.  You</span>
<span class="quote">&gt; flush 2GB.  Is that intentional?</span>

I don&#39;t think so; seems like we should be flushing
(1 &lt;&lt; order) * PAGE_SIZE instead.
<span class="quote">
&gt; &gt; +</span>
<span class="quote">&gt; &gt; +void xpfo_free_pages(struct page *page, int order)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; ...</span>
<span class="quote">&gt; &gt; +		/*</span>
<span class="quote">&gt; &gt; +		 * Map the page back into the kernel if it was previously</span>
<span class="quote">&gt; &gt; +		 * allocated to user space.</span>
<span class="quote">&gt; &gt; +		 */</span>
<span class="quote">&gt; &gt; +		if (test_and_clear_bit(XPFO_PAGE_USER, &amp;xpfo-&gt;flags)) {</span>
<span class="quote">&gt; &gt; +			clear_bit(XPFO_PAGE_UNMAPPED, &amp;xpfo-&gt;flags);</span>
<span class="quote">&gt; &gt; +			set_kpte(page_address(page + i), page + i,</span>
<span class="quote">&gt; &gt; +				 PAGE_KERNEL);</span>
<span class="quote">&gt; &gt; +		}</span>
<span class="quote">&gt; &gt; +	}</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This seems like a bad idea, performance-wise.  Kernel and userspace</span>
<span class="quote">&gt; pages tend to be separated by migrate types.  So, a given physical page</span>
<span class="quote">&gt; will tend to be used as kernel *or* for userspace.  With this nugget,</span>
<span class="quote">&gt; every time a userspace page is freed, we will go to the trouble of</span>
<span class="quote">&gt; making it *back* into a kernel page.  Then, when it is allocated again</span>
<span class="quote">&gt; (probably as userspace), we will re-make it into a userspace page.  That</span>
<span class="quote">&gt; seems horribly inefficient.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Also, this weakens the security guarantees.  Let&#39;s say you&#39;re mounting a</span>
<span class="quote">&gt; ret2dir attack.  You populate a page with your evil data and you know</span>
<span class="quote">&gt; the kernel address for the page.  All you have to do is coordinate your</span>
<span class="quote">&gt; attack with freeing the page.  You can control when it gets freed.  Now,</span>
<span class="quote">&gt; the xpfo_free_pages() helpfully just mapped your attack code back into</span>
<span class="quote">&gt; the kernel.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Why not *just* do these moves at allocation time?</span>

Yes, this is a great point, thanks. I think this can be a no-op, and
with the fixed up v7 logic for alloc pages that I linked to above it
should work out correctly.

Tycho
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=65121">Dave Hansen</a> - Sept. 20, 2017, 11:21 p.m.</div>
<pre class="content">
On 09/20/2017 03:34 PM, Tycho Andersen wrote:
<span class="quote">&gt;&gt; I really have to wonder whether there are better ret2dir defenses than</span>
<span class="quote">&gt;&gt; this.  The allocator just seems like the *wrong* place to be doing this</span>
<span class="quote">&gt;&gt; because it&#39;s such a hot path.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This might be crazy, but what if we defer flushing of the kernel</span>
<span class="quote">&gt; ranges until just before we return to userspace? We&#39;d still manipulate</span>
<span class="quote">&gt; the prot/xpfo bits for the pages, but then just keep a list of which</span>
<span class="quote">&gt; ranges need to be flushed, and do the right thing before we return.</span>
<span class="quote">&gt; This leaves a little window between the actual allocation and the</span>
<span class="quote">&gt; flush, but userspace would need another thread in its threadgroup to</span>
<span class="quote">&gt; predict the next allocation, write the bad stuff there, and do the</span>
<span class="quote">&gt; exploit all in that window.</span>

I think the common case is still that you enter the kernel, allocate a
single page (or very few) and then exit.  So, you don&#39;t really reduce
the total number of flushes.

Just think of this in terms of IPIs to do the remote TLB flushes.  A CPU
can do roughly 1 million page faults and allocations a second.  Say you
have a 2-socket x 28-core x 2 hyperthead system = 112 CPU threads.
That&#39;s 111M IPI interrupts/second, just for the TLB flushes, *ON* *EACH*
*CPU*.

I think the only thing that will really help here is if you batch the
allocations.  For instance, you could make sure that the per-cpu-pageset
lists always contain either all kernel or all user data.  Then remap the
entire list at once and do a single flush after the entire list is consumed.
<span class="quote">
&gt;&gt; Why do you even bother keeping large pages around?  Won&#39;t the entire</span>
<span class="quote">&gt;&gt; kernel just degrade to using 4k everywhere, eventually?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Isn&#39;t that true of large pages in general? Is there something about</span>
<span class="quote">&gt; xpfo that makes this worse? I thought this would only split things if</span>
<span class="quote">&gt; they had already been split somewhere else, and the protection can&#39;t</span>
<span class="quote">&gt; apply to the whole huge page.</span>

Even though the kernel gives out 4k pages, it still *maps* them in the
kernel linear direct map with the largest size available.  My 16GB
laptop, for instance, has 3GB of 2MB transparent huge pages, but the
rest is used as 4k pages.  Yet, from /proc/meminfo:

DirectMap4k:      665280 kB
DirectMap2M:    11315200 kB
DirectMap1G:     4194304 kB

Your code pretty much forces 4k pages coming out of the allocator to be
mapped with 4k mappings.
<span class="quote">&gt;&gt;&gt; +inline void xpfo_flush_kernel_tlb(struct page *page, int order)</span>
<span class="quote">&gt;&gt;&gt; +{</span>
<span class="quote">&gt;&gt;&gt; +	int level;</span>
<span class="quote">&gt;&gt;&gt; +	unsigned long size, kaddr;</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; +	kaddr = (unsigned long)page_address(page);</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; +	if (unlikely(!lookup_address(kaddr, &amp;level))) {</span>
<span class="quote">&gt;&gt;&gt; +		WARN(1, &quot;xpfo: invalid address to flush %lx %d\n&quot;, kaddr, level);</span>
<span class="quote">&gt;&gt;&gt; +		return;</span>
<span class="quote">&gt;&gt;&gt; +	}</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; +	switch (level) {</span>
<span class="quote">&gt;&gt;&gt; +	case PG_LEVEL_4K:</span>
<span class="quote">&gt;&gt;&gt; +		size = PAGE_SIZE;</span>
<span class="quote">&gt;&gt;&gt; +		break;</span>
<span class="quote">&gt;&gt;&gt; +	case PG_LEVEL_2M:</span>
<span class="quote">&gt;&gt;&gt; +		size = PMD_SIZE;</span>
<span class="quote">&gt;&gt;&gt; +		break;</span>
<span class="quote">&gt;&gt;&gt; +	case PG_LEVEL_1G:</span>
<span class="quote">&gt;&gt;&gt; +		size = PUD_SIZE;</span>
<span class="quote">&gt;&gt;&gt; +		break;</span>
<span class="quote">&gt;&gt;&gt; +	default:</span>
<span class="quote">&gt;&gt;&gt; +		WARN(1, &quot;xpfo: unsupported page level %x\n&quot;, level);</span>
<span class="quote">&gt;&gt;&gt; +		return;</span>
<span class="quote">&gt;&gt;&gt; +	}</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; +	flush_tlb_kernel_range(kaddr, kaddr + (1 &lt;&lt; order) * size);</span>
<span class="quote">&gt;&gt;&gt; +}</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; I&#39;m not sure flush_tlb_kernel_range() is the best primitive to be</span>
<span class="quote">&gt;&gt; calling here.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Let&#39;s say you walk the page tables and find level=PG_LEVEL_1G.  You call</span>
<span class="quote">&gt;&gt; flush_tlb_kernel_range(), you will be above</span>
<span class="quote">&gt;&gt; tlb_single_page_flush_ceiling, and you will do a full TLB flush.  But,</span>
<span class="quote">&gt;&gt; with a 1GB page, you could have just used a single INVLPG and skipped</span>
<span class="quote">&gt;&gt; the global flush.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; I guess the cost of the IPI is way more than the flush itself, but it&#39;s</span>
<span class="quote">&gt;&gt; still a shame to toss the entire TLB when you don&#39;t have to.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Ok, do you think it&#39;s worth making a new helper for others to use? Or</span>
<span class="quote">&gt; should I just keep the logic in this function?</span>

I&#39;d just leave it in place.  Most folks already have a PTE when they do
the invalidation, so this is a bit of a weirdo.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=65121">Dave Hansen</a> - Sept. 21, 2017, 12:03 a.m.</div>
<pre class="content">
On 09/07/2017 10:36 AM, Tycho Andersen wrote:
<span class="quote">&gt; +		/*</span>
<span class="quote">&gt; +		 * Map the page back into the kernel if it was previously</span>
<span class="quote">&gt; +		 * allocated to user space.</span>
<span class="quote">&gt; +		 */</span>
<span class="quote">&gt; +		if (test_and_clear_bit(XPFO_PAGE_USER, &amp;xpfo-&gt;flags)) {</span>
<span class="quote">&gt; +			clear_bit(XPFO_PAGE_UNMAPPED, &amp;xpfo-&gt;flags);</span>
<span class="quote">&gt; +			set_kpte(page_address(page + i), page + i,</span>
<span class="quote">&gt; +				 PAGE_KERNEL);</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +	}</span>

It might also be a really good idea to clear the page here.  Otherwise,
the page still might have attack code in it and now it is mapped into
the kernel again, ready to be exploited.

Think of it this way: pages either trusted data and are mapped all the
time, or they have potentially bad data and are unmapped mostly.  If we
want to take a bad page and map it always, we have to make sure the
contents are not evil.  0&#39;s are not evil.
<span class="quote">
&gt;  static inline void *kmap(struct page *page)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; +	void *kaddr;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	might_sleep();</span>
<span class="quote">&gt; -	return page_address(page);</span>
<span class="quote">&gt; +	kaddr = page_address(page);</span>
<span class="quote">&gt; +	xpfo_kmap(kaddr, page);</span>
<span class="quote">&gt; +	return kaddr;</span>
<span class="quote">&gt;  }</span>

The time between kmap() and kunmap() is potentially a really long
operation.  I think we, for instance, keep some pages kmap()&#39;d while we
do I/O to them, or wait for I/O elsewhere.

IOW, this will map predictable data at a predictable location and it
will do it for a long time.  While that&#39;s better than the current state
(mapped always), it still seems rather risky.

Could you, for instance, turn kmap(page) into vmap(&amp;page, 1, ...)?  That
way, at least the address may be different each time.  Even if an
attacker knows the physical address, they don&#39;t know where it will be
mapped.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=173139">Tycho Andersen</a> - Sept. 21, 2017, 12:09 a.m.</div>
<pre class="content">
On Wed, Sep 20, 2017 at 04:21:15PM -0700, Dave Hansen wrote:
<span class="quote">&gt; On 09/20/2017 03:34 PM, Tycho Andersen wrote:</span>
<span class="quote">&gt; &gt;&gt; I really have to wonder whether there are better ret2dir defenses than</span>
<span class="quote">&gt; &gt;&gt; this.  The allocator just seems like the *wrong* place to be doing this</span>
<span class="quote">&gt; &gt;&gt; because it&#39;s such a hot path.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; This might be crazy, but what if we defer flushing of the kernel</span>
<span class="quote">&gt; &gt; ranges until just before we return to userspace? We&#39;d still manipulate</span>
<span class="quote">&gt; &gt; the prot/xpfo bits for the pages, but then just keep a list of which</span>
<span class="quote">&gt; &gt; ranges need to be flushed, and do the right thing before we return.</span>
<span class="quote">&gt; &gt; This leaves a little window between the actual allocation and the</span>
<span class="quote">&gt; &gt; flush, but userspace would need another thread in its threadgroup to</span>
<span class="quote">&gt; &gt; predict the next allocation, write the bad stuff there, and do the</span>
<span class="quote">&gt; &gt; exploit all in that window.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I think the common case is still that you enter the kernel, allocate a</span>
<span class="quote">&gt; single page (or very few) and then exit.  So, you don&#39;t really reduce</span>
<span class="quote">&gt; the total number of flushes.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Just think of this in terms of IPIs to do the remote TLB flushes.  A CPU</span>
<span class="quote">&gt; can do roughly 1 million page faults and allocations a second.  Say you</span>
<span class="quote">&gt; have a 2-socket x 28-core x 2 hyperthead system = 112 CPU threads.</span>
<span class="quote">&gt; That&#39;s 111M IPI interrupts/second, just for the TLB flushes, *ON* *EACH*</span>
<span class="quote">&gt; *CPU*.</span>

Since we only need to flush when something switches from a userspace
to a kernel page or back, hopefully it&#39;s not this bad, but point
taken.
<span class="quote">
&gt; I think the only thing that will really help here is if you batch the</span>
<span class="quote">&gt; allocations.  For instance, you could make sure that the per-cpu-pageset</span>
<span class="quote">&gt; lists always contain either all kernel or all user data.  Then remap the</span>
<span class="quote">&gt; entire list at once and do a single flush after the entire list is consumed.</span>

Just so I understand, the idea would be that we only flush when the
type of allocation alternates, so:

kmalloc(..., GFP_KERNEL);
kmalloc(..., GFP_KERNEL);
/* remap+flush here */
kmalloc(..., GFP_HIGHUSER);
/* remap+flush here */
kmalloc(..., GFP_KERNEL);

?

Tycho
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=65121">Dave Hansen</a> - Sept. 21, 2017, 12:27 a.m.</div>
<pre class="content">
On 09/20/2017 05:09 PM, Tycho Andersen wrote:
<span class="quote">&gt;&gt; I think the only thing that will really help here is if you batch the</span>
<span class="quote">&gt;&gt; allocations.  For instance, you could make sure that the per-cpu-pageset</span>
<span class="quote">&gt;&gt; lists always contain either all kernel or all user data.  Then remap the</span>
<span class="quote">&gt;&gt; entire list at once and do a single flush after the entire list is consumed.</span>
<span class="quote">&gt; Just so I understand, the idea would be that we only flush when the</span>
<span class="quote">&gt; type of allocation alternates, so:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; kmalloc(..., GFP_KERNEL);</span>
<span class="quote">&gt; kmalloc(..., GFP_KERNEL);</span>
<span class="quote">&gt; /* remap+flush here */</span>
<span class="quote">&gt; kmalloc(..., GFP_HIGHUSER);</span>
<span class="quote">&gt; /* remap+flush here */</span>
<span class="quote">&gt; kmalloc(..., GFP_KERNEL);</span>

Not really.  We keep a free list per migrate type, and a per_cpu_pages
(pcp) list per migratetype:
<span class="quote">
&gt; struct per_cpu_pages {</span>
<span class="quote">&gt;         int count;              /* number of pages in the list */</span>
<span class="quote">&gt;         int high;               /* high watermark, emptying needed */</span>
<span class="quote">&gt;         int batch;              /* chunk size for buddy add/remove */</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;         /* Lists of pages, one per migrate type stored on the pcp-lists */</span>
<span class="quote">&gt;         struct list_head lists[MIGRATE_PCPTYPES];</span>
<span class="quote">&gt; };</span>

The migratetype is derived from the GFP flags in
gfpflags_to_migratetype().  In general, GFP_HIGHUSER and GFP_KERNEL come
from different migratetypes, so they come from different free lists.

In your case above, the GFP_HIGHUSER allocation come through the
MIGRATE_MOVABLE pcp list while the GFP_KERNEL ones come from the
MIGRATE_UNMOVABLE one.  Since we add a bunch of pages to those lists at
once, you could do all the mapping/unmapping/flushing on a bunch of
pages at once

Or, you could hook your code into the places where the migratetype of
memory is changed (set_pageblock_migratetype(), plus where we fall
back).  Those changes are much more rare than page allocation.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=65121">Dave Hansen</a> - Sept. 21, 2017, 12:28 a.m.</div>
<pre class="content">
At a high level, does this approach keep an attacker from being able to
determine the address of data in the linear map, or does it keep them
from being able to *exploit* it?  Can you have a ret2dir attack if the
attacker doesn&#39;t know the address, for instance?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=173139">Tycho Andersen</a> - Sept. 21, 2017, 1:04 a.m.</div>
<pre class="content">
On Wed, Sep 20, 2017 at 05:28:11PM -0700, Dave Hansen wrote:
<span class="quote">&gt; At a high level, does this approach keep an attacker from being able to</span>
<span class="quote">&gt; determine the address of data in the linear map, or does it keep them</span>
<span class="quote">&gt; from being able to *exploit* it?</span>

It keeps them from exploiting it, by faulting when a physmap alias is
used.
<span class="quote">
&gt; Can you have a ret2dir attack if the attacker doesn&#39;t know the</span>
<span class="quote">&gt; address, for instance?</span>

Yes, through a technique similar to heap spraying. The original paper
has a study of this, section 5.2 outlines the attack and 7.2 describes
their success rate:

http://www.cs.columbia.edu/~vpk/papers/ret2dir.sec14.pdf

Tycho
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=173139">Tycho Andersen</a> - Sept. 21, 2017, 1:37 a.m.</div>
<pre class="content">
On Wed, Sep 20, 2017 at 05:27:02PM -0700, Dave Hansen wrote:
<span class="quote">&gt; On 09/20/2017 05:09 PM, Tycho Andersen wrote:</span>
<span class="quote">&gt; &gt;&gt; I think the only thing that will really help here is if you batch the</span>
<span class="quote">&gt; &gt;&gt; allocations.  For instance, you could make sure that the per-cpu-pageset</span>
<span class="quote">&gt; &gt;&gt; lists always contain either all kernel or all user data.  Then remap the</span>
<span class="quote">&gt; &gt;&gt; entire list at once and do a single flush after the entire list is consumed.</span>
<span class="quote">&gt; &gt; Just so I understand, the idea would be that we only flush when the</span>
<span class="quote">&gt; &gt; type of allocation alternates, so:</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; kmalloc(..., GFP_KERNEL);</span>
<span class="quote">&gt; &gt; kmalloc(..., GFP_KERNEL);</span>
<span class="quote">&gt; &gt; /* remap+flush here */</span>
<span class="quote">&gt; &gt; kmalloc(..., GFP_HIGHUSER);</span>
<span class="quote">&gt; &gt; /* remap+flush here */</span>
<span class="quote">&gt; &gt; kmalloc(..., GFP_KERNEL);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Not really.  We keep a free list per migrate type, and a per_cpu_pages</span>
<span class="quote">&gt; (pcp) list per migratetype:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; struct per_cpu_pages {</span>
<span class="quote">&gt; &gt;         int count;              /* number of pages in the list */</span>
<span class="quote">&gt; &gt;         int high;               /* high watermark, emptying needed */</span>
<span class="quote">&gt; &gt;         int batch;              /* chunk size for buddy add/remove */</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt;         /* Lists of pages, one per migrate type stored on the pcp-lists */</span>
<span class="quote">&gt; &gt;         struct list_head lists[MIGRATE_PCPTYPES];</span>
<span class="quote">&gt; &gt; };</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The migratetype is derived from the GFP flags in</span>
<span class="quote">&gt; gfpflags_to_migratetype().  In general, GFP_HIGHUSER and GFP_KERNEL come</span>
<span class="quote">&gt; from different migratetypes, so they come from different free lists.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; In your case above, the GFP_HIGHUSER allocation come through the</span>
<span class="quote">&gt; MIGRATE_MOVABLE pcp list while the GFP_KERNEL ones come from the</span>
<span class="quote">&gt; MIGRATE_UNMOVABLE one.  Since we add a bunch of pages to those lists at</span>
<span class="quote">&gt; once, you could do all the mapping/unmapping/flushing on a bunch of</span>
<span class="quote">&gt; pages at once</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Or, you could hook your code into the places where the migratetype of</span>
<span class="quote">&gt; memory is changed (set_pageblock_migratetype(), plus where we fall</span>
<span class="quote">&gt; back).  Those changes are much more rare than page allocation.</span>

I see, thanks for all this discussion. It has been very helpful!

Tycho
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/Documentation/admin-guide/kernel-parameters.txt b/Documentation/admin-guide/kernel-parameters.txt</span>
<span class="p_header">index d9c171ce4190..444d83183f75 100644</span>
<span class="p_header">--- a/Documentation/admin-guide/kernel-parameters.txt</span>
<span class="p_header">+++ b/Documentation/admin-guide/kernel-parameters.txt</span>
<span class="p_chunk">@@ -2736,6 +2736,8 @@</span> <span class="p_context"></span>
 
 	nox2apic	[X86-64,APIC] Do not enable x2APIC mode.
 
<span class="p_add">+	noxpfo		[X86-64] Disable XPFO when CONFIG_XPFO is on.</span>
<span class="p_add">+</span>
 	cpu0_hotplug	[X86] Turn on CPU0 hotplug feature when
 			CONFIG_BOOTPARAM_HOTPLUG_CPU0 is off.
 			Some features depend on CPU0. Known dependencies are:
<span class="p_header">diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig</span>
<span class="p_header">index 323cb065be5e..d78a0d538900 100644</span>
<span class="p_header">--- a/arch/x86/Kconfig</span>
<span class="p_header">+++ b/arch/x86/Kconfig</span>
<span class="p_chunk">@@ -185,6 +185,7 @@</span> <span class="p_context"> config X86</span>
 	select USER_STACKTRACE_SUPPORT
 	select VIRT_TO_BUS
 	select X86_FEATURE_NAMES		if PROC_FS
<span class="p_add">+	select ARCH_SUPPORTS_XPFO		if X86_64</span>
 
 config INSTRUCTION_DECODER
 	def_bool y
<span class="p_header">diff --git a/arch/x86/include/asm/pgtable.h b/arch/x86/include/asm/pgtable.h</span>
<span class="p_header">index 77037b6f1caa..c2eb40f7a74b 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/pgtable.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/pgtable.h</span>
<span class="p_chunk">@@ -1238,6 +1238,31 @@</span> <span class="p_context"> static inline bool pud_access_permitted(pud_t pud, bool write)</span>
 	return __pte_access_permitted(pud_val(pud), write);
 }
 
<span class="p_add">+/*</span>
<span class="p_add">+ * The current flushing context - we pass it instead of 5 arguments:</span>
<span class="p_add">+ */</span>
<span class="p_add">+struct cpa_data {</span>
<span class="p_add">+	unsigned long	*vaddr;</span>
<span class="p_add">+	pgd_t		*pgd;</span>
<span class="p_add">+	pgprot_t	mask_set;</span>
<span class="p_add">+	pgprot_t	mask_clr;</span>
<span class="p_add">+	unsigned long	numpages;</span>
<span class="p_add">+	int		flags;</span>
<span class="p_add">+	unsigned long	pfn;</span>
<span class="p_add">+	unsigned	force_split : 1;</span>
<span class="p_add">+	int		curpage;</span>
<span class="p_add">+	struct page	**pages;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+int</span>
<span class="p_add">+try_preserve_large_page(pte_t *kpte, unsigned long address,</span>
<span class="p_add">+			struct cpa_data *cpa);</span>
<span class="p_add">+extern spinlock_t cpa_lock;</span>
<span class="p_add">+int</span>
<span class="p_add">+__split_large_page(struct cpa_data *cpa, pte_t *kpte, unsigned long address,</span>
<span class="p_add">+		   struct page *base);</span>
<span class="p_add">+</span>
 #include &lt;asm-generic/pgtable.h&gt;
 #endif	/* __ASSEMBLY__ */
 
<span class="p_header">diff --git a/arch/x86/mm/Makefile b/arch/x86/mm/Makefile</span>
<span class="p_header">index 0fbdcb64f9f8..89ba6d25fb51 100644</span>
<span class="p_header">--- a/arch/x86/mm/Makefile</span>
<span class="p_header">+++ b/arch/x86/mm/Makefile</span>
<span class="p_chunk">@@ -39,3 +39,4 @@</span> <span class="p_context"> obj-$(CONFIG_X86_INTEL_MPX)	+= mpx.o</span>
 obj-$(CONFIG_X86_INTEL_MEMORY_PROTECTION_KEYS) += pkeys.o
 obj-$(CONFIG_RANDOMIZE_MEMORY) += kaslr.o
 
<span class="p_add">+obj-$(CONFIG_XPFO)		+= xpfo.o</span>
<span class="p_header">diff --git a/arch/x86/mm/pageattr.c b/arch/x86/mm/pageattr.c</span>
<span class="p_header">index 757b0bcdf712..f25d07191e60 100644</span>
<span class="p_header">--- a/arch/x86/mm/pageattr.c</span>
<span class="p_header">+++ b/arch/x86/mm/pageattr.c</span>
<span class="p_chunk">@@ -27,28 +27,12 @@</span> <span class="p_context"></span>
 #include &lt;asm/set_memory.h&gt;
 
 /*
<span class="p_del">- * The current flushing context - we pass it instead of 5 arguments:</span>
<span class="p_del">- */</span>
<span class="p_del">-struct cpa_data {</span>
<span class="p_del">-	unsigned long	*vaddr;</span>
<span class="p_del">-	pgd_t		*pgd;</span>
<span class="p_del">-	pgprot_t	mask_set;</span>
<span class="p_del">-	pgprot_t	mask_clr;</span>
<span class="p_del">-	unsigned long	numpages;</span>
<span class="p_del">-	int		flags;</span>
<span class="p_del">-	unsigned long	pfn;</span>
<span class="p_del">-	unsigned	force_split : 1;</span>
<span class="p_del">-	int		curpage;</span>
<span class="p_del">-	struct page	**pages;</span>
<span class="p_del">-};</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
  * Serialize cpa() (for !DEBUG_PAGEALLOC which uses large identity mappings)
  * using cpa_lock. So that we don&#39;t allow any other cpu, with stale large tlb
  * entries change the page attribute in parallel to some other cpu
  * splitting a large page entry along with changing the attribute.
  */
<span class="p_del">-static DEFINE_SPINLOCK(cpa_lock);</span>
<span class="p_add">+DEFINE_SPINLOCK(cpa_lock);</span>
 
 #define CPA_FLUSHTLB 1
 #define CPA_ARRAY 2
<span class="p_chunk">@@ -512,7 +496,7 @@</span> <span class="p_context"> static void __set_pmd_pte(pte_t *kpte, unsigned long address, pte_t pte)</span>
 #endif
 }
 
<span class="p_del">-static int</span>
<span class="p_add">+int</span>
 try_preserve_large_page(pte_t *kpte, unsigned long address,
 			struct cpa_data *cpa)
 {
<span class="p_chunk">@@ -648,7 +632,7 @@</span> <span class="p_context"> try_preserve_large_page(pte_t *kpte, unsigned long address,</span>
 	return do_split;
 }
 
<span class="p_del">-static int</span>
<span class="p_add">+int</span>
 __split_large_page(struct cpa_data *cpa, pte_t *kpte, unsigned long address,
 		   struct page *base)
 {
<span class="p_header">diff --git a/arch/x86/mm/xpfo.c b/arch/x86/mm/xpfo.c</span>
new file mode 100644
<span class="p_header">index 000000000000..6794d6724ab5</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/x86/mm/xpfo.c</span>
<span class="p_chunk">@@ -0,0 +1,114 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2017 Hewlett Packard Enterprise Development, L.P.</span>
<span class="p_add">+ * Copyright (C) 2016 Brown University. All rights reserved.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Authors:</span>
<span class="p_add">+ *   Juerg Haefliger &lt;juerg.haefliger@hpe.com&gt;</span>
<span class="p_add">+ *   Vasileios P. Kemerlis &lt;vpk@cs.brown.edu&gt;</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify it</span>
<span class="p_add">+ * under the terms of the GNU General Public License version 2 as published by</span>
<span class="p_add">+ * the Free Software Foundation.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/mm.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/tlbflush.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+extern spinlock_t cpa_lock;</span>
<span class="p_add">+</span>
<span class="p_add">+/* Update a single kernel page table entry */</span>
<span class="p_add">+inline void set_kpte(void *kaddr, struct page *page, pgprot_t prot)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned int level;</span>
<span class="p_add">+	pgprot_t msk_clr;</span>
<span class="p_add">+	pte_t *pte = lookup_address((unsigned long)kaddr, &amp;level);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (unlikely(!pte)) {</span>
<span class="p_add">+		WARN(1, &quot;xpfo: invalid address %p\n&quot;, kaddr);</span>
<span class="p_add">+		return;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	switch (level) {</span>
<span class="p_add">+	case PG_LEVEL_4K:</span>
<span class="p_add">+		set_pte_atomic(pte, pfn_pte(page_to_pfn(page), canon_pgprot(prot)));</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case PG_LEVEL_2M:</span>
<span class="p_add">+	case PG_LEVEL_1G: {</span>
<span class="p_add">+		struct cpa_data cpa = { };</span>
<span class="p_add">+		int do_split;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (level == PG_LEVEL_2M)</span>
<span class="p_add">+			msk_clr = pmd_pgprot(*(pmd_t*)pte);</span>
<span class="p_add">+		else</span>
<span class="p_add">+			msk_clr = pud_pgprot(*(pud_t*)pte);</span>
<span class="p_add">+</span>
<span class="p_add">+		cpa.vaddr = kaddr;</span>
<span class="p_add">+		cpa.pages = &amp;page;</span>
<span class="p_add">+		cpa.mask_set = prot;</span>
<span class="p_add">+		cpa.mask_clr = msk_clr;</span>
<span class="p_add">+		cpa.numpages = 1;</span>
<span class="p_add">+		cpa.flags = 0;</span>
<span class="p_add">+		cpa.curpage = 0;</span>
<span class="p_add">+		cpa.force_split = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+		do_split = try_preserve_large_page(pte, (unsigned long)kaddr,</span>
<span class="p_add">+						   &amp;cpa);</span>
<span class="p_add">+		if (do_split) {</span>
<span class="p_add">+			struct page *base;</span>
<span class="p_add">+</span>
<span class="p_add">+			base = alloc_pages(GFP_ATOMIC | __GFP_NOTRACK, 0);</span>
<span class="p_add">+			if (!base) {</span>
<span class="p_add">+				WARN(1, &quot;xpfo: failed to split large page\n&quot;);</span>
<span class="p_add">+				break;</span>
<span class="p_add">+			}</span>
<span class="p_add">+</span>
<span class="p_add">+			if (!debug_pagealloc_enabled())</span>
<span class="p_add">+				spin_lock(&amp;cpa_lock);</span>
<span class="p_add">+			if  (__split_large_page(&amp;cpa, pte, (unsigned long)kaddr, base) &lt; 0)</span>
<span class="p_add">+				WARN(1, &quot;xpfo: failed to split large page\n&quot;);</span>
<span class="p_add">+			if (!debug_pagealloc_enabled())</span>
<span class="p_add">+				spin_unlock(&amp;cpa_lock);</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	case PG_LEVEL_512G:</span>
<span class="p_add">+		/* fallthrough, splitting infrastructure doesn&#39;t</span>
<span class="p_add">+		 * support 512G pages. */</span>
<span class="p_add">+	default:</span>
<span class="p_add">+		WARN(1, &quot;xpfo: unsupported page level %x\n&quot;, level);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+inline void xpfo_flush_kernel_tlb(struct page *page, int order)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int level;</span>
<span class="p_add">+	unsigned long size, kaddr;</span>
<span class="p_add">+</span>
<span class="p_add">+	kaddr = (unsigned long)page_address(page);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (unlikely(!lookup_address(kaddr, &amp;level))) {</span>
<span class="p_add">+		WARN(1, &quot;xpfo: invalid address to flush %lx %d\n&quot;, kaddr, level);</span>
<span class="p_add">+		return;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	switch (level) {</span>
<span class="p_add">+	case PG_LEVEL_4K:</span>
<span class="p_add">+		size = PAGE_SIZE;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case PG_LEVEL_2M:</span>
<span class="p_add">+		size = PMD_SIZE;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case PG_LEVEL_1G:</span>
<span class="p_add">+		size = PUD_SIZE;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	default:</span>
<span class="p_add">+		WARN(1, &quot;xpfo: unsupported page level %x\n&quot;, level);</span>
<span class="p_add">+		return;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	flush_tlb_kernel_range(kaddr, kaddr + (1 &lt;&lt; order) * size);</span>
<span class="p_add">+}</span>
<span class="p_header">diff --git a/include/linux/highmem.h b/include/linux/highmem.h</span>
<span class="p_header">index bb3f3297062a..7a17c166532f 100644</span>
<span class="p_header">--- a/include/linux/highmem.h</span>
<span class="p_header">+++ b/include/linux/highmem.h</span>
<span class="p_chunk">@@ -7,6 +7,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/mm.h&gt;
 #include &lt;linux/uaccess.h&gt;
 #include &lt;linux/hardirq.h&gt;
<span class="p_add">+#include &lt;linux/xpfo.h&gt;</span>
 
 #include &lt;asm/cacheflush.h&gt;
 
<span class="p_chunk">@@ -55,24 +56,34 @@</span> <span class="p_context"> static inline struct page *kmap_to_page(void *addr)</span>
 #ifndef ARCH_HAS_KMAP
 static inline void *kmap(struct page *page)
 {
<span class="p_add">+	void *kaddr;</span>
<span class="p_add">+</span>
 	might_sleep();
<span class="p_del">-	return page_address(page);</span>
<span class="p_add">+	kaddr = page_address(page);</span>
<span class="p_add">+	xpfo_kmap(kaddr, page);</span>
<span class="p_add">+	return kaddr;</span>
 }
 
 static inline void kunmap(struct page *page)
 {
<span class="p_add">+	xpfo_kunmap(page_address(page), page);</span>
 }
 
 static inline void *kmap_atomic(struct page *page)
 {
<span class="p_add">+	void *kaddr;</span>
<span class="p_add">+</span>
 	preempt_disable();
 	pagefault_disable();
<span class="p_del">-	return page_address(page);</span>
<span class="p_add">+	kaddr = page_address(page);</span>
<span class="p_add">+	xpfo_kmap(kaddr, page);</span>
<span class="p_add">+	return kaddr;</span>
 }
 #define kmap_atomic_prot(page, prot)	kmap_atomic(page)
 
 static inline void __kunmap_atomic(void *addr)
 {
<span class="p_add">+	xpfo_kunmap(addr, virt_to_page(addr));</span>
 	pagefault_enable();
 	preempt_enable();
 }
<span class="p_header">diff --git a/include/linux/xpfo.h b/include/linux/xpfo.h</span>
new file mode 100644
<span class="p_header">index 000000000000..442c58ee930e</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/include/linux/xpfo.h</span>
<span class="p_chunk">@@ -0,0 +1,42 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2017 Docker, Inc.</span>
<span class="p_add">+ * Copyright (C) 2017 Hewlett Packard Enterprise Development, L.P.</span>
<span class="p_add">+ * Copyright (C) 2016 Brown University. All rights reserved.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Authors:</span>
<span class="p_add">+ *   Juerg Haefliger &lt;juerg.haefliger@hpe.com&gt;</span>
<span class="p_add">+ *   Vasileios P. Kemerlis &lt;vpk@cs.brown.edu&gt;</span>
<span class="p_add">+ *   Tycho Andersen &lt;tycho@docker.com&gt;</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify it</span>
<span class="p_add">+ * under the terms of the GNU General Public License version 2 as published by</span>
<span class="p_add">+ * the Free Software Foundation.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _LINUX_XPFO_H</span>
<span class="p_add">+#define _LINUX_XPFO_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_XPFO</span>
<span class="p_add">+</span>
<span class="p_add">+extern struct page_ext_operations page_xpfo_ops;</span>
<span class="p_add">+</span>
<span class="p_add">+void set_kpte(void *kaddr, struct page *page, pgprot_t prot);</span>
<span class="p_add">+void xpfo_dma_map_unmap_area(bool map, const void *addr, size_t size,</span>
<span class="p_add">+				    enum dma_data_direction dir);</span>
<span class="p_add">+void xpfo_flush_kernel_tlb(struct page *page, int order);</span>
<span class="p_add">+</span>
<span class="p_add">+void xpfo_kmap(void *kaddr, struct page *page);</span>
<span class="p_add">+void xpfo_kunmap(void *kaddr, struct page *page);</span>
<span class="p_add">+void xpfo_alloc_pages(struct page *page, int order, gfp_t gfp);</span>
<span class="p_add">+void xpfo_free_pages(struct page *page, int order);</span>
<span class="p_add">+</span>
<span class="p_add">+#else /* !CONFIG_XPFO */</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void xpfo_kmap(void *kaddr, struct page *page) { }</span>
<span class="p_add">+static inline void xpfo_kunmap(void *kaddr, struct page *page) { }</span>
<span class="p_add">+static inline void xpfo_alloc_pages(struct page *page, int order, gfp_t gfp) { }</span>
<span class="p_add">+static inline void xpfo_free_pages(struct page *page, int order) { }</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* CONFIG_XPFO */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _LINUX_XPFO_H */</span>
<span class="p_header">diff --git a/mm/Makefile b/mm/Makefile</span>
<span class="p_header">index 411bd24d4a7c..0be67cac8f6c 100644</span>
<span class="p_header">--- a/mm/Makefile</span>
<span class="p_header">+++ b/mm/Makefile</span>
<span class="p_chunk">@@ -104,3 +104,4 @@</span> <span class="p_context"> obj-$(CONFIG_FRAME_VECTOR) += frame_vector.o</span>
 obj-$(CONFIG_DEBUG_PAGE_REF) += debug_page_ref.o
 obj-$(CONFIG_HARDENED_USERCOPY) += usercopy.o
 obj-$(CONFIG_PERCPU_STATS) += percpu-stats.o
<span class="p_add">+obj-$(CONFIG_XPFO) += xpfo.o</span>
<span class="p_header">diff --git a/mm/page_alloc.c b/mm/page_alloc.c</span>
<span class="p_header">index 1423da8dd16f..09fdf1bad21f 100644</span>
<span class="p_header">--- a/mm/page_alloc.c</span>
<span class="p_header">+++ b/mm/page_alloc.c</span>
<span class="p_chunk">@@ -1059,6 +1059,7 @@</span> <span class="p_context"> static __always_inline bool free_pages_prepare(struct page *page,</span>
 	kernel_poison_pages(page, 1 &lt;&lt; order, 0);
 	kernel_map_pages(page, 1 &lt;&lt; order, 0);
 	kasan_free_pages(page, order);
<span class="p_add">+	xpfo_free_pages(page, order);</span>
 
 	return true;
 }
<span class="p_chunk">@@ -1758,6 +1759,7 @@</span> <span class="p_context"> inline void post_alloc_hook(struct page *page, unsigned int order,</span>
 	kernel_map_pages(page, 1 &lt;&lt; order, 1);
 	kernel_poison_pages(page, 1 &lt;&lt; order, 1);
 	kasan_alloc_pages(page, order);
<span class="p_add">+	xpfo_alloc_pages(page, order, gfp_flags);</span>
 	set_page_owner(page, order, gfp_flags);
 }
 
<span class="p_header">diff --git a/mm/page_ext.c b/mm/page_ext.c</span>
<span class="p_header">index 88ccc044b09a..4899df1f5d66 100644</span>
<span class="p_header">--- a/mm/page_ext.c</span>
<span class="p_header">+++ b/mm/page_ext.c</span>
<span class="p_chunk">@@ -7,6 +7,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/kmemleak.h&gt;
 #include &lt;linux/page_owner.h&gt;
 #include &lt;linux/page_idle.h&gt;
<span class="p_add">+#include &lt;linux/xpfo.h&gt;</span>
 
 /*
  * struct page extension
<span class="p_chunk">@@ -65,6 +66,9 @@</span> <span class="p_context"> static struct page_ext_operations *page_ext_ops[] = {</span>
 #if defined(CONFIG_IDLE_PAGE_TRACKING) &amp;&amp; !defined(CONFIG_64BIT)
 	&amp;page_idle_ops,
 #endif
<span class="p_add">+#ifdef CONFIG_XPFO</span>
<span class="p_add">+	&amp;page_xpfo_ops,</span>
<span class="p_add">+#endif</span>
 };
 
 static unsigned long total_usage;
<span class="p_header">diff --git a/mm/xpfo.c b/mm/xpfo.c</span>
new file mode 100644
<span class="p_header">index 000000000000..bff24afcaa2e</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/mm/xpfo.c</span>
<span class="p_chunk">@@ -0,0 +1,222 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2017 Docker, Inc.</span>
<span class="p_add">+ * Copyright (C) 2017 Hewlett Packard Enterprise Development, L.P.</span>
<span class="p_add">+ * Copyright (C) 2016 Brown University. All rights reserved.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Authors:</span>
<span class="p_add">+ *   Juerg Haefliger &lt;juerg.haefliger@hpe.com&gt;</span>
<span class="p_add">+ *   Vasileios P. Kemerlis &lt;vpk@cs.brown.edu&gt;</span>
<span class="p_add">+ *   Tycho Andersen &lt;tycho@docker.com&gt;</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify it</span>
<span class="p_add">+ * under the terms of the GNU General Public License version 2 as published by</span>
<span class="p_add">+ * the Free Software Foundation.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/mm.h&gt;</span>
<span class="p_add">+#include &lt;linux/module.h&gt;</span>
<span class="p_add">+#include &lt;linux/page_ext.h&gt;</span>
<span class="p_add">+#include &lt;linux/xpfo.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/tlbflush.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+/* XPFO page state flags */</span>
<span class="p_add">+enum xpfo_flags {</span>
<span class="p_add">+	XPFO_PAGE_USER,		/* Page is allocated to user-space */</span>
<span class="p_add">+	XPFO_PAGE_UNMAPPED,	/* Page is unmapped from the linear map */</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+/* Per-page XPFO house-keeping data */</span>
<span class="p_add">+struct xpfo {</span>
<span class="p_add">+	unsigned long flags;	/* Page state */</span>
<span class="p_add">+	bool inited;		/* Map counter and lock initialized */</span>
<span class="p_add">+	atomic_t mapcount;	/* Counter for balancing map/unmap requests */</span>
<span class="p_add">+	spinlock_t maplock;	/* Lock to serialize map/unmap requests */</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+DEFINE_STATIC_KEY_FALSE(xpfo_inited);</span>
<span class="p_add">+</span>
<span class="p_add">+static bool xpfo_disabled __initdata;</span>
<span class="p_add">+</span>
<span class="p_add">+static int __init noxpfo_param(char *str)</span>
<span class="p_add">+{</span>
<span class="p_add">+	xpfo_disabled = true;</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+early_param(&quot;noxpfo&quot;, noxpfo_param);</span>
<span class="p_add">+</span>
<span class="p_add">+static bool __init need_xpfo(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (xpfo_disabled) {</span>
<span class="p_add">+		printk(KERN_INFO &quot;XPFO disabled\n&quot;);</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return true;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void init_xpfo(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	printk(KERN_INFO &quot;XPFO enabled\n&quot;);</span>
<span class="p_add">+	static_branch_enable(&amp;xpfo_inited);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+struct page_ext_operations page_xpfo_ops = {</span>
<span class="p_add">+	.size = sizeof(struct xpfo),</span>
<span class="p_add">+	.need = need_xpfo,</span>
<span class="p_add">+	.init = init_xpfo,</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+static inline struct xpfo *lookup_xpfo(struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct page_ext *page_ext = lookup_page_ext(page);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (unlikely(!page_ext)) {</span>
<span class="p_add">+		WARN(1, &quot;xpfo: failed to get page ext&quot;);</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return (void *)page_ext + page_xpfo_ops.offset;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void xpfo_alloc_pages(struct page *page, int order, gfp_t gfp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int i, flush_tlb = 0;</span>
<span class="p_add">+	struct xpfo *xpfo;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!static_branch_unlikely(&amp;xpfo_inited))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i &lt; (1 &lt;&lt; order); i++)  {</span>
<span class="p_add">+		xpfo = lookup_xpfo(page + i);</span>
<span class="p_add">+		if (!xpfo)</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		WARN(test_bit(XPFO_PAGE_UNMAPPED, &amp;xpfo-&gt;flags),</span>
<span class="p_add">+		     &quot;xpfo: unmapped page being allocated\n&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+		/* Initialize the map lock and map counter */</span>
<span class="p_add">+		if (unlikely(!xpfo-&gt;inited)) {</span>
<span class="p_add">+			spin_lock_init(&amp;xpfo-&gt;maplock);</span>
<span class="p_add">+			atomic_set(&amp;xpfo-&gt;mapcount, 0);</span>
<span class="p_add">+			xpfo-&gt;inited = true;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		WARN(atomic_read(&amp;xpfo-&gt;mapcount),</span>
<span class="p_add">+		     &quot;xpfo: already mapped page being allocated\n&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+		if ((gfp &amp; GFP_HIGHUSER) == GFP_HIGHUSER) {</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * Tag the page as a user page and flush the TLB if it</span>
<span class="p_add">+			 * was previously allocated to the kernel.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			if (!test_and_set_bit(XPFO_PAGE_USER, &amp;xpfo-&gt;flags))</span>
<span class="p_add">+				flush_tlb = 1;</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			/* Tag the page as a non-user (kernel) page */</span>
<span class="p_add">+			clear_bit(XPFO_PAGE_USER, &amp;xpfo-&gt;flags);</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (flush_tlb)</span>
<span class="p_add">+		xpfo_flush_kernel_tlb(page, order);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void xpfo_free_pages(struct page *page, int order)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+	struct xpfo *xpfo;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!static_branch_unlikely(&amp;xpfo_inited))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i &lt; (1 &lt;&lt; order); i++) {</span>
<span class="p_add">+		xpfo = lookup_xpfo(page + i);</span>
<span class="p_add">+		if (!xpfo || unlikely(!xpfo-&gt;inited)) {</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * The page was allocated before page_ext was</span>
<span class="p_add">+			 * initialized, so it is a kernel page.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Map the page back into the kernel if it was previously</span>
<span class="p_add">+		 * allocated to user space.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (test_and_clear_bit(XPFO_PAGE_USER, &amp;xpfo-&gt;flags)) {</span>
<span class="p_add">+			clear_bit(XPFO_PAGE_UNMAPPED, &amp;xpfo-&gt;flags);</span>
<span class="p_add">+			set_kpte(page_address(page + i), page + i,</span>
<span class="p_add">+				 PAGE_KERNEL);</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void xpfo_kmap(void *kaddr, struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct xpfo *xpfo;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!static_branch_unlikely(&amp;xpfo_inited))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	xpfo = lookup_xpfo(page);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The page was allocated before page_ext was initialized (which means</span>
<span class="p_add">+	 * it&#39;s a kernel page) or it&#39;s allocated to the kernel, so nothing to</span>
<span class="p_add">+	 * do.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (!xpfo || unlikely(!xpfo-&gt;inited) ||</span>
<span class="p_add">+	    !test_bit(XPFO_PAGE_USER, &amp;xpfo-&gt;flags))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	spin_lock(&amp;xpfo-&gt;maplock);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The page was previously allocated to user space, so map it back</span>
<span class="p_add">+	 * into the kernel. No TLB flush required.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if ((atomic_inc_return(&amp;xpfo-&gt;mapcount) == 1) &amp;&amp;</span>
<span class="p_add">+	    test_and_clear_bit(XPFO_PAGE_UNMAPPED, &amp;xpfo-&gt;flags))</span>
<span class="p_add">+		set_kpte(kaddr, page, PAGE_KERNEL);</span>
<span class="p_add">+</span>
<span class="p_add">+	spin_unlock(&amp;xpfo-&gt;maplock);</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(xpfo_kmap);</span>
<span class="p_add">+</span>
<span class="p_add">+void xpfo_kunmap(void *kaddr, struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct xpfo *xpfo;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!static_branch_unlikely(&amp;xpfo_inited))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	xpfo = lookup_xpfo(page);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The page was allocated before page_ext was initialized (which means</span>
<span class="p_add">+	 * it&#39;s a kernel page) or it&#39;s allocated to the kernel, so nothing to</span>
<span class="p_add">+	 * do.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (!xpfo || unlikely(!xpfo-&gt;inited) ||</span>
<span class="p_add">+	    !test_bit(XPFO_PAGE_USER, &amp;xpfo-&gt;flags))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	spin_lock(&amp;xpfo-&gt;maplock);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The page is to be allocated back to user space, so unmap it from the</span>
<span class="p_add">+	 * kernel, flush the TLB and tag it as a user page.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (atomic_dec_return(&amp;xpfo-&gt;mapcount) == 0) {</span>
<span class="p_add">+		WARN(test_bit(XPFO_PAGE_UNMAPPED, &amp;xpfo-&gt;flags),</span>
<span class="p_add">+		     &quot;xpfo: unmapping already unmapped page\n&quot;);</span>
<span class="p_add">+		set_bit(XPFO_PAGE_UNMAPPED, &amp;xpfo-&gt;flags);</span>
<span class="p_add">+		set_kpte(kaddr, page, __pgprot(0));</span>
<span class="p_add">+		xpfo_flush_kernel_tlb(page, 0);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	spin_unlock(&amp;xpfo-&gt;maplock);</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(xpfo_kunmap);</span>
<span class="p_header">diff --git a/security/Kconfig b/security/Kconfig</span>
<span class="p_header">index e8e449444e65..be5145eeed7d 100644</span>
<span class="p_header">--- a/security/Kconfig</span>
<span class="p_header">+++ b/security/Kconfig</span>
<span class="p_chunk">@@ -6,6 +6,25 @@</span> <span class="p_context"> menu &quot;Security options&quot;</span>
 
 source security/keys/Kconfig
 
<span class="p_add">+config ARCH_SUPPORTS_XPFO</span>
<span class="p_add">+	bool</span>
<span class="p_add">+</span>
<span class="p_add">+config XPFO</span>
<span class="p_add">+	bool &quot;Enable eXclusive Page Frame Ownership (XPFO)&quot;</span>
<span class="p_add">+	default n</span>
<span class="p_add">+	depends on ARCH_SUPPORTS_XPFO</span>
<span class="p_add">+	select PAGE_EXTENSION</span>
<span class="p_add">+	help</span>
<span class="p_add">+	  This option offers protection against &#39;ret2dir&#39; kernel attacks.</span>
<span class="p_add">+	  When enabled, every time a page frame is allocated to user space, it</span>
<span class="p_add">+	  is unmapped from the direct mapped RAM region in kernel space</span>
<span class="p_add">+	  (physmap). Similarly, when a page frame is freed/reclaimed, it is</span>
<span class="p_add">+	  mapped back to physmap.</span>
<span class="p_add">+</span>
<span class="p_add">+	  There is a slight performance impact when this option is enabled.</span>
<span class="p_add">+</span>
<span class="p_add">+	  If in doubt, say &quot;N&quot;.</span>
<span class="p_add">+</span>
 config SECURITY_DMESG_RESTRICT
 	bool &quot;Restrict unprivileged access to the kernel syslog&quot;
 	default n

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



