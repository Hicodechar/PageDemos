
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[2/9] RISC-V: Atomic and Locking Code - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [2/9] RISC-V: Atomic and Locking Code</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=90941">Palmer Dabbelt</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>June 28, 2017, 6:55 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20170628185538.1804-3-palmer@dabbelt.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9815055/mbox/"
   >mbox</a>
|
   <a href="/patch/9815055/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9815055/">/patch/9815055/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	4F3CC603F2 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 28 Jun 2017 18:56:36 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 442911FFAD
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 28 Jun 2017 18:56:36 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 38BB32844B; Wed, 28 Jun 2017 18:56:36 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.4 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	DKIM_VALID, RCVD_IN_DNSWL_HI,
	RCVD_IN_SORBS_SPAM autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id C404D1FFAD
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 28 Jun 2017 18:56:33 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1751979AbdF1S42 (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Wed, 28 Jun 2017 14:56:28 -0400
Received: from mail-pf0-f196.google.com ([209.85.192.196]:33094 &quot;EHLO
	mail-pf0-f196.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1751609AbdF1Sz4 (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Wed, 28 Jun 2017 14:55:56 -0400
Received: by mail-pf0-f196.google.com with SMTP id e199so10073671pfh.0
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Wed, 28 Jun 2017 11:55:56 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=dabbelt-com.20150623.gappssmtp.com; s=20150623;
	h=from:to:to:to:to:to:to:to:to:to:to:to:to:to:to:to:to:cc:subject
	:date:message-id:in-reply-to:references;
	bh=gUzdyHktbFH2eJLmh+iG7NxeY0P+TgCgmP2zhH4H4oM=;
	b=d97QdqckHEh/KVSjaOF9uWtKdHm2nfpDcYneFojNSA8eAzVpwOVI42O0PUfPKISAkT
	rNfK/dLxN/U+ayuv196DmGSyVYtx6slLu2JsLv9QzL1aupfERum93dFKq5Lbmyk1XKVW
	YSNGG62ycikXMY/qhIX1ZZGHnb3ssGHbUpBbakXz7ezDPZC5fhz+BHDQaaQxagUuhFTU
	OB31dasEookT0E6l7gMRbMKdZJvE0bHq0LZ9uXe7i0zDLi0sS6jd/YM5s5Sqk96zU656
	92yRtJk2p3OxFvDO7Lnq/I2PWoI+QH2oullv6/IqQtRMt+DfvmJdAZ3yFOGIS5OhWX+d
	MeRg==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20161025;
	h=x-gm-message-state:from:to:to:to:to:to:to:to:to:to:to:to:to:to:to
	:to:to:cc:subject:date:message-id:in-reply-to:references;
	bh=gUzdyHktbFH2eJLmh+iG7NxeY0P+TgCgmP2zhH4H4oM=;
	b=Jbdg9p9o6eqShVxAjeXKb+ZBzJr6RqE95fciAEJslVf1xmU6hPz1jkKX3A7N/NVd6e
	H9J0Ry6ZlnatOFqh2ebUGmgmNUZ+1Epb2lu0q6sLOLpMbbYaQ8a+EslcklbUM+LPtEf/
	JGnHoaJMjNKBTkrIOD40docr9NURLgrxlvNKRqMU7xLMYCU4hAsFkQjkQlbjdMW+VN/x
	55T872O0fePM4ZA9dz29ugT/JnEdriblBy6V+3LyzCatftuAbD09kufJiBhGeKYKpKdN
	C5cSaCSGoI9qDsONaaIMAIBeP5j3aw3yxFdCNHb6l0bJbGyu2t0elnFDbMXjGMsBvP6p
	VUzw==
X-Gm-Message-State: AKS2vOw1CeQnqVecEX+4k7smPMlby68qYL8cIJDyS/ikZpyyqqMUjrO/
	9hg6ZrqzlwV3OPAC
X-Received: by 10.84.128.47 with SMTP id 44mr13293774pla.213.1498676154985; 
	Wed, 28 Jun 2017 11:55:54 -0700 (PDT)
Received: from localhost ([216.38.154.21]) by smtp.gmail.com with ESMTPSA id
	t5sm4952567pgt.19.2017.06.28.11.55.54
	(version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
	Wed, 28 Jun 2017 11:55:54 -0700 (PDT)
From: Palmer Dabbelt &lt;palmer@dabbelt.com&gt;
To: peterz@infradead.org
To: mingo@redhat.com
To: mcgrof@kernel.org
To: viro@zeniv.linux.org.uk
To: sfr@canb.auug.org.au
To: nicolas.dichtel@6wind.com
To: rmk+kernel@armlinux.org.uk
To: msalter@redhat.com
To: tklauser@distanz.ch
To: will.deacon@arm.com
To: james.hogan@imgtec.com
To: paul.gortmaker@windriver.com
To: linux@roeck-us.net
To: linux-kernel@vger.kernel.org
To: linux-arch@vger.kernel.org
To: albert@sifive.com
Cc: Palmer Dabbelt &lt;palmer@dabbelt.com&gt;
Subject: [PATCH 2/9] RISC-V: Atomic and Locking Code
Date: Wed, 28 Jun 2017 11:55:31 -0700
Message-Id: &lt;20170628185538.1804-3-palmer@dabbelt.com&gt;
X-Mailer: git-send-email 2.13.0
In-Reply-To: &lt;20170628185538.1804-1-palmer@dabbelt.com&gt;
References: &lt;20170606230007.19101-1-palmer@dabbelt.com&gt;
	&lt;20170628185538.1804-1-palmer@dabbelt.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=90941">Palmer Dabbelt</a> - June 28, 2017, 6:55 p.m.</div>
<pre class="content">
This contains all the code that directly interfaces with the RISC-V
memory model.  While this code corforms to the current RISC-V ISA
specifications (user 2.2 and priv 1.10), the memory model is somewhat
underspecified in those documents.  There is a working group that hopes
to produce a formal memory model by the end of the year, but my
understanding is that the basic definitions we&#39;re relying on here won&#39;t
change significantly.
<span class="signed-off-by">
Signed-off-by: Palmer Dabbelt &lt;palmer@dabbelt.com&gt;</span>
---
 arch/riscv/include/asm/atomic.h         | 330 ++++++++++++++++++++++++++++++++
 arch/riscv/include/asm/barrier.h        | 139 ++++++++++++++
 arch/riscv/include/asm/bitops.h         | 228 ++++++++++++++++++++++
 arch/riscv/include/asm/cacheflush.h     |  39 ++++
 arch/riscv/include/asm/cmpxchg.h        | 138 +++++++++++++
 arch/riscv/include/asm/io.h             | 178 +++++++++++++++++
 arch/riscv/include/asm/spinlock.h       | 167 ++++++++++++++++
 arch/riscv/include/asm/spinlock_types.h |  33 ++++
 arch/riscv/include/asm/tlb.h            |  24 +++
 arch/riscv/include/asm/tlbflush.h       |  64 +++++++
 10 files changed, 1340 insertions(+)
 create mode 100644 arch/riscv/include/asm/atomic.h
 create mode 100644 arch/riscv/include/asm/barrier.h
 create mode 100644 arch/riscv/include/asm/bitops.h
 create mode 100644 arch/riscv/include/asm/cacheflush.h
 create mode 100644 arch/riscv/include/asm/cmpxchg.h
 create mode 100644 arch/riscv/include/asm/io.h
 create mode 100644 arch/riscv/include/asm/spinlock.h
 create mode 100644 arch/riscv/include/asm/spinlock_types.h
 create mode 100644 arch/riscv/include/asm/tlb.h
 create mode 100644 arch/riscv/include/asm/tlbflush.h
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/riscv/include/asm/atomic.h b/arch/riscv/include/asm/atomic.h</span>
new file mode 100644
<span class="p_header">index 000000000000..7c5bfd18fe4e</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/atomic.h</span>
<span class="p_chunk">@@ -0,0 +1,330 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2007 Red Hat, Inc. All Rights Reserved.</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ * Copyright (C) 2017 SiFive</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ * modify it under the terms of the GNU General Public Licence</span>
<span class="p_add">+ * as published by the Free Software Foundation; either version</span>
<span class="p_add">+ * 2 of the Licence, or (at your option) any later version.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_ATOMIC_H</span>
<span class="p_add">+#define _ASM_RISCV_ATOMIC_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_GENERIC_ATOMIC64</span>
<span class="p_add">+# include &lt;asm-generic/atomic64.h&gt;</span>
<span class="p_add">+#else</span>
<span class="p_add">+# if (__riscv_xlen &lt; 64)</span>
<span class="p_add">+#  error &quot;64-bit atomics require XLEN to be at least 64&quot;</span>
<span class="p_add">+# endif</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_ISA_A</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/cmpxchg.h&gt;</span>
<span class="p_add">+#include &lt;asm/barrier.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#define ATOMIC_INIT(i)	{ (i) }</span>
<span class="p_add">+static __always_inline int atomic_read(const atomic_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return READ_ONCE(v-&gt;counter);</span>
<span class="p_add">+}</span>
<span class="p_add">+static __always_inline void atomic_set(atomic_t *v, int i)</span>
<span class="p_add">+{</span>
<span class="p_add">+	WRITE_ONCE(v-&gt;counter, i);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef CONFIG_GENERIC_ATOMIC64</span>
<span class="p_add">+#define ATOMIC64_INIT(i) { (i) }</span>
<span class="p_add">+static __always_inline int atomic64_read(const atomic64_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return READ_ONCE(v-&gt;counter);</span>
<span class="p_add">+}</span>
<span class="p_add">+static __always_inline void atomic64_set(atomic64_t *v, int i)</span>
<span class="p_add">+{</span>
<span class="p_add">+	WRITE_ONCE(v-&gt;counter, i);</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+/* First, the atomic ops that have no ordering constraints and therefor don&#39;t</span>
<span class="p_add">+ * have the AQ or RL bits set.  These don&#39;t return anything, so there&#39;s only</span>
<span class="p_add">+ * one version to worry about.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define ATOMIC_OP(op, asm_op, c_op, I, asm_type, c_type, prefix)				\</span>
<span class="p_add">+static __always_inline void atomic##prefix##_##op(c_type i, atomic##prefix##_t *v)		\</span>
<span class="p_add">+{												\</span>
<span class="p_add">+	__asm__ __volatile__ (									\</span>
<span class="p_add">+		&quot;amo&quot; #asm_op &quot;.&quot; #asm_type &quot; zero, %1, %0&quot;					\</span>
<span class="p_add">+		: &quot;+A&quot; (v-&gt;counter)								\</span>
<span class="p_add">+		: &quot;r&quot; (I));									\</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_GENERIC_ATOMIC64</span>
<span class="p_add">+#define ATOMIC_OPS(op, asm_op, c_op, I)			\</span>
<span class="p_add">+        ATOMIC_OP (op, asm_op, c_op, I, w,  int,   )</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define ATOMIC_OPS(op, asm_op, c_op, I)			\</span>
<span class="p_add">+        ATOMIC_OP (op, asm_op, c_op, I, w,  int,   )	\</span>
<span class="p_add">+        ATOMIC_OP (op, asm_op, c_op, I, d, long, 64)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+ATOMIC_OPS(add, add, +,  i)</span>
<span class="p_add">+ATOMIC_OPS(sub, add, +, -i)</span>
<span class="p_add">+/* FIXME: I could only find documentation that atomic_{add,sub,inc,dec} are</span>
<span class="p_add">+ * barrier-free.  I&#39;m assuming that and/or/xor have the same constraints as the</span>
<span class="p_add">+ * others.</span>
<span class="p_add">+ */</span>
<span class="p_add">+ATOMIC_OPS(and, and, &amp;,  i)</span>
<span class="p_add">+ATOMIC_OPS( or,  or, |,  i)</span>
<span class="p_add">+ATOMIC_OPS(xor, xor, ^,  i)</span>
<span class="p_add">+</span>
<span class="p_add">+#undef ATOMIC_OP</span>
<span class="p_add">+#undef ATOMIC_OPS</span>
<span class="p_add">+</span>
<span class="p_add">+/* Atomic ops that have ordered, relaxed, acquire, and relese variants.</span>
<span class="p_add">+ * There&#39;s two flavors of these: the arithmatic ops have both fetch and return</span>
<span class="p_add">+ * versions, while the logical ops only have fetch versions. */</span>
<span class="p_add">+#define ATOMIC_FETCH_OP(op, asm_op, c_op, I, asm_or, c_or, asm_type, c_type, prefix)			\</span>
<span class="p_add">+static __always_inline c_type atomic##prefix##_fetch_##op##c_or(c_type i, atomic##prefix##_t *v)	\</span>
<span class="p_add">+{													\</span>
<span class="p_add">+	register c_type ret;										\</span>
<span class="p_add">+	__asm__ __volatile__ (										\</span>
<span class="p_add">+		&quot;amo&quot; #asm_op &quot;.&quot; #asm_type #asm_or &quot; %2, %1, %0&quot;					\</span>
<span class="p_add">+		: &quot;+A&quot; (v-&gt;counter), &quot;=r&quot; (ret)								\</span>
<span class="p_add">+		: &quot;r&quot; (I));										\</span>
<span class="p_add">+	return ret;											\</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define ATOMIC_OP_RETURN(op, asm_op, c_op, I, asm_or, c_or, asm_type, c_type, prefix)			\</span>
<span class="p_add">+static __always_inline c_type atomic##prefix##_##op##_return##c_or(int i, atomic##prefix##_t *v)	\</span>
<span class="p_add">+{													\</span>
<span class="p_add">+        return atomic##prefix##_fetch_##op##c_or(i, v) c_op I;						\</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_GENERIC_ATOMIC64</span>
<span class="p_add">+#define ATOMIC_OPS(op, asm_op, c_op, I, asm_or, c_or)				\</span>
<span class="p_add">+        ATOMIC_FETCH_OP (op, asm_op, c_op, I, asm_or, c_or, w,  int,   )	\</span>
<span class="p_add">+        ATOMIC_OP_RETURN(op, asm_op, c_op, I, asm_or, c_or, w,  int,   )</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define ATOMIC_OPS(op, asm_op, c_op, I, asm_or, c_or)				\</span>
<span class="p_add">+        ATOMIC_FETCH_OP (op, asm_op, c_op, I, asm_or, c_or, w,  int,   )	\</span>
<span class="p_add">+        ATOMIC_OP_RETURN(op, asm_op, c_op, I, asm_or, c_or, w,  int,   )	\</span>
<span class="p_add">+        ATOMIC_FETCH_OP (op, asm_op, c_op, I, asm_or, c_or, d, long, 64)	\</span>
<span class="p_add">+        ATOMIC_OP_RETURN(op, asm_op, c_op, I, asm_or, c_or, d, long, 64)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+ATOMIC_OPS(add, add, +,  i,      , _relaxed)</span>
<span class="p_add">+ATOMIC_OPS(add, add, +,  i, .aq  , _acquire)</span>
<span class="p_add">+ATOMIC_OPS(add, add, +,  i, .rl  , _release)</span>
<span class="p_add">+ATOMIC_OPS(add, add, +,  i, .aqrl,         )</span>
<span class="p_add">+</span>
<span class="p_add">+ATOMIC_OPS(sub, add, +, -i,      , _relaxed)</span>
<span class="p_add">+ATOMIC_OPS(sub, add, +, -i, .aq  , _acquire)</span>
<span class="p_add">+ATOMIC_OPS(sub, add, +, -i, .rl  , _release)</span>
<span class="p_add">+ATOMIC_OPS(sub, add, +, -i, .aqrl,         )</span>
<span class="p_add">+</span>
<span class="p_add">+#undef ATOMIC_OPS</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_GENERIC_ATOMIC64</span>
<span class="p_add">+#define ATOMIC_OPS(op, asm_op, c_op, I, asm_or, c_or)				\</span>
<span class="p_add">+        ATOMIC_FETCH_OP(op, asm_op, c_op, I, asm_or, c_or, w,  int,   )</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define ATOMIC_OPS(op, asm_op, c_op, I, asm_or, c_or)				\</span>
<span class="p_add">+        ATOMIC_FETCH_OP(op, asm_op, c_op, I, asm_or, c_or, w,  int,   )		\</span>
<span class="p_add">+        ATOMIC_FETCH_OP(op, asm_op, c_op, I, asm_or, c_or, d, long, 64)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+ATOMIC_OPS(and, and, &amp;,  i,      , _relaxed)</span>
<span class="p_add">+ATOMIC_OPS(and, and, &amp;,  i, .aq  , _acquire)</span>
<span class="p_add">+ATOMIC_OPS(and, and, &amp;,  i, .rl  , _release)</span>
<span class="p_add">+ATOMIC_OPS(and, and, &amp;,  i, .aqrl,         )</span>
<span class="p_add">+</span>
<span class="p_add">+ATOMIC_OPS( or,  or, |,  i,      , _relaxed)</span>
<span class="p_add">+ATOMIC_OPS( or,  or, |,  i, .aq  , _acquire)</span>
<span class="p_add">+ATOMIC_OPS( or,  or, |,  i, .rl  , _release)</span>
<span class="p_add">+ATOMIC_OPS( or,  or, |,  i, .aqrl,         )</span>
<span class="p_add">+</span>
<span class="p_add">+ATOMIC_OPS(xor, xor, ^,  i,      , _relaxed)</span>
<span class="p_add">+ATOMIC_OPS(xor, xor, ^,  i, .aq  , _acquire)</span>
<span class="p_add">+ATOMIC_OPS(xor, xor, ^,  i, .rl  , _release)</span>
<span class="p_add">+ATOMIC_OPS(xor, xor, ^,  i, .aqrl,         )</span>
<span class="p_add">+</span>
<span class="p_add">+#undef ATOMIC_OPS</span>
<span class="p_add">+</span>
<span class="p_add">+#undef ATOMIC_FETCH_OP</span>
<span class="p_add">+#undef ATOMIC_OP_RETURN</span>
<span class="p_add">+</span>
<span class="p_add">+/* The extra atomic operations that are constructed from one of the core</span>
<span class="p_add">+ * AMO-based operations above (aside from sub, which is easier to fit above).</span>
<span class="p_add">+ * These are required to perform a barrier, but they&#39;re OK this way because</span>
<span class="p_add">+ * atomic_*_return is also required to perform a barrier.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define ATOMIC_OP(op, func_op, comp_op, I, c_type, prefix)			\</span>
<span class="p_add">+static __always_inline bool atomic##prefix##_##op(c_type i, atomic##prefix##_t *v) \</span>
<span class="p_add">+{										\</span>
<span class="p_add">+	return atomic##prefix##_##func_op##_return(i, v) comp_op I;		\</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_GENERIC_ATOMIC64</span>
<span class="p_add">+#define ATOMIC_OPS(op, func_op, comp_op, I)			\</span>
<span class="p_add">+        ATOMIC_OP (op, func_op, comp_op, I,  int,   )</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define ATOMIC_OPS(op, func_op, comp_op, I)			\</span>
<span class="p_add">+        ATOMIC_OP (op, func_op, comp_op, I,  int,   )		\</span>
<span class="p_add">+        ATOMIC_OP (op, func_op, comp_op, I, long, 64)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+ATOMIC_OPS(add_and_test, add, ==, 0)</span>
<span class="p_add">+ATOMIC_OPS(sub_and_test, sub, ==, 0)</span>
<span class="p_add">+ATOMIC_OPS(add_negative, add,  &lt;, 0)</span>
<span class="p_add">+</span>
<span class="p_add">+#undef ATOMIC_OP</span>
<span class="p_add">+#undef ATOMIC_OPS</span>
<span class="p_add">+</span>
<span class="p_add">+#define ATOMIC_OP(op, func_op, c_op, I, prefix)					\</span>
<span class="p_add">+static __always_inline void atomic##prefix##_##op(atomic##prefix##_t *v)	\</span>
<span class="p_add">+{										\</span>
<span class="p_add">+	atomic##prefix##_##func_op(I, v);					\</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define ATOMIC_FETCH_OP(op, func_op, c_op, I, prefix)				\</span>
<span class="p_add">+static __always_inline int atomic##prefix##_fetch_##op(atomic##prefix##_t *v)	\</span>
<span class="p_add">+{										\</span>
<span class="p_add">+	return atomic##prefix##_fetch_##func_op(I, v);				\</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define ATOMIC_OP_RETURN(op, asm_op, c_op, I, prefix)				\</span>
<span class="p_add">+static __always_inline int atomic##prefix##_##op##_return(atomic##prefix##_t *v) \</span>
<span class="p_add">+{										\</span>
<span class="p_add">+        return atomic##prefix##_fetch_##op(v) c_op I;				\</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_GENERIC_ATOMIC64</span>
<span class="p_add">+#define ATOMIC_OPS(op, asm_op, c_op, I)						\</span>
<span class="p_add">+        ATOMIC_OP       (op, asm_op, c_op, I,   )				\</span>
<span class="p_add">+        ATOMIC_FETCH_OP (op, asm_op, c_op, I,   )				\</span>
<span class="p_add">+        ATOMIC_OP_RETURN(op, asm_op, c_op, I,   )</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define ATOMIC_OPS(op, asm_op, c_op, I)						\</span>
<span class="p_add">+        ATOMIC_OP       (op, asm_op, c_op, I,   )				\</span>
<span class="p_add">+        ATOMIC_FETCH_OP (op, asm_op, c_op, I,   )				\</span>
<span class="p_add">+        ATOMIC_OP_RETURN(op, asm_op, c_op, I,   )				\</span>
<span class="p_add">+        ATOMIC_OP       (op, asm_op, c_op, I, 64)				\</span>
<span class="p_add">+        ATOMIC_FETCH_OP (op, asm_op, c_op, I, 64)				\</span>
<span class="p_add">+        ATOMIC_OP_RETURN(op, asm_op, c_op, I, 64)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+ATOMIC_OPS(inc, add, +,  1)</span>
<span class="p_add">+ATOMIC_OPS(dec, add, +, -1)</span>
<span class="p_add">+</span>
<span class="p_add">+#undef ATOMIC_OPS</span>
<span class="p_add">+#undef ATOMIC_OP</span>
<span class="p_add">+#undef ATOMIC_FETCH_OP</span>
<span class="p_add">+#undef ATOMIC_OP_RETURN</span>
<span class="p_add">+</span>
<span class="p_add">+#define ATOMIC_OP(op, func_op, comp_op, I, prefix)				\</span>
<span class="p_add">+static __always_inline bool atomic##prefix##_##op(atomic##prefix##_t *v)	\</span>
<span class="p_add">+{										\</span>
<span class="p_add">+	return atomic##prefix##_##func_op##_return(v) comp_op I;		\</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+ATOMIC_OP(inc_and_test, inc, ==, 0,   )</span>
<span class="p_add">+ATOMIC_OP(dec_and_test, dec, ==, 0,   )</span>
<span class="p_add">+#ifndef CONFIG_GENERIC_ATOMIC64</span>
<span class="p_add">+ATOMIC_OP(inc_and_test, inc, ==, 0, 64)</span>
<span class="p_add">+ATOMIC_OP(dec_and_test, dec, ==, 0, 64)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#undef ATOMIC_OP</span>
<span class="p_add">+</span>
<span class="p_add">+/* This is required to provide a barrier on success. */</span>
<span class="p_add">+static __always_inline int __atomic_add_unless(atomic_t *v, int a, int u)</span>
<span class="p_add">+{</span>
<span class="p_add">+       register int prev, rc;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;0:\n\t&quot;</span>
<span class="p_add">+		&quot;lr.w.aqrl %0, %2\n\t&quot;</span>
<span class="p_add">+		&quot;beq       %0, %4, 1f\n\t&quot;</span>
<span class="p_add">+		&quot;add       %1, %0, %3\n\t&quot;</span>
<span class="p_add">+		&quot;sc.w.aqrl %1, %1, %2\n\t&quot;</span>
<span class="p_add">+		&quot;bnez      %1, 0b\n\t&quot;</span>
<span class="p_add">+		&quot;1:&quot;</span>
<span class="p_add">+		: &quot;=&amp;r&quot; (prev), &quot;=&amp;r&quot; (rc), &quot;+A&quot; (v-&gt;counter)</span>
<span class="p_add">+		: &quot;r&quot; (a), &quot;r&quot; (u));</span>
<span class="p_add">+	return prev;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef CONFIG_GENERIC_ATOMIC64</span>
<span class="p_add">+static __always_inline long atomic64_add_unless(atomic64_t *v, long a, long u)</span>
<span class="p_add">+{</span>
<span class="p_add">+       register long prev, rc;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;0:\n\t&quot;</span>
<span class="p_add">+		&quot;lr.d.aqrl %0, %2\n\t&quot;</span>
<span class="p_add">+		&quot;beq       %0, %4, 1f\n\t&quot;</span>
<span class="p_add">+		&quot;add       %1, %0, %3\n\t&quot;</span>
<span class="p_add">+		&quot;sc.d.aqrl %1, %1, %2\n\t&quot;</span>
<span class="p_add">+		&quot;bnez      %1, 0b\n\t&quot;</span>
<span class="p_add">+		&quot;1:&quot;</span>
<span class="p_add">+		: &quot;=&amp;r&quot; (prev), &quot;=&amp;r&quot; (rc), &quot;+A&quot; (v-&gt;counter)</span>
<span class="p_add">+		: &quot;r&quot; (a), &quot;r&quot; (u));</span>
<span class="p_add">+	return prev;</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+/* The extra atomic operations that are constructed from one of the core</span>
<span class="p_add">+ * LR/SC-based operations above.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static __always_inline int atomic_inc_not_zero(atomic_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+        return __atomic_add_unless(v, 1, 0);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef CONFIG_GENERIC_ATOMIC64</span>
<span class="p_add">+static __always_inline int atomic64_inc_not_zero(atomic64_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+        return atomic64_add_unless(v, 1, 0);</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+/* atomic_{cmp,}xchg is required to have exactly the same ordering semantics as</span>
<span class="p_add">+ * {cmp,}xchg and the operations that return, so they need a barrier.  We just</span>
<span class="p_add">+ * use the other implementations directly.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define ATOMIC_OP(c_t, prefix, c_or, size, asm_or)						\</span>
<span class="p_add">+static __always_inline c_t atomic##prefix##_cmpxchg##c_or(atomic##prefix##_t *v, c_t o, c_t n) 	\</span>
<span class="p_add">+{												\</span>
<span class="p_add">+	return __cmpxchg(&amp;(v-&gt;counter), o, n, size, asm_or, asm_or);				\</span>
<span class="p_add">+}												\</span>
<span class="p_add">+static __always_inline c_t atomic##prefix##_xchg##c_or(atomic##prefix##_t *v, c_t n) 		\</span>
<span class="p_add">+{												\</span>
<span class="p_add">+	return __xchg(n, &amp;(v-&gt;counter), size, asm_or);						\</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_GENERIC_ATOMIC64</span>
<span class="p_add">+#define ATOMIC_OPS(c_or, asm_or)			\</span>
<span class="p_add">+	ATOMIC_OP( int,   , c_or, 4, asm_or)</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define ATOMIC_OPS(c_or, asm_or)			\</span>
<span class="p_add">+	ATOMIC_OP( int,   , c_or, 4, asm_or)		\</span>
<span class="p_add">+	ATOMIC_OP(long, 64, c_or, 8, asm_or)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+ATOMIC_OPS(        , .aqrl)</span>
<span class="p_add">+ATOMIC_OPS(_acquire,   .aq)</span>
<span class="p_add">+ATOMIC_OPS(_release,   .rl)</span>
<span class="p_add">+ATOMIC_OPS(_relaxed,      )</span>
<span class="p_add">+</span>
<span class="p_add">+#undef ATOMIC_OPS</span>
<span class="p_add">+#undef ATOMIC_OP</span>
<span class="p_add">+</span>
<span class="p_add">+#else /* !CONFIG_ISA_A */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/atomic.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* CONFIG_ISA_A */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_ATOMIC_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/barrier.h b/arch/riscv/include/asm/barrier.h</span>
new file mode 100644
<span class="p_header">index 000000000000..fe9c26f49e70</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/barrier.h</span>
<span class="p_chunk">@@ -0,0 +1,139 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Based on arch/arm/include/asm/barrier.h</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Copyright (C) 2012 ARM Ltd.</span>
<span class="p_add">+ * Copyright (C) 2013 Regents of the University of California</span>
<span class="p_add">+ * Copyright (C) 2017 SiFive</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of the GNU General Public License version 2 as</span>
<span class="p_add">+ * published by the Free Software Foundation.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ * GNU General Public License for more details.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * You should have received a copy of the GNU General Public License</span>
<span class="p_add">+ * along with this program.  If not, see &lt;http://www.gnu.org/licenses/&gt;.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_BARRIER_H</span>
<span class="p_add">+#define _ASM_RISCV_BARRIER_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __ASSEMBLY__</span>
<span class="p_add">+</span>
<span class="p_add">+#define nop()		__asm__ __volatile__ (&quot;nop&quot;)</span>
<span class="p_add">+</span>
<span class="p_add">+#define RISCV_FENCE(p, s) \</span>
<span class="p_add">+	__asm__ __volatile__ (&quot;fence &quot; #p &quot;,&quot; #s : : : &quot;memory&quot;)</span>
<span class="p_add">+</span>
<span class="p_add">+/* These barries need to enforce ordering on both devices or memory. */</span>
<span class="p_add">+#define mb()		RISCV_FENCE(iorw,iorw)</span>
<span class="p_add">+#define rmb()		RISCV_FENCE(ir,ir)</span>
<span class="p_add">+#define wmb()		RISCV_FENCE(ow,ow)</span>
<span class="p_add">+</span>
<span class="p_add">+/* These barries do not need to enforce ordering on devices, just memory. */</span>
<span class="p_add">+#define smp_mb()	RISCV_FENCE(rw,rw)</span>
<span class="p_add">+#define smp_rmb()	RISCV_FENCE(r,r)</span>
<span class="p_add">+#define smp_wmb()	RISCV_FENCE(w,w)</span>
<span class="p_add">+</span>
<span class="p_add">+/* Our atomic operations set the AQ and RL bits and therefor we don&#39;t need to</span>
<span class="p_add">+ * fence around atomics.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define __smb_mb__before_atomic()	barrier()</span>
<span class="p_add">+#define __smb_mb__after_atomic()	barrier()</span>
<span class="p_add">+</span>
<span class="p_add">+/* These barries are meant to prevent memory operations inside a spinlock from</span>
<span class="p_add">+ * moving outside of that spinlock.  Since we set the AQ and RL bits when</span>
<span class="p_add">+ * entering or leaving spinlocks, no additional fence needs to be performed.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define smb_mb__before_spinlock()	barrier()</span>
<span class="p_add">+#define smb_mb__after_spinlock()	barrier()</span>
<span class="p_add">+</span>
<span class="p_add">+/* FIXME: I don&#39;t think RISC-V is allowed to perform a speculative load. */</span>
<span class="p_add">+#define smp_acquire__after_ctrl_dep()	barrier()</span>
<span class="p_add">+</span>
<span class="p_add">+/* The RISC-V ISA doesn&#39;t support byte or half-word AMOs, so we fall back to a</span>
<span class="p_add">+ * regular store and a fence here.  Otherwise we emit an AMO with an AQ or RL</span>
<span class="p_add">+ * bit set and allow the microarchitecture to avoid the other half of the AMO.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define __smp_store_release(p, v)					\</span>
<span class="p_add">+do {									\</span>
<span class="p_add">+	union { typeof(*p) __val; char __c[1]; } __u =			\</span>
<span class="p_add">+		{ .__val = (__force typeof(*p)) (v) };			\</span>
<span class="p_add">+	compiletime_assert_atomic_type(*p);				\</span>
<span class="p_add">+	switch (sizeof(*p)) {						\</span>
<span class="p_add">+	case 1:								\</span>
<span class="p_add">+	case 2:								\</span>
<span class="p_add">+		smb_mb();						\</span>
<span class="p_add">+		WRITE_ONCE(*p, __u.__val);				\</span>
<span class="p_add">+		break;							\</span>
<span class="p_add">+	case 4:								\</span>
<span class="p_add">+		__asm__ __volatile__ (					\</span>
<span class="p_add">+			&quot;amoswap.w.rl zero, %1, %0&quot;			\</span>
<span class="p_add">+			: &quot;+A&quot; (*p), &quot;r&quot; (__u.__val)			\</span>
<span class="p_add">+			: 						\</span>
<span class="p_add">+			: &quot;memory&quot;);					\</span>
<span class="p_add">+		break;							\</span>
<span class="p_add">+	case 8:								\</span>
<span class="p_add">+		__asm__ __volatile__ (					\</span>
<span class="p_add">+			&quot;amoswap.d.rl zero, %1, %0&quot;			\</span>
<span class="p_add">+			: &quot;+A&quot; (*p), &quot;r&quot; (__u.__val)			\</span>
<span class="p_add">+			: 						\</span>
<span class="p_add">+			: &quot;memory&quot;);					\</span>
<span class="p_add">+		break;							\</span>
<span class="p_add">+	}								\</span>
<span class="p_add">+} while (0)</span>
<span class="p_add">+</span>
<span class="p_add">+#define __smp_load_acquire(p)						\</span>
<span class="p_add">+do {									\</span>
<span class="p_add">+	union { typeof(*p) __val; char __c[1]; } __u =			\</span>
<span class="p_add">+		{ .__val = (__force typeof(*p)) (v) };			\</span>
<span class="p_add">+	compiletime_assert_atomic_type(*p);				\</span>
<span class="p_add">+	switch (sizeof(*p)) {						\</span>
<span class="p_add">+	case 1:								\</span>
<span class="p_add">+	case 2:								\</span>
<span class="p_add">+		__u.__val = READ_ONCE(*p);				\</span>
<span class="p_add">+		smb_mb();						\</span>
<span class="p_add">+		break;							\</span>
<span class="p_add">+	case 4:								\</span>
<span class="p_add">+		__asm__ __volatile__ (					\</span>
<span class="p_add">+			&quot;amoor.w.aq %1, zero, %0&quot;			\</span>
<span class="p_add">+			: &quot;+A&quot; (*p)					\</span>
<span class="p_add">+			: &quot;=r&quot; (__u.__val)				\</span>
<span class="p_add">+			: &quot;memory&quot;);					\</span>
<span class="p_add">+		break;							\</span>
<span class="p_add">+	case 8:								\</span>
<span class="p_add">+		__asm__ __volatile__ (					\</span>
<span class="p_add">+			&quot;amoor.d.aq %1, zero, %0&quot;			\</span>
<span class="p_add">+			: &quot;+A&quot; (*p)					\</span>
<span class="p_add">+			: &quot;=r&quot; (__u.__val)				\</span>
<span class="p_add">+			: &quot;memory&quot;);					\</span>
<span class="p_add">+		break;							\</span>
<span class="p_add">+	}								\</span>
<span class="p_add">+	__u.__val;							\</span>
<span class="p_add">+} while (0)</span>
<span class="p_add">+</span>
<span class="p_add">+/* The default implementation of this uses READ_ONCE and</span>
<span class="p_add">+ * smp_acquire__after_ctrl_dep, but since we can directly do an ACQUIRE load we</span>
<span class="p_add">+ * can avoid the extra barrier.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define smp_cond_load_acquire(ptr, cond_expr) ({			\</span>
<span class="p_add">+	typeof(ptr) __PTR = (ptr);					\</span>
<span class="p_add">+	typeof(*ptr) VAL;						\</span>
<span class="p_add">+	for (;;) {							\</span>
<span class="p_add">+		VAL = __smp_load_acquire(__PTR);			\</span>
<span class="p_add">+		if (cond_expr)						\</span>
<span class="p_add">+			break;						\</span>
<span class="p_add">+		cpu_relax();						\</span>
<span class="p_add">+	}								\</span>
<span class="p_add">+	smp_acquire__after_ctrl_dep();					\</span>
<span class="p_add">+	VAL;								\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/barrier.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* __ASSEMBLY__ */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_BARRIER_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/bitops.h b/arch/riscv/include/asm/bitops.h</span>
new file mode 100644
<span class="p_header">index 000000000000..27e47858c6b1</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/bitops.h</span>
<span class="p_chunk">@@ -0,0 +1,228 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_BITOPS_H</span>
<span class="p_add">+#define _ASM_RISCV_BITOPS_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _LINUX_BITOPS_H</span>
<span class="p_add">+#error &quot;Only &lt;linux/bitops.h&gt; can be included directly&quot;</span>
<span class="p_add">+#endif /* _LINUX_BITOPS_H */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef __KERNEL__</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/compiler.h&gt;</span>
<span class="p_add">+#include &lt;linux/irqflags.h&gt;</span>
<span class="p_add">+#include &lt;asm/barrier.h&gt;</span>
<span class="p_add">+#include &lt;asm/bitsperlong.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_ISA_A</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef smp_mb__before_clear_bit</span>
<span class="p_add">+#define smp_mb__before_clear_bit()  smp_mb()</span>
<span class="p_add">+#define smp_mb__after_clear_bit()   smp_mb()</span>
<span class="p_add">+#endif /* smp_mb__before_clear_bit */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/bitops/__ffs.h&gt;</span>
<span class="p_add">+#include &lt;asm-generic/bitops/ffz.h&gt;</span>
<span class="p_add">+#include &lt;asm-generic/bitops/fls.h&gt;</span>
<span class="p_add">+#include &lt;asm-generic/bitops/__fls.h&gt;</span>
<span class="p_add">+#include &lt;asm-generic/bitops/fls64.h&gt;</span>
<span class="p_add">+#include &lt;asm-generic/bitops/find.h&gt;</span>
<span class="p_add">+#include &lt;asm-generic/bitops/sched.h&gt;</span>
<span class="p_add">+#include &lt;asm-generic/bitops/ffs.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/bitops/hweight.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#if (BITS_PER_LONG == 64)</span>
<span class="p_add">+#define __AMO(op)	&quot;amo&quot; #op &quot;.d&quot;</span>
<span class="p_add">+#elif (BITS_PER_LONG == 32)</span>
<span class="p_add">+#define __AMO(op)	&quot;amo&quot; #op &quot;.w&quot;</span>
<span class="p_add">+#else</span>
<span class="p_add">+#error &quot;Unexpected BITS_PER_LONG&quot;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#define __test_and_op_bit(op, mod, nr, addr)			\</span>
<span class="p_add">+({								\</span>
<span class="p_add">+	unsigned long __res, __mask;				\</span>
<span class="p_add">+	__mask = BIT_MASK(nr);					\</span>
<span class="p_add">+	__asm__ __volatile__ (					\</span>
<span class="p_add">+		__AMO(op) &quot; %0, %2, %1&quot;				\</span>
<span class="p_add">+		: &quot;=r&quot; (__res), &quot;+A&quot; (addr[BIT_WORD(nr)])	\</span>
<span class="p_add">+		: &quot;r&quot; (mod(__mask)));				\</span>
<span class="p_add">+	((__res &amp; __mask) != 0);				\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+#define __op_bit(op, mod, nr, addr)				\</span>
<span class="p_add">+	__asm__ __volatile__ (					\</span>
<span class="p_add">+		__AMO(op) &quot; zero, %1, %0&quot;			\</span>
<span class="p_add">+		: &quot;+A&quot; (addr[BIT_WORD(nr)])			\</span>
<span class="p_add">+		: &quot;r&quot; (mod(BIT_MASK(nr))))</span>
<span class="p_add">+</span>
<span class="p_add">+/* Bitmask modifiers */</span>
<span class="p_add">+#define __NOP(x)	(x)</span>
<span class="p_add">+#define __NOT(x)	(~(x))</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * test_and_set_bit - Set a bit and return its old value</span>
<span class="p_add">+ * @nr: Bit to set</span>
<span class="p_add">+ * @addr: Address to count from</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This operation is atomic and cannot be reordered.</span>
<span class="p_add">+ * It may be reordered on other architectures than x86.</span>
<span class="p_add">+ * It also implies a memory barrier.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline int test_and_set_bit(int nr, volatile unsigned long *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __test_and_op_bit(or, __NOP, nr, addr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * test_and_clear_bit - Clear a bit and return its old value</span>
<span class="p_add">+ * @nr: Bit to clear</span>
<span class="p_add">+ * @addr: Address to count from</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This operation is atomic and cannot be reordered.</span>
<span class="p_add">+ * It can be reordered on other architectures other than x86.</span>
<span class="p_add">+ * It also implies a memory barrier.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline int test_and_clear_bit(int nr, volatile unsigned long *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __test_and_op_bit(and, __NOT, nr, addr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * test_and_change_bit - Change a bit and return its old value</span>
<span class="p_add">+ * @nr: Bit to change</span>
<span class="p_add">+ * @addr: Address to count from</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This operation is atomic and cannot be reordered.</span>
<span class="p_add">+ * It also implies a memory barrier.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline int test_and_change_bit(int nr, volatile unsigned long *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __test_and_op_bit(xor, __NOP, nr, addr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * set_bit - Atomically set a bit in memory</span>
<span class="p_add">+ * @nr: the bit to set</span>
<span class="p_add">+ * @addr: the address to start counting from</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This function is atomic and may not be reordered.  See __set_bit()</span>
<span class="p_add">+ * if you do not require the atomic guarantees.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Note: there are no guarantees that this function will not be reordered</span>
<span class="p_add">+ * on non x86 architectures, so if you are writing portable code,</span>
<span class="p_add">+ * make sure not to rely on its reordering guarantees.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Note that @nr may be almost arbitrarily large; this function is not</span>
<span class="p_add">+ * restricted to acting on a single-word quantity.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void set_bit(int nr, volatile unsigned long *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__op_bit(or, __NOP, nr, addr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * clear_bit - Clears a bit in memory</span>
<span class="p_add">+ * @nr: Bit to clear</span>
<span class="p_add">+ * @addr: Address to start counting from</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * clear_bit() is atomic and may not be reordered.  However, it does</span>
<span class="p_add">+ * not contain a memory barrier, so if it is used for locking purposes,</span>
<span class="p_add">+ * you should call smp_mb__before_clear_bit() and/or smp_mb__after_clear_bit()</span>
<span class="p_add">+ * in order to ensure changes are visible on other processors.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void clear_bit(int nr, volatile unsigned long *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__op_bit(and, __NOT, nr, addr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * change_bit - Toggle a bit in memory</span>
<span class="p_add">+ * @nr: Bit to change</span>
<span class="p_add">+ * @addr: Address to start counting from</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * change_bit() is atomic and may not be reordered. It may be</span>
<span class="p_add">+ * reordered on other architectures than x86.</span>
<span class="p_add">+ * Note that @nr may be almost arbitrarily large; this function is not</span>
<span class="p_add">+ * restricted to acting on a single-word quantity.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void change_bit(int nr, volatile unsigned long *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__op_bit(xor, __NOP, nr, addr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * test_and_set_bit_lock - Set a bit and return its old value, for lock</span>
<span class="p_add">+ * @nr: Bit to set</span>
<span class="p_add">+ * @addr: Address to count from</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This operation is atomic and provides acquire barrier semantics.</span>
<span class="p_add">+ * It can be used to implement bit locks.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline int test_and_set_bit_lock(</span>
<span class="p_add">+	unsigned long nr, volatile unsigned long *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return test_and_set_bit(nr, addr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * clear_bit_unlock - Clear a bit in memory, for unlock</span>
<span class="p_add">+ * @nr: the bit to set</span>
<span class="p_add">+ * @addr: the address to start counting from</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This operation is atomic and provides release barrier semantics.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void clear_bit_unlock(</span>
<span class="p_add">+	unsigned long nr, volatile unsigned long *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	clear_bit(nr, addr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * __clear_bit_unlock - Clear a bit in memory, for unlock</span>
<span class="p_add">+ * @nr: the bit to set</span>
<span class="p_add">+ * @addr: the address to start counting from</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This operation is like clear_bit_unlock, however it is not atomic.</span>
<span class="p_add">+ * It does provide release barrier semantics so it can be used to unlock</span>
<span class="p_add">+ * a bit lock, however it would only be used if no other CPU can modify</span>
<span class="p_add">+ * any bits in the memory until the lock is released (a good example is</span>
<span class="p_add">+ * if the bit lock itself protects access to the other bits in the word).</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void __clear_bit_unlock(</span>
<span class="p_add">+	unsigned long nr, volatile unsigned long *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	clear_bit(nr, addr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#undef __test_and_op_bit</span>
<span class="p_add">+#undef __op_bit</span>
<span class="p_add">+#undef __NOP</span>
<span class="p_add">+#undef __NOT</span>
<span class="p_add">+#undef __AMO</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/bitops/non-atomic.h&gt;</span>
<span class="p_add">+#include &lt;asm-generic/bitops/le.h&gt;</span>
<span class="p_add">+#include &lt;asm-generic/bitops/ext2-atomic.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#else /* !CONFIG_ISA_A */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/bitops.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* CONFIG_ISA_A */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* __KERNEL__ */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_BITOPS_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/cacheflush.h b/arch/riscv/include/asm/cacheflush.h</span>
new file mode 100644
<span class="p_header">index 000000000000..0595585013b0</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/cacheflush.h</span>
<span class="p_chunk">@@ -0,0 +1,39 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2015 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_CACHEFLUSH_H</span>
<span class="p_add">+#define _ASM_RISCV_CACHEFLUSH_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/cacheflush.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#undef flush_icache_range</span>
<span class="p_add">+#undef flush_icache_user_range</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void local_flush_icache_all(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	asm volatile (&quot;fence.i&quot; ::: &quot;memory&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef CONFIG_SMP</span>
<span class="p_add">+</span>
<span class="p_add">+#define flush_icache_range(start, end) local_flush_icache_all()</span>
<span class="p_add">+#define flush_icache_user_range(vma, pg, addr, len) local_flush_icache_all()</span>
<span class="p_add">+</span>
<span class="p_add">+#else /* CONFIG_SMP */</span>
<span class="p_add">+</span>
<span class="p_add">+#define flush_icache_range(start, end) sbi_remote_fence_i(0)</span>
<span class="p_add">+#define flush_icache_user_range(vma, pg, addr, len) sbi_remote_fence_i(0)</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* CONFIG_SMP */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_CACHEFLUSH_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/cmpxchg.h b/arch/riscv/include/asm/cmpxchg.h</span>
new file mode 100644
<span class="p_header">index 000000000000..81025c056412</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/cmpxchg.h</span>
<span class="p_chunk">@@ -0,0 +1,138 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2014 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_CMPXCHG_H</span>
<span class="p_add">+#define _ASM_RISCV_CMPXCHG_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/bug.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_ISA_A</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/barrier.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#define __xchg(new, ptr, size, asm_or)				\</span>
<span class="p_add">+({								\</span>
<span class="p_add">+	__typeof__(ptr) __ptr = (ptr);				\</span>
<span class="p_add">+	__typeof__(new) __new = (new);				\</span>
<span class="p_add">+	__typeof__(*(ptr)) __ret;				\</span>
<span class="p_add">+	switch (size) {						\</span>
<span class="p_add">+	case 4:							\</span>
<span class="p_add">+		__asm__ __volatile__ (				\</span>
<span class="p_add">+			&quot;amoswap.w&quot; #asm_or &quot; %0, %2, %1&quot;	\</span>
<span class="p_add">+			: &quot;=r&quot; (__ret), &quot;+A&quot; (*__ptr)		\</span>
<span class="p_add">+			: &quot;r&quot; (__new));				\</span>
<span class="p_add">+		break;						\</span>
<span class="p_add">+	case 8:							\</span>
<span class="p_add">+		__asm__ __volatile__ (				\</span>
<span class="p_add">+			&quot;amoswap.d&quot; #asm_or &quot; %0, %2, %1&quot;	\</span>
<span class="p_add">+			: &quot;=r&quot; (__ret), &quot;+A&quot; (*__ptr)		\</span>
<span class="p_add">+			: &quot;r&quot; (__new));				\</span>
<span class="p_add">+		break;						\</span>
<span class="p_add">+	default:						\</span>
<span class="p_add">+		BUILD_BUG();					\</span>
<span class="p_add">+	}							\</span>
<span class="p_add">+	__ret;							\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+#define xchg(ptr, x)    (__xchg((x), (ptr), sizeof(*(ptr)), .aqrl))</span>
<span class="p_add">+</span>
<span class="p_add">+#define xchg32(ptr, x)				\</span>
<span class="p_add">+({						\</span>
<span class="p_add">+	BUILD_BUG_ON(sizeof(*(ptr)) != 4);	\</span>
<span class="p_add">+	xchg((ptr), (x));			\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+#define xchg64(ptr, x)				\</span>
<span class="p_add">+({						\</span>
<span class="p_add">+	BUILD_BUG_ON(sizeof(*(ptr)) != 8);	\</span>
<span class="p_add">+	xchg((ptr), (x));			\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Atomic compare and exchange.  Compare OLD with MEM, if identical,</span>
<span class="p_add">+ * store NEW in MEM.  Return the initial value in MEM.  Success is</span>
<span class="p_add">+ * indicated by comparing RETURN with OLD.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define __cmpxchg(ptr, old, new, size, lrb, scb)			\</span>
<span class="p_add">+({									\</span>
<span class="p_add">+	__typeof__(ptr) __ptr = (ptr);					\</span>
<span class="p_add">+	__typeof__(old) __old = (old);					\</span>
<span class="p_add">+	__typeof__(new) __new = (new);					\</span>
<span class="p_add">+	__typeof__(*(ptr)) __ret;					\</span>
<span class="p_add">+	register unsigned int __rc;					\</span>
<span class="p_add">+	switch (size) {							\</span>
<span class="p_add">+	case 4:								\</span>
<span class="p_add">+		__asm__ __volatile__ (					\</span>
<span class="p_add">+		&quot;0:&quot;							\</span>
<span class="p_add">+			&quot;lr.w&quot; #scb &quot; %0, %2\n&quot;				\</span>
<span class="p_add">+			&quot;bne         %0, %z3, 1f\n&quot;			\</span>
<span class="p_add">+			&quot;sc.w&quot; #lrb &quot; %1, %z4, %2\n&quot;			\</span>
<span class="p_add">+			&quot;bnez        %1, 0b\n&quot;				\</span>
<span class="p_add">+		&quot;1:&quot;							\</span>
<span class="p_add">+			: &quot;=&amp;r&quot; (__ret), &quot;=&amp;r&quot; (__rc), &quot;+A&quot; (*__ptr)	\</span>
<span class="p_add">+			: &quot;rJ&quot; (__old), &quot;rJ&quot; (__new));			\</span>
<span class="p_add">+		break;							\</span>
<span class="p_add">+	case 8:								\</span>
<span class="p_add">+		__asm__ __volatile__ (					\</span>
<span class="p_add">+		&quot;0:&quot;							\</span>
<span class="p_add">+			&quot;lr.d&quot; #scb &quot; %0, %2\n&quot;				\</span>
<span class="p_add">+			&quot;bne         %0, %z3, 1f\n&quot;			\</span>
<span class="p_add">+			&quot;sc.d&quot; #lrb &quot; %1, %z4, %2\n&quot;			\</span>
<span class="p_add">+			&quot;bnez        %1, 0b\n&quot;				\</span>
<span class="p_add">+		&quot;1:&quot;							\</span>
<span class="p_add">+			: &quot;=&amp;r&quot; (__ret), &quot;=&amp;r&quot; (__rc), &quot;+A&quot; (*__ptr)	\</span>
<span class="p_add">+			: &quot;rJ&quot; (__old), &quot;rJ&quot; (__new));			\</span>
<span class="p_add">+		break;							\</span>
<span class="p_add">+	default:							\</span>
<span class="p_add">+		BUILD_BUG();						\</span>
<span class="p_add">+	}								\</span>
<span class="p_add">+	__ret;								\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+#define cmpxchg(ptr, o, n) \</span>
<span class="p_add">+	(__cmpxchg((ptr), (o), (n), sizeof(*(ptr)), .aqrl, .aqrl))</span>
<span class="p_add">+</span>
<span class="p_add">+#define cmpxchg_local(ptr, o, n) \</span>
<span class="p_add">+	(__cmpxchg((ptr), (o), (n), sizeof(*(ptr)), , ))</span>
<span class="p_add">+</span>
<span class="p_add">+#define cmpxchg32(ptr, o, n)			\</span>
<span class="p_add">+({						\</span>
<span class="p_add">+	BUILD_BUG_ON(sizeof(*(ptr)) != 4);	\</span>
<span class="p_add">+	cmpxchg((ptr), (o), (n));		\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+#define cmpxchg32_local(ptr, o, n)		\</span>
<span class="p_add">+({						\</span>
<span class="p_add">+	BUILD_BUG_ON(sizeof(*(ptr)) != 4);	\</span>
<span class="p_add">+	cmpxchg_local((ptr), (o), (n));		\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+#define cmpxchg64(ptr, o, n)			\</span>
<span class="p_add">+({						\</span>
<span class="p_add">+	BUILD_BUG_ON(sizeof(*(ptr)) != 8);	\</span>
<span class="p_add">+	cmpxchg((ptr), (o), (n));		\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+#define cmpxchg64_local(ptr, o, n)		\</span>
<span class="p_add">+({						\</span>
<span class="p_add">+	BUILD_BUG_ON(sizeof(*(ptr)) != 8);	\</span>
<span class="p_add">+	cmpxchg_local((ptr), (o), (n));		\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+#else /* !CONFIG_ISA_A */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/cmpxchg.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* CONFIG_ISA_A */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_CMPXCHG_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/io.h b/arch/riscv/include/asm/io.h</span>
new file mode 100644
<span class="p_header">index 000000000000..9c76ecfd203d</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/io.h</span>
<span class="p_chunk">@@ -0,0 +1,178 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * {read,write}{b,w,l,q} based on arch/arm64/include/asm/io.h</span>
<span class="p_add">+ *   which was based on arch/arm/include/io.h</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Copyright (C) 1996-2000 Russell King</span>
<span class="p_add">+ * Copyright (C) 2012 ARM Ltd.</span>
<span class="p_add">+ * Copyright (C) 2014 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_IO_H</span>
<span class="p_add">+#define _ASM_RISCV_IO_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef __KERNEL__</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_MMU</span>
<span class="p_add">+</span>
<span class="p_add">+extern void __iomem *ioremap(phys_addr_t offset, unsigned long size);</span>
<span class="p_add">+</span>
<span class="p_add">+/* The RISC-V ISA doesn&#39;t yet specify how to query of modify PMAs, so we can&#39;t</span>
<span class="p_add">+ * change the properties of memory regions.  This should be fixed by the</span>
<span class="p_add">+ * upcoming platform spec.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define ioremap_nocache(addr, size) ioremap((addr), (size))</span>
<span class="p_add">+#define ioremap_wc(addr, size) ioremap((addr), (size))</span>
<span class="p_add">+#define ioremap_wt(addr, size) ioremap((addr), (size))</span>
<span class="p_add">+</span>
<span class="p_add">+extern void iounmap(void __iomem *addr);</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* CONFIG_MMU */</span>
<span class="p_add">+</span>
<span class="p_add">+/* Generic IO read/write.  These perform native-endian accesses. */</span>
<span class="p_add">+#define __raw_writeb __raw_writeb</span>
<span class="p_add">+static inline void __raw_writeb(u8 val, volatile void __iomem *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	asm volatile(&quot;sb %0, 0(%1)&quot; : : &quot;r&quot; (val), &quot;r&quot; (addr));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define __raw_writew __raw_writew</span>
<span class="p_add">+static inline void __raw_writew(u16 val, volatile void __iomem *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	asm volatile(&quot;sh %0, 0(%1)&quot; : : &quot;r&quot; (val), &quot;r&quot; (addr));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define __raw_writel __raw_writel</span>
<span class="p_add">+static inline void __raw_writel(u32 val, volatile void __iomem *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	asm volatile(&quot;sw %0, 0(%1)&quot; : : &quot;r&quot; (val), &quot;r&quot; (addr));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef __riscv64</span>
<span class="p_add">+#define __raw_writeq __raw_writeq</span>
<span class="p_add">+static inline void __raw_writeq(u64 val, volatile void __iomem *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	asm volatile(&quot;sd %0, 0(%1)&quot; : : &quot;r&quot; (val), &quot;r&quot; (addr));</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#define __raw_readb __raw_readb</span>
<span class="p_add">+static inline u8 __raw_readb(const volatile void __iomem *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	u8 val;</span>
<span class="p_add">+</span>
<span class="p_add">+	asm volatile(&quot;lb %0, 0(%1)&quot; : &quot;=r&quot; (val) : &quot;r&quot; (addr));</span>
<span class="p_add">+	return val;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define __raw_readw __raw_readw</span>
<span class="p_add">+static inline u16 __raw_readw(const volatile void __iomem *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	u16 val;</span>
<span class="p_add">+</span>
<span class="p_add">+	asm volatile(&quot;lh %0, 0(%1)&quot; : &quot;=r&quot; (val) : &quot;r&quot; (addr));</span>
<span class="p_add">+	return val;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define __raw_readl __raw_readl</span>
<span class="p_add">+static inline u32 __raw_readl(const volatile void __iomem *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	u32 val;</span>
<span class="p_add">+</span>
<span class="p_add">+	asm volatile(&quot;lw %0, 0(%1)&quot; : &quot;=r&quot; (val) : &quot;r&quot; (addr));</span>
<span class="p_add">+	return val;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef __riscv64</span>
<span class="p_add">+#define __raw_readq __raw_readq</span>
<span class="p_add">+static inline u64 __raw_readq(const volatile void __iomem *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	u64 val;</span>
<span class="p_add">+</span>
<span class="p_add">+	asm volatile(&quot;ld %0, 0(%1)&quot; : &quot;=r&quot; (val) : &quot;r&quot; (addr));</span>
<span class="p_add">+	return val;</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+/* FIXME: I&#39;m flip-flopping on whether or not we should keep this or enforce</span>
<span class="p_add">+ * the ordering with I/O on spinlocks.  The worry is that drivers won&#39;t get</span>
<span class="p_add">+ * this correct, but I also don&#39;t want to introduce a fence into the lock code</span>
<span class="p_add">+ * that otherwise only uses AMOs and LR/SC.   For now I&#39;m leaving this here:</span>
<span class="p_add">+ * &quot;w,o&quot; is sufficient to ensure that all writes to the device has completed</span>
<span class="p_add">+ * before the write to the spinlock is allowed to commit.  I surmised this from</span>
<span class="p_add">+ * reading &quot;ACQUIRES VS I/O ACCESSES&quot; in memory-barries.txt.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define mmiowb()	__asm__ __volatile__ (&quot;fence o,w&quot; : : : &quot;memory&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+/* Unordered I/O memory access primitives.  These are even more relaxed than</span>
<span class="p_add">+ * the relaxed versions, as they don&#39;t even order accesses between successive</span>
<span class="p_add">+ * operations to the I/O regions.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define readb_cpu(c)		({ u8  __r = __raw_readb(c); __r; })</span>
<span class="p_add">+#define readw_cpu(c)		({ u16 __r = le16_to_cpu((__force __le16)__raw_readw(c)); __r; })</span>
<span class="p_add">+#define readl_cpu(c)		({ u32 __r = le32_to_cpu((__force __le32)__raw_readl(c)); __r; })</span>
<span class="p_add">+#define readq_cpu(c)		({ u64 __r = le64_to_cpu((__force __le64)__raw_readq(c)); __r; })</span>
<span class="p_add">+</span>
<span class="p_add">+#define writeb_cpu(v,c)		((void)__raw_writeb((v),(c)))</span>
<span class="p_add">+#define writew_cpu(v,c)		((void)__raw_writew((__force u16)cpu_to_le16(v),(c)))</span>
<span class="p_add">+#define writel_cpu(v,c)		((void)__raw_writel((__force u32)cpu_to_le32(v),(c)))</span>
<span class="p_add">+#define writeq_cpu(v,c)		((void)__raw_writeq((__force u64)cpu_to_le64(v),(c)))</span>
<span class="p_add">+</span>
<span class="p_add">+/* Relaxed I/O memory access primitives. These follow the Device memory</span>
<span class="p_add">+ * ordering rules but do not guarantee any ordering relative to Normal memory</span>
<span class="p_add">+ * accesses.  The memory barries here are necessary as RISC-V doesn&#39;t define</span>
<span class="p_add">+ * any ordering constraints on accesses to the device I/O space.  These are</span>
<span class="p_add">+ * defined to order the indicated access (either a read or write) with all</span>
<span class="p_add">+ * other I/O memory accesses.</span>
<span class="p_add">+ */</span>
<span class="p_add">+/* FIXME: The platform spec will define the default Linux-capable platform to</span>
<span class="p_add">+ * have some extra IO ordering constraints that will make these fences</span>
<span class="p_add">+ * unnecessary.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define __iorrmb()	__asm__ __volatile__ (&quot;fence i,io&quot; : : : &quot;memory&quot;);</span>
<span class="p_add">+#define __iorwmb()	__asm__ __volatile__ (&quot;fence io,o&quot; : : : &quot;memory&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+#define readb_relaxed(c)	({ u8  __v = readb_cpu(c); __iorrmb(); __v; })</span>
<span class="p_add">+#define readw_relaxed(c)	({ u16 __v = readw_cpu(c); __iorrmb(); __v; })</span>
<span class="p_add">+#define readl_relaxed(c)	({ u32 __v = readl_cpu(c); __iorrmb(); __v; })</span>
<span class="p_add">+#define readq_relaxed(c)	({ u64 __v = readq_cpu(c); __iorrmb(); __v; })</span>
<span class="p_add">+</span>
<span class="p_add">+#define writeb_relaxed(v,c)	({ __iorwmb(); writeb_cpu((v),(c)); })</span>
<span class="p_add">+#define writew_relaxed(v,c)	({ __iorwmb(); writew_cpu((v),(c)); })</span>
<span class="p_add">+#define writel_relaxed(v,c)	({ __iorwmb(); writel_cpu((v),(c)); })</span>
<span class="p_add">+#define writeq_relaxed(v,c)	({ __iorwmb(); writeq_cpu((v),(c)); })</span>
<span class="p_add">+</span>
<span class="p_add">+/* I/O memory access primitives. Reads are ordered relative to any</span>
<span class="p_add">+ * following Normal memory access. Writes are ordered relative to any prior</span>
<span class="p_add">+ * Normal memory access.  The memory barriers here are necessary as RISC-V</span>
<span class="p_add">+ * doesn&#39;t define any ordering between the memory space and the I/O space.</span>
<span class="p_add">+ * They may be stronger than necessary (&quot;i,r&quot; and &quot;w,o&quot; might be sufficient),</span>
<span class="p_add">+ * but I feel kind of queasy making these weaker in any manner than the relaxed</span>
<span class="p_add">+ * versions above.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define __iormb()	__asm__ __volatile__ (&quot;fence i,ior&quot; : : : &quot;memory&quot;);</span>
<span class="p_add">+#define __iowmb()	__asm__ __volatile__ (&quot;fence iow,o&quot; : : : &quot;memory&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+#define readb(c)		({ u8  __v = readb_cpu(c); __iormb(); __v; })</span>
<span class="p_add">+#define readw(c)		({ u16 __v = readw_cpu(c); __iormb(); __v; })</span>
<span class="p_add">+#define readl(c)		({ u32 __v = readl_cpu(c); __iormb(); __v; })</span>
<span class="p_add">+#define readq(c)		({ u64 __v = readq_cpu(c); __iormb(); __v; })</span>
<span class="p_add">+</span>
<span class="p_add">+#define writeb(v,c)		({ __iowmb(); writeb_cpu((v),(c)); })</span>
<span class="p_add">+#define writew(v,c)		({ __iowmb(); writew_cpu((v),(c)); })</span>
<span class="p_add">+#define writel(v,c)		({ __iowmb(); writel_cpu((v),(c)); })</span>
<span class="p_add">+#define writeq(v,c)		({ __iowmb(); writeq_cpu((v),(c)); })</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/io.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* __KERNEL__ */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_IO_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/spinlock.h b/arch/riscv/include/asm/spinlock.h</span>
new file mode 100644
<span class="p_header">index 000000000000..54ed73bfa972</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/spinlock.h</span>
<span class="p_chunk">@@ -0,0 +1,167 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2015 Regents of the University of California</span>
<span class="p_add">+ * Copyright (C) 2017 SiFive</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_SPINLOCK_H</span>
<span class="p_add">+#define _ASM_RISCV_SPINLOCK_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/kernel.h&gt;</span>
<span class="p_add">+#include &lt;asm/current.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Simple spin lock operations.  These provide no fairness guarantees.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+/* FIXME: Replace this with a ticket lock, like MIPS. */</span>
<span class="p_add">+</span>
<span class="p_add">+#define arch_spin_lock_flags(lock, flags) arch_spin_lock(lock)</span>
<span class="p_add">+#define arch_spin_is_locked(x)	((x)-&gt;lock != 0)</span>
<span class="p_add">+#define arch_spin_unlock_wait(x) \</span>
<span class="p_add">+		do { cpu_relax(); } while ((x)-&gt;lock)</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void arch_spin_unlock(arch_spinlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;amoswap.w.rl x0, x0, %0&quot;</span>
<span class="p_add">+		: &quot;=A&quot; (lock-&gt;lock)</span>
<span class="p_add">+		:: &quot;memory&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int arch_spin_trylock(arch_spinlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int tmp = 1, busy;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;amoswap.w.aq %0, %2, %1&quot;</span>
<span class="p_add">+		: &quot;=r&quot; (busy), &quot;+A&quot; (lock-&gt;lock)</span>
<span class="p_add">+		: &quot;r&quot; (tmp)</span>
<span class="p_add">+		: &quot;memory&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+	return !busy;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void arch_spin_lock(arch_spinlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	while (1) {</span>
<span class="p_add">+		if (arch_spin_is_locked(lock))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (arch_spin_trylock(lock))</span>
<span class="p_add">+			break;</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void arch_spin_unlock_wait(arch_spinlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	smp_rmb();</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		cpu_relax();</span>
<span class="p_add">+	} while (arch_spin_is_locked(lock));</span>
<span class="p_add">+	smp_acquire__after_ctrl_dep();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/***********************************************************/</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int arch_read_can_lock(arch_rwlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return lock-&gt;lock &gt;= 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int arch_write_can_lock(arch_rwlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return lock-&gt;lock == 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void arch_read_lock(arch_rwlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int tmp;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__(</span>
<span class="p_add">+		&quot;1:	lr.w	%1, %0\n&quot;</span>
<span class="p_add">+		&quot;	bltz	%1, 1b\n&quot;</span>
<span class="p_add">+		&quot;	addi	%1, %1, 1\n&quot;</span>
<span class="p_add">+		&quot;	sc.w.aq	%1, %1, %0\n&quot;</span>
<span class="p_add">+		&quot;	bnez	%1, 1b\n&quot;</span>
<span class="p_add">+		: &quot;+A&quot; (lock-&gt;lock), &quot;=&amp;r&quot; (tmp)</span>
<span class="p_add">+		:: &quot;memory&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void arch_write_lock(arch_rwlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int tmp;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__(</span>
<span class="p_add">+		&quot;1:	lr.w	%1, %0\n&quot;</span>
<span class="p_add">+		&quot;	bnez	%1, 1b\n&quot;</span>
<span class="p_add">+		&quot;	li	%1, -1\n&quot;</span>
<span class="p_add">+		&quot;	sc.w.aq	%1, %1, %0\n&quot;</span>
<span class="p_add">+		&quot;	bnez	%1, 1b\n&quot;</span>
<span class="p_add">+		: &quot;+A&quot; (lock-&gt;lock), &quot;=&amp;r&quot; (tmp)</span>
<span class="p_add">+		:: &quot;memory&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int arch_read_trylock(arch_rwlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int busy;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__(</span>
<span class="p_add">+		&quot;1:	lr.w	%1, %0\n&quot;</span>
<span class="p_add">+		&quot;	bltz	%1, 1f\n&quot;</span>
<span class="p_add">+		&quot;	addi	%1, %1, 1\n&quot;</span>
<span class="p_add">+		&quot;	sc.w.aq	%1, %1, %0\n&quot;</span>
<span class="p_add">+		&quot;	bnez	%1, 1b\n&quot;</span>
<span class="p_add">+		&quot;1:\n&quot;</span>
<span class="p_add">+		: &quot;+A&quot; (lock-&gt;lock), &quot;=&amp;r&quot; (busy)</span>
<span class="p_add">+		:: &quot;memory&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+	return !busy;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int arch_write_trylock(arch_rwlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int busy;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__(</span>
<span class="p_add">+		&quot;1:	lr.w	%1, %0\n&quot;</span>
<span class="p_add">+		&quot;	bnez	%1, 1f\n&quot;</span>
<span class="p_add">+		&quot;	li	%1, -1\n&quot;</span>
<span class="p_add">+		&quot;	sc.w.aq	%1, %1, %0\n&quot;</span>
<span class="p_add">+		&quot;	bnez	%1, 1b\n&quot;</span>
<span class="p_add">+		&quot;1:\n&quot;</span>
<span class="p_add">+		: &quot;+A&quot; (lock-&gt;lock), &quot;=&amp;r&quot; (busy)</span>
<span class="p_add">+		:: &quot;memory&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+	return !busy;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void arch_read_unlock(arch_rwlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__asm__ __volatile__(</span>
<span class="p_add">+		&quot;amoadd.w.rl x0, %1, %0&quot;</span>
<span class="p_add">+		: &quot;+A&quot; (lock-&gt;lock)</span>
<span class="p_add">+		: &quot;r&quot; (-1)</span>
<span class="p_add">+		: &quot;memory&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void arch_write_unlock(arch_rwlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;amoswap.w.rl x0, x0, %0&quot;</span>
<span class="p_add">+		: &quot;=A&quot; (lock-&gt;lock)</span>
<span class="p_add">+		:: &quot;memory&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define arch_read_lock_flags(lock, flags) arch_read_lock(lock)</span>
<span class="p_add">+#define arch_write_lock_flags(lock, flags) arch_write_lock(lock)</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_SPINLOCK_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/spinlock_types.h b/arch/riscv/include/asm/spinlock_types.h</span>
new file mode 100644
<span class="p_header">index 000000000000..83ac4ac9e2ac</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/spinlock_types.h</span>
<span class="p_chunk">@@ -0,0 +1,33 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2015 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_SPINLOCK_TYPES_H</span>
<span class="p_add">+#define _ASM_RISCV_SPINLOCK_TYPES_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __LINUX_SPINLOCK_TYPES_H</span>
<span class="p_add">+# error &quot;please don&#39;t include this file directly&quot;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+typedef struct {</span>
<span class="p_add">+	volatile unsigned int lock;</span>
<span class="p_add">+} arch_spinlock_t;</span>
<span class="p_add">+</span>
<span class="p_add">+#define __ARCH_SPIN_LOCK_UNLOCKED	{ 0 }</span>
<span class="p_add">+</span>
<span class="p_add">+typedef struct {</span>
<span class="p_add">+	volatile unsigned int lock;</span>
<span class="p_add">+} arch_rwlock_t;</span>
<span class="p_add">+</span>
<span class="p_add">+#define __ARCH_RW_LOCK_UNLOCKED		{ 0 }</span>
<span class="p_add">+</span>
<span class="p_add">+#endif</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/tlb.h b/arch/riscv/include/asm/tlb.h</span>
new file mode 100644
<span class="p_header">index 000000000000..c229509288ea</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/tlb.h</span>
<span class="p_chunk">@@ -0,0 +1,24 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_TLB_H</span>
<span class="p_add">+#define _ASM_RISCV_TLB_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/tlb.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void tlb_flush(struct mmu_gather *tlb)</span>
<span class="p_add">+{</span>
<span class="p_add">+	flush_tlb_mm(tlb-&gt;mm);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_TLB_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/tlbflush.h b/arch/riscv/include/asm/tlbflush.h</span>
new file mode 100644
<span class="p_header">index 000000000000..5ee4ae370b5e</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/tlbflush.h</span>
<span class="p_chunk">@@ -0,0 +1,64 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2009 Chen Liqin &lt;liqin.chen@sunplusct.com&gt;</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_TLBFLUSH_H</span>
<span class="p_add">+#define _ASM_RISCV_TLBFLUSH_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_MMU</span>
<span class="p_add">+</span>
<span class="p_add">+/* Flush entire local TLB */</span>
<span class="p_add">+static inline void local_flush_tlb_all(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__asm__ __volatile__ (&quot;sfence.vma&quot; : : : &quot;memory&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Flush one page from local TLB */</span>
<span class="p_add">+static inline void local_flush_tlb_page(unsigned long addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__asm__ __volatile__ (&quot;sfence.vma %0&quot; : : &quot;r&quot; (addr) : &quot;memory&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef CONFIG_SMP</span>
<span class="p_add">+</span>
<span class="p_add">+#define flush_tlb_all() local_flush_tlb_all()</span>
<span class="p_add">+#define flush_tlb_page(vma, addr) local_flush_tlb_page(addr)</span>
<span class="p_add">+#define flush_tlb_range(vma, start, end) local_flush_tlb_all()</span>
<span class="p_add">+</span>
<span class="p_add">+#else /* CONFIG_SMP */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/sbi.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#define flush_tlb_all() sbi_remote_sfence_vma(0, 0, -1)</span>
<span class="p_add">+#define flush_tlb_page(vma, addr) flush_tlb_range(vma, addr, 0)</span>
<span class="p_add">+#define flush_tlb_range(vma, start, end) \</span>
<span class="p_add">+	sbi_remote_sfence_vma(0, start, (end) - (start))</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* CONFIG_SMP */</span>
<span class="p_add">+</span>
<span class="p_add">+/* Flush the TLB entries of the specified mm context */</span>
<span class="p_add">+static inline void flush_tlb_mm(struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+	flush_tlb_all();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Flush a range of kernel pages */</span>
<span class="p_add">+static inline void flush_tlb_kernel_range(unsigned long start,</span>
<span class="p_add">+	unsigned long end)</span>
<span class="p_add">+{</span>
<span class="p_add">+	flush_tlb_all();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* CONFIG_MMU */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_TLBFLUSH_H */</span>

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



