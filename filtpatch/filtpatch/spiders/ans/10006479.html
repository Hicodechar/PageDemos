
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[GIT,PULL] x86 fixes - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [GIT,PULL] x86 fixes</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=35552">Ingo Molnar</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Oct. 14, 2017, 4:16 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20171014161636.tufvzolmzst64jaa@gmail.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10006479/mbox/"
   >mbox</a>
|
   <a href="/patch/10006479/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10006479/">/patch/10006479/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	E1066601E9 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Sat, 14 Oct 2017 16:16:47 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id B6432290D1
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Sat, 14 Oct 2017 16:16:47 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id AABBE291E2; Sat, 14 Oct 2017 16:16:47 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.8 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	RCVD_IN_DNSWL_HI,T_DKIM_INVALID autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id EB35A290D1
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Sat, 14 Oct 2017 16:16:45 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1753958AbdJNQQn (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Sat, 14 Oct 2017 12:16:43 -0400
Received: from mail-wr0-f180.google.com ([209.85.128.180]:52905 &quot;EHLO
	mail-wr0-f180.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1753666AbdJNQQl (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Sat, 14 Oct 2017 12:16:41 -0400
Received: by mail-wr0-f180.google.com with SMTP id k62so2806545wrc.9
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Sat, 14 Oct 2017 09:16:40 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=gmail.com; s=20161025;
	h=sender:date:from:to:cc:subject:message-id:mime-version
	:content-disposition:user-agent;
	bh=fthB5Yx2/wWdkoow3+eb6Unw7ulxtMlRZqCFHgMssUQ=;
	b=esrZIWNLNJDSiWDnuTPMFuvTJu7r7Ax1z9U2w9qBrtVFw7dl2HskbmCPR+stgECDky
	mYZCG2aNTtanBfaBlyIZKpM89F3y/qhtSM+EgK/nIfedIo7MIgn0T8UHsvXa2fHX9sLN
	2GYgHGnqWRAYIU8mrizdsaZcuUWP+RPvCxeO41m6nHbUOc/Q3jml6AGuiWBXH7DljRqi
	t02Sg25723x4j95CiSrK4n9mN4eK2pxyKD8UsJloaNcnPe8Dx8XNqs/jAaykUMFGxb0P
	yZ7s7pYXZkezjFSQjhVu0Lce4YGTFi9K1QKGIhY3nRR36JToU1f8xtmCTREfbqcne9pJ
	TjMQ==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20161025;
	h=x-gm-message-state:sender:date:from:to:cc:subject:message-id
	:mime-version:content-disposition:user-agent;
	bh=fthB5Yx2/wWdkoow3+eb6Unw7ulxtMlRZqCFHgMssUQ=;
	b=hZ5TCXtTZmFQ8EcK2Bug3mB6Y35KNmwqAvo7del7lC6Mlgen4U9ur92N84YFSl+Oua
	L2+QNq2+A6fYYclxtnNDaWk89P28oKipH1m7vdCe7Q+MmBUqNNqE1VF/sVIpK0tsqEao
	D5M+TGSQoCmsRQXg6CVKw45JG4/EMiA38It194J/Cj758glzaVGvgZqPWFv+oJsSnWdk
	SLJQvZNixXnwu0zMrPvf8e3WjER8b8rwtgygzwbiYXeb5eScMXKaeIuFSn+GiOhdEPUh
	s/NL+UncNcJE2Z+Ir9NbBZSgMKgaQOdrG5bCUxHVUBLwsJOufuNk2gTMxWx+D+rmZH3k
	XP1Q==
X-Gm-Message-State: AMCzsaVjF0C7OeJDFujbXTYK3o+QhvzPWS3SY4/OcO6rdkmZkIRDvtmv
	GQJL/LnqRFzTBMTBFBsGmuRGmQ==
X-Google-Smtp-Source: AOwi7QCNOYZfT5vM9fNH4iRbslrCW9sa1mooUu0GpTmHORMkaKwdxvdbq5nflLpjJ1tDxbE4+XqfQA==
X-Received: by 10.223.182.12 with SMTP id f12mr3833725wre.136.1507997799828; 
	Sat, 14 Oct 2017 09:16:39 -0700 (PDT)
Received: from gmail.com (2E8B0CD5.catv.pool.telekom.hu. [46.139.12.213])
	by smtp.gmail.com with ESMTPSA id
	h8sm2758957wme.30.2017.10.14.09.16.38
	(version=TLS1_2 cipher=ECDHE-RSA-CHACHA20-POLY1305 bits=256/256);
	Sat, 14 Oct 2017 09:16:38 -0700 (PDT)
Date: Sat, 14 Oct 2017 18:16:37 +0200
From: Ingo Molnar &lt;mingo@kernel.org&gt;
To: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;
Cc: linux-kernel@vger.kernel.org, Thomas Gleixner &lt;tglx@linutronix.de&gt;,
	&quot;H. Peter Anvin&quot; &lt;hpa@zytor.com&gt;,
	Peter Zijlstra &lt;a.p.zijlstra@chello.nl&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;
Subject: [GIT PULL] x86 fixes
Message-ID: &lt;20171014161636.tufvzolmzst64jaa@gmail.com&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Disposition: inline
User-Agent: NeoMutt/20170113 (1.7.2)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=35552">Ingo Molnar</a> - Oct. 14, 2017, 4:16 p.m.</div>
<pre class="content">
Linus,

Please pull the latest x86-urgent-for-linus git tree from:

   git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip.git x86-urgent-for-linus

   # HEAD: 1f161f67a272cc4f29f27934dd3f74cb657eb5c4 x86/microcode: Do the family check first

A landry list of fixes:

 - fix reboot breakage on some PCID-enabled system
 - fix crashes/hangs on some PCID-enabled systems
 - fix microcode loading on certain older CPUs
 - various unwinder fixes
 - extend an APIC quirk to more hardware systems and disable APIC related warning 
   on virtualized systems
 - various Hyper-V fixes
 - a macro definition robustness fix
 - remove jprobes IRQ disabling
 - various mem-encryption fixes

 Thanks,

	Ingo

------------------&gt;
Andy Lutomirski (2):
      x86/mm/64: Fix reboot interaction with CR4.PCIDE
      x86/mm: Flush more aggressively in lazy TLB mode

Borislav Petkov (1):
      x86/microcode: Do the family check first

Josh Poimboeuf (5):
      kprobes/x86: Set up frame pointer in kprobe trampoline
      x86/unwind: Fix dereference of untrusted pointer
      x86/unwind: Use MSB for frame pointer encoding on 32-bit
      x86/unwind: Align stack pointer in unwinder dump
      x86/unwind: Disable unwinder warnings on 32-bit

Len Brown (1):
      x86/apic: Update TSC_DEADLINE quirk with additional SKX stepping

Marcelo Henrique Cerri (1):
      x86/hyperv: Fix hypercalls with extended CPU ranges for TLB flushing

Masami Hiramatsu (1):
      kprobes/x86: Remove IRQ disabling from jprobe handlers

Mathias Krause (1):
      x86/alternatives: Fix alt_max_short macro to really be a max()

Paolo Bonzini (1):
      x86/apic: Silence &quot;FW_BUG TSC_DEADLINE disabled due to Errata&quot; on hypervisors

Tom Lendacky (1):
      x86/mm: Disable various instrumentations of mm/mem_encrypt.c and mm/tlb.c

Vitaly Kuznetsov (2):
      x86/hyperv: Clear vCPU banks between calls to avoid flushing unneeded vCPUs
      x86/hyperv: Don&#39;t use percpu areas for pcpu_flush/pcpu_flush_ex structures


 arch/x86/entry/entry_32.S              |   4 +-
 arch/x86/hyperv/hv_init.c              |   5 ++
 arch/x86/hyperv/mmu.c                  |  57 +++++++++---
 arch/x86/include/asm/alternative-asm.h |   4 +-
 arch/x86/include/asm/alternative.h     |   6 +-
 arch/x86/include/asm/mmu_context.h     |   8 +-
 arch/x86/include/asm/mshyperv.h        |   1 +
 arch/x86/include/asm/tlbflush.h        |  24 ++++++
 arch/x86/kernel/apic/apic.c            |  15 +++-
 arch/x86/kernel/cpu/microcode/core.c   |  27 ++++--
 arch/x86/kernel/kprobes/common.h       |  13 ++-
 arch/x86/kernel/kprobes/core.c         |   2 -
 arch/x86/kernel/reboot.c               |   4 +
 arch/x86/kernel/unwind_frame.c         |  38 +++++++-
 arch/x86/mm/Makefile                   |  11 ++-
 arch/x86/mm/tlb.c                      | 153 ++++++++++++++++++++++++---------
 16 files changed, 284 insertions(+), 88 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/entry/entry_32.S b/arch/x86/entry/entry_32.S</span>
<span class="p_header">index 8a13d468635a..50e0d2bc4528 100644</span>
<span class="p_header">--- a/arch/x86/entry/entry_32.S</span>
<span class="p_header">+++ b/arch/x86/entry/entry_32.S</span>
<span class="p_chunk">@@ -176,7 +176,7 @@</span> <span class="p_context"></span>
 /*
  * This is a sneaky trick to help the unwinder find pt_regs on the stack.  The
  * frame pointer is replaced with an encoded pointer to pt_regs.  The encoding
<span class="p_del">- * is just setting the LSB, which makes it an invalid stack address and is also</span>
<span class="p_add">+ * is just clearing the MSB, which makes it an invalid stack address and is also</span>
  * a signal to the unwinder that it&#39;s a pt_regs pointer in disguise.
  *
  * NOTE: This macro must be used *after* SAVE_ALL because it corrupts the
<span class="p_chunk">@@ -185,7 +185,7 @@</span> <span class="p_context"></span>
 .macro ENCODE_FRAME_POINTER
 #ifdef CONFIG_FRAME_POINTER
 	mov %esp, %ebp
<span class="p_del">-	orl $0x1, %ebp</span>
<span class="p_add">+	andl $0x7fffffff, %ebp</span>
 #endif
 .endm
 
<span class="p_header">diff --git a/arch/x86/hyperv/hv_init.c b/arch/x86/hyperv/hv_init.c</span>
<span class="p_header">index 1a8eb550c40f..a5db63f728a2 100644</span>
<span class="p_header">--- a/arch/x86/hyperv/hv_init.c</span>
<span class="p_header">+++ b/arch/x86/hyperv/hv_init.c</span>
<span class="p_chunk">@@ -85,6 +85,8 @@</span> <span class="p_context"> EXPORT_SYMBOL_GPL(hyperv_cs);</span>
 u32 *hv_vp_index;
 EXPORT_SYMBOL_GPL(hv_vp_index);
 
<span class="p_add">+u32 hv_max_vp_index;</span>
<span class="p_add">+</span>
 static int hv_cpu_init(unsigned int cpu)
 {
 	u64 msr_vp_index;
<span class="p_chunk">@@ -93,6 +95,9 @@</span> <span class="p_context"> static int hv_cpu_init(unsigned int cpu)</span>
 
 	hv_vp_index[smp_processor_id()] = msr_vp_index;
 
<span class="p_add">+	if (msr_vp_index &gt; hv_max_vp_index)</span>
<span class="p_add">+		hv_max_vp_index = msr_vp_index;</span>
<span class="p_add">+</span>
 	return 0;
 }
 
<span class="p_header">diff --git a/arch/x86/hyperv/mmu.c b/arch/x86/hyperv/mmu.c</span>
<span class="p_header">index 39e7f6e50919..9cc9e1c1e2db 100644</span>
<span class="p_header">--- a/arch/x86/hyperv/mmu.c</span>
<span class="p_header">+++ b/arch/x86/hyperv/mmu.c</span>
<span class="p_chunk">@@ -36,9 +36,9 @@</span> <span class="p_context"> struct hv_flush_pcpu_ex {</span>
 /* Each gva in gva_list encodes up to 4096 pages to flush */
 #define HV_TLB_FLUSH_UNIT (4096 * PAGE_SIZE)
 
<span class="p_del">-static struct hv_flush_pcpu __percpu *pcpu_flush;</span>
<span class="p_add">+static struct hv_flush_pcpu __percpu **pcpu_flush;</span>
 
<span class="p_del">-static struct hv_flush_pcpu_ex __percpu *pcpu_flush_ex;</span>
<span class="p_add">+static struct hv_flush_pcpu_ex __percpu **pcpu_flush_ex;</span>
 
 /*
  * Fills in gva_list starting from offset. Returns the number of items added.
<span class="p_chunk">@@ -76,6 +76,18 @@</span> <span class="p_context"> static inline int cpumask_to_vp_set(struct hv_flush_pcpu_ex *flush,</span>
 {
 	int cpu, vcpu, vcpu_bank, vcpu_offset, nr_bank = 1;
 
<span class="p_add">+	/* valid_bank_mask can represent up to 64 banks */</span>
<span class="p_add">+	if (hv_max_vp_index / 64 &gt;= 64)</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Clear all banks up to the maximum possible bank as hv_flush_pcpu_ex</span>
<span class="p_add">+	 * structs are not cleared between calls, we risk flushing unneeded</span>
<span class="p_add">+	 * vCPUs otherwise.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	for (vcpu_bank = 0; vcpu_bank &lt;= hv_max_vp_index / 64; vcpu_bank++)</span>
<span class="p_add">+		flush-&gt;hv_vp_set.bank_contents[vcpu_bank] = 0;</span>
<span class="p_add">+</span>
 	/*
 	 * Some banks may end up being empty but this is acceptable.
 	 */
<span class="p_chunk">@@ -83,11 +95,6 @@</span> <span class="p_context"> static inline int cpumask_to_vp_set(struct hv_flush_pcpu_ex *flush,</span>
 		vcpu = hv_cpu_number_to_vp_number(cpu);
 		vcpu_bank = vcpu / 64;
 		vcpu_offset = vcpu % 64;
<span class="p_del">-</span>
<span class="p_del">-		/* valid_bank_mask can represent up to 64 banks */</span>
<span class="p_del">-		if (vcpu_bank &gt;= 64)</span>
<span class="p_del">-			return 0;</span>
<span class="p_del">-</span>
 		__set_bit(vcpu_offset, (unsigned long *)
 			  &amp;flush-&gt;hv_vp_set.bank_contents[vcpu_bank]);
 		if (vcpu_bank &gt;= nr_bank)
<span class="p_chunk">@@ -102,6 +109,7 @@</span> <span class="p_context"> static void hyperv_flush_tlb_others(const struct cpumask *cpus,</span>
 				    const struct flush_tlb_info *info)
 {
 	int cpu, vcpu, gva_n, max_gvas;
<span class="p_add">+	struct hv_flush_pcpu **flush_pcpu;</span>
 	struct hv_flush_pcpu *flush;
 	u64 status = U64_MAX;
 	unsigned long flags;
<span class="p_chunk">@@ -116,7 +124,17 @@</span> <span class="p_context"> static void hyperv_flush_tlb_others(const struct cpumask *cpus,</span>
 
 	local_irq_save(flags);
 
<span class="p_del">-	flush = this_cpu_ptr(pcpu_flush);</span>
<span class="p_add">+	flush_pcpu = this_cpu_ptr(pcpu_flush);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (unlikely(!*flush_pcpu))</span>
<span class="p_add">+		*flush_pcpu = page_address(alloc_page(GFP_ATOMIC));</span>
<span class="p_add">+</span>
<span class="p_add">+	flush = *flush_pcpu;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (unlikely(!flush)) {</span>
<span class="p_add">+		local_irq_restore(flags);</span>
<span class="p_add">+		goto do_native;</span>
<span class="p_add">+	}</span>
 
 	if (info-&gt;mm) {
 		flush-&gt;address_space = virt_to_phys(info-&gt;mm-&gt;pgd);
<span class="p_chunk">@@ -173,6 +191,7 @@</span> <span class="p_context"> static void hyperv_flush_tlb_others_ex(const struct cpumask *cpus,</span>
 				       const struct flush_tlb_info *info)
 {
 	int nr_bank = 0, max_gvas, gva_n;
<span class="p_add">+	struct hv_flush_pcpu_ex **flush_pcpu;</span>
 	struct hv_flush_pcpu_ex *flush;
 	u64 status = U64_MAX;
 	unsigned long flags;
<span class="p_chunk">@@ -187,7 +206,17 @@</span> <span class="p_context"> static void hyperv_flush_tlb_others_ex(const struct cpumask *cpus,</span>
 
 	local_irq_save(flags);
 
<span class="p_del">-	flush = this_cpu_ptr(pcpu_flush_ex);</span>
<span class="p_add">+	flush_pcpu = this_cpu_ptr(pcpu_flush_ex);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (unlikely(!*flush_pcpu))</span>
<span class="p_add">+		*flush_pcpu = page_address(alloc_page(GFP_ATOMIC));</span>
<span class="p_add">+</span>
<span class="p_add">+	flush = *flush_pcpu;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (unlikely(!flush)) {</span>
<span class="p_add">+		local_irq_restore(flags);</span>
<span class="p_add">+		goto do_native;</span>
<span class="p_add">+	}</span>
 
 	if (info-&gt;mm) {
 		flush-&gt;address_space = virt_to_phys(info-&gt;mm-&gt;pgd);
<span class="p_chunk">@@ -222,18 +251,18 @@</span> <span class="p_context"> static void hyperv_flush_tlb_others_ex(const struct cpumask *cpus,</span>
 		flush-&gt;flags |= HV_FLUSH_NON_GLOBAL_MAPPINGS_ONLY;
 		status = hv_do_rep_hypercall(
 			HVCALL_FLUSH_VIRTUAL_ADDRESS_SPACE_EX,
<span class="p_del">-			0, nr_bank + 2, flush, NULL);</span>
<span class="p_add">+			0, nr_bank, flush, NULL);</span>
 	} else if (info-&gt;end &amp;&amp;
 		   ((info-&gt;end - info-&gt;start)/HV_TLB_FLUSH_UNIT) &gt; max_gvas) {
 		status = hv_do_rep_hypercall(
 			HVCALL_FLUSH_VIRTUAL_ADDRESS_SPACE_EX,
<span class="p_del">-			0, nr_bank + 2, flush, NULL);</span>
<span class="p_add">+			0, nr_bank, flush, NULL);</span>
 	} else {
 		gva_n = fill_gva_list(flush-&gt;gva_list, nr_bank,
 				      info-&gt;start, info-&gt;end);
 		status = hv_do_rep_hypercall(
 			HVCALL_FLUSH_VIRTUAL_ADDRESS_LIST_EX,
<span class="p_del">-			gva_n, nr_bank + 2, flush, NULL);</span>
<span class="p_add">+			gva_n, nr_bank, flush, NULL);</span>
 	}
 
 	local_irq_restore(flags);
<span class="p_chunk">@@ -266,7 +295,7 @@</span> <span class="p_context"> void hyper_alloc_mmu(void)</span>
 		return;
 
 	if (!(ms_hyperv.hints &amp; HV_X64_EX_PROCESSOR_MASKS_RECOMMENDED))
<span class="p_del">-		pcpu_flush = __alloc_percpu(PAGE_SIZE, PAGE_SIZE);</span>
<span class="p_add">+		pcpu_flush = alloc_percpu(struct hv_flush_pcpu *);</span>
 	else
<span class="p_del">-		pcpu_flush_ex = __alloc_percpu(PAGE_SIZE, PAGE_SIZE);</span>
<span class="p_add">+		pcpu_flush_ex = alloc_percpu(struct hv_flush_pcpu_ex *);</span>
 }
<span class="p_header">diff --git a/arch/x86/include/asm/alternative-asm.h b/arch/x86/include/asm/alternative-asm.h</span>
<span class="p_header">index e7636bac7372..6c98821fef5e 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/alternative-asm.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/alternative-asm.h</span>
<span class="p_chunk">@@ -62,8 +62,10 @@</span> <span class="p_context"></span>
 #define new_len2		145f-144f
 
 /*
<span class="p_del">- * max without conditionals. Idea adapted from:</span>
<span class="p_add">+ * gas compatible max based on the idea from:</span>
  * http://graphics.stanford.edu/~seander/bithacks.html#IntegerMinOrMax
<span class="p_add">+ *</span>
<span class="p_add">+ * The additional &quot;-&quot; is needed because gas uses a &quot;true&quot; value of -1.</span>
  */
 #define alt_max_short(a, b)	((a) ^ (((a) ^ (b)) &amp; -(-((a) &lt; (b)))))
 
<span class="p_header">diff --git a/arch/x86/include/asm/alternative.h b/arch/x86/include/asm/alternative.h</span>
<span class="p_header">index c096624137ae..ccbe24e697c4 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/alternative.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/alternative.h</span>
<span class="p_chunk">@@ -103,12 +103,12 @@</span> <span class="p_context"> static inline int alternatives_text_reserved(void *start, void *end)</span>
 	alt_end_marker &quot;:\n&quot;
 
 /*
<span class="p_del">- * max without conditionals. Idea adapted from:</span>
<span class="p_add">+ * gas compatible max based on the idea from:</span>
  * http://graphics.stanford.edu/~seander/bithacks.html#IntegerMinOrMax
  *
<span class="p_del">- * The additional &quot;-&quot; is needed because gas works with s32s.</span>
<span class="p_add">+ * The additional &quot;-&quot; is needed because gas uses a &quot;true&quot; value of -1.</span>
  */
<span class="p_del">-#define alt_max_short(a, b)	&quot;((&quot; a &quot;) ^ (((&quot; a &quot;) ^ (&quot; b &quot;)) &amp; -(-((&quot; a &quot;) - (&quot; b &quot;)))))&quot;</span>
<span class="p_add">+#define alt_max_short(a, b)	&quot;((&quot; a &quot;) ^ (((&quot; a &quot;) ^ (&quot; b &quot;)) &amp; -(-((&quot; a &quot;) &lt; (&quot; b &quot;)))))&quot;</span>
 
 /*
  * Pad the second replacement alternative with additional NOPs if it is
<span class="p_header">diff --git a/arch/x86/include/asm/mmu_context.h b/arch/x86/include/asm/mmu_context.h</span>
<span class="p_header">index c120b5db178a..3c856a15b98e 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/mmu_context.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/mmu_context.h</span>
<span class="p_chunk">@@ -126,13 +126,7 @@</span> <span class="p_context"> static inline void switch_ldt(struct mm_struct *prev, struct mm_struct *next)</span>
 	DEBUG_LOCKS_WARN_ON(preemptible());
 }
 
<span class="p_del">-static inline void enter_lazy_tlb(struct mm_struct *mm, struct task_struct *tsk)</span>
<span class="p_del">-{</span>
<span class="p_del">-	int cpu = smp_processor_id();</span>
<span class="p_del">-</span>
<span class="p_del">-	if (cpumask_test_cpu(cpu, mm_cpumask(mm)))</span>
<span class="p_del">-		cpumask_clear_cpu(cpu, mm_cpumask(mm));</span>
<span class="p_del">-}</span>
<span class="p_add">+void enter_lazy_tlb(struct mm_struct *mm, struct task_struct *tsk);</span>
 
 static inline int init_new_context(struct task_struct *tsk,
 				   struct mm_struct *mm)
<span class="p_header">diff --git a/arch/x86/include/asm/mshyperv.h b/arch/x86/include/asm/mshyperv.h</span>
<span class="p_header">index 738503e1f80c..530f448fddaf 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/mshyperv.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/mshyperv.h</span>
<span class="p_chunk">@@ -289,6 +289,7 @@</span> <span class="p_context"> static inline u64 hv_do_rep_hypercall(u16 code, u16 rep_count, u16 varhead_size,</span>
  * to this information.
  */
 extern u32 *hv_vp_index;
<span class="p_add">+extern u32 hv_max_vp_index;</span>
 
 /**
  * hv_cpu_number_to_vp_number() - Map CPU to VP.
<span class="p_header">diff --git a/arch/x86/include/asm/tlbflush.h b/arch/x86/include/asm/tlbflush.h</span>
<span class="p_header">index 4893abf7f74f..d362161d3291 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/tlbflush.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/tlbflush.h</span>
<span class="p_chunk">@@ -83,6 +83,13 @@</span> <span class="p_context"> static inline u64 inc_mm_tlb_gen(struct mm_struct *mm)</span>
 #endif
 
 /*
<span class="p_add">+ * If tlb_use_lazy_mode is true, then we try to avoid switching CR3 to point</span>
<span class="p_add">+ * to init_mm when we switch to a kernel thread (e.g. the idle thread).  If</span>
<span class="p_add">+ * it&#39;s false, then we immediately switch CR3 when entering a kernel thread.</span>
<span class="p_add">+ */</span>
<span class="p_add">+DECLARE_STATIC_KEY_TRUE(tlb_use_lazy_mode);</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
  * 6 because 6 should be plenty and struct tlb_state will fit in
  * two cache lines.
  */
<span class="p_chunk">@@ -105,6 +112,23 @@</span> <span class="p_context"> struct tlb_state {</span>
 	u16 next_asid;
 
 	/*
<span class="p_add">+	 * We can be in one of several states:</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 *  - Actively using an mm.  Our CPU&#39;s bit will be set in</span>
<span class="p_add">+	 *    mm_cpumask(loaded_mm) and is_lazy == false;</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 *  - Not using a real mm.  loaded_mm == &amp;init_mm.  Our CPU&#39;s bit</span>
<span class="p_add">+	 *    will not be set in mm_cpumask(&amp;init_mm) and is_lazy == false.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 *  - Lazily using a real mm.  loaded_mm != &amp;init_mm, our bit</span>
<span class="p_add">+	 *    is set in mm_cpumask(loaded_mm), but is_lazy == true.</span>
<span class="p_add">+	 *    We&#39;re heuristically guessing that the CR3 load we</span>
<span class="p_add">+	 *    skipped more than makes up for the overhead added by</span>
<span class="p_add">+	 *    lazy mode.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	bool is_lazy;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
 	 * Access to this CR4 shadow and to H/W CR4 is protected by
 	 * disabling interrupts when modifying either one.
 	 */
<span class="p_header">diff --git a/arch/x86/kernel/apic/apic.c b/arch/x86/kernel/apic/apic.c</span>
<span class="p_header">index d705c769f77d..ff891772c9f8 100644</span>
<span class="p_header">--- a/arch/x86/kernel/apic/apic.c</span>
<span class="p_header">+++ b/arch/x86/kernel/apic/apic.c</span>
<span class="p_chunk">@@ -573,11 +573,21 @@</span> <span class="p_context"> static u32 bdx_deadline_rev(void)</span>
 	return ~0U;
 }
 
<span class="p_add">+static u32 skx_deadline_rev(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	switch (boot_cpu_data.x86_mask) {</span>
<span class="p_add">+	case 0x03: return 0x01000136;</span>
<span class="p_add">+	case 0x04: return 0x02000014;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return ~0U;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static const struct x86_cpu_id deadline_match[] = {
 	DEADLINE_MODEL_MATCH_FUNC( INTEL_FAM6_HASWELL_X,	hsx_deadline_rev),
 	DEADLINE_MODEL_MATCH_REV ( INTEL_FAM6_BROADWELL_X,	0x0b000020),
 	DEADLINE_MODEL_MATCH_FUNC( INTEL_FAM6_BROADWELL_XEON_D,	bdx_deadline_rev),
<span class="p_del">-	DEADLINE_MODEL_MATCH_REV ( INTEL_FAM6_SKYLAKE_X,	0x02000014),</span>
<span class="p_add">+	DEADLINE_MODEL_MATCH_FUNC( INTEL_FAM6_SKYLAKE_X,	skx_deadline_rev),</span>
 
 	DEADLINE_MODEL_MATCH_REV ( INTEL_FAM6_HASWELL_CORE,	0x22),
 	DEADLINE_MODEL_MATCH_REV ( INTEL_FAM6_HASWELL_ULT,	0x20),
<span class="p_chunk">@@ -600,7 +610,8 @@</span> <span class="p_context"> static void apic_check_deadline_errata(void)</span>
 	const struct x86_cpu_id *m;
 	u32 rev;
 
<span class="p_del">-	if (!boot_cpu_has(X86_FEATURE_TSC_DEADLINE_TIMER))</span>
<span class="p_add">+	if (!boot_cpu_has(X86_FEATURE_TSC_DEADLINE_TIMER) ||</span>
<span class="p_add">+	    boot_cpu_has(X86_FEATURE_HYPERVISOR))</span>
 		return;
 
 	m = x86_match_cpu(deadline_match);
<span class="p_header">diff --git a/arch/x86/kernel/cpu/microcode/core.c b/arch/x86/kernel/cpu/microcode/core.c</span>
<span class="p_header">index 86e8f0b2537b..c4fa4a85d4cb 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/microcode/core.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/microcode/core.c</span>
<span class="p_chunk">@@ -122,9 +122,6 @@</span> <span class="p_context"> static bool __init check_loader_disabled_bsp(void)</span>
 	bool *res = &amp;dis_ucode_ldr;
 #endif
 
<span class="p_del">-	if (!have_cpuid_p())</span>
<span class="p_del">-		return *res;</span>
<span class="p_del">-</span>
 	/*
 	 * CPUID(1).ECX[31]: reserved for hypervisor use. This is still not
 	 * completely accurate as xen pv guests don&#39;t see that CPUID bit set but
<span class="p_chunk">@@ -166,24 +163,36 @@</span> <span class="p_context"> bool get_builtin_firmware(struct cpio_data *cd, const char *name)</span>
 void __init load_ucode_bsp(void)
 {
 	unsigned int cpuid_1_eax;
<span class="p_add">+	bool intel = true;</span>
 
<span class="p_del">-	if (check_loader_disabled_bsp())</span>
<span class="p_add">+	if (!have_cpuid_p())</span>
 		return;
 
 	cpuid_1_eax = native_cpuid_eax(1);
 
 	switch (x86_cpuid_vendor()) {
 	case X86_VENDOR_INTEL:
<span class="p_del">-		if (x86_family(cpuid_1_eax) &gt;= 6)</span>
<span class="p_del">-			load_ucode_intel_bsp();</span>
<span class="p_add">+		if (x86_family(cpuid_1_eax) &lt; 6)</span>
<span class="p_add">+			return;</span>
 		break;
<span class="p_add">+</span>
 	case X86_VENDOR_AMD:
<span class="p_del">-		if (x86_family(cpuid_1_eax) &gt;= 0x10)</span>
<span class="p_del">-			load_ucode_amd_bsp(cpuid_1_eax);</span>
<span class="p_add">+		if (x86_family(cpuid_1_eax) &lt; 0x10)</span>
<span class="p_add">+			return;</span>
<span class="p_add">+		intel = false;</span>
 		break;
<span class="p_add">+</span>
 	default:
<span class="p_del">-		break;</span>
<span class="p_add">+		return;</span>
 	}
<span class="p_add">+</span>
<span class="p_add">+	if (check_loader_disabled_bsp())</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (intel)</span>
<span class="p_add">+		load_ucode_intel_bsp();</span>
<span class="p_add">+	else</span>
<span class="p_add">+		load_ucode_amd_bsp(cpuid_1_eax);</span>
 }
 
 static bool check_loader_disabled_ap(void)
<span class="p_header">diff --git a/arch/x86/kernel/kprobes/common.h b/arch/x86/kernel/kprobes/common.h</span>
<span class="p_header">index db2182d63ed0..3fc0f9a794cb 100644</span>
<span class="p_header">--- a/arch/x86/kernel/kprobes/common.h</span>
<span class="p_header">+++ b/arch/x86/kernel/kprobes/common.h</span>
<span class="p_chunk">@@ -3,6 +3,15 @@</span> <span class="p_context"></span>
 
 /* Kprobes and Optprobes common header */
 
<span class="p_add">+#include &lt;asm/asm.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_FRAME_POINTER</span>
<span class="p_add">+# define SAVE_RBP_STRING &quot;	push %&quot; _ASM_BP &quot;\n&quot; \</span>
<span class="p_add">+			 &quot;	mov  %&quot; _ASM_SP &quot;, %&quot; _ASM_BP &quot;\n&quot;</span>
<span class="p_add">+#else</span>
<span class="p_add">+# define SAVE_RBP_STRING &quot;	push %&quot; _ASM_BP &quot;\n&quot;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 #ifdef CONFIG_X86_64
 #define SAVE_REGS_STRING			\
 	/* Skip cs, ip, orig_ax. */		\
<span class="p_chunk">@@ -17,7 +26,7 @@</span> <span class="p_context"></span>
 	&quot;	pushq %r10\n&quot;			\
 	&quot;	pushq %r11\n&quot;			\
 	&quot;	pushq %rbx\n&quot;			\
<span class="p_del">-	&quot;	pushq %rbp\n&quot;			\</span>
<span class="p_add">+	SAVE_RBP_STRING				\</span>
 	&quot;	pushq %r12\n&quot;			\
 	&quot;	pushq %r13\n&quot;			\
 	&quot;	pushq %r14\n&quot;			\
<span class="p_chunk">@@ -48,7 +57,7 @@</span> <span class="p_context"></span>
 	&quot;	pushl %es\n&quot;			\
 	&quot;	pushl %ds\n&quot;			\
 	&quot;	pushl %eax\n&quot;			\
<span class="p_del">-	&quot;	pushl %ebp\n&quot;			\</span>
<span class="p_add">+	SAVE_RBP_STRING				\</span>
 	&quot;	pushl %edi\n&quot;			\
 	&quot;	pushl %esi\n&quot;			\
 	&quot;	pushl %edx\n&quot;			\
<span class="p_header">diff --git a/arch/x86/kernel/kprobes/core.c b/arch/x86/kernel/kprobes/core.c</span>
<span class="p_header">index f0153714ddac..0742491cbb73 100644</span>
<span class="p_header">--- a/arch/x86/kernel/kprobes/core.c</span>
<span class="p_header">+++ b/arch/x86/kernel/kprobes/core.c</span>
<span class="p_chunk">@@ -1080,8 +1080,6 @@</span> <span class="p_context"> int setjmp_pre_handler(struct kprobe *p, struct pt_regs *regs)</span>
 	 * raw stack chunk with redzones:
 	 */
 	__memcpy(kcb-&gt;jprobes_stack, (kprobe_opcode_t *)addr, MIN_STACK_SIZE(addr));
<span class="p_del">-	regs-&gt;flags &amp;= ~X86_EFLAGS_IF;</span>
<span class="p_del">-	trace_hardirqs_off();</span>
 	regs-&gt;ip = (unsigned long)(jp-&gt;entry);
 
 	/*
<span class="p_header">diff --git a/arch/x86/kernel/reboot.c b/arch/x86/kernel/reboot.c</span>
<span class="p_header">index 54180fa6f66f..add33f600531 100644</span>
<span class="p_header">--- a/arch/x86/kernel/reboot.c</span>
<span class="p_header">+++ b/arch/x86/kernel/reboot.c</span>
<span class="p_chunk">@@ -105,6 +105,10 @@</span> <span class="p_context"> void __noreturn machine_real_restart(unsigned int type)</span>
 	load_cr3(initial_page_table);
 #else
 	write_cr3(real_mode_header-&gt;trampoline_pgd);
<span class="p_add">+</span>
<span class="p_add">+	/* Exiting long mode will fail if CR4.PCIDE is set. */</span>
<span class="p_add">+	if (static_cpu_has(X86_FEATURE_PCID))</span>
<span class="p_add">+		cr4_clear_bits(X86_CR4_PCIDE);</span>
 #endif
 
 	/* Jump to the identity-mapped low memory code */
<span class="p_header">diff --git a/arch/x86/kernel/unwind_frame.c b/arch/x86/kernel/unwind_frame.c</span>
<span class="p_header">index d145a0b1f529..3dc26f95d46e 100644</span>
<span class="p_header">--- a/arch/x86/kernel/unwind_frame.c</span>
<span class="p_header">+++ b/arch/x86/kernel/unwind_frame.c</span>
<span class="p_chunk">@@ -44,7 +44,8 @@</span> <span class="p_context"> static void unwind_dump(struct unwind_state *state)</span>
 			state-&gt;stack_info.type, state-&gt;stack_info.next_sp,
 			state-&gt;stack_mask, state-&gt;graph_idx);
 
<span class="p_del">-	for (sp = state-&gt;orig_sp; sp; sp = PTR_ALIGN(stack_info.next_sp, sizeof(long))) {</span>
<span class="p_add">+	for (sp = PTR_ALIGN(state-&gt;orig_sp, sizeof(long)); sp;</span>
<span class="p_add">+	     sp = PTR_ALIGN(stack_info.next_sp, sizeof(long))) {</span>
 		if (get_stack_info(sp, state-&gt;task, &amp;stack_info, &amp;visit_mask))
 			break;
 
<span class="p_chunk">@@ -174,6 +175,7 @@</span> <span class="p_context"> static bool is_last_task_frame(struct unwind_state *state)</span>
  * This determines if the frame pointer actually contains an encoded pointer to
  * pt_regs on the stack.  See ENCODE_FRAME_POINTER.
  */
<span class="p_add">+#ifdef CONFIG_X86_64</span>
 static struct pt_regs *decode_frame_pointer(unsigned long *bp)
 {
 	unsigned long regs = (unsigned long)bp;
<span class="p_chunk">@@ -183,6 +185,23 @@</span> <span class="p_context"> static struct pt_regs *decode_frame_pointer(unsigned long *bp)</span>
 
 	return (struct pt_regs *)(regs &amp; ~0x1);
 }
<span class="p_add">+#else</span>
<span class="p_add">+static struct pt_regs *decode_frame_pointer(unsigned long *bp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long regs = (unsigned long)bp;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (regs &amp; 0x80000000)</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+	return (struct pt_regs *)(regs | 0x80000000);</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_X86_32</span>
<span class="p_add">+#define KERNEL_REGS_SIZE (sizeof(struct pt_regs) - 2*sizeof(long))</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define KERNEL_REGS_SIZE (sizeof(struct pt_regs))</span>
<span class="p_add">+#endif</span>
 
 static bool update_stack_state(struct unwind_state *state,
 			       unsigned long *next_bp)
<span class="p_chunk">@@ -202,7 +221,7 @@</span> <span class="p_context"> static bool update_stack_state(struct unwind_state *state,</span>
 	regs = decode_frame_pointer(next_bp);
 	if (regs) {
 		frame = (unsigned long *)regs;
<span class="p_del">-		len = regs_size(regs);</span>
<span class="p_add">+		len = KERNEL_REGS_SIZE;</span>
 		state-&gt;got_irq = true;
 	} else {
 		frame = next_bp;
<span class="p_chunk">@@ -226,6 +245,14 @@</span> <span class="p_context"> static bool update_stack_state(struct unwind_state *state,</span>
 	    frame &lt; prev_frame_end)
 		return false;
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * On 32-bit with user mode regs, make sure the last two regs are safe</span>
<span class="p_add">+	 * to access:</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (IS_ENABLED(CONFIG_X86_32) &amp;&amp; regs &amp;&amp; user_mode(regs) &amp;&amp;</span>
<span class="p_add">+	    !on_stack(info, frame, len + 2*sizeof(long)))</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+</span>
 	/* Move state to the next frame: */
 	if (regs) {
 		state-&gt;regs = regs;
<span class="p_chunk">@@ -328,6 +355,13 @@</span> <span class="p_context"> bool unwind_next_frame(struct unwind_state *state)</span>
 	    state-&gt;regs-&gt;sp &lt; (unsigned long)task_pt_regs(state-&gt;task))
 		goto the_end;
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * There are some known frame pointer issues on 32-bit.  Disable</span>
<span class="p_add">+	 * unwinder warnings on 32-bit until it gets objtool support.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (IS_ENABLED(CONFIG_X86_32))</span>
<span class="p_add">+		goto the_end;</span>
<span class="p_add">+</span>
 	if (state-&gt;regs) {
 		printk_deferred_once(KERN_WARNING
 			&quot;WARNING: kernel stack regs at %p in %s:%d has bad &#39;bp&#39; value %p\n&quot;,
<span class="p_header">diff --git a/arch/x86/mm/Makefile b/arch/x86/mm/Makefile</span>
<span class="p_header">index 72bf8c01c6e3..e1f095884386 100644</span>
<span class="p_header">--- a/arch/x86/mm/Makefile</span>
<span class="p_header">+++ b/arch/x86/mm/Makefile</span>
<span class="p_chunk">@@ -1,5 +1,12 @@</span> <span class="p_context"></span>
<span class="p_del">-# Kernel does not boot with instrumentation of tlb.c.</span>
<span class="p_del">-KCOV_INSTRUMENT_tlb.o	:= n</span>
<span class="p_add">+# Kernel does not boot with instrumentation of tlb.c and mem_encrypt.c</span>
<span class="p_add">+KCOV_INSTRUMENT_tlb.o		:= n</span>
<span class="p_add">+KCOV_INSTRUMENT_mem_encrypt.o	:= n</span>
<span class="p_add">+</span>
<span class="p_add">+KASAN_SANITIZE_mem_encrypt.o	:= n</span>
<span class="p_add">+</span>
<span class="p_add">+ifdef CONFIG_FUNCTION_TRACER</span>
<span class="p_add">+CFLAGS_REMOVE_mem_encrypt.o	= -pg</span>
<span class="p_add">+endif</span>
 
 obj-y	:=  init.o init_$(BITS).o fault.o ioremap.o extable.o pageattr.o mmap.o \
 	    pat.o pgtable.o physaddr.o setup_nx.o tlb.o
<span class="p_header">diff --git a/arch/x86/mm/tlb.c b/arch/x86/mm/tlb.c</span>
<span class="p_header">index 49d9778376d7..658bf0090565 100644</span>
<span class="p_header">--- a/arch/x86/mm/tlb.c</span>
<span class="p_header">+++ b/arch/x86/mm/tlb.c</span>
<span class="p_chunk">@@ -30,6 +30,8 @@</span> <span class="p_context"></span>
 
 atomic64_t last_mm_ctx_id = ATOMIC64_INIT(1);
 
<span class="p_add">+DEFINE_STATIC_KEY_TRUE(tlb_use_lazy_mode);</span>
<span class="p_add">+</span>
 static void choose_new_asid(struct mm_struct *next, u64 next_tlb_gen,
 			    u16 *new_asid, bool *need_flush)
 {
<span class="p_chunk">@@ -80,7 +82,7 @@</span> <span class="p_context"> void leave_mm(int cpu)</span>
 		return;
 
 	/* Warn if we&#39;re not lazy. */
<span class="p_del">-	WARN_ON(cpumask_test_cpu(smp_processor_id(), mm_cpumask(loaded_mm)));</span>
<span class="p_add">+	WARN_ON(!this_cpu_read(cpu_tlbstate.is_lazy));</span>
 
 	switch_mm(NULL, &amp;init_mm, NULL);
 }
<span class="p_chunk">@@ -142,45 +144,24 @@</span> <span class="p_context"> void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,</span>
 		__flush_tlb_all();
 	}
 #endif
<span class="p_add">+	this_cpu_write(cpu_tlbstate.is_lazy, false);</span>
 
 	if (real_prev == next) {
 		VM_BUG_ON(this_cpu_read(cpu_tlbstate.ctxs[prev_asid].ctx_id) !=
 			  next-&gt;context.ctx_id);
 
<span class="p_del">-		if (cpumask_test_cpu(cpu, mm_cpumask(next))) {</span>
<span class="p_del">-			/*</span>
<span class="p_del">-			 * There&#39;s nothing to do: we weren&#39;t lazy, and we</span>
<span class="p_del">-			 * aren&#39;t changing our mm.  We don&#39;t need to flush</span>
<span class="p_del">-			 * anything, nor do we need to update CR3, CR4, or</span>
<span class="p_del">-			 * LDTR.</span>
<span class="p_del">-			 */</span>
<span class="p_del">-			return;</span>
<span class="p_del">-		}</span>
<span class="p_del">-</span>
<span class="p_del">-		/* Resume remote flushes and then read tlb_gen. */</span>
<span class="p_del">-		cpumask_set_cpu(cpu, mm_cpumask(next));</span>
<span class="p_del">-		next_tlb_gen = atomic64_read(&amp;next-&gt;context.tlb_gen);</span>
<span class="p_del">-</span>
<span class="p_del">-		if (this_cpu_read(cpu_tlbstate.ctxs[prev_asid].tlb_gen) &lt;</span>
<span class="p_del">-		    next_tlb_gen) {</span>
<span class="p_del">-			/*</span>
<span class="p_del">-			 * Ideally, we&#39;d have a flush_tlb() variant that</span>
<span class="p_del">-			 * takes the known CR3 value as input.  This would</span>
<span class="p_del">-			 * be faster on Xen PV and on hypothetical CPUs</span>
<span class="p_del">-			 * on which INVPCID is fast.</span>
<span class="p_del">-			 */</span>
<span class="p_del">-			this_cpu_write(cpu_tlbstate.ctxs[prev_asid].tlb_gen,</span>
<span class="p_del">-				       next_tlb_gen);</span>
<span class="p_del">-			write_cr3(build_cr3(next, prev_asid));</span>
<span class="p_del">-			trace_tlb_flush(TLB_FLUSH_ON_TASK_SWITCH,</span>
<span class="p_del">-					TLB_FLUSH_ALL);</span>
<span class="p_del">-		}</span>
<span class="p_del">-</span>
 		/*
<span class="p_del">-		 * We just exited lazy mode, which means that CR4 and/or LDTR</span>
<span class="p_del">-		 * may be stale.  (Changes to the required CR4 and LDTR states</span>
<span class="p_del">-		 * are not reflected in tlb_gen.)</span>
<span class="p_add">+		 * We don&#39;t currently support having a real mm loaded without</span>
<span class="p_add">+		 * our cpu set in mm_cpumask().  We have all the bookkeeping</span>
<span class="p_add">+		 * in place to figure out whether we would need to flush</span>
<span class="p_add">+		 * if our cpu were cleared in mm_cpumask(), but we don&#39;t</span>
<span class="p_add">+		 * currently use it.</span>
 		 */
<span class="p_add">+		if (WARN_ON_ONCE(real_prev != &amp;init_mm &amp;&amp;</span>
<span class="p_add">+				 !cpumask_test_cpu(cpu, mm_cpumask(next))))</span>
<span class="p_add">+			cpumask_set_cpu(cpu, mm_cpumask(next));</span>
<span class="p_add">+</span>
<span class="p_add">+		return;</span>
 	} else {
 		u16 new_asid;
 		bool need_flush;
<span class="p_chunk">@@ -199,10 +180,9 @@</span> <span class="p_context"> void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,</span>
 		}
 
 		/* Stop remote flushes for the previous mm */
<span class="p_del">-		if (cpumask_test_cpu(cpu, mm_cpumask(real_prev)))</span>
<span class="p_del">-			cpumask_clear_cpu(cpu, mm_cpumask(real_prev));</span>
<span class="p_del">-</span>
<span class="p_del">-		VM_WARN_ON_ONCE(cpumask_test_cpu(cpu, mm_cpumask(next)));</span>
<span class="p_add">+		VM_WARN_ON_ONCE(!cpumask_test_cpu(cpu, mm_cpumask(real_prev)) &amp;&amp;</span>
<span class="p_add">+				real_prev != &amp;init_mm);</span>
<span class="p_add">+		cpumask_clear_cpu(cpu, mm_cpumask(real_prev));</span>
 
 		/*
 		 * Start remote flushes and then read tlb_gen.
<span class="p_chunk">@@ -233,6 +213,37 @@</span> <span class="p_context"> void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,</span>
 }
 
 /*
<span class="p_add">+ * enter_lazy_tlb() is a hint from the scheduler that we are entering a</span>
<span class="p_add">+ * kernel thread or other context without an mm.  Acceptable implementations</span>
<span class="p_add">+ * include doing nothing whatsoever, switching to init_mm, or various clever</span>
<span class="p_add">+ * lazy tricks to try to minimize TLB flushes.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * The scheduler reserves the right to call enter_lazy_tlb() several times</span>
<span class="p_add">+ * in a row.  It will notify us that we&#39;re going back to a real mm by</span>
<span class="p_add">+ * calling switch_mm_irqs_off().</span>
<span class="p_add">+ */</span>
<span class="p_add">+void enter_lazy_tlb(struct mm_struct *mm, struct task_struct *tsk)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (this_cpu_read(cpu_tlbstate.loaded_mm) == &amp;init_mm)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (static_branch_unlikely(&amp;tlb_use_lazy_mode)) {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * There&#39;s a significant optimization that may be possible</span>
<span class="p_add">+		 * here.  We have accurate enough TLB flush tracking that we</span>
<span class="p_add">+		 * don&#39;t need to maintain coherence of TLB per se when we&#39;re</span>
<span class="p_add">+		 * lazy.  We do, however, need to maintain coherence of</span>
<span class="p_add">+		 * paging-structure caches.  We could, in principle, leave our</span>
<span class="p_add">+		 * old mm loaded and only switch to init_mm when</span>
<span class="p_add">+		 * tlb_remove_page() happens.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		this_cpu_write(cpu_tlbstate.is_lazy, true);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		switch_mm(NULL, &amp;init_mm, NULL);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
  * Call this when reinitializing a CPU.  It fixes the following potential
  * problems:
  *
<span class="p_chunk">@@ -303,16 +314,20 @@</span> <span class="p_context"> static void flush_tlb_func_common(const struct flush_tlb_info *f,</span>
 	/* This code cannot presently handle being reentered. */
 	VM_WARN_ON(!irqs_disabled());
 
<span class="p_add">+	if (unlikely(loaded_mm == &amp;init_mm))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
 	VM_WARN_ON(this_cpu_read(cpu_tlbstate.ctxs[loaded_mm_asid].ctx_id) !=
 		   loaded_mm-&gt;context.ctx_id);
 
<span class="p_del">-	if (!cpumask_test_cpu(smp_processor_id(), mm_cpumask(loaded_mm))) {</span>
<span class="p_add">+	if (this_cpu_read(cpu_tlbstate.is_lazy)) {</span>
 		/*
<span class="p_del">-		 * We&#39;re in lazy mode -- don&#39;t flush.  We can get here on</span>
<span class="p_del">-		 * remote flushes due to races and on local flushes if a</span>
<span class="p_del">-		 * kernel thread coincidentally flushes the mm it&#39;s lazily</span>
<span class="p_del">-		 * still using.</span>
<span class="p_add">+		 * We&#39;re in lazy mode.  We need to at least flush our</span>
<span class="p_add">+		 * paging-structure cache to avoid speculatively reading</span>
<span class="p_add">+		 * garbage into our TLB.  Since switching to init_mm is barely</span>
<span class="p_add">+		 * slower than a minimal flush, just switch to init_mm.</span>
 		 */
<span class="p_add">+		switch_mm_irqs_off(NULL, &amp;init_mm, NULL);</span>
 		return;
 	}
 
<span class="p_chunk">@@ -611,3 +626,57 @@</span> <span class="p_context"> static int __init create_tlb_single_page_flush_ceiling(void)</span>
 	return 0;
 }
 late_initcall(create_tlb_single_page_flush_ceiling);
<span class="p_add">+</span>
<span class="p_add">+static ssize_t tlblazy_read_file(struct file *file, char __user *user_buf,</span>
<span class="p_add">+				 size_t count, loff_t *ppos)</span>
<span class="p_add">+{</span>
<span class="p_add">+	char buf[2];</span>
<span class="p_add">+</span>
<span class="p_add">+	buf[0] = static_branch_likely(&amp;tlb_use_lazy_mode) ? &#39;1&#39; : &#39;0&#39;;</span>
<span class="p_add">+	buf[1] = &#39;\n&#39;;</span>
<span class="p_add">+</span>
<span class="p_add">+	return simple_read_from_buffer(user_buf, count, ppos, buf, 2);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static ssize_t tlblazy_write_file(struct file *file,</span>
<span class="p_add">+		 const char __user *user_buf, size_t count, loff_t *ppos)</span>
<span class="p_add">+{</span>
<span class="p_add">+	bool val;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (kstrtobool_from_user(user_buf, count, &amp;val))</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (val)</span>
<span class="p_add">+		static_branch_enable(&amp;tlb_use_lazy_mode);</span>
<span class="p_add">+	else</span>
<span class="p_add">+		static_branch_disable(&amp;tlb_use_lazy_mode);</span>
<span class="p_add">+</span>
<span class="p_add">+	return count;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static const struct file_operations fops_tlblazy = {</span>
<span class="p_add">+	.read = tlblazy_read_file,</span>
<span class="p_add">+	.write = tlblazy_write_file,</span>
<span class="p_add">+	.llseek = default_llseek,</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+static int __init init_tlb_use_lazy_mode(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (boot_cpu_has(X86_FEATURE_PCID)) {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Heuristic: with PCID on, switching to and from</span>
<span class="p_add">+		 * init_mm is reasonably fast, but remote flush IPIs</span>
<span class="p_add">+		 * as expensive as ever, so turn off lazy TLB mode.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * We can&#39;t do this in setup_pcid() because static keys</span>
<span class="p_add">+		 * haven&#39;t been initialized yet, and it would blow up</span>
<span class="p_add">+		 * badly.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		static_branch_disable(&amp;tlb_use_lazy_mode);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	debugfs_create_file(&quot;tlb_use_lazy_mode&quot;, S_IRUSR | S_IWUSR,</span>
<span class="p_add">+			    arch_debugfs_dir, NULL, &amp;fops_tlblazy);</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+late_initcall(init_tlb_use_lazy_mode);</span>

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



