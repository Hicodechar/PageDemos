
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>mremap: fix race between mremap() and page cleanning - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    mremap: fix race between mremap() and page cleanning</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=45931">Aaron Lu</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Nov. 17, 2016, 7:45 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20161117074538.GA1713@aaronlu.sh.intel.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9433593/mbox/"
   >mbox</a>
|
   <a href="/patch/9433593/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9433593/">/patch/9433593/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	79D6A60238 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 17 Nov 2016 07:45:52 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 69032292C1
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 17 Nov 2016 07:45:52 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 5DB55292C3; Thu, 17 Nov 2016 07:45:52 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 0569A292C1
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 17 Nov 2016 07:45:51 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752149AbcKQHpn (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 17 Nov 2016 02:45:43 -0500
Received: from mga05.intel.com ([192.55.52.43]:2875 &quot;EHLO mga05.intel.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1750785AbcKQHpl (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 17 Nov 2016 02:45:41 -0500
Received: from orsmga004.jf.intel.com ([10.7.209.38])
	by fmsmga105.fm.intel.com with ESMTP; 16 Nov 2016 23:45:40 -0800
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i=&quot;5.31,504,1473145200&quot;; 
	d=&quot;c&#39;?scan&#39;208,223&quot;;a=&quot;31816076&quot;
Received: from aaronlu.sh.intel.com ([10.239.13.119])
	by orsmga004.jf.intel.com with ESMTP; 16 Nov 2016 23:45:38 -0800
Date: Thu, 17 Nov 2016 15:45:38 +0800
From: Aaron Lu &lt;aaron.lu@intel.com&gt;
To: Linux Memory Management List &lt;linux-mm@kvack.org&gt;,
	LKML &lt;linux-kernel@vger.kernel.org&gt;
Cc: Dave Hansen &lt;dave.hansen@intel.com&gt;,
	Linus Torvalds &lt;torvalds@linux-foundation.org&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	&quot;&#39;Kirill A. Shutemov&#39;&quot; &lt;kirill.shutemov@linux.intel.com&gt;,
	Huang Ying &lt;ying.huang@intel.com&gt;
Subject: Re: [PATCH] mremap: fix race between mremap() and page cleanning
Message-ID: &lt;20161117074538.GA1713@aaronlu.sh.intel.com&gt;
References: &lt;026b73f6-ca1d-e7bb-766c-4aaeb7071ce6@intel.com&gt;
MIME-Version: 1.0
Content-Type: multipart/mixed; boundary=&quot;AqsLC8rIMeq19msA&quot;
Content-Disposition: inline
In-Reply-To: &lt;026b73f6-ca1d-e7bb-766c-4aaeb7071ce6@intel.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=45931">Aaron Lu</a> - Nov. 17, 2016, 7:45 a.m.</div>
<pre class="content">
+LKML.

Also attached the kernel patch that enlarges the race window and the
user space test code raceremap.c, which you can put in will-it-scale&#39;s
tests directory and run it as:
# ./raceremap_threads -t 2 -s 1

Make sure &quot;cpu0 runs&quot; appeared in the first line.

If you get SEGFAULT:
[aaron@aaronlu will-it-scale]$ sudo ./raceremap_threads -t 2 -s 1
cpu0 runs
cpu1 runs
cpu0: going to remap
testcase:mremap
warmup
cpu1: going to clean the page
cpu1: going to write 2
min:0 max:0 total:0
min:0 max:0 total:0
cpu0: remap done
Segmentation fault

That means the race doesn&#39;t occur.

If you get &quot;*cpu1 wrote 2 gets lost&quot;:
[aaron@aaronlu will-it-scale]$ sudo ./raceremap_threads -t 2 -s 1
cpu1 runs
testcase:mremap
warmup
cpu0 runs
cpu0: going to remap
cpu1: going to clean the page
cpu1: going to write 2
cpu1 wrote 2
min:0 max:0 total:0
min:0 max:0 total:0
cpu0: remap done
*cpu1 wrote 2 gets lost

That means the race occurred.

Thanks,
Aaron

On Thu, Nov 10, 2016 at 05:16:33PM +0800, Aaron Lu wrote:
<span class="quote">&gt; Prior to 3.15, there was a race between zap_pte_range() and</span>
<span class="quote">&gt; page_mkclean() where writes to a page could be lost.  Dave Hansen</span>
<span class="quote">&gt; discovered by inspection that there is a similar race between</span>
<span class="quote">&gt; move_ptes() and  page_mkclean().</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; We&#39;ve been able to reproduce the issue by enlarging the race window with</span>
<span class="quote">&gt; a msleep(), but have not been able to hit it without modifying the code.</span>
<span class="quote">&gt; So, we think it&#39;s a real issue, but is difficult or impossible to hit</span>
<span class="quote">&gt; in practice.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The zap_pte_range() issue is fixed by commit 1cf35d47712d(&quot;mm: split</span>
<span class="quote">&gt; &#39;tlb_flush_mmu()&#39; into tlb flushing and memory freeing parts&quot;). And</span>
<span class="quote">&gt; this patch is to fix the race between page_mkclean() and mremap().</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Here is one possible way to hit the race: suppose a process mmapped a</span>
<span class="quote">&gt; file with READ | WRITE and SHARED, it has two threads and they are bound</span>
<span class="quote">&gt; to 2 different CPUs, e.g. CPU1 and CPU2. mmap returned X, then thread 1</span>
<span class="quote">&gt; did a write to addr X so that CPU1 now has a writable TLB for addr X</span>
<span class="quote">&gt; on it. Thread 2 starts mremaping from addr X to Y while thread 1 cleaned</span>
<span class="quote">&gt; the page and then did another write to the old addr X again. The 2nd</span>
<span class="quote">&gt; write from thread 1 could succeed but the value will get lost.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;         thread 1                           thread 2</span>
<span class="quote">&gt;      (bound to CPU1)                    (bound to CPU2)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;   1: write 1 to addr X to get a</span>
<span class="quote">&gt;      writeable TLB on this CPU</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;                                         2: mremap starts</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;                                         3: move_ptes emptied PTE for addr X</span>
<span class="quote">&gt;                                            and setup new PTE for addr Y and</span>
<span class="quote">&gt;                                            then dropped PTL for X and Y</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;   4: page laundering for N by doing</span>
<span class="quote">&gt;      fadvise FADV_DONTNEED. When done,</span>
<span class="quote">&gt;      pageframe N is deemed clean.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;   5: *write 2 to addr X</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;                                         6: tlb flush for addr X</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;   7: munmap (Y, pagesize) to make the</span>
<span class="quote">&gt;      page unmapped</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;   8: fadvise with FADV_DONTNEED again</span>
<span class="quote">&gt;      to kick the page off the pagecache</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;   9: pread the page from file to verify</span>
<span class="quote">&gt;      the value. If 1 is there, it means</span>
<span class="quote">&gt;      we have lost the written 2.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;   *the write may or may not cause segmentation fault, it depends on</span>
<span class="quote">&gt;   if the TLB is still on the CPU.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Please note that this is only one specific way of how the race could</span>
<span class="quote">&gt; occur, it didn&#39;t mean that the race could only occur in exact the above</span>
<span class="quote">&gt; config, e.g. more than 2 threads could be involved and fadvise() could</span>
<span class="quote">&gt; be done in another thread, etc.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; For anonymous pages, they could race between mremap() and page reclaim:</span>
<span class="quote">&gt; THP: a huge PMD is moved by mremap to a new huge PMD, then the new huge PMD</span>
<span class="quote">&gt; gets unmapped/splitted/pagedout before the flush tlb happened for the old</span>
<span class="quote">&gt; huge PMD in move_page_tables() and we could still write data to it.</span>
<span class="quote">&gt; The normal anonymous page has similar situation.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; To fix this, check for any dirty PTE in move_ptes()/move_huge_pmd() and</span>
<span class="quote">&gt; if any, did the flush before dropping the PTL. If we did the flush for</span>
<span class="quote">&gt; every move_ptes()/move_huge_pmd() call then we do not need to do the</span>
<span class="quote">&gt; flush in move_pages_tables() for the whole range. But if we didn&#39;t, we</span>
<span class="quote">&gt; still need to do the whole range flush.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Alternatively, we can track which part of the range is flushed in</span>
<span class="quote">&gt; move_ptes()/move_huge_pmd() and which didn&#39;t to avoid flushing the whole</span>
<span class="quote">&gt; range in move_page_tables(). But that would require multiple tlb flushes</span>
<span class="quote">&gt; for the different sub-ranges and should be less efficient than the</span>
<span class="quote">&gt; single whole range flush.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; KBuild test on my Sandybridge desktop doesn&#39;t show any noticeable change.</span>
<span class="quote">&gt; v4.9-rc4:</span>
<span class="quote">&gt; real    5m14.048s</span>
<span class="quote">&gt; user    32m19.800s</span>
<span class="quote">&gt; sys     4m50.320s</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; With this commit:</span>
<span class="quote">&gt; real    5m13.888s</span>
<span class="quote">&gt; user    32m19.330s</span>
<span class="quote">&gt; sys     4m51.200s</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Reported-by: Dave Hansen &lt;dave.hansen@intel.com&gt;</span>
<span class="quote">&gt; Signed-off-by: Aaron Lu &lt;aaron.lu@intel.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  include/linux/huge_mm.h |  2 +-</span>
<span class="quote">&gt;  mm/huge_memory.c        |  9 ++++++++-</span>
<span class="quote">&gt;  mm/mremap.c             | 30 +++++++++++++++++++++---------</span>
<span class="quote">&gt;  3 files changed, 30 insertions(+), 11 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/include/linux/huge_mm.h b/include/linux/huge_mm.h</span>
<span class="quote">&gt; index 9b9f65d99873..e35e6de633b9 100644</span>
<span class="quote">&gt; --- a/include/linux/huge_mm.h</span>
<span class="quote">&gt; +++ b/include/linux/huge_mm.h</span>
<span class="quote">&gt; @@ -22,7 +22,7 @@ extern int mincore_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,</span>
<span class="quote">&gt;  			unsigned char *vec);</span>
<span class="quote">&gt;  extern bool move_huge_pmd(struct vm_area_struct *vma, unsigned long old_addr,</span>
<span class="quote">&gt;  			 unsigned long new_addr, unsigned long old_end,</span>
<span class="quote">&gt; -			 pmd_t *old_pmd, pmd_t *new_pmd);</span>
<span class="quote">&gt; +			 pmd_t *old_pmd, pmd_t *new_pmd, bool *need_flush);</span>
<span class="quote">&gt;  extern int change_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,</span>
<span class="quote">&gt;  			unsigned long addr, pgprot_t newprot,</span>
<span class="quote">&gt;  			int prot_numa);</span>
<span class="quote">&gt; diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="quote">&gt; index cdcd25cb30fe..eff3de359d50 100644</span>
<span class="quote">&gt; --- a/mm/huge_memory.c</span>
<span class="quote">&gt; +++ b/mm/huge_memory.c</span>
<span class="quote">&gt; @@ -1426,11 +1426,12 @@ int zap_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  bool move_huge_pmd(struct vm_area_struct *vma, unsigned long old_addr,</span>
<span class="quote">&gt;  		  unsigned long new_addr, unsigned long old_end,</span>
<span class="quote">&gt; -		  pmd_t *old_pmd, pmd_t *new_pmd)</span>
<span class="quote">&gt; +		  pmd_t *old_pmd, pmd_t *new_pmd, bool *need_flush)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	spinlock_t *old_ptl, *new_ptl;</span>
<span class="quote">&gt;  	pmd_t pmd;</span>
<span class="quote">&gt;  	struct mm_struct *mm = vma-&gt;vm_mm;</span>
<span class="quote">&gt; +	bool force_flush = false;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if ((old_addr &amp; ~HPAGE_PMD_MASK) ||</span>
<span class="quote">&gt;  	    (new_addr &amp; ~HPAGE_PMD_MASK) ||</span>
<span class="quote">&gt; @@ -1455,6 +1456,8 @@ bool move_huge_pmd(struct vm_area_struct *vma, unsigned long old_addr,</span>
<span class="quote">&gt;  		new_ptl = pmd_lockptr(mm, new_pmd);</span>
<span class="quote">&gt;  		if (new_ptl != old_ptl)</span>
<span class="quote">&gt;  			spin_lock_nested(new_ptl, SINGLE_DEPTH_NESTING);</span>
<span class="quote">&gt; +		if (pmd_present(*old_pmd) &amp;&amp; pmd_dirty(*old_pmd))</span>
<span class="quote">&gt; +			force_flush = true;</span>
<span class="quote">&gt;  		pmd = pmdp_huge_get_and_clear(mm, old_addr, old_pmd);</span>
<span class="quote">&gt;  		VM_BUG_ON(!pmd_none(*new_pmd));</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -1467,6 +1470,10 @@ bool move_huge_pmd(struct vm_area_struct *vma, unsigned long old_addr,</span>
<span class="quote">&gt;  		set_pmd_at(mm, new_addr, new_pmd, pmd_mksoft_dirty(pmd));</span>
<span class="quote">&gt;  		if (new_ptl != old_ptl)</span>
<span class="quote">&gt;  			spin_unlock(new_ptl);</span>
<span class="quote">&gt; +		if (force_flush)</span>
<span class="quote">&gt; +			flush_tlb_range(vma, old_addr, old_addr + PMD_SIZE);</span>
<span class="quote">&gt; +		else</span>
<span class="quote">&gt; +			*need_flush = true;</span>
<span class="quote">&gt;  		spin_unlock(old_ptl);</span>
<span class="quote">&gt;  		return true;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; diff --git a/mm/mremap.c b/mm/mremap.c</span>
<span class="quote">&gt; index da22ad2a5678..6ccecc03f56a 100644</span>
<span class="quote">&gt; --- a/mm/mremap.c</span>
<span class="quote">&gt; +++ b/mm/mremap.c</span>
<span class="quote">&gt; @@ -104,11 +104,13 @@ static pte_t move_soft_dirty_pte(pte_t pte)</span>
<span class="quote">&gt;  static void move_ptes(struct vm_area_struct *vma, pmd_t *old_pmd,</span>
<span class="quote">&gt;  		unsigned long old_addr, unsigned long old_end,</span>
<span class="quote">&gt;  		struct vm_area_struct *new_vma, pmd_t *new_pmd,</span>
<span class="quote">&gt; -		unsigned long new_addr, bool need_rmap_locks)</span>
<span class="quote">&gt; +		unsigned long new_addr, bool need_rmap_locks, bool *need_flush)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct mm_struct *mm = vma-&gt;vm_mm;</span>
<span class="quote">&gt;  	pte_t *old_pte, *new_pte, pte;</span>
<span class="quote">&gt;  	spinlock_t *old_ptl, *new_ptl;</span>
<span class="quote">&gt; +	bool force_flush = false;</span>
<span class="quote">&gt; +	unsigned long len = old_end - old_addr;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt;  	 * When need_rmap_locks is true, we take the i_mmap_rwsem and anon_vma</span>
<span class="quote">&gt; @@ -146,6 +148,14 @@ static void move_ptes(struct vm_area_struct *vma, pmd_t *old_pmd,</span>
<span class="quote">&gt;  				   new_pte++, new_addr += PAGE_SIZE) {</span>
<span class="quote">&gt;  		if (pte_none(*old_pte))</span>
<span class="quote">&gt;  			continue;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		/*</span>
<span class="quote">&gt; +		 * We are remapping a dirty PTE, make sure to</span>
<span class="quote">&gt; +		 * flush TLB before we drop the PTL for the</span>
<span class="quote">&gt; +		 * old PTE or we may race with page_mkclean().</span>
<span class="quote">&gt; +		 */</span>
<span class="quote">&gt; +		if (pte_present(*old_pte) &amp;&amp; pte_dirty(*old_pte))</span>
<span class="quote">&gt; +			force_flush = true;</span>
<span class="quote">&gt;  		pte = ptep_get_and_clear(mm, old_addr, old_pte);</span>
<span class="quote">&gt;  		pte = move_pte(pte, new_vma-&gt;vm_page_prot, old_addr, new_addr);</span>
<span class="quote">&gt;  		pte = move_soft_dirty_pte(pte);</span>
<span class="quote">&gt; @@ -156,6 +166,10 @@ static void move_ptes(struct vm_area_struct *vma, pmd_t *old_pmd,</span>
<span class="quote">&gt;  	if (new_ptl != old_ptl)</span>
<span class="quote">&gt;  		spin_unlock(new_ptl);</span>
<span class="quote">&gt;  	pte_unmap(new_pte - 1);</span>
<span class="quote">&gt; +	if (force_flush)</span>
<span class="quote">&gt; +		flush_tlb_range(vma, old_end - len, old_end);</span>
<span class="quote">&gt; +	else</span>
<span class="quote">&gt; +		*need_flush = true;</span>
<span class="quote">&gt;  	pte_unmap_unlock(old_pte - 1, old_ptl);</span>
<span class="quote">&gt;  	if (need_rmap_locks)</span>
<span class="quote">&gt;  		drop_rmap_locks(vma);</span>
<span class="quote">&gt; @@ -201,13 +215,12 @@ unsigned long move_page_tables(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  				if (need_rmap_locks)</span>
<span class="quote">&gt;  					take_rmap_locks(vma);</span>
<span class="quote">&gt;  				moved = move_huge_pmd(vma, old_addr, new_addr,</span>
<span class="quote">&gt; -						    old_end, old_pmd, new_pmd);</span>
<span class="quote">&gt; +						    old_end, old_pmd, new_pmd,</span>
<span class="quote">&gt; +						    &amp;need_flush);</span>
<span class="quote">&gt;  				if (need_rmap_locks)</span>
<span class="quote">&gt;  					drop_rmap_locks(vma);</span>
<span class="quote">&gt; -				if (moved) {</span>
<span class="quote">&gt; -					need_flush = true;</span>
<span class="quote">&gt; +				if (moved)</span>
<span class="quote">&gt;  					continue;</span>
<span class="quote">&gt; -				}</span>
<span class="quote">&gt;  			}</span>
<span class="quote">&gt;  			split_huge_pmd(vma, old_pmd, old_addr);</span>
<span class="quote">&gt;  			if (pmd_trans_unstable(old_pmd))</span>
<span class="quote">&gt; @@ -220,11 +233,10 @@ unsigned long move_page_tables(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  			extent = next - new_addr;</span>
<span class="quote">&gt;  		if (extent &gt; LATENCY_LIMIT)</span>
<span class="quote">&gt;  			extent = LATENCY_LIMIT;</span>
<span class="quote">&gt; -		move_ptes(vma, old_pmd, old_addr, old_addr + extent,</span>
<span class="quote">&gt; -			  new_vma, new_pmd, new_addr, need_rmap_locks);</span>
<span class="quote">&gt; -		need_flush = true;</span>
<span class="quote">&gt; +		move_ptes(vma, old_pmd, old_addr, old_addr + extent, new_vma,</span>
<span class="quote">&gt; +			  new_pmd, new_addr, need_rmap_locks, &amp;need_flush);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; -	if (likely(need_flush))</span>
<span class="quote">&gt; +	if (need_flush)</span>
<span class="quote">&gt;  		flush_tlb_range(vma, old_end-len, old_addr);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	mmu_notifier_invalidate_range_end(vma-&gt;vm_mm, mmun_start, mmun_end);</span>
<span class="quote">&gt; -- </span>
<span class="quote">&gt; 2.5.5</span>
<span class="quote">&gt;</span>
From c529dfa6bdfc643a9c3debb4b61b9b0c13b0862e Mon Sep 17 00:00:00 2001
<span class="from">From: Aaron Lu &lt;aaron.lu@intel.com&gt;</span>
Date: Thu, 17 Nov 2016 15:11:08 +0800
Subject: [PATCH] mremap: add a 2s delay for MAP_FIXED case

Add a 2s delay for MAP_FIXED case to enlarge the race window so that we
can hit the race in user space.
<span class="signed-off-by">
Signed-off-by: Aaron Lu &lt;aaron.lu@intel.com&gt;</span>
---
 fs/exec.c          |  2 +-
 include/linux/mm.h |  2 +-
 mm/mremap.c        | 19 ++++++++++++-------
 3 files changed, 14 insertions(+), 9 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/fs/exec.c b/fs/exec.c</span>
<span class="p_header">index 4e497b9ee71e..1e49ce9a23bd 100644</span>
<span class="p_header">--- a/fs/exec.c</span>
<span class="p_header">+++ b/fs/exec.c</span>
<span class="p_chunk">@@ -619,7 +619,7 @@</span> <span class="p_context"> static int shift_arg_pages(struct vm_area_struct *vma, unsigned long shift)</span>
 	 * process cleanup to remove whatever mess we made.
 	 */
 	if (length != move_page_tables(vma, old_start,
<span class="p_del">-				       vma, new_start, length, false))</span>
<span class="p_add">+				       vma, new_start, length, false, false))</span>
 		return -ENOMEM;
 
 	lru_add_drain();
<span class="p_header">diff --git a/include/linux/mm.h b/include/linux/mm.h</span>
<span class="p_header">index a92c8d73aeaf..5e35fe3d914a 100644</span>
<span class="p_header">--- a/include/linux/mm.h</span>
<span class="p_header">+++ b/include/linux/mm.h</span>
<span class="p_chunk">@@ -1392,7 +1392,7 @@</span> <span class="p_context"> int vma_is_stack_for_current(struct vm_area_struct *vma);</span>
 extern unsigned long move_page_tables(struct vm_area_struct *vma,
 		unsigned long old_addr, struct vm_area_struct *new_vma,
 		unsigned long new_addr, unsigned long len,
<span class="p_del">-		bool need_rmap_locks);</span>
<span class="p_add">+		bool need_rmap_locks, bool delay);</span>
 extern unsigned long change_protection(struct vm_area_struct *vma, unsigned long start,
 			      unsigned long end, pgprot_t newprot,
 			      int dirty_accountable, int prot_numa);
<span class="p_header">diff --git a/mm/mremap.c b/mm/mremap.c</span>
<span class="p_header">index da22ad2a5678..8e35279ca622 100644</span>
<span class="p_header">--- a/mm/mremap.c</span>
<span class="p_header">+++ b/mm/mremap.c</span>
<span class="p_chunk">@@ -22,6 +22,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/mmu_notifier.h&gt;
 #include &lt;linux/uaccess.h&gt;
 #include &lt;linux/mm-arch-hooks.h&gt;
<span class="p_add">+#include &lt;linux/delay.h&gt;</span>
 
 #include &lt;asm/cacheflush.h&gt;
 #include &lt;asm/tlbflush.h&gt;
<span class="p_chunk">@@ -166,7 +167,7 @@</span> <span class="p_context"> static void move_ptes(struct vm_area_struct *vma, pmd_t *old_pmd,</span>
 unsigned long move_page_tables(struct vm_area_struct *vma,
 		unsigned long old_addr, struct vm_area_struct *new_vma,
 		unsigned long new_addr, unsigned long len,
<span class="p_del">-		bool need_rmap_locks)</span>
<span class="p_add">+		bool need_rmap_locks, bool delay)</span>
 {
 	unsigned long extent, next, old_end;
 	pmd_t *old_pmd, *new_pmd;
<span class="p_chunk">@@ -224,8 +225,11 @@</span> <span class="p_context"> unsigned long move_page_tables(struct vm_area_struct *vma,</span>
 			  new_vma, new_pmd, new_addr, need_rmap_locks);
 		need_flush = true;
 	}
<span class="p_del">-	if (likely(need_flush))</span>
<span class="p_add">+	if (likely(need_flush)) {</span>
<span class="p_add">+		if (delay)</span>
<span class="p_add">+			msleep(2000);</span>
 		flush_tlb_range(vma, old_end-len, old_addr);
<span class="p_add">+	}</span>
 
 	mmu_notifier_invalidate_range_end(vma-&gt;vm_mm, mmun_start, mmun_end);
 
<span class="p_chunk">@@ -234,7 +238,8 @@</span> <span class="p_context"> unsigned long move_page_tables(struct vm_area_struct *vma,</span>
 
 static unsigned long move_vma(struct vm_area_struct *vma,
 		unsigned long old_addr, unsigned long old_len,
<span class="p_del">-		unsigned long new_len, unsigned long new_addr, bool *locked)</span>
<span class="p_add">+		unsigned long new_len, unsigned long new_addr,</span>
<span class="p_add">+		bool *locked, bool delay)</span>
 {
 	struct mm_struct *mm = vma-&gt;vm_mm;
 	struct vm_area_struct *new_vma;
<span class="p_chunk">@@ -273,7 +278,7 @@</span> <span class="p_context"> static unsigned long move_vma(struct vm_area_struct *vma,</span>
 		return -ENOMEM;
 
 	moved_len = move_page_tables(vma, old_addr, new_vma, new_addr, old_len,
<span class="p_del">-				     need_rmap_locks);</span>
<span class="p_add">+				     need_rmap_locks, delay);</span>
 	if (moved_len &lt; old_len) {
 		err = -ENOMEM;
 	} else if (vma-&gt;vm_ops &amp;&amp; vma-&gt;vm_ops-&gt;mremap) {
<span class="p_chunk">@@ -287,7 +292,7 @@</span> <span class="p_context"> static unsigned long move_vma(struct vm_area_struct *vma,</span>
 		 * and then proceed to unmap new area instead of old.
 		 */
 		move_page_tables(new_vma, new_addr, vma, old_addr, moved_len,
<span class="p_del">-				 true);</span>
<span class="p_add">+				 true, delay);</span>
 		vma = new_vma;
 		old_len = new_len;
 		old_addr = new_addr;
<span class="p_chunk">@@ -442,7 +447,7 @@</span> <span class="p_context"> static unsigned long mremap_to(unsigned long addr, unsigned long old_len,</span>
 	if (offset_in_page(ret))
 		goto out1;
 
<span class="p_del">-	ret = move_vma(vma, addr, old_len, new_len, new_addr, locked);</span>
<span class="p_add">+	ret = move_vma(vma, addr, old_len, new_len, new_addr, locked, true);</span>
 	if (!(offset_in_page(ret)))
 		goto out;
 out1:
<span class="p_chunk">@@ -576,7 +581,7 @@</span> <span class="p_context"> SYSCALL_DEFINE5(mremap, unsigned long, addr, unsigned long, old_len,</span>
 			goto out;
 		}
 
<span class="p_del">-		ret = move_vma(vma, addr, old_len, new_len, new_addr, &amp;locked);</span>
<span class="p_add">+		ret = move_vma(vma, addr, old_len, new_len, new_addr, &amp;locked, false);</span>
 	}
 out:
 	if (offset_in_page(ret)) {

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



