
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[7/8] memcg: get rid of mm_struct::owner - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [7/8] memcg: get rid of mm_struct::owner</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>July 8, 2015, 12:27 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1436358472-29137-8-git-send-email-mhocko@kernel.org&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/6746551/mbox/"
   >mbox</a>
|
   <a href="/patch/6746551/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/6746551/">/patch/6746551/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork1.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork1.web.kernel.org (Postfix) with ESMTP id CED749F380
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed,  8 Jul 2015 12:29:10 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 3318420665
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed,  8 Jul 2015 12:29:09 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id 5F28620676
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed,  8 Jul 2015 12:29:07 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1758365AbbGHM2e (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Wed, 8 Jul 2015 08:28:34 -0400
Received: from cantor2.suse.de ([195.135.220.15]:51166 &quot;EHLO mx2.suse.de&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1754805AbbGHM2F (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Wed, 8 Jul 2015 08:28:05 -0400
X-Virus-Scanned: by amavisd-new at test-mx.suse.de
Received: from relay2.suse.de (charybdis-ext.suse.de [195.135.220.254])
	by mx2.suse.de (Postfix) with ESMTP id 01A32AC95;
	Wed,  8 Jul 2015 12:28:03 +0000 (UTC)
From: Michal Hocko &lt;mhocko@kernel.org&gt;
To: Johannes Weiner &lt;hannes@cmpxchg.org&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;
Cc: Tejun Heo &lt;tj@kernel.org&gt;, Oleg Nesterov &lt;oleg@redhat.com&gt;,
	Vladimir Davydov &lt;vdavydov@parallels.com&gt;,
	Greg Thelen &lt;gthelen@google.com&gt;,
	KAMEZAWA Hiroyuki &lt;kamezawa.hiroyu@jp.fujitsu.com&gt;,
	KOSAKI Motohiro &lt;kosaki.motohiro@jp.fujitsu.com&gt;,
	&lt;linux-mm@kvack.org&gt;, LKML &lt;linux-kernel@vger.kernel.org&gt;,
	Michal Hocko &lt;mhocko@suse.cz&gt;
Subject: [PATCH 7/8] memcg: get rid of mm_struct::owner
Date: Wed,  8 Jul 2015 14:27:51 +0200
Message-Id: &lt;1436358472-29137-8-git-send-email-mhocko@kernel.org&gt;
X-Mailer: git-send-email 2.1.4
In-Reply-To: &lt;1436358472-29137-1-git-send-email-mhocko@kernel.org&gt;
References: &lt;1436358472-29137-1-git-send-email-mhocko@kernel.org&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-7.6 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - July 8, 2015, 12:27 p.m.</div>
<pre class="content">
<span class="from">From: Michal Hocko &lt;mhocko@suse.cz&gt;</span>

mm_struct::owner keeps track of the task which is in charge for the
specific mm. This is usually the thread group leader of the process but
there are exotic cases where this doesn&#39;t hold.

The most prominent one is when separate tasks (not in the same thread
group) share the address space (by using clone with CLONE_VM without
CLONE_THREAD). The first task will be the owner until it exits.
mm_update_next_owner will then try to find a new owner - a task which
points to the same mm_struct. There is no guarantee a new owner will
be a thread group leader though because the leader for that thread
group might have exited. Even though such a thread will be still around
waiting for the remaining threads from its group, it&#39;s mm will be NULL
so it cannot be chosen.

cgroup migration code, however assumes only group leaders when migrating
via cgroup.procs (which will be the only mode in the unified hierarchy
API) while mem_cgroup_can_attach considers only those tasks which are
owner of the mm. So we might end up with tasks which cannot be migrated.
mm_update_next_owner could be tweaked to try harder and use a group
leader whenever possible but this will never be 100% because all the
leaders might be dead. It seems that getting rid of the mm-&gt;owner sounds
like a better and less hacky option.

The whole concept of the mm owner is a bit artificial and too tricky to
get right. All the memcg code needs is to find struct mem_cgroup from
a given mm_struct and there are only two events when the association
is either built or changed
	- a new mm is created - dup_mmm resp exec_mmap - when the memcg
	  is inherited from the oldmm
	- task associated with the mm is moved to another memcg
So it is much more easier to bind mm_struct with the mem_cgroup directly
rather than indirectly via a task. This is exactly what this patch does.

mm_inherit_memcg and mm_drop_memcg are exported for the core kernel
to bind an old memcg during dup_mm (fork) resp. exec_mmap (exec) and
releasing that memcg in mmput after the last reference is dropped and no
task sees the mm anymore. We have to be careful and take a reference to
the memcg-&gt;css so that it doesn&#39;t vanish from under our feet.

The only remaining part is to catch task migration and change the
association. This is done in mem_cgroup_move_task before charges get
moved because mem_cgroup_can_attach is too early and other controllers
might fail and we would have to handle the rollback.

mm-&gt;memcg conforms to standard mem_cgroup locking rules. It has to be
used inside rcu_read_{un}lock() and a reference has to be taken before the
unlock if the memcg is supposed to be used outside.

Finally mem_cgroup_can_attach will allow task migration only for the
thread group leaders to conform with cgroup core requirements.

Please note that this patch introduces a USER VISIBLE CHANGE OF BEHAVIOR.
Without mm-&gt;owner _all_ tasks (group leaders to be precise) associated
with the mm_struct would initiate memcg migration while previously
only owner of the mm_struct could do that. The original behavior was
awkward though because the user task didn&#39;t have any means to find out
the current owner (esp. after mm_update_next_owner) so the migration
behavior was not well defined in general.
New cgroup API (unified hierarchy) will discontinue tasks cgroup file
which means that migrating threads will no longer be possible. In such
a case having CLONE_VM without CLONE_THREAD could emulate the thread
behavior but this patch prevents from isolating memcg controllers from
others. Nevertheless I am not convinced such a use case would really
deserve complications on the memcg code side.

Suggested-by: Oleg Nesterov &lt;oleg@redhat.com&gt;
<span class="signed-off-by">Signed-off-by: Michal Hocko &lt;mhocko@suse.cz&gt;</span>
---
 fs/exec.c                  |   2 +-
 include/linux/memcontrol.h |  58 ++++++++++++++++++++++++--
 include/linux/mm_types.h   |  12 +-----
 kernel/exit.c              |  89 ---------------------------------------
 kernel/fork.c              |  10 +----
 mm/debug.c                 |   4 +-
 mm/memcontrol.c            | 101 ++++++++++++++++++++++++++++-----------------
 7 files changed, 123 insertions(+), 153 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=26262">Vladimir Davydov</a> - July 8, 2015, 5:32 p.m.</div>
<pre class="content">
I like the gist of this patch. A few comments below.

On Wed, Jul 08, 2015 at 02:27:51PM +0200, Michal Hocko wrote:
[...]
<span class="quote">&gt; diff --git a/fs/exec.c b/fs/exec.c</span>
<span class="quote">&gt; index 1977c2a553ac..3ed9c0abc9f5 100644</span>
<span class="quote">&gt; --- a/fs/exec.c</span>
<span class="quote">&gt; +++ b/fs/exec.c</span>
<span class="quote">&gt; @@ -870,7 +870,7 @@ static int exec_mmap(struct mm_struct *mm)</span>
<span class="quote">&gt;  		up_read(&amp;old_mm-&gt;mmap_sem);</span>
<span class="quote">&gt;  		BUG_ON(active_mm != old_mm);</span>
<span class="quote">&gt;  		setmax_mm_hiwater_rss(&amp;tsk-&gt;signal-&gt;maxrss, old_mm);</span>
<span class="quote">&gt; -		mm_update_next_owner(old_mm);</span>
<span class="quote">&gt; +		mm_inherit_memcg(mm, old_mm);</span>
<span class="quote">&gt;  		mmput(old_mm);</span>
<span class="quote">&gt;  		return 0;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; diff --git a/include/linux/memcontrol.h b/include/linux/memcontrol.h</span>
<span class="quote">&gt; index 78e9d4ac57a1..8e6b2444ebfe 100644</span>
<span class="quote">&gt; --- a/include/linux/memcontrol.h</span>
<span class="quote">&gt; +++ b/include/linux/memcontrol.h</span>
<span class="quote">&gt; @@ -274,6 +274,52 @@ struct mem_cgroup {</span>
<span class="quote">&gt;  extern struct cgroup_subsys_state *mem_cgroup_root_css;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /**</span>
<span class="quote">&gt; + * __mm_set_memcg - Set mm_struct:memcg to a given memcg.</span>
<span class="quote">&gt; + * @mm: mm struct</span>
<span class="quote">&gt; + * @memcg: mem_cgroup to be used</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Note that this function doesn&#39;t clean up the previous mm-&gt;memcg.</span>
<span class="quote">&gt; + * This should be done by caller when necessary (e.g. when moving</span>
<span class="quote">&gt; + * mm from one memcg to another).</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +static inline</span>
<span class="quote">&gt; +void __mm_set_memcg(struct mm_struct *mm, struct mem_cgroup *memcg)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	if (memcg)</span>
<span class="quote">&gt; +		css_get(&amp;memcg-&gt;css);</span>
<span class="quote">&gt; +	rcu_assign_pointer(mm-&gt;memcg, memcg);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/**</span>
<span class="quote">&gt; + * mm_inherit_memcg - Initialize mm_struct::memcg from an existing mm_struct</span>
<span class="quote">&gt; + * @newmm: new mm struct</span>
<span class="quote">&gt; + * @oldmm: old mm struct to inherit from</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Should be called for each new mm_struct.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +static inline</span>
<span class="quote">&gt; +void mm_inherit_memcg(struct mm_struct *newmm, struct mm_struct *oldmm)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct mem_cgroup *memcg = oldmm-&gt;memcg;</span>

FWIW, if CONFIG_SPARSE_RCU_POINTER is on, this will trigger a compile
time warning, as well as any unannotated dereference of mm_struct-&gt;memcg
below.
<span class="quote">
&gt; +</span>
<span class="quote">&gt; +	__mm_set_memcg(newmm, memcg);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/**</span>
<span class="quote">&gt; + * mm_drop_iter - drop mm_struct::memcg association</span>

s/mm_drop_iter/mm_drop_memcg
<span class="quote">
&gt; + * @mm: mm struct</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Should be called after the mm has been removed from all tasks</span>
<span class="quote">&gt; + * and before it is freed (e.g. from mmput)</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +static inline void mm_drop_memcg(struct mm_struct *mm)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	if (mm-&gt;memcg)</span>
<span class="quote">&gt; +		css_put(&amp;mm-&gt;memcg-&gt;css);</span>
<span class="quote">&gt; +	mm-&gt;memcg = NULL;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/**</span>
<span class="quote">&gt;   * mem_cgroup_events - count memory events against a cgroup</span>
<span class="quote">&gt;   * @memcg: the memory cgroup</span>
<span class="quote">&gt;   * @idx: the event index</span>
<span class="quote">&gt; @@ -305,7 +351,6 @@ struct lruvec *mem_cgroup_page_lruvec(struct page *, struct zone *);</span>
<span class="quote">&gt;  bool task_in_mem_cgroup(struct task_struct *task, struct mem_cgroup *memcg);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  struct mem_cgroup *try_get_mem_cgroup_from_page(struct page *page);</span>
<span class="quote">&gt; -struct mem_cgroup *mem_cgroup_from_task(struct task_struct *p);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  struct mem_cgroup *parent_mem_cgroup(struct mem_cgroup *memcg);</span>
<span class="quote">&gt;  static inline</span>
<span class="quote">&gt; @@ -335,7 +380,7 @@ static inline bool mm_match_cgroup(struct mm_struct *mm,</span>
<span class="quote">&gt;  	bool match = false;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	rcu_read_lock();</span>
<span class="quote">&gt; -	task_memcg = mem_cgroup_from_task(rcu_dereference(mm-&gt;owner));</span>
<span class="quote">&gt; +	task_memcg = rcu_dereference(mm-&gt;memcg);</span>
<span class="quote">&gt;  	if (task_memcg)</span>
<span class="quote">&gt;  		match = mem_cgroup_is_descendant(task_memcg, memcg);</span>
<span class="quote">&gt;  	rcu_read_unlock();</span>
<span class="quote">&gt; @@ -474,7 +519,7 @@ static inline void mem_cgroup_count_vm_event(struct mm_struct *mm,</span>
<span class="quote">&gt;  		return;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	rcu_read_lock();</span>
<span class="quote">&gt; -	memcg = mem_cgroup_from_task(rcu_dereference(mm-&gt;owner));</span>
<span class="quote">&gt; +	memcg = rcu_dereference(mm-&gt;memcg);</span>
<span class="quote">&gt;  	if (unlikely(!memcg))</span>
<span class="quote">&gt;  		goto out;</span>
<span class="quote">&gt;  </span>

If I&#39;m not mistaken, mm-&gt;memcg equals NULL for any task in the root
memory cgroup (BTW, it it&#39;s true, it&#39;s worth mentioning in the comment
to mm-&gt;memcg definition IMO). As a result, we won&#39;t account the stats
for such tasks, will we?

[...]
<span class="quote">&gt; @@ -4749,37 +4748,49 @@ static int mem_cgroup_can_attach(struct cgroup_subsys_state *css,</span>
<span class="quote">&gt;  	 * tunable will only affect upcoming migrations, not the current one.</span>
<span class="quote">&gt;  	 * So we need to save it, and keep it going.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt; -	move_flags = READ_ONCE(memcg-&gt;move_charge_at_immigrate);</span>
<span class="quote">&gt; +	move_flags = READ_ONCE(to-&gt;move_charge_at_immigrate);</span>
<span class="quote">&gt;  	if (!move_flags)</span>
<span class="quote">&gt;  		return 0;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	p = cgroup_taskset_first(tset);</span>
<span class="quote">&gt; -	from = mem_cgroup_from_task(p);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	VM_BUG_ON(from == memcg);</span>
<span class="quote">&gt; +	if (!thread_group_leader(p))</span>
<span class="quote">&gt; +		return 0;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	mm = get_task_mm(p);</span>
<span class="quote">&gt;  	if (!mm)</span>
<span class="quote">&gt;  		return 0;</span>
<span class="quote">&gt; -	/* We move charges only when we move a owner of the mm */</span>
<span class="quote">&gt; -	if (mm-&gt;owner == p) {</span>
<span class="quote">&gt; -		VM_BUG_ON(mc.from);</span>
<span class="quote">&gt; -		VM_BUG_ON(mc.to);</span>
<span class="quote">&gt; -		VM_BUG_ON(mc.precharge);</span>
<span class="quote">&gt; -		VM_BUG_ON(mc.moved_charge);</span>
<span class="quote">&gt; -		VM_BUG_ON(mc.moved_swap);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -		spin_lock(&amp;mc.lock);</span>
<span class="quote">&gt; -		mc.from = from;</span>
<span class="quote">&gt; -		mc.to = memcg;</span>
<span class="quote">&gt; -		mc.flags = move_flags;</span>
<span class="quote">&gt; -		spin_unlock(&amp;mc.lock);</span>
<span class="quote">&gt; -		/* We set mc.moving_task later */</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -		ret = mem_cgroup_precharge_mc(mm);</span>
<span class="quote">&gt; -		if (ret)</span>
<span class="quote">&gt; -			mem_cgroup_clear_mc();</span>
<span class="quote">&gt; -	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * tasks&#39; cgroup might be different from the one p-&gt;mm is associated</span>
<span class="quote">&gt; +	 * with because CLONE_VM is allowed without CLONE_THREAD. The task is</span>
<span class="quote">&gt; +	 * moving so we have to migrate from the memcg associated with its</span>
<span class="quote">&gt; +	 * address space.</span>
<span class="quote">
&gt; +	 * No need to take a reference here because the memcg is pinned by the</span>
<span class="quote">&gt; +	 * mm_struct.</span>
<span class="quote">&gt; +	 */</span>

But after we drop the reference to the mm below, mc.from can pass away
and we can get use-after-free in mem_cgroup_move_task, can&#39;t we?

AFAIU the real reason why we can skip taking a reference to mc.from, as
well as to mc.to, is that task migration proceeds under cgroup_mutex,
which blocks cgroup destruction. Am I missing something? If not, please
remove this comment, because it&#39;s confusing.
<span class="quote">
&gt; +	from = READ_ONCE(mm-&gt;memcg);</span>
<span class="quote">&gt; +	if (!from)</span>
<span class="quote">&gt; +		from = root_mem_cgroup;</span>
<span class="quote">&gt; +	if (from == to)</span>
<span class="quote">&gt; +		goto out;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	VM_BUG_ON(mc.from);</span>
<span class="quote">&gt; +	VM_BUG_ON(mc.to);</span>
<span class="quote">&gt; +	VM_BUG_ON(mc.precharge);</span>
<span class="quote">&gt; +	VM_BUG_ON(mc.moved_charge);</span>
<span class="quote">&gt; +	VM_BUG_ON(mc.moved_swap);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	spin_lock(&amp;mc.lock);</span>
<span class="quote">&gt; +	mc.from = from;</span>
<span class="quote">&gt; +	mc.to = to;</span>
<span class="quote">&gt; +	mc.flags = move_flags;</span>
<span class="quote">&gt; +	spin_unlock(&amp;mc.lock);</span>
<span class="quote">&gt; +	/* We set mc.moving_task later */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	ret = mem_cgroup_precharge_mc(mm);</span>
<span class="quote">&gt; +	if (ret)</span>
<span class="quote">&gt; +		mem_cgroup_clear_mc();</span>
<span class="quote">&gt; +out:</span>
<span class="quote">&gt;  	mmput(mm);</span>
<span class="quote">&gt;  	return ret;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; @@ -4932,14 +4943,26 @@ static void mem_cgroup_move_task(struct cgroup_subsys_state *css,</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct task_struct *p = cgroup_taskset_first(tset);</span>
<span class="quote">&gt;  	struct mm_struct *mm = get_task_mm(p);</span>
<span class="quote">&gt; +	struct mem_cgroup *old_memcg = NULL;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (mm) {</span>
<span class="quote">&gt; +		old_memcg = READ_ONCE(mm-&gt;memcg);</span>
<span class="quote">&gt; +		__mm_set_memcg(mm, mem_cgroup_from_css(css));</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  		if (mc.to)</span>
<span class="quote">&gt;  			mem_cgroup_move_charge(mm);</span>
<span class="quote">&gt;  		mmput(mm);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  	if (mc.to)</span>
<span class="quote">&gt;  		mem_cgroup_clear_mc();</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * Be careful and drop the reference only after we are done because</span>
<span class="quote">&gt; +	 * p&#39;s task_css memcg might be different from p-&gt;memcg and nothing else</span>
<span class="quote">&gt; +	 * might be pinning the old memcg.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	if (old_memcg)</span>
<span class="quote">&gt; +		css_put(&amp;old_memcg-&gt;css);</span>

Please explain why the following race is impossible:

CPU0					CPU1
----					----
[current = T]
dup_mm or exec_mmap
 mm_inherit_memcg
  memcg = current-&gt;mm-&gt;memcg;
					mem_cgroup_move_task
					 p = T;
					 mm = get_task_mm(p);
					 old_memcg = mm-&gt;memcg;
					 css_put(&amp;old_memcg-&gt;css);
					 /* old_memcg can be freed now */
  css_get(memcg); /* BUG */
<span class="quote">
&gt;  }</span>
<span class="quote">&gt;  #else	/* !CONFIG_MMU */</span>
<span class="quote">&gt;  static int mem_cgroup_can_attach(struct cgroup_subsys_state *css,</span>

Thanks,
Vladimir
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - July 10, 2015, 2:05 p.m.</div>
<pre class="content">
JFYI: I&#39;ve found some more issues while hamerring this more. Please
ignore this and the follow up patch for now. If others are OK with the
cleanups preceding this patch I will repost with the changes based on
the feedback so far and let them merge into mm tree before I settle
with this much more tricky part.

On Wed 08-07-15 14:27:51, Michal Hocko wrote:
<span class="quote">&gt; From: Michal Hocko &lt;mhocko@suse.cz&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; mm_struct::owner keeps track of the task which is in charge for the</span>
<span class="quote">&gt; specific mm. This is usually the thread group leader of the process but</span>
<span class="quote">&gt; there are exotic cases where this doesn&#39;t hold.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The most prominent one is when separate tasks (not in the same thread</span>
<span class="quote">&gt; group) share the address space (by using clone with CLONE_VM without</span>
<span class="quote">&gt; CLONE_THREAD). The first task will be the owner until it exits.</span>
<span class="quote">&gt; mm_update_next_owner will then try to find a new owner - a task which</span>
<span class="quote">&gt; points to the same mm_struct. There is no guarantee a new owner will</span>
<span class="quote">&gt; be a thread group leader though because the leader for that thread</span>
<span class="quote">&gt; group might have exited. Even though such a thread will be still around</span>
<span class="quote">&gt; waiting for the remaining threads from its group, it&#39;s mm will be NULL</span>
<span class="quote">&gt; so it cannot be chosen.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; cgroup migration code, however assumes only group leaders when migrating</span>
<span class="quote">&gt; via cgroup.procs (which will be the only mode in the unified hierarchy</span>
<span class="quote">&gt; API) while mem_cgroup_can_attach considers only those tasks which are</span>
<span class="quote">&gt; owner of the mm. So we might end up with tasks which cannot be migrated.</span>
<span class="quote">&gt; mm_update_next_owner could be tweaked to try harder and use a group</span>
<span class="quote">&gt; leader whenever possible but this will never be 100% because all the</span>
<span class="quote">&gt; leaders might be dead. It seems that getting rid of the mm-&gt;owner sounds</span>
<span class="quote">&gt; like a better and less hacky option.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The whole concept of the mm owner is a bit artificial and too tricky to</span>
<span class="quote">&gt; get right. All the memcg code needs is to find struct mem_cgroup from</span>
<span class="quote">&gt; a given mm_struct and there are only two events when the association</span>
<span class="quote">&gt; is either built or changed</span>
<span class="quote">&gt; 	- a new mm is created - dup_mmm resp exec_mmap - when the memcg</span>
<span class="quote">&gt; 	  is inherited from the oldmm</span>
<span class="quote">&gt; 	- task associated with the mm is moved to another memcg</span>
<span class="quote">&gt; So it is much more easier to bind mm_struct with the mem_cgroup directly</span>
<span class="quote">&gt; rather than indirectly via a task. This is exactly what this patch does.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; mm_inherit_memcg and mm_drop_memcg are exported for the core kernel</span>
<span class="quote">&gt; to bind an old memcg during dup_mm (fork) resp. exec_mmap (exec) and</span>
<span class="quote">&gt; releasing that memcg in mmput after the last reference is dropped and no</span>
<span class="quote">&gt; task sees the mm anymore. We have to be careful and take a reference to</span>
<span class="quote">&gt; the memcg-&gt;css so that it doesn&#39;t vanish from under our feet.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The only remaining part is to catch task migration and change the</span>
<span class="quote">&gt; association. This is done in mem_cgroup_move_task before charges get</span>
<span class="quote">&gt; moved because mem_cgroup_can_attach is too early and other controllers</span>
<span class="quote">&gt; might fail and we would have to handle the rollback.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; mm-&gt;memcg conforms to standard mem_cgroup locking rules. It has to be</span>
<span class="quote">&gt; used inside rcu_read_{un}lock() and a reference has to be taken before the</span>
<span class="quote">&gt; unlock if the memcg is supposed to be used outside.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Finally mem_cgroup_can_attach will allow task migration only for the</span>
<span class="quote">&gt; thread group leaders to conform with cgroup core requirements.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Please note that this patch introduces a USER VISIBLE CHANGE OF BEHAVIOR.</span>
<span class="quote">&gt; Without mm-&gt;owner _all_ tasks (group leaders to be precise) associated</span>
<span class="quote">&gt; with the mm_struct would initiate memcg migration while previously</span>
<span class="quote">&gt; only owner of the mm_struct could do that. The original behavior was</span>
<span class="quote">&gt; awkward though because the user task didn&#39;t have any means to find out</span>
<span class="quote">&gt; the current owner (esp. after mm_update_next_owner) so the migration</span>
<span class="quote">&gt; behavior was not well defined in general.</span>
<span class="quote">&gt; New cgroup API (unified hierarchy) will discontinue tasks cgroup file</span>
<span class="quote">&gt; which means that migrating threads will no longer be possible. In such</span>
<span class="quote">&gt; a case having CLONE_VM without CLONE_THREAD could emulate the thread</span>
<span class="quote">&gt; behavior but this patch prevents from isolating memcg controllers from</span>
<span class="quote">&gt; others. Nevertheless I am not convinced such a use case would really</span>
<span class="quote">&gt; deserve complications on the memcg code side.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Suggested-by: Oleg Nesterov &lt;oleg@redhat.com&gt;</span>
<span class="quote">&gt; Signed-off-by: Michal Hocko &lt;mhocko@suse.cz&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  fs/exec.c                  |   2 +-</span>
<span class="quote">&gt;  include/linux/memcontrol.h |  58 ++++++++++++++++++++++++--</span>
<span class="quote">&gt;  include/linux/mm_types.h   |  12 +-----</span>
<span class="quote">&gt;  kernel/exit.c              |  89 ---------------------------------------</span>
<span class="quote">&gt;  kernel/fork.c              |  10 +----</span>
<span class="quote">&gt;  mm/debug.c                 |   4 +-</span>
<span class="quote">&gt;  mm/memcontrol.c            | 101 ++++++++++++++++++++++++++++-----------------</span>
<span class="quote">&gt;  7 files changed, 123 insertions(+), 153 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/fs/exec.c b/fs/exec.c</span>
<span class="quote">&gt; index 1977c2a553ac..3ed9c0abc9f5 100644</span>
<span class="quote">&gt; --- a/fs/exec.c</span>
<span class="quote">&gt; +++ b/fs/exec.c</span>
<span class="quote">&gt; @@ -870,7 +870,7 @@ static int exec_mmap(struct mm_struct *mm)</span>
<span class="quote">&gt;  		up_read(&amp;old_mm-&gt;mmap_sem);</span>
<span class="quote">&gt;  		BUG_ON(active_mm != old_mm);</span>
<span class="quote">&gt;  		setmax_mm_hiwater_rss(&amp;tsk-&gt;signal-&gt;maxrss, old_mm);</span>
<span class="quote">&gt; -		mm_update_next_owner(old_mm);</span>
<span class="quote">&gt; +		mm_inherit_memcg(mm, old_mm);</span>
<span class="quote">&gt;  		mmput(old_mm);</span>
<span class="quote">&gt;  		return 0;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; diff --git a/include/linux/memcontrol.h b/include/linux/memcontrol.h</span>
<span class="quote">&gt; index 78e9d4ac57a1..8e6b2444ebfe 100644</span>
<span class="quote">&gt; --- a/include/linux/memcontrol.h</span>
<span class="quote">&gt; +++ b/include/linux/memcontrol.h</span>
<span class="quote">&gt; @@ -274,6 +274,52 @@ struct mem_cgroup {</span>
<span class="quote">&gt;  extern struct cgroup_subsys_state *mem_cgroup_root_css;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /**</span>
<span class="quote">&gt; + * __mm_set_memcg - Set mm_struct:memcg to a given memcg.</span>
<span class="quote">&gt; + * @mm: mm struct</span>
<span class="quote">&gt; + * @memcg: mem_cgroup to be used</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Note that this function doesn&#39;t clean up the previous mm-&gt;memcg.</span>
<span class="quote">&gt; + * This should be done by caller when necessary (e.g. when moving</span>
<span class="quote">&gt; + * mm from one memcg to another).</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +static inline</span>
<span class="quote">&gt; +void __mm_set_memcg(struct mm_struct *mm, struct mem_cgroup *memcg)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	if (memcg)</span>
<span class="quote">&gt; +		css_get(&amp;memcg-&gt;css);</span>
<span class="quote">&gt; +	rcu_assign_pointer(mm-&gt;memcg, memcg);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/**</span>
<span class="quote">&gt; + * mm_inherit_memcg - Initialize mm_struct::memcg from an existing mm_struct</span>
<span class="quote">&gt; + * @newmm: new mm struct</span>
<span class="quote">&gt; + * @oldmm: old mm struct to inherit from</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Should be called for each new mm_struct.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +static inline</span>
<span class="quote">&gt; +void mm_inherit_memcg(struct mm_struct *newmm, struct mm_struct *oldmm)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct mem_cgroup *memcg = oldmm-&gt;memcg;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	__mm_set_memcg(newmm, memcg);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/**</span>
<span class="quote">&gt; + * mm_drop_iter - drop mm_struct::memcg association</span>
<span class="quote">&gt; + * @mm: mm struct</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Should be called after the mm has been removed from all tasks</span>
<span class="quote">&gt; + * and before it is freed (e.g. from mmput)</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +static inline void mm_drop_memcg(struct mm_struct *mm)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	if (mm-&gt;memcg)</span>
<span class="quote">&gt; +		css_put(&amp;mm-&gt;memcg-&gt;css);</span>
<span class="quote">&gt; +	mm-&gt;memcg = NULL;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/**</span>
<span class="quote">&gt;   * mem_cgroup_events - count memory events against a cgroup</span>
<span class="quote">&gt;   * @memcg: the memory cgroup</span>
<span class="quote">&gt;   * @idx: the event index</span>
<span class="quote">&gt; @@ -305,7 +351,6 @@ struct lruvec *mem_cgroup_page_lruvec(struct page *, struct zone *);</span>
<span class="quote">&gt;  bool task_in_mem_cgroup(struct task_struct *task, struct mem_cgroup *memcg);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  struct mem_cgroup *try_get_mem_cgroup_from_page(struct page *page);</span>
<span class="quote">&gt; -struct mem_cgroup *mem_cgroup_from_task(struct task_struct *p);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  struct mem_cgroup *parent_mem_cgroup(struct mem_cgroup *memcg);</span>
<span class="quote">&gt;  static inline</span>
<span class="quote">&gt; @@ -335,7 +380,7 @@ static inline bool mm_match_cgroup(struct mm_struct *mm,</span>
<span class="quote">&gt;  	bool match = false;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	rcu_read_lock();</span>
<span class="quote">&gt; -	task_memcg = mem_cgroup_from_task(rcu_dereference(mm-&gt;owner));</span>
<span class="quote">&gt; +	task_memcg = rcu_dereference(mm-&gt;memcg);</span>
<span class="quote">&gt;  	if (task_memcg)</span>
<span class="quote">&gt;  		match = mem_cgroup_is_descendant(task_memcg, memcg);</span>
<span class="quote">&gt;  	rcu_read_unlock();</span>
<span class="quote">&gt; @@ -474,7 +519,7 @@ static inline void mem_cgroup_count_vm_event(struct mm_struct *mm,</span>
<span class="quote">&gt;  		return;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	rcu_read_lock();</span>
<span class="quote">&gt; -	memcg = mem_cgroup_from_task(rcu_dereference(mm-&gt;owner));</span>
<span class="quote">&gt; +	memcg = rcu_dereference(mm-&gt;memcg);</span>
<span class="quote">&gt;  	if (unlikely(!memcg))</span>
<span class="quote">&gt;  		goto out;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -498,6 +543,13 @@ void mem_cgroup_split_huge_fixup(struct page *head);</span>
<span class="quote">&gt;  #else /* CONFIG_MEMCG */</span>
<span class="quote">&gt;  struct mem_cgroup;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +static inline void mm_inherit_memcg(struct mm_struct *newmm, struct mm_struct *oldmm)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +static inline void mm_drop_memcg(struct mm_struct *mm)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  static inline void mem_cgroup_events(struct mem_cgroup *memcg,</span>
<span class="quote">&gt;  				     enum mem_cgroup_events_index idx,</span>
<span class="quote">&gt;  				     unsigned int nr)</span>
<span class="quote">&gt; diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h</span>
<span class="quote">&gt; index f6266742ce1f..93dc8cb9c636 100644</span>
<span class="quote">&gt; --- a/include/linux/mm_types.h</span>
<span class="quote">&gt; +++ b/include/linux/mm_types.h</span>
<span class="quote">&gt; @@ -426,17 +426,7 @@ struct mm_struct {</span>
<span class="quote">&gt;  	struct kioctx_table __rcu	*ioctx_table;</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt;  #ifdef CONFIG_MEMCG</span>
<span class="quote">&gt; -	/*</span>
<span class="quote">&gt; -	 * &quot;owner&quot; points to a task that is regarded as the canonical</span>
<span class="quote">&gt; -	 * user/owner of this mm. All of the following must be true in</span>
<span class="quote">&gt; -	 * order for it to be changed:</span>
<span class="quote">&gt; -	 *</span>
<span class="quote">&gt; -	 * current == mm-&gt;owner</span>
<span class="quote">&gt; -	 * current-&gt;mm != mm</span>
<span class="quote">&gt; -	 * new_owner-&gt;mm == mm</span>
<span class="quote">&gt; -	 * new_owner-&gt;alloc_lock is held</span>
<span class="quote">&gt; -	 */</span>
<span class="quote">&gt; -	struct task_struct __rcu *owner;</span>
<span class="quote">&gt; +	struct mem_cgroup __rcu *memcg;</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/* store ref to file /proc/&lt;pid&gt;/exe symlink points to */</span>
<span class="quote">&gt; diff --git a/kernel/exit.c b/kernel/exit.c</span>
<span class="quote">&gt; index 185752a729f6..339554612677 100644</span>
<span class="quote">&gt; --- a/kernel/exit.c</span>
<span class="quote">&gt; +++ b/kernel/exit.c</span>
<span class="quote">&gt; @@ -292,94 +292,6 @@ kill_orphaned_pgrp(struct task_struct *tsk, struct task_struct *parent)</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -#ifdef CONFIG_MEMCG</span>
<span class="quote">&gt; -/*</span>
<span class="quote">&gt; - * A task is exiting.   If it owned this mm, find a new owner for the mm.</span>
<span class="quote">&gt; - */</span>
<span class="quote">&gt; -void mm_update_next_owner(struct mm_struct *mm)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	struct task_struct *c, *g, *p = current;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -retry:</span>
<span class="quote">&gt; -	/*</span>
<span class="quote">&gt; -	 * If the exiting or execing task is not the owner, it&#39;s</span>
<span class="quote">&gt; -	 * someone else&#39;s problem.</span>
<span class="quote">&gt; -	 */</span>
<span class="quote">&gt; -	if (mm-&gt;owner != p)</span>
<span class="quote">&gt; -		return;</span>
<span class="quote">&gt; -	/*</span>
<span class="quote">&gt; -	 * The current owner is exiting/execing and there are no other</span>
<span class="quote">&gt; -	 * candidates.  Do not leave the mm pointing to a possibly</span>
<span class="quote">&gt; -	 * freed task structure.</span>
<span class="quote">&gt; -	 */</span>
<span class="quote">&gt; -	if (atomic_read(&amp;mm-&gt;mm_users) &lt;= 1) {</span>
<span class="quote">&gt; -		mm-&gt;owner = NULL;</span>
<span class="quote">&gt; -		return;</span>
<span class="quote">&gt; -	}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	read_lock(&amp;tasklist_lock);</span>
<span class="quote">&gt; -	/*</span>
<span class="quote">&gt; -	 * Search in the children</span>
<span class="quote">&gt; -	 */</span>
<span class="quote">&gt; -	list_for_each_entry(c, &amp;p-&gt;children, sibling) {</span>
<span class="quote">&gt; -		if (c-&gt;mm == mm)</span>
<span class="quote">&gt; -			goto assign_new_owner;</span>
<span class="quote">&gt; -	}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	/*</span>
<span class="quote">&gt; -	 * Search in the siblings</span>
<span class="quote">&gt; -	 */</span>
<span class="quote">&gt; -	list_for_each_entry(c, &amp;p-&gt;real_parent-&gt;children, sibling) {</span>
<span class="quote">&gt; -		if (c-&gt;mm == mm)</span>
<span class="quote">&gt; -			goto assign_new_owner;</span>
<span class="quote">&gt; -	}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	/*</span>
<span class="quote">&gt; -	 * Search through everything else, we should not get here often.</span>
<span class="quote">&gt; -	 */</span>
<span class="quote">&gt; -	for_each_process(g) {</span>
<span class="quote">&gt; -		if (g-&gt;flags &amp; PF_KTHREAD)</span>
<span class="quote">&gt; -			continue;</span>
<span class="quote">&gt; -		for_each_thread(g, c) {</span>
<span class="quote">&gt; -			if (c-&gt;mm == mm)</span>
<span class="quote">&gt; -				goto assign_new_owner;</span>
<span class="quote">&gt; -			if (c-&gt;mm)</span>
<span class="quote">&gt; -				break;</span>
<span class="quote">&gt; -		}</span>
<span class="quote">&gt; -	}</span>
<span class="quote">&gt; -	read_unlock(&amp;tasklist_lock);</span>
<span class="quote">&gt; -	/*</span>
<span class="quote">&gt; -	 * We found no owner yet mm_users &gt; 1: this implies that we are</span>
<span class="quote">&gt; -	 * most likely racing with swapoff (try_to_unuse()) or /proc or</span>
<span class="quote">&gt; -	 * ptrace or page migration (get_task_mm()).  Mark owner as NULL.</span>
<span class="quote">&gt; -	 */</span>
<span class="quote">&gt; -	mm-&gt;owner = NULL;</span>
<span class="quote">&gt; -	return;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -assign_new_owner:</span>
<span class="quote">&gt; -	BUG_ON(c == p);</span>
<span class="quote">&gt; -	get_task_struct(c);</span>
<span class="quote">&gt; -	/*</span>
<span class="quote">&gt; -	 * The task_lock protects c-&gt;mm from changing.</span>
<span class="quote">&gt; -	 * We always want mm-&gt;owner-&gt;mm == mm</span>
<span class="quote">&gt; -	 */</span>
<span class="quote">&gt; -	task_lock(c);</span>
<span class="quote">&gt; -	/*</span>
<span class="quote">&gt; -	 * Delay read_unlock() till we have the task_lock()</span>
<span class="quote">&gt; -	 * to ensure that c does not slip away underneath us</span>
<span class="quote">&gt; -	 */</span>
<span class="quote">&gt; -	read_unlock(&amp;tasklist_lock);</span>
<span class="quote">&gt; -	if (c-&gt;mm != mm) {</span>
<span class="quote">&gt; -		task_unlock(c);</span>
<span class="quote">&gt; -		put_task_struct(c);</span>
<span class="quote">&gt; -		goto retry;</span>
<span class="quote">&gt; -	}</span>
<span class="quote">&gt; -	mm-&gt;owner = c;</span>
<span class="quote">&gt; -	task_unlock(c);</span>
<span class="quote">&gt; -	put_task_struct(c);</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -#endif /* CONFIG_MEMCG */</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt;   * Turn us into a lazy TLB process if we</span>
<span class="quote">&gt;   * aren&#39;t already..</span>
<span class="quote">&gt; @@ -433,7 +345,6 @@ static void exit_mm(struct task_struct *tsk)</span>
<span class="quote">&gt;  	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt;  	enter_lazy_tlb(mm, current);</span>
<span class="quote">&gt;  	task_unlock(tsk);</span>
<span class="quote">&gt; -	mm_update_next_owner(mm);</span>
<span class="quote">&gt;  	mmput(mm);</span>
<span class="quote">&gt;  	if (test_thread_flag(TIF_MEMDIE))</span>
<span class="quote">&gt;  		exit_oom_victim();</span>
<span class="quote">&gt; diff --git a/kernel/fork.c b/kernel/fork.c</span>
<span class="quote">&gt; index 16e0f872f084..d073b6249d98 100644</span>
<span class="quote">&gt; --- a/kernel/fork.c</span>
<span class="quote">&gt; +++ b/kernel/fork.c</span>
<span class="quote">&gt; @@ -570,13 +570,6 @@ static void mm_init_aio(struct mm_struct *mm)</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static void mm_init_owner(struct mm_struct *mm, struct task_struct *p)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -#ifdef CONFIG_MEMCG</span>
<span class="quote">&gt; -	mm-&gt;owner = p;</span>
<span class="quote">&gt; -#endif</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt;  static struct mm_struct *mm_init(struct mm_struct *mm, struct task_struct *p)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	mm-&gt;mmap = NULL;</span>
<span class="quote">&gt; @@ -596,7 +589,6 @@ static struct mm_struct *mm_init(struct mm_struct *mm, struct task_struct *p)</span>
<span class="quote">&gt;  	spin_lock_init(&amp;mm-&gt;page_table_lock);</span>
<span class="quote">&gt;  	mm_init_cpumask(mm);</span>
<span class="quote">&gt;  	mm_init_aio(mm);</span>
<span class="quote">&gt; -	mm_init_owner(mm, p);</span>
<span class="quote">&gt;  	mmu_notifier_mm_init(mm);</span>
<span class="quote">&gt;  	clear_tlb_flush_pending(mm);</span>
<span class="quote">&gt;  #if defined(CONFIG_TRANSPARENT_HUGEPAGE) &amp;&amp; !USE_SPLIT_PMD_PTLOCKS</span>
<span class="quote">&gt; @@ -702,6 +694,7 @@ void mmput(struct mm_struct *mm)</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  		if (mm-&gt;binfmt)</span>
<span class="quote">&gt;  			module_put(mm-&gt;binfmt-&gt;module);</span>
<span class="quote">&gt; +		mm_drop_memcg(mm);</span>
<span class="quote">&gt;  		mmdrop(mm);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; @@ -926,6 +919,7 @@ static struct mm_struct *dup_mm(struct task_struct *tsk)</span>
<span class="quote">&gt;  	if (mm-&gt;binfmt &amp;&amp; !try_module_get(mm-&gt;binfmt-&gt;module))</span>
<span class="quote">&gt;  		goto free_pt;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +	mm_inherit_memcg(mm, oldmm);</span>
<span class="quote">&gt;  	return mm;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  free_pt:</span>
<span class="quote">&gt; diff --git a/mm/debug.c b/mm/debug.c</span>
<span class="quote">&gt; index 3eb3ac2fcee7..d0347a168651 100644</span>
<span class="quote">&gt; --- a/mm/debug.c</span>
<span class="quote">&gt; +++ b/mm/debug.c</span>
<span class="quote">&gt; @@ -184,7 +184,7 @@ void dump_mm(const struct mm_struct *mm)</span>
<span class="quote">&gt;  		&quot;ioctx_table %p\n&quot;</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt;  #ifdef CONFIG_MEMCG</span>
<span class="quote">&gt; -		&quot;owner %p &quot;</span>
<span class="quote">&gt; +		&quot;memcg %p &quot;</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt;  		&quot;exe_file %p\n&quot;</span>
<span class="quote">&gt;  #ifdef CONFIG_MMU_NOTIFIER</span>
<span class="quote">&gt; @@ -218,7 +218,7 @@ void dump_mm(const struct mm_struct *mm)</span>
<span class="quote">&gt;  		mm-&gt;ioctx_table,</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt;  #ifdef CONFIG_MEMCG</span>
<span class="quote">&gt; -		mm-&gt;owner,</span>
<span class="quote">&gt; +		mm-&gt;memcg,</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt;  		mm-&gt;exe_file,</span>
<span class="quote">&gt;  #ifdef CONFIG_MMU_NOTIFIER</span>
<span class="quote">&gt; diff --git a/mm/memcontrol.c b/mm/memcontrol.c</span>
<span class="quote">&gt; index 19ffae804076..4069ec8f52be 100644</span>
<span class="quote">&gt; --- a/mm/memcontrol.c</span>
<span class="quote">&gt; +++ b/mm/memcontrol.c</span>
<span class="quote">&gt; @@ -294,6 +294,18 @@ static inline struct mem_cgroup *mem_cgroup_from_id(unsigned short id)</span>
<span class="quote">&gt;  	return mem_cgroup_from_css(css);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +static struct mem_cgroup *mem_cgroup_from_task(struct task_struct *p)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	if (p-&gt;mm)</span>
<span class="quote">&gt; +		return rcu_dereference(p-&gt;mm-&gt;memcg);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * If the process doesn&#39;t have mm struct anymore we have to fallback</span>
<span class="quote">&gt; +	 * to the task_css.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	return mem_cgroup_from_css(task_css(p, memory_cgrp_id));</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  /* Writing them here to avoid exposing memcg&#39;s inner layout */</span>
<span class="quote">&gt;  #if defined(CONFIG_INET) &amp;&amp; defined(CONFIG_MEMCG_KMEM)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -783,19 +795,6 @@ static void memcg_check_events(struct mem_cgroup *memcg, struct page *page)</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -struct mem_cgroup *mem_cgroup_from_task(struct task_struct *p)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	/*</span>
<span class="quote">&gt; -	 * mm_update_next_owner() may clear mm-&gt;owner to NULL</span>
<span class="quote">&gt; -	 * if it races with swapoff, page migration, etc.</span>
<span class="quote">&gt; -	 * So this can be called with p == NULL.</span>
<span class="quote">&gt; -	 */</span>
<span class="quote">&gt; -	if (unlikely(!p))</span>
<span class="quote">&gt; -		return NULL;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	return mem_cgroup_from_css(task_css(p, memory_cgrp_id));</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt;  static struct mem_cgroup *get_mem_cgroup_from_mm(struct mm_struct *mm)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct mem_cgroup *memcg = NULL;</span>
<span class="quote">&gt; @@ -810,7 +809,7 @@ static struct mem_cgroup *get_mem_cgroup_from_mm(struct mm_struct *mm)</span>
<span class="quote">&gt;  		if (unlikely(!mm))</span>
<span class="quote">&gt;  			memcg = root_mem_cgroup;</span>
<span class="quote">&gt;  		else {</span>
<span class="quote">&gt; -			memcg = mem_cgroup_from_task(rcu_dereference(mm-&gt;owner));</span>
<span class="quote">&gt; +			memcg = rcu_dereference(mm-&gt;memcg);</span>
<span class="quote">&gt;  			if (unlikely(!memcg))</span>
<span class="quote">&gt;  				memcg = root_mem_cgroup;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt; @@ -2286,7 +2285,7 @@ void __memcg_kmem_put_cache(struct kmem_cache *cachep)</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt; - * We need to verify if the allocation against current-&gt;mm-&gt;owner&#39;s memcg is</span>
<span class="quote">&gt; + * We need to verify if the allocation against current-&gt;mm-&gt;memcg is</span>
<span class="quote">&gt;   * possible for the given order. But the page is not allocated yet, so we&#39;ll</span>
<span class="quote">&gt;   * need a further commit step to do the final arrangements.</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt; @@ -4737,7 +4736,7 @@ static void mem_cgroup_clear_mc(void)</span>
<span class="quote">&gt;  static int mem_cgroup_can_attach(struct cgroup_subsys_state *css,</span>
<span class="quote">&gt;  				 struct cgroup_taskset *tset)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	struct mem_cgroup *memcg = mem_cgroup_from_css(css);</span>
<span class="quote">&gt; +	struct mem_cgroup *to = mem_cgroup_from_css(css);</span>
<span class="quote">&gt;  	struct mem_cgroup *from;</span>
<span class="quote">&gt;  	struct task_struct *p;</span>
<span class="quote">&gt;  	struct mm_struct *mm;</span>
<span class="quote">&gt; @@ -4749,37 +4748,49 @@ static int mem_cgroup_can_attach(struct cgroup_subsys_state *css,</span>
<span class="quote">&gt;  	 * tunable will only affect upcoming migrations, not the current one.</span>
<span class="quote">&gt;  	 * So we need to save it, and keep it going.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt; -	move_flags = READ_ONCE(memcg-&gt;move_charge_at_immigrate);</span>
<span class="quote">&gt; +	move_flags = READ_ONCE(to-&gt;move_charge_at_immigrate);</span>
<span class="quote">&gt;  	if (!move_flags)</span>
<span class="quote">&gt;  		return 0;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	p = cgroup_taskset_first(tset);</span>
<span class="quote">&gt; -	from = mem_cgroup_from_task(p);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	VM_BUG_ON(from == memcg);</span>
<span class="quote">&gt; +	if (!thread_group_leader(p))</span>
<span class="quote">&gt; +		return 0;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	mm = get_task_mm(p);</span>
<span class="quote">&gt;  	if (!mm)</span>
<span class="quote">&gt;  		return 0;</span>
<span class="quote">&gt; -	/* We move charges only when we move a owner of the mm */</span>
<span class="quote">&gt; -	if (mm-&gt;owner == p) {</span>
<span class="quote">&gt; -		VM_BUG_ON(mc.from);</span>
<span class="quote">&gt; -		VM_BUG_ON(mc.to);</span>
<span class="quote">&gt; -		VM_BUG_ON(mc.precharge);</span>
<span class="quote">&gt; -		VM_BUG_ON(mc.moved_charge);</span>
<span class="quote">&gt; -		VM_BUG_ON(mc.moved_swap);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -		spin_lock(&amp;mc.lock);</span>
<span class="quote">&gt; -		mc.from = from;</span>
<span class="quote">&gt; -		mc.to = memcg;</span>
<span class="quote">&gt; -		mc.flags = move_flags;</span>
<span class="quote">&gt; -		spin_unlock(&amp;mc.lock);</span>
<span class="quote">&gt; -		/* We set mc.moving_task later */</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -		ret = mem_cgroup_precharge_mc(mm);</span>
<span class="quote">&gt; -		if (ret)</span>
<span class="quote">&gt; -			mem_cgroup_clear_mc();</span>
<span class="quote">&gt; -	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * tasks&#39; cgroup might be different from the one p-&gt;mm is associated</span>
<span class="quote">&gt; +	 * with because CLONE_VM is allowed without CLONE_THREAD. The task is</span>
<span class="quote">&gt; +	 * moving so we have to migrate from the memcg associated with its</span>
<span class="quote">&gt; +	 * address space.</span>
<span class="quote">&gt; +	 * No need to take a reference here because the memcg is pinned by the</span>
<span class="quote">&gt; +	 * mm_struct.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	from = READ_ONCE(mm-&gt;memcg);</span>
<span class="quote">&gt; +	if (!from)</span>
<span class="quote">&gt; +		from = root_mem_cgroup;</span>
<span class="quote">&gt; +	if (from == to)</span>
<span class="quote">&gt; +		goto out;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	VM_BUG_ON(mc.from);</span>
<span class="quote">&gt; +	VM_BUG_ON(mc.to);</span>
<span class="quote">&gt; +	VM_BUG_ON(mc.precharge);</span>
<span class="quote">&gt; +	VM_BUG_ON(mc.moved_charge);</span>
<span class="quote">&gt; +	VM_BUG_ON(mc.moved_swap);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	spin_lock(&amp;mc.lock);</span>
<span class="quote">&gt; +	mc.from = from;</span>
<span class="quote">&gt; +	mc.to = to;</span>
<span class="quote">&gt; +	mc.flags = move_flags;</span>
<span class="quote">&gt; +	spin_unlock(&amp;mc.lock);</span>
<span class="quote">&gt; +	/* We set mc.moving_task later */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	ret = mem_cgroup_precharge_mc(mm);</span>
<span class="quote">&gt; +	if (ret)</span>
<span class="quote">&gt; +		mem_cgroup_clear_mc();</span>
<span class="quote">&gt; +out:</span>
<span class="quote">&gt;  	mmput(mm);</span>
<span class="quote">&gt;  	return ret;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; @@ -4932,14 +4943,26 @@ static void mem_cgroup_move_task(struct cgroup_subsys_state *css,</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct task_struct *p = cgroup_taskset_first(tset);</span>
<span class="quote">&gt;  	struct mm_struct *mm = get_task_mm(p);</span>
<span class="quote">&gt; +	struct mem_cgroup *old_memcg = NULL;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (mm) {</span>
<span class="quote">&gt; +		old_memcg = READ_ONCE(mm-&gt;memcg);</span>
<span class="quote">&gt; +		__mm_set_memcg(mm, mem_cgroup_from_css(css));</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  		if (mc.to)</span>
<span class="quote">&gt;  			mem_cgroup_move_charge(mm);</span>
<span class="quote">&gt;  		mmput(mm);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  	if (mc.to)</span>
<span class="quote">&gt;  		mem_cgroup_clear_mc();</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * Be careful and drop the reference only after we are done because</span>
<span class="quote">&gt; +	 * p&#39;s task_css memcg might be different from p-&gt;memcg and nothing else</span>
<span class="quote">&gt; +	 * might be pinning the old memcg.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	if (old_memcg)</span>
<span class="quote">&gt; +		css_put(&amp;old_memcg-&gt;css);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  #else	/* !CONFIG_MMU */</span>
<span class="quote">&gt;  static int mem_cgroup_can_attach(struct cgroup_subsys_state *css,</span>
<span class="quote">&gt; -- </span>
<span class="quote">&gt; 2.1.4</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - July 14, 2015, 3:18 p.m.</div>
<pre class="content">
On Fri 10-07-15 16:05:33, Michal Hocko wrote:
<span class="quote">&gt; JFYI: I&#39;ve found some more issues while hamerring this more.</span>

OK so the main issue is quite simple but I have completely missed it when
thinking about the patch before. clone(CLONE_VM) without CLONE_THREAD is
really nasty and it will easily lockup the machine with preempt. disabled
for ever. It goes like this:
taskA (in memcg A)
  taskB = clone(CLONE_VM)
				taskB
				  A -&gt; B	# Both tasks charge to B now
				  exit()	# No tasks in B -&gt; can be
				  		# offlined now
				css_offline()
  mem_cgroup_try_charge
    get_mem_cgroup_from_mm
      rcu_read_lock()
      do {
      } while css_tryget_online(mm-&gt;memcg)	# will never succeed
      rcu_read_unlock()

taskA and taskB are basically independent entities wrt. the life
cycle (unlike threads which are bound to the group leader). The
previous code handles this by re-ownering during exit by the monster
mm_update_next_owner.

I can see the following options without reintroducing reintroducing
some form of mm_update_next_owner:

1) Do not allow offlining a cgroup if we have active users in it.  This
would require a callback from the cgroup core to the subsystem called if
there are no active tasks tracked by the cgroup core. Tracking on the memcg
side doesn&#39;t sound terribly hard - just mark a mm_struct as an alien and
count the number of aliens during the move in mem_cgroup. mm_drop_memcg
then drops the counter. We could end up with EBUSY cgroup without any
visible tasks which is a bit awkward.

2) update get_mem_cgroup_from_mm and others to fallback to the parent
memcg if the current one is offline. This would be in line with charge
reparenting we used to do. I cannot say I would like this because it
allows for easy runaway to the root memcg if the hierarchy is not
configured cautiously. The code would be also quite tricky because each
direct consumer of mm-&gt;memcg would have to be aware of this. This is
awkward.

3) fail mem_cgroup_can_attach if we are trying to migrate a task sharing
mm_struct with a process outside of the tset. If I understand the
tset properly this would require all the sharing tasks to be migrated
together and we would never end up with task_css != &amp;task-&gt;mm-&gt;css.
__cgroup_procs_write doesn&#39;t seem to support multi pid move currently
AFAICS, though. cgroup_migrate_add_src, however, seems to be intended
for this purpose so this should be doable. Without that support we would
basically disallow migrating these tasks - I wouldn&#39;t object if you ask
me.

Do you see other options? From the above three options the 3rd one
sounds the most sane to me and the 1st quite easy to implement. Both will
require some cgroup core work though. But maybe we would be good enough
with 3rd option without supporting moving schizophrenic tasks and that
would be reduced to memcg code.

Or we can, of course, stay with the current state but I think it would
be much saner to get rid of the schizophrenia.

What do you think?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - July 29, 2015, 11:58 a.m.</div>
<pre class="content">
On Tue 14-07-15 17:18:23, Michal Hocko wrote:
<span class="quote">&gt; On Fri 10-07-15 16:05:33, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; JFYI: I&#39;ve found some more issues while hamerring this more.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; OK so the main issue is quite simple but I have completely missed it when</span>
<span class="quote">&gt; thinking about the patch before. clone(CLONE_VM) without CLONE_THREAD is</span>
<span class="quote">&gt; really nasty and it will easily lockup the machine with preempt. disabled</span>
<span class="quote">&gt; for ever. It goes like this:</span>
<span class="quote">&gt; taskA (in memcg A)</span>
<span class="quote">&gt;   taskB = clone(CLONE_VM)</span>
<span class="quote">&gt; 				taskB</span>
<span class="quote">&gt; 				  A -&gt; B	# Both tasks charge to B now</span>
<span class="quote">&gt; 				  exit()	# No tasks in B -&gt; can be</span>
<span class="quote">&gt; 				  		# offlined now</span>
<span class="quote">&gt; 				css_offline()</span>
<span class="quote">&gt;   mem_cgroup_try_charge</span>
<span class="quote">&gt;     get_mem_cgroup_from_mm</span>
<span class="quote">&gt;       rcu_read_lock()</span>
<span class="quote">&gt;       do {</span>
<span class="quote">&gt;       } while css_tryget_online(mm-&gt;memcg)	# will never succeed</span>
<span class="quote">&gt;       rcu_read_unlock()</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; taskA and taskB are basically independent entities wrt. the life</span>
<span class="quote">&gt; cycle (unlike threads which are bound to the group leader). The</span>
<span class="quote">&gt; previous code handles this by re-ownering during exit by the monster</span>
<span class="quote">&gt; mm_update_next_owner.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I can see the following options without reintroducing reintroducing</span>
<span class="quote">&gt; some form of mm_update_next_owner:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 1) Do not allow offlining a cgroup if we have active users in it.  This</span>
<span class="quote">&gt; would require a callback from the cgroup core to the subsystem called if</span>
<span class="quote">&gt; there are no active tasks tracked by the cgroup core. Tracking on the memcg</span>
<span class="quote">&gt; side doesn&#39;t sound terribly hard - just mark a mm_struct as an alien and</span>
<span class="quote">&gt; count the number of aliens during the move in mem_cgroup. mm_drop_memcg</span>
<span class="quote">&gt; then drops the counter. We could end up with EBUSY cgroup without any</span>
<span class="quote">&gt; visible tasks which is a bit awkward.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 2) update get_mem_cgroup_from_mm and others to fallback to the parent</span>
<span class="quote">&gt; memcg if the current one is offline. This would be in line with charge</span>
<span class="quote">&gt; reparenting we used to do. I cannot say I would like this because it</span>
<span class="quote">&gt; allows for easy runaway to the root memcg if the hierarchy is not</span>
<span class="quote">&gt; configured cautiously. The code would be also quite tricky because each</span>
<span class="quote">&gt; direct consumer of mm-&gt;memcg would have to be aware of this. This is</span>
<span class="quote">&gt; awkward.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 3) fail mem_cgroup_can_attach if we are trying to migrate a task sharing</span>
<span class="quote">&gt; mm_struct with a process outside of the tset. If I understand the</span>
<span class="quote">&gt; tset properly this would require all the sharing tasks to be migrated</span>
<span class="quote">&gt; together and we would never end up with task_css != &amp;task-&gt;mm-&gt;css.</span>
<span class="quote">&gt; __cgroup_procs_write doesn&#39;t seem to support multi pid move currently</span>
<span class="quote">&gt; AFAICS, though. cgroup_migrate_add_src, however, seems to be intended</span>
<span class="quote">&gt; for this purpose so this should be doable. Without that support we would</span>
<span class="quote">&gt; basically disallow migrating these tasks - I wouldn&#39;t object if you ask</span>
<span class="quote">&gt; me.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Do you see other options? From the above three options the 3rd one</span>
<span class="quote">&gt; sounds the most sane to me and the 1st quite easy to implement. Both will</span>
<span class="quote">&gt; require some cgroup core work though. But maybe we would be good enough</span>
<span class="quote">&gt; with 3rd option without supporting moving schizophrenic tasks and that</span>
<span class="quote">&gt; would be reduced to memcg code.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Or we can, of course, stay with the current state but I think it would</span>
<span class="quote">&gt; be much saner to get rid of the schizophrenia.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; What do you think?</span>

Ideas, thoughs? Anybody?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=45">Johannes Weiner</a> - July 29, 2015, 1:14 p.m.</div>
<pre class="content">
On Tue, Jul 14, 2015 at 05:18:23PM +0200, Michal Hocko wrote:
<span class="quote">&gt; On Fri 10-07-15 16:05:33, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; JFYI: I&#39;ve found some more issues while hamerring this more.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; OK so the main issue is quite simple but I have completely missed it when</span>
<span class="quote">&gt; thinking about the patch before. clone(CLONE_VM) without CLONE_THREAD is</span>
<span class="quote">&gt; really nasty and it will easily lockup the machine with preempt. disabled</span>
<span class="quote">&gt; for ever. It goes like this:</span>
<span class="quote">&gt; taskA (in memcg A)</span>
<span class="quote">&gt;   taskB = clone(CLONE_VM)</span>
<span class="quote">&gt; 				taskB</span>
<span class="quote">&gt; 				  A -&gt; B	# Both tasks charge to B now</span>
<span class="quote">&gt; 				  exit()	# No tasks in B -&gt; can be</span>
<span class="quote">&gt; 				  		# offlined now</span>
<span class="quote">&gt; 				css_offline()</span>
<span class="quote">&gt;   mem_cgroup_try_charge</span>
<span class="quote">&gt;     get_mem_cgroup_from_mm</span>
<span class="quote">&gt;       rcu_read_lock()</span>
<span class="quote">&gt;       do {</span>
<span class="quote">&gt;       } while css_tryget_online(mm-&gt;memcg)	# will never succeed</span>
<span class="quote">&gt;       rcu_read_unlock()</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; taskA and taskB are basically independent entities wrt. the life</span>
<span class="quote">&gt; cycle (unlike threads which are bound to the group leader). The</span>
<span class="quote">&gt; previous code handles this by re-ownering during exit by the monster</span>
<span class="quote">&gt; mm_update_next_owner.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I can see the following options without reintroducing reintroducing</span>
<span class="quote">&gt; some form of mm_update_next_owner:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 1) Do not allow offlining a cgroup if we have active users in it.  This</span>
<span class="quote">&gt; would require a callback from the cgroup core to the subsystem called if</span>
<span class="quote">&gt; there are no active tasks tracked by the cgroup core. Tracking on the memcg</span>
<span class="quote">&gt; side doesn&#39;t sound terribly hard - just mark a mm_struct as an alien and</span>
<span class="quote">&gt; count the number of aliens during the move in mem_cgroup. mm_drop_memcg</span>
<span class="quote">&gt; then drops the counter. We could end up with EBUSY cgroup without any</span>
<span class="quote">&gt; visible tasks which is a bit awkward.</span>

You couldn&#39;t remove the group, and you wouldn&#39;t know which task needs
to move to get the mm out of there. That&#39;s no good.
<span class="quote">
&gt; 2) update get_mem_cgroup_from_mm and others to fallback to the parent</span>
<span class="quote">&gt; memcg if the current one is offline. This would be in line with charge</span>
<span class="quote">&gt; reparenting we used to do. I cannot say I would like this because it</span>
<span class="quote">&gt; allows for easy runaway to the root memcg if the hierarchy is not</span>
<span class="quote">&gt; configured cautiously. The code would be also quite tricky because each</span>
<span class="quote">&gt; direct consumer of mm-&gt;memcg would have to be aware of this. This is</span>
<span class="quote">&gt; awkward.</span>

In the unified hierarchy, there won&#39;t be tasks inside intermediate
nodes, so reparenting would lead to surprising behavior.
<span class="quote">
&gt; 3) fail mem_cgroup_can_attach if we are trying to migrate a task sharing</span>
<span class="quote">&gt; mm_struct with a process outside of the tset. If I understand the</span>
<span class="quote">&gt; tset properly this would require all the sharing tasks to be migrated</span>
<span class="quote">&gt; together and we would never end up with task_css != &amp;task-&gt;mm-&gt;css.</span>
<span class="quote">&gt; __cgroup_procs_write doesn&#39;t seem to support multi pid move currently</span>
<span class="quote">&gt; AFAICS, though. cgroup_migrate_add_src, however, seems to be intended</span>
<span class="quote">&gt; for this purpose so this should be doable. Without that support we would</span>
<span class="quote">&gt; basically disallow migrating these tasks - I wouldn&#39;t object if you ask</span>
<span class="quote">&gt; me.</span>

I&#39;d prefer not adding controller-specific failure modes for attaching,
and this too would lead to very non-obvious behavior.
<span class="quote">
&gt; Do you see other options? From the above three options the 3rd one</span>
<span class="quote">&gt; sounds the most sane to me and the 1st quite easy to implement. Both will</span>
<span class="quote">&gt; require some cgroup core work though. But maybe we would be good enough</span>
<span class="quote">&gt; with 3rd option without supporting moving schizophrenic tasks and that</span>
<span class="quote">&gt; would be reduced to memcg code.</span>

A modified form of 1) would be to track the mms referring to a memcg
but during offline search the process tree for a matching task. This
is heavy-handed, but it&#39;s a rare case and this work would be done in
the cgroup removal path rather than during task exit. This is stolen
from the current mm_update_next_owner():

list_for_each_entry(mm, memcg-&gt;mms, memcg_list) {
    for_each_process(g) {
        if (g-&gt;flags &amp; PF_KTHREAD)
            continue;
        for_each_thread(g, c) {
            if (c-&gt;mm == mm)
                goto assign;
            if (c-&gt;mm)
                break;
        }
    }
assign:
    memcg = mem_cgroup_from_task(c);
    mm-&gt;memcg = memcg;
    list_move(&amp;mm-&gt;memcg_list, &amp;memcg-&gt;mms);
}

(plus appropriate synchronization, of course)
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - July 29, 2015, 3:05 p.m.</div>
<pre class="content">
On Wed 29-07-15 09:14:54, Johannes Weiner wrote:
<span class="quote">&gt; On Tue, Jul 14, 2015 at 05:18:23PM +0200, Michal Hocko wrote:</span>
[...]
<span class="quote">&gt; &gt; 3) fail mem_cgroup_can_attach if we are trying to migrate a task sharing</span>
<span class="quote">&gt; &gt; mm_struct with a process outside of the tset. If I understand the</span>
<span class="quote">&gt; &gt; tset properly this would require all the sharing tasks to be migrated</span>
<span class="quote">&gt; &gt; together and we would never end up with task_css != &amp;task-&gt;mm-&gt;css.</span>
<span class="quote">&gt; &gt; __cgroup_procs_write doesn&#39;t seem to support multi pid move currently</span>
<span class="quote">&gt; &gt; AFAICS, though. cgroup_migrate_add_src, however, seems to be intended</span>
<span class="quote">&gt; &gt; for this purpose so this should be doable. Without that support we would</span>
<span class="quote">&gt; &gt; basically disallow migrating these tasks - I wouldn&#39;t object if you ask</span>
<span class="quote">&gt; &gt; me.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I&#39;d prefer not adding controller-specific failure modes for attaching,</span>

Does this mean that there is a plan to drop the return value from
can_attach? I can see that both cpuset and cpu controllers currently
allow to fail to attach. Are those going to change? I remember some
discussions but no clear outcome of those.
<span class="quote">
&gt; and this too would lead to very non-obvious behavior.</span>

Yeah, the user will not get an error source with the current API but
this is an inherent restriction currently. Maybe we can add a knob with
the error source?

If there is a clear consensus that can_attach failures are clearly a no
go then what about &quot;silent&quot; moving of the associated tasks? This would
be similar to thread group except the group would be more generic term.
<span class="quote">
&gt; &gt; Do you see other options? From the above three options the 3rd one</span>
<span class="quote">&gt; &gt; sounds the most sane to me and the 1st quite easy to implement. Both will</span>
<span class="quote">&gt; &gt; require some cgroup core work though. But maybe we would be good enough</span>
<span class="quote">&gt; &gt; with 3rd option without supporting moving schizophrenic tasks and that</span>
<span class="quote">&gt; &gt; would be reduced to memcg code.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; A modified form of 1) would be to track the mms referring to a memcg</span>
<span class="quote">&gt; but during offline search the process tree for a matching task.</span>

But we might have many of those and all of them living in different
cgroups. So which one do we take? The first encountered, the one with
the majority? I am not sure this is much better.

I would really prefer if we could get rid of the schizophrenia if it is
possible.
<span class="quote">
&gt; This is heavy-handed, but it&#39;s a rare case and this work would be done</span>
<span class="quote">&gt; in the cgroup removal path rather than during task exit.</span>

Yes it would be lighter a bit.
<span class="quote">
&gt; This is stolen</span>
<span class="quote">&gt; from the current mm_update_next_owner():</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; list_for_each_entry(mm, memcg-&gt;mms, memcg_list) {</span>
<span class="quote">&gt;     for_each_process(g) {</span>
<span class="quote">&gt;         if (g-&gt;flags &amp; PF_KTHREAD)</span>
<span class="quote">&gt;             continue;</span>
<span class="quote">&gt;         for_each_thread(g, c) {</span>
<span class="quote">&gt;             if (c-&gt;mm == mm)</span>
<span class="quote">&gt;                 goto assign;</span>
<span class="quote">&gt;             if (c-&gt;mm)</span>
<span class="quote">&gt;                 break;</span>
<span class="quote">&gt;         }</span>
<span class="quote">&gt;     }</span>
<span class="quote">&gt; assign:</span>
<span class="quote">&gt;     memcg = mem_cgroup_from_task(c);</span>
<span class="quote">&gt;     mm-&gt;memcg = memcg;</span>
<span class="quote">&gt;     list_move(&amp;mm-&gt;memcg_list, &amp;memcg-&gt;mms);</span>
<span class="quote">&gt; }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; (plus appropriate synchronization, of course)</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=45">Johannes Weiner</a> - July 29, 2015, 4:42 p.m.</div>
<pre class="content">
On Wed, Jul 29, 2015 at 05:05:49PM +0200, Michal Hocko wrote:
<span class="quote">&gt; On Wed 29-07-15 09:14:54, Johannes Weiner wrote:</span>
<span class="quote">&gt; &gt; On Tue, Jul 14, 2015 at 05:18:23PM +0200, Michal Hocko wrote:</span>
<span class="quote">&gt; [...]</span>
<span class="quote">&gt; &gt; &gt; 3) fail mem_cgroup_can_attach if we are trying to migrate a task sharing</span>
<span class="quote">&gt; &gt; &gt; mm_struct with a process outside of the tset. If I understand the</span>
<span class="quote">&gt; &gt; &gt; tset properly this would require all the sharing tasks to be migrated</span>
<span class="quote">&gt; &gt; &gt; together and we would never end up with task_css != &amp;task-&gt;mm-&gt;css.</span>
<span class="quote">&gt; &gt; &gt; __cgroup_procs_write doesn&#39;t seem to support multi pid move currently</span>
<span class="quote">&gt; &gt; &gt; AFAICS, though. cgroup_migrate_add_src, however, seems to be intended</span>
<span class="quote">&gt; &gt; &gt; for this purpose so this should be doable. Without that support we would</span>
<span class="quote">&gt; &gt; &gt; basically disallow migrating these tasks - I wouldn&#39;t object if you ask</span>
<span class="quote">&gt; &gt; &gt; me.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I&#39;d prefer not adding controller-specific failure modes for attaching,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Does this mean that there is a plan to drop the return value from</span>
<span class="quote">&gt; can_attach? I can see that both cpuset and cpu controllers currently</span>
<span class="quote">&gt; allow to fail to attach. Are those going to change? I remember some</span>
<span class="quote">&gt; discussions but no clear outcome of those.</span>

Nothing but the realtime stuff needs to be able to fail migration due
to controller restraints. This should probably remain a fringe thing,
because it does make for a much more ambiguous interface.

So I think can_attach() will have to stay, but it should be avoided.
<span class="quote">
&gt; &gt; and this too would lead to very non-obvious behavior.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Yeah, the user will not get an error source with the current API but</span>
<span class="quote">&gt; this is an inherent restriction currently. Maybe we can add a knob with</span>
<span class="quote">&gt; the error source?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; If there is a clear consensus that can_attach failures are clearly a no</span>
<span class="quote">&gt; go then what about &quot;silent&quot; moving of the associated tasks? This would</span>
<span class="quote">&gt; be similar to thread group except the group would be more generic term.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; &gt; Do you see other options? From the above three options the 3rd one</span>
<span class="quote">&gt; &gt; &gt; sounds the most sane to me and the 1st quite easy to implement. Both will</span>
<span class="quote">&gt; &gt; &gt; require some cgroup core work though. But maybe we would be good enough</span>
<span class="quote">&gt; &gt; &gt; with 3rd option without supporting moving schizophrenic tasks and that</span>
<span class="quote">&gt; &gt; &gt; would be reduced to memcg code.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; A modified form of 1) would be to track the mms referring to a memcg</span>
<span class="quote">&gt; &gt; but during offline search the process tree for a matching task.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; But we might have many of those and all of them living in different</span>
<span class="quote">&gt; cgroups. So which one do we take? The first encountered, the one with</span>
<span class="quote">&gt; the majority? I am not sure this is much better.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I would really prefer if we could get rid of the schizophrenia if it is</span>
<span class="quote">&gt; possible.</span>

The first encountered.

This is just our model for sharing memory across groups. Page cache,
writeback, address space--we have always accounted based on who&#39;s
touching it first. We might as well stick with it for shared mms.
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/fs/exec.c b/fs/exec.c</span>
<span class="p_header">index 1977c2a553ac..3ed9c0abc9f5 100644</span>
<span class="p_header">--- a/fs/exec.c</span>
<span class="p_header">+++ b/fs/exec.c</span>
<span class="p_chunk">@@ -870,7 +870,7 @@</span> <span class="p_context"> static int exec_mmap(struct mm_struct *mm)</span>
 		up_read(&amp;old_mm-&gt;mmap_sem);
 		BUG_ON(active_mm != old_mm);
 		setmax_mm_hiwater_rss(&amp;tsk-&gt;signal-&gt;maxrss, old_mm);
<span class="p_del">-		mm_update_next_owner(old_mm);</span>
<span class="p_add">+		mm_inherit_memcg(mm, old_mm);</span>
 		mmput(old_mm);
 		return 0;
 	}
<span class="p_header">diff --git a/include/linux/memcontrol.h b/include/linux/memcontrol.h</span>
<span class="p_header">index 78e9d4ac57a1..8e6b2444ebfe 100644</span>
<span class="p_header">--- a/include/linux/memcontrol.h</span>
<span class="p_header">+++ b/include/linux/memcontrol.h</span>
<span class="p_chunk">@@ -274,6 +274,52 @@</span> <span class="p_context"> struct mem_cgroup {</span>
 extern struct cgroup_subsys_state *mem_cgroup_root_css;
 
 /**
<span class="p_add">+ * __mm_set_memcg - Set mm_struct:memcg to a given memcg.</span>
<span class="p_add">+ * @mm: mm struct</span>
<span class="p_add">+ * @memcg: mem_cgroup to be used</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Note that this function doesn&#39;t clean up the previous mm-&gt;memcg.</span>
<span class="p_add">+ * This should be done by caller when necessary (e.g. when moving</span>
<span class="p_add">+ * mm from one memcg to another).</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline</span>
<span class="p_add">+void __mm_set_memcg(struct mm_struct *mm, struct mem_cgroup *memcg)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (memcg)</span>
<span class="p_add">+		css_get(&amp;memcg-&gt;css);</span>
<span class="p_add">+	rcu_assign_pointer(mm-&gt;memcg, memcg);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * mm_inherit_memcg - Initialize mm_struct::memcg from an existing mm_struct</span>
<span class="p_add">+ * @newmm: new mm struct</span>
<span class="p_add">+ * @oldmm: old mm struct to inherit from</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Should be called for each new mm_struct.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline</span>
<span class="p_add">+void mm_inherit_memcg(struct mm_struct *newmm, struct mm_struct *oldmm)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct mem_cgroup *memcg = oldmm-&gt;memcg;</span>
<span class="p_add">+</span>
<span class="p_add">+	__mm_set_memcg(newmm, memcg);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * mm_drop_iter - drop mm_struct::memcg association</span>
<span class="p_add">+ * @mm: mm struct</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Should be called after the mm has been removed from all tasks</span>
<span class="p_add">+ * and before it is freed (e.g. from mmput)</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void mm_drop_memcg(struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (mm-&gt;memcg)</span>
<span class="p_add">+		css_put(&amp;mm-&gt;memcg-&gt;css);</span>
<span class="p_add">+	mm-&gt;memcg = NULL;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
  * mem_cgroup_events - count memory events against a cgroup
  * @memcg: the memory cgroup
  * @idx: the event index
<span class="p_chunk">@@ -305,7 +351,6 @@</span> <span class="p_context"> struct lruvec *mem_cgroup_page_lruvec(struct page *, struct zone *);</span>
 bool task_in_mem_cgroup(struct task_struct *task, struct mem_cgroup *memcg);
 
 struct mem_cgroup *try_get_mem_cgroup_from_page(struct page *page);
<span class="p_del">-struct mem_cgroup *mem_cgroup_from_task(struct task_struct *p);</span>
 
 struct mem_cgroup *parent_mem_cgroup(struct mem_cgroup *memcg);
 static inline
<span class="p_chunk">@@ -335,7 +380,7 @@</span> <span class="p_context"> static inline bool mm_match_cgroup(struct mm_struct *mm,</span>
 	bool match = false;
 
 	rcu_read_lock();
<span class="p_del">-	task_memcg = mem_cgroup_from_task(rcu_dereference(mm-&gt;owner));</span>
<span class="p_add">+	task_memcg = rcu_dereference(mm-&gt;memcg);</span>
 	if (task_memcg)
 		match = mem_cgroup_is_descendant(task_memcg, memcg);
 	rcu_read_unlock();
<span class="p_chunk">@@ -474,7 +519,7 @@</span> <span class="p_context"> static inline void mem_cgroup_count_vm_event(struct mm_struct *mm,</span>
 		return;
 
 	rcu_read_lock();
<span class="p_del">-	memcg = mem_cgroup_from_task(rcu_dereference(mm-&gt;owner));</span>
<span class="p_add">+	memcg = rcu_dereference(mm-&gt;memcg);</span>
 	if (unlikely(!memcg))
 		goto out;
 
<span class="p_chunk">@@ -498,6 +543,13 @@</span> <span class="p_context"> void mem_cgroup_split_huge_fixup(struct page *head);</span>
 #else /* CONFIG_MEMCG */
 struct mem_cgroup;
 
<span class="p_add">+static inline void mm_inherit_memcg(struct mm_struct *newmm, struct mm_struct *oldmm)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+static inline void mm_drop_memcg(struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static inline void mem_cgroup_events(struct mem_cgroup *memcg,
 				     enum mem_cgroup_events_index idx,
 				     unsigned int nr)
<span class="p_header">diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h</span>
<span class="p_header">index f6266742ce1f..93dc8cb9c636 100644</span>
<span class="p_header">--- a/include/linux/mm_types.h</span>
<span class="p_header">+++ b/include/linux/mm_types.h</span>
<span class="p_chunk">@@ -426,17 +426,7 @@</span> <span class="p_context"> struct mm_struct {</span>
 	struct kioctx_table __rcu	*ioctx_table;
 #endif
 #ifdef CONFIG_MEMCG
<span class="p_del">-	/*</span>
<span class="p_del">-	 * &quot;owner&quot; points to a task that is regarded as the canonical</span>
<span class="p_del">-	 * user/owner of this mm. All of the following must be true in</span>
<span class="p_del">-	 * order for it to be changed:</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 * current == mm-&gt;owner</span>
<span class="p_del">-	 * current-&gt;mm != mm</span>
<span class="p_del">-	 * new_owner-&gt;mm == mm</span>
<span class="p_del">-	 * new_owner-&gt;alloc_lock is held</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	struct task_struct __rcu *owner;</span>
<span class="p_add">+	struct mem_cgroup __rcu *memcg;</span>
 #endif
 
 	/* store ref to file /proc/&lt;pid&gt;/exe symlink points to */
<span class="p_header">diff --git a/kernel/exit.c b/kernel/exit.c</span>
<span class="p_header">index 185752a729f6..339554612677 100644</span>
<span class="p_header">--- a/kernel/exit.c</span>
<span class="p_header">+++ b/kernel/exit.c</span>
<span class="p_chunk">@@ -292,94 +292,6 @@</span> <span class="p_context"> kill_orphaned_pgrp(struct task_struct *tsk, struct task_struct *parent)</span>
 	}
 }
 
<span class="p_del">-#ifdef CONFIG_MEMCG</span>
<span class="p_del">-/*</span>
<span class="p_del">- * A task is exiting.   If it owned this mm, find a new owner for the mm.</span>
<span class="p_del">- */</span>
<span class="p_del">-void mm_update_next_owner(struct mm_struct *mm)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct task_struct *c, *g, *p = current;</span>
<span class="p_del">-</span>
<span class="p_del">-retry:</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * If the exiting or execing task is not the owner, it&#39;s</span>
<span class="p_del">-	 * someone else&#39;s problem.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	if (mm-&gt;owner != p)</span>
<span class="p_del">-		return;</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * The current owner is exiting/execing and there are no other</span>
<span class="p_del">-	 * candidates.  Do not leave the mm pointing to a possibly</span>
<span class="p_del">-	 * freed task structure.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	if (atomic_read(&amp;mm-&gt;mm_users) &lt;= 1) {</span>
<span class="p_del">-		mm-&gt;owner = NULL;</span>
<span class="p_del">-		return;</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	read_lock(&amp;tasklist_lock);</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Search in the children</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	list_for_each_entry(c, &amp;p-&gt;children, sibling) {</span>
<span class="p_del">-		if (c-&gt;mm == mm)</span>
<span class="p_del">-			goto assign_new_owner;</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Search in the siblings</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	list_for_each_entry(c, &amp;p-&gt;real_parent-&gt;children, sibling) {</span>
<span class="p_del">-		if (c-&gt;mm == mm)</span>
<span class="p_del">-			goto assign_new_owner;</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Search through everything else, we should not get here often.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	for_each_process(g) {</span>
<span class="p_del">-		if (g-&gt;flags &amp; PF_KTHREAD)</span>
<span class="p_del">-			continue;</span>
<span class="p_del">-		for_each_thread(g, c) {</span>
<span class="p_del">-			if (c-&gt;mm == mm)</span>
<span class="p_del">-				goto assign_new_owner;</span>
<span class="p_del">-			if (c-&gt;mm)</span>
<span class="p_del">-				break;</span>
<span class="p_del">-		}</span>
<span class="p_del">-	}</span>
<span class="p_del">-	read_unlock(&amp;tasklist_lock);</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * We found no owner yet mm_users &gt; 1: this implies that we are</span>
<span class="p_del">-	 * most likely racing with swapoff (try_to_unuse()) or /proc or</span>
<span class="p_del">-	 * ptrace or page migration (get_task_mm()).  Mark owner as NULL.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	mm-&gt;owner = NULL;</span>
<span class="p_del">-	return;</span>
<span class="p_del">-</span>
<span class="p_del">-assign_new_owner:</span>
<span class="p_del">-	BUG_ON(c == p);</span>
<span class="p_del">-	get_task_struct(c);</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * The task_lock protects c-&gt;mm from changing.</span>
<span class="p_del">-	 * We always want mm-&gt;owner-&gt;mm == mm</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	task_lock(c);</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Delay read_unlock() till we have the task_lock()</span>
<span class="p_del">-	 * to ensure that c does not slip away underneath us</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	read_unlock(&amp;tasklist_lock);</span>
<span class="p_del">-	if (c-&gt;mm != mm) {</span>
<span class="p_del">-		task_unlock(c);</span>
<span class="p_del">-		put_task_struct(c);</span>
<span class="p_del">-		goto retry;</span>
<span class="p_del">-	}</span>
<span class="p_del">-	mm-&gt;owner = c;</span>
<span class="p_del">-	task_unlock(c);</span>
<span class="p_del">-	put_task_struct(c);</span>
<span class="p_del">-}</span>
<span class="p_del">-#endif /* CONFIG_MEMCG */</span>
<span class="p_del">-</span>
 /*
  * Turn us into a lazy TLB process if we
  * aren&#39;t already..
<span class="p_chunk">@@ -433,7 +345,6 @@</span> <span class="p_context"> static void exit_mm(struct task_struct *tsk)</span>
 	up_read(&amp;mm-&gt;mmap_sem);
 	enter_lazy_tlb(mm, current);
 	task_unlock(tsk);
<span class="p_del">-	mm_update_next_owner(mm);</span>
 	mmput(mm);
 	if (test_thread_flag(TIF_MEMDIE))
 		exit_oom_victim();
<span class="p_header">diff --git a/kernel/fork.c b/kernel/fork.c</span>
<span class="p_header">index 16e0f872f084..d073b6249d98 100644</span>
<span class="p_header">--- a/kernel/fork.c</span>
<span class="p_header">+++ b/kernel/fork.c</span>
<span class="p_chunk">@@ -570,13 +570,6 @@</span> <span class="p_context"> static void mm_init_aio(struct mm_struct *mm)</span>
 #endif
 }
 
<span class="p_del">-static void mm_init_owner(struct mm_struct *mm, struct task_struct *p)</span>
<span class="p_del">-{</span>
<span class="p_del">-#ifdef CONFIG_MEMCG</span>
<span class="p_del">-	mm-&gt;owner = p;</span>
<span class="p_del">-#endif</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 static struct mm_struct *mm_init(struct mm_struct *mm, struct task_struct *p)
 {
 	mm-&gt;mmap = NULL;
<span class="p_chunk">@@ -596,7 +589,6 @@</span> <span class="p_context"> static struct mm_struct *mm_init(struct mm_struct *mm, struct task_struct *p)</span>
 	spin_lock_init(&amp;mm-&gt;page_table_lock);
 	mm_init_cpumask(mm);
 	mm_init_aio(mm);
<span class="p_del">-	mm_init_owner(mm, p);</span>
 	mmu_notifier_mm_init(mm);
 	clear_tlb_flush_pending(mm);
 #if defined(CONFIG_TRANSPARENT_HUGEPAGE) &amp;&amp; !USE_SPLIT_PMD_PTLOCKS
<span class="p_chunk">@@ -702,6 +694,7 @@</span> <span class="p_context"> void mmput(struct mm_struct *mm)</span>
 		}
 		if (mm-&gt;binfmt)
 			module_put(mm-&gt;binfmt-&gt;module);
<span class="p_add">+		mm_drop_memcg(mm);</span>
 		mmdrop(mm);
 	}
 }
<span class="p_chunk">@@ -926,6 +919,7 @@</span> <span class="p_context"> static struct mm_struct *dup_mm(struct task_struct *tsk)</span>
 	if (mm-&gt;binfmt &amp;&amp; !try_module_get(mm-&gt;binfmt-&gt;module))
 		goto free_pt;
 
<span class="p_add">+	mm_inherit_memcg(mm, oldmm);</span>
 	return mm;
 
 free_pt:
<span class="p_header">diff --git a/mm/debug.c b/mm/debug.c</span>
<span class="p_header">index 3eb3ac2fcee7..d0347a168651 100644</span>
<span class="p_header">--- a/mm/debug.c</span>
<span class="p_header">+++ b/mm/debug.c</span>
<span class="p_chunk">@@ -184,7 +184,7 @@</span> <span class="p_context"> void dump_mm(const struct mm_struct *mm)</span>
 		&quot;ioctx_table %p\n&quot;
 #endif
 #ifdef CONFIG_MEMCG
<span class="p_del">-		&quot;owner %p &quot;</span>
<span class="p_add">+		&quot;memcg %p &quot;</span>
 #endif
 		&quot;exe_file %p\n&quot;
 #ifdef CONFIG_MMU_NOTIFIER
<span class="p_chunk">@@ -218,7 +218,7 @@</span> <span class="p_context"> void dump_mm(const struct mm_struct *mm)</span>
 		mm-&gt;ioctx_table,
 #endif
 #ifdef CONFIG_MEMCG
<span class="p_del">-		mm-&gt;owner,</span>
<span class="p_add">+		mm-&gt;memcg,</span>
 #endif
 		mm-&gt;exe_file,
 #ifdef CONFIG_MMU_NOTIFIER
<span class="p_header">diff --git a/mm/memcontrol.c b/mm/memcontrol.c</span>
<span class="p_header">index 19ffae804076..4069ec8f52be 100644</span>
<span class="p_header">--- a/mm/memcontrol.c</span>
<span class="p_header">+++ b/mm/memcontrol.c</span>
<span class="p_chunk">@@ -294,6 +294,18 @@</span> <span class="p_context"> static inline struct mem_cgroup *mem_cgroup_from_id(unsigned short id)</span>
 	return mem_cgroup_from_css(css);
 }
 
<span class="p_add">+static struct mem_cgroup *mem_cgroup_from_task(struct task_struct *p)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (p-&gt;mm)</span>
<span class="p_add">+		return rcu_dereference(p-&gt;mm-&gt;memcg);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If the process doesn&#39;t have mm struct anymore we have to fallback</span>
<span class="p_add">+	 * to the task_css.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	return mem_cgroup_from_css(task_css(p, memory_cgrp_id));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /* Writing them here to avoid exposing memcg&#39;s inner layout */
 #if defined(CONFIG_INET) &amp;&amp; defined(CONFIG_MEMCG_KMEM)
 
<span class="p_chunk">@@ -783,19 +795,6 @@</span> <span class="p_context"> static void memcg_check_events(struct mem_cgroup *memcg, struct page *page)</span>
 	}
 }
 
<span class="p_del">-struct mem_cgroup *mem_cgroup_from_task(struct task_struct *p)</span>
<span class="p_del">-{</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * mm_update_next_owner() may clear mm-&gt;owner to NULL</span>
<span class="p_del">-	 * if it races with swapoff, page migration, etc.</span>
<span class="p_del">-	 * So this can be called with p == NULL.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	if (unlikely(!p))</span>
<span class="p_del">-		return NULL;</span>
<span class="p_del">-</span>
<span class="p_del">-	return mem_cgroup_from_css(task_css(p, memory_cgrp_id));</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 static struct mem_cgroup *get_mem_cgroup_from_mm(struct mm_struct *mm)
 {
 	struct mem_cgroup *memcg = NULL;
<span class="p_chunk">@@ -810,7 +809,7 @@</span> <span class="p_context"> static struct mem_cgroup *get_mem_cgroup_from_mm(struct mm_struct *mm)</span>
 		if (unlikely(!mm))
 			memcg = root_mem_cgroup;
 		else {
<span class="p_del">-			memcg = mem_cgroup_from_task(rcu_dereference(mm-&gt;owner));</span>
<span class="p_add">+			memcg = rcu_dereference(mm-&gt;memcg);</span>
 			if (unlikely(!memcg))
 				memcg = root_mem_cgroup;
 		}
<span class="p_chunk">@@ -2286,7 +2285,7 @@</span> <span class="p_context"> void __memcg_kmem_put_cache(struct kmem_cache *cachep)</span>
 }
 
 /*
<span class="p_del">- * We need to verify if the allocation against current-&gt;mm-&gt;owner&#39;s memcg is</span>
<span class="p_add">+ * We need to verify if the allocation against current-&gt;mm-&gt;memcg is</span>
  * possible for the given order. But the page is not allocated yet, so we&#39;ll
  * need a further commit step to do the final arrangements.
  *
<span class="p_chunk">@@ -4737,7 +4736,7 @@</span> <span class="p_context"> static void mem_cgroup_clear_mc(void)</span>
 static int mem_cgroup_can_attach(struct cgroup_subsys_state *css,
 				 struct cgroup_taskset *tset)
 {
<span class="p_del">-	struct mem_cgroup *memcg = mem_cgroup_from_css(css);</span>
<span class="p_add">+	struct mem_cgroup *to = mem_cgroup_from_css(css);</span>
 	struct mem_cgroup *from;
 	struct task_struct *p;
 	struct mm_struct *mm;
<span class="p_chunk">@@ -4749,37 +4748,49 @@</span> <span class="p_context"> static int mem_cgroup_can_attach(struct cgroup_subsys_state *css,</span>
 	 * tunable will only affect upcoming migrations, not the current one.
 	 * So we need to save it, and keep it going.
 	 */
<span class="p_del">-	move_flags = READ_ONCE(memcg-&gt;move_charge_at_immigrate);</span>
<span class="p_add">+	move_flags = READ_ONCE(to-&gt;move_charge_at_immigrate);</span>
 	if (!move_flags)
 		return 0;
 
 	p = cgroup_taskset_first(tset);
<span class="p_del">-	from = mem_cgroup_from_task(p);</span>
<span class="p_del">-</span>
<span class="p_del">-	VM_BUG_ON(from == memcg);</span>
<span class="p_add">+	if (!thread_group_leader(p))</span>
<span class="p_add">+		return 0;</span>
 
 	mm = get_task_mm(p);
 	if (!mm)
 		return 0;
<span class="p_del">-	/* We move charges only when we move a owner of the mm */</span>
<span class="p_del">-	if (mm-&gt;owner == p) {</span>
<span class="p_del">-		VM_BUG_ON(mc.from);</span>
<span class="p_del">-		VM_BUG_ON(mc.to);</span>
<span class="p_del">-		VM_BUG_ON(mc.precharge);</span>
<span class="p_del">-		VM_BUG_ON(mc.moved_charge);</span>
<span class="p_del">-		VM_BUG_ON(mc.moved_swap);</span>
<span class="p_del">-</span>
<span class="p_del">-		spin_lock(&amp;mc.lock);</span>
<span class="p_del">-		mc.from = from;</span>
<span class="p_del">-		mc.to = memcg;</span>
<span class="p_del">-		mc.flags = move_flags;</span>
<span class="p_del">-		spin_unlock(&amp;mc.lock);</span>
<span class="p_del">-		/* We set mc.moving_task later */</span>
<span class="p_del">-</span>
<span class="p_del">-		ret = mem_cgroup_precharge_mc(mm);</span>
<span class="p_del">-		if (ret)</span>
<span class="p_del">-			mem_cgroup_clear_mc();</span>
<span class="p_del">-	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * tasks&#39; cgroup might be different from the one p-&gt;mm is associated</span>
<span class="p_add">+	 * with because CLONE_VM is allowed without CLONE_THREAD. The task is</span>
<span class="p_add">+	 * moving so we have to migrate from the memcg associated with its</span>
<span class="p_add">+	 * address space.</span>
<span class="p_add">+	 * No need to take a reference here because the memcg is pinned by the</span>
<span class="p_add">+	 * mm_struct.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	from = READ_ONCE(mm-&gt;memcg);</span>
<span class="p_add">+	if (!from)</span>
<span class="p_add">+		from = root_mem_cgroup;</span>
<span class="p_add">+	if (from == to)</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+</span>
<span class="p_add">+	VM_BUG_ON(mc.from);</span>
<span class="p_add">+	VM_BUG_ON(mc.to);</span>
<span class="p_add">+	VM_BUG_ON(mc.precharge);</span>
<span class="p_add">+	VM_BUG_ON(mc.moved_charge);</span>
<span class="p_add">+	VM_BUG_ON(mc.moved_swap);</span>
<span class="p_add">+</span>
<span class="p_add">+	spin_lock(&amp;mc.lock);</span>
<span class="p_add">+	mc.from = from;</span>
<span class="p_add">+	mc.to = to;</span>
<span class="p_add">+	mc.flags = move_flags;</span>
<span class="p_add">+	spin_unlock(&amp;mc.lock);</span>
<span class="p_add">+	/* We set mc.moving_task later */</span>
<span class="p_add">+</span>
<span class="p_add">+	ret = mem_cgroup_precharge_mc(mm);</span>
<span class="p_add">+	if (ret)</span>
<span class="p_add">+		mem_cgroup_clear_mc();</span>
<span class="p_add">+out:</span>
 	mmput(mm);
 	return ret;
 }
<span class="p_chunk">@@ -4932,14 +4943,26 @@</span> <span class="p_context"> static void mem_cgroup_move_task(struct cgroup_subsys_state *css,</span>
 {
 	struct task_struct *p = cgroup_taskset_first(tset);
 	struct mm_struct *mm = get_task_mm(p);
<span class="p_add">+	struct mem_cgroup *old_memcg = NULL;</span>
 
 	if (mm) {
<span class="p_add">+		old_memcg = READ_ONCE(mm-&gt;memcg);</span>
<span class="p_add">+		__mm_set_memcg(mm, mem_cgroup_from_css(css));</span>
<span class="p_add">+</span>
 		if (mc.to)
 			mem_cgroup_move_charge(mm);
 		mmput(mm);
 	}
 	if (mc.to)
 		mem_cgroup_clear_mc();
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Be careful and drop the reference only after we are done because</span>
<span class="p_add">+	 * p&#39;s task_css memcg might be different from p-&gt;memcg and nothing else</span>
<span class="p_add">+	 * might be pinning the old memcg.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (old_memcg)</span>
<span class="p_add">+		css_put(&amp;old_memcg-&gt;css);</span>
 }
 #else	/* !CONFIG_MMU */
 static int mem_cgroup_can_attach(struct cgroup_subsys_state *css,

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



