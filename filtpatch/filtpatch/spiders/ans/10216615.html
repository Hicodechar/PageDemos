
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[tip:x86/mm] x86/mm/encrypt: Move page table helpers into separate translation unit - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [tip:x86/mm] x86/mm/encrypt: Move page table helpers into separate translation unit</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=60001">tip-bot for Jacob Shin</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Feb. 13, 2018, 3:30 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;tip-1cd9c22fee3ac21db52a0997d08cf2f065d2c0c0@git.kernel.org&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10216615/mbox/"
   >mbox</a>
|
   <a href="/patch/10216615/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10216615/">/patch/10216615/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	05A9460329 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 13 Feb 2018 15:43:13 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id DEC80288EC
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 13 Feb 2018 15:43:12 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id D2F92289F7; Tue, 13 Feb 2018 15:43:12 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id C3A46288EC
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 13 Feb 2018 15:43:10 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S934298AbeBMPnF (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 13 Feb 2018 10:43:05 -0500
Received: from terminus.zytor.com ([198.137.202.136]:34689 &quot;EHLO
	terminus.zytor.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S933848AbeBMPnC (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 13 Feb 2018 10:43:02 -0500
Received: from terminus.zytor.com (localhost [127.0.0.1])
	by terminus.zytor.com (8.15.2/8.15.2) with ESMTP id w1DFUSF0005256;
	Tue, 13 Feb 2018 07:30:28 -0800
Received: (from tipbot@localhost)
	by terminus.zytor.com (8.15.2/8.15.2/Submit) id w1DFUSvX005253;
	Tue, 13 Feb 2018 07:30:28 -0800
Date: Tue, 13 Feb 2018 07:30:28 -0800
X-Authentication-Warning: terminus.zytor.com: tipbot set sender to
	tipbot@zytor.com using -f
From: &quot;tip-bot for Kirill A. Shutemov&quot; &lt;tipbot@zytor.com&gt;
Message-ID: &lt;tip-1cd9c22fee3ac21db52a0997d08cf2f065d2c0c0@git.kernel.org&gt;
Cc: tglx@linutronix.de, luto@kernel.org, hpa@zytor.com,
	thomas.lendacky@amd.com, linux-kernel@vger.kernel.org,
	bp@alien8.de, mingo@kernel.org, peterz@infradead.org,
	torvalds@linux-foundation.org, kirill.shutemov@linux.intel.com
Reply-To: bp@alien8.de, hpa@zytor.com, linux-kernel@vger.kernel.org,
	thomas.lendacky@amd.com, tglx@linutronix.de, luto@kernel.org,
	kirill.shutemov@linux.intel.com, torvalds@linux-foundation.org,
	peterz@infradead.org, mingo@kernel.org
In-Reply-To: &lt;20180131135404.40692-2-kirill.shutemov@linux.intel.com&gt;
References: &lt;20180131135404.40692-2-kirill.shutemov@linux.intel.com&gt;
To: linux-tip-commits@vger.kernel.org
Subject: [tip:x86/mm] x86/mm/encrypt: Move page table helpers into separate
	translation unit
Git-Commit-ID: 1cd9c22fee3ac21db52a0997d08cf2f065d2c0c0
X-Mailer: tip-git-log-daemon
Robot-ID: &lt;tip-bot.git.kernel.org&gt;
Robot-Unsubscribe: Contact &lt;mailto:hpa@kernel.org&gt; to get blacklisted from
	these emails
MIME-Version: 1.0
Content-Transfer-Encoding: 8bit
Content-Type: text/plain; charset=UTF-8
Content-Disposition: inline
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=60001">tip-bot for Jacob Shin</a> - Feb. 13, 2018, 3:30 p.m.</div>
<pre class="content">
Commit-ID:  1cd9c22fee3ac21db52a0997d08cf2f065d2c0c0
Gitweb:     https://git.kernel.org/tip/1cd9c22fee3ac21db52a0997d08cf2f065d2c0c0
Author:     Kirill A. Shutemov &lt;kirill.shutemov@linux.intel.com&gt;
AuthorDate: Wed, 31 Jan 2018 16:54:02 +0300
Committer:  Ingo Molnar &lt;mingo@kernel.org&gt;
CommitDate: Tue, 13 Feb 2018 15:59:48 +0100

x86/mm/encrypt: Move page table helpers into separate translation unit

There are bunch of functions in mem_encrypt.c that operate on the
identity mapping, which means they want virtual addresses to be equal to
physical one, without PAGE_OFFSET shift.

We also need to avoid paravirtualizaion call there.

Getting this done is tricky. We cannot use usual page table helpers.
It forces us to open-code a lot of things. It makes code ugly and hard
to modify.

We can get it work with the page table helpers, but it requires few
preprocessor tricks. These tricks may have side effects for the rest of
the file.

Let&#39;s isolate such functions into own translation unit.
<span class="tested-by">
Tested-by: Tom Lendacky &lt;thomas.lendacky@amd.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Kirill A. Shutemov &lt;kirill.shutemov@linux.intel.com&gt;</span>
<span class="reviewed-by">Reviewed-by: Tom Lendacky &lt;thomas.lendacky@amd.com&gt;</span>
Cc: Andy Lutomirski &lt;luto@kernel.org&gt;
Cc: Borislav Petkov &lt;bp@alien8.de&gt;
Cc: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;
Cc: Peter Zijlstra &lt;peterz@infradead.org&gt;
Cc: Thomas Gleixner &lt;tglx@linutronix.de&gt;
Cc: linux-mm@kvack.org
Link: http://lkml.kernel.org/r/20180131135404.40692-2-kirill.shutemov@linux.intel.com
<span class="signed-off-by">Signed-off-by: Ingo Molnar &lt;mingo@kernel.org&gt;</span>
---
 arch/x86/include/asm/mem_encrypt.h                 |   1 +
 arch/x86/mm/Makefile                               |  14 +-
 arch/x86/mm/mem_encrypt.c                          | 578 +--------------------
 .../mm/{mem_encrypt.c =&gt; mem_encrypt_identity.c}   | 479 +----------------
 4 files changed, 31 insertions(+), 1041 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/include/asm/mem_encrypt.h b/arch/x86/include/asm/mem_encrypt.h</span>
<span class="p_header">index 22c5f3e..8fe61ad 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/mem_encrypt.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/mem_encrypt.h</span>
<span class="p_chunk">@@ -22,6 +22,7 @@</span> <span class="p_context"></span>
 #ifdef CONFIG_AMD_MEM_ENCRYPT
 
 extern u64 sme_me_mask;
<span class="p_add">+extern bool sev_enabled;</span>
 
 void sme_encrypt_execute(unsigned long encrypted_kernel_vaddr,
 			 unsigned long decrypted_kernel_vaddr,
<span class="p_header">diff --git a/arch/x86/mm/Makefile b/arch/x86/mm/Makefile</span>
<span class="p_header">index 27e9e90..03c6c85 100644</span>
<span class="p_header">--- a/arch/x86/mm/Makefile</span>
<span class="p_header">+++ b/arch/x86/mm/Makefile</span>
<span class="p_chunk">@@ -1,12 +1,15 @@</span> <span class="p_context"></span>
 # SPDX-License-Identifier: GPL-2.0
<span class="p_del">-# Kernel does not boot with instrumentation of tlb.c and mem_encrypt.c</span>
<span class="p_del">-KCOV_INSTRUMENT_tlb.o		:= n</span>
<span class="p_del">-KCOV_INSTRUMENT_mem_encrypt.o	:= n</span>
<span class="p_add">+# Kernel does not boot with instrumentation of tlb.c and mem_encrypt*.c</span>
<span class="p_add">+KCOV_INSTRUMENT_tlb.o			:= n</span>
<span class="p_add">+KCOV_INSTRUMENT_mem_encrypt.o		:= n</span>
<span class="p_add">+KCOV_INSTRUMENT_mem_encrypt_identity.o	:= n</span>
 
<span class="p_del">-KASAN_SANITIZE_mem_encrypt.o	:= n</span>
<span class="p_add">+KASAN_SANITIZE_mem_encrypt.o		:= n</span>
<span class="p_add">+KASAN_SANITIZE_mem_encrypt_identity.o	:= n</span>
 
 ifdef CONFIG_FUNCTION_TRACER
<span class="p_del">-CFLAGS_REMOVE_mem_encrypt.o	= -pg</span>
<span class="p_add">+CFLAGS_REMOVE_mem_encrypt.o		= -pg</span>
<span class="p_add">+CFLAGS_REMOVE_mem_encrypt_identity.o	= -pg</span>
 endif
 
 obj-y	:=  init.o init_$(BITS).o fault.o ioremap.o extable.o pageattr.o mmap.o \
<span class="p_chunk">@@ -47,4 +50,5 @@</span> <span class="p_context"> obj-$(CONFIG_RANDOMIZE_MEMORY)			+= kaslr.o</span>
 obj-$(CONFIG_PAGE_TABLE_ISOLATION)		+= pti.o
 
 obj-$(CONFIG_AMD_MEM_ENCRYPT)	+= mem_encrypt.o
<span class="p_add">+obj-$(CONFIG_AMD_MEM_ENCRYPT)	+= mem_encrypt_identity.o</span>
 obj-$(CONFIG_AMD_MEM_ENCRYPT)	+= mem_encrypt_boot.o
<span class="p_header">diff --git a/arch/x86/mm/mem_encrypt.c b/arch/x86/mm/mem_encrypt.c</span>
<span class="p_header">index 1a53071..3a1b5fe 100644</span>
<span class="p_header">--- a/arch/x86/mm/mem_encrypt.c</span>
<span class="p_header">+++ b/arch/x86/mm/mem_encrypt.c</span>
<span class="p_chunk">@@ -25,17 +25,12 @@</span> <span class="p_context"></span>
 #include &lt;asm/bootparam.h&gt;
 #include &lt;asm/set_memory.h&gt;
 #include &lt;asm/cacheflush.h&gt;
<span class="p_del">-#include &lt;asm/sections.h&gt;</span>
 #include &lt;asm/processor-flags.h&gt;
 #include &lt;asm/msr.h&gt;
 #include &lt;asm/cmdline.h&gt;
 
 #include &quot;mm_internal.h&quot;
 
<span class="p_del">-static char sme_cmdline_arg[] __initdata = &quot;mem_encrypt&quot;;</span>
<span class="p_del">-static char sme_cmdline_on[]  __initdata = &quot;on&quot;;</span>
<span class="p_del">-static char sme_cmdline_off[] __initdata = &quot;off&quot;;</span>
<span class="p_del">-</span>
 /*
  * Since SME related variables are set early in the boot process they must
  * reside in the .data section so as not to be zeroed out when the .bss
<span class="p_chunk">@@ -46,7 +41,7 @@</span> <span class="p_context"> EXPORT_SYMBOL(sme_me_mask);</span>
 DEFINE_STATIC_KEY_FALSE(sev_enable_key);
 EXPORT_SYMBOL_GPL(sev_enable_key);
 
<span class="p_del">-static bool sev_enabled __section(.data);</span>
<span class="p_add">+bool sev_enabled __section(.data);</span>
 
 /* Buffer used for early in-place encryption by BSP, no locking needed */
 static char sme_early_buffer[PAGE_SIZE] __aligned(PAGE_SIZE);
<span class="p_chunk">@@ -463,574 +458,3 @@</span> <span class="p_context"> void swiotlb_set_mem_attributes(void *vaddr, unsigned long size)</span>
 	/* Make the SWIOTLB buffer area decrypted */
 	set_memory_decrypted((unsigned long)vaddr, size &gt;&gt; PAGE_SHIFT);
 }
<span class="p_del">-</span>
<span class="p_del">-struct sme_populate_pgd_data {</span>
<span class="p_del">-	void	*pgtable_area;</span>
<span class="p_del">-	pgd_t	*pgd;</span>
<span class="p_del">-</span>
<span class="p_del">-	pmdval_t pmd_flags;</span>
<span class="p_del">-	pteval_t pte_flags;</span>
<span class="p_del">-	unsigned long paddr;</span>
<span class="p_del">-</span>
<span class="p_del">-	unsigned long vaddr;</span>
<span class="p_del">-	unsigned long vaddr_end;</span>
<span class="p_del">-};</span>
<span class="p_del">-</span>
<span class="p_del">-static void __init sme_clear_pgd(struct sme_populate_pgd_data *ppd)</span>
<span class="p_del">-{</span>
<span class="p_del">-	unsigned long pgd_start, pgd_end, pgd_size;</span>
<span class="p_del">-	pgd_t *pgd_p;</span>
<span class="p_del">-</span>
<span class="p_del">-	pgd_start = ppd-&gt;vaddr &amp; PGDIR_MASK;</span>
<span class="p_del">-	pgd_end = ppd-&gt;vaddr_end &amp; PGDIR_MASK;</span>
<span class="p_del">-</span>
<span class="p_del">-	pgd_size = (((pgd_end - pgd_start) / PGDIR_SIZE) + 1) * sizeof(pgd_t);</span>
<span class="p_del">-</span>
<span class="p_del">-	pgd_p = ppd-&gt;pgd + pgd_index(ppd-&gt;vaddr);</span>
<span class="p_del">-</span>
<span class="p_del">-	memset(pgd_p, 0, pgd_size);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-#define PGD_FLAGS		_KERNPG_TABLE_NOENC</span>
<span class="p_del">-#define P4D_FLAGS		_KERNPG_TABLE_NOENC</span>
<span class="p_del">-#define PUD_FLAGS		_KERNPG_TABLE_NOENC</span>
<span class="p_del">-#define PMD_FLAGS		_KERNPG_TABLE_NOENC</span>
<span class="p_del">-</span>
<span class="p_del">-#define PMD_FLAGS_LARGE		(__PAGE_KERNEL_LARGE_EXEC &amp; ~_PAGE_GLOBAL)</span>
<span class="p_del">-</span>
<span class="p_del">-#define PMD_FLAGS_DEC		PMD_FLAGS_LARGE</span>
<span class="p_del">-#define PMD_FLAGS_DEC_WP	((PMD_FLAGS_DEC &amp; ~_PAGE_CACHE_MASK) | \</span>
<span class="p_del">-				 (_PAGE_PAT | _PAGE_PWT))</span>
<span class="p_del">-</span>
<span class="p_del">-#define PMD_FLAGS_ENC		(PMD_FLAGS_LARGE | _PAGE_ENC)</span>
<span class="p_del">-</span>
<span class="p_del">-#define PTE_FLAGS		(__PAGE_KERNEL_EXEC &amp; ~_PAGE_GLOBAL)</span>
<span class="p_del">-</span>
<span class="p_del">-#define PTE_FLAGS_DEC		PTE_FLAGS</span>
<span class="p_del">-#define PTE_FLAGS_DEC_WP	((PTE_FLAGS_DEC &amp; ~_PAGE_CACHE_MASK) | \</span>
<span class="p_del">-				 (_PAGE_PAT | _PAGE_PWT))</span>
<span class="p_del">-</span>
<span class="p_del">-#define PTE_FLAGS_ENC		(PTE_FLAGS | _PAGE_ENC)</span>
<span class="p_del">-</span>
<span class="p_del">-static pmd_t __init *sme_prepare_pgd(struct sme_populate_pgd_data *ppd)</span>
<span class="p_del">-{</span>
<span class="p_del">-	pgd_t *pgd_p;</span>
<span class="p_del">-	p4d_t *p4d_p;</span>
<span class="p_del">-	pud_t *pud_p;</span>
<span class="p_del">-	pmd_t *pmd_p;</span>
<span class="p_del">-</span>
<span class="p_del">-	pgd_p = ppd-&gt;pgd + pgd_index(ppd-&gt;vaddr);</span>
<span class="p_del">-	if (native_pgd_val(*pgd_p)) {</span>
<span class="p_del">-		if (IS_ENABLED(CONFIG_X86_5LEVEL))</span>
<span class="p_del">-			p4d_p = (p4d_t *)(native_pgd_val(*pgd_p) &amp; ~PTE_FLAGS_MASK);</span>
<span class="p_del">-		else</span>
<span class="p_del">-			pud_p = (pud_t *)(native_pgd_val(*pgd_p) &amp; ~PTE_FLAGS_MASK);</span>
<span class="p_del">-	} else {</span>
<span class="p_del">-		pgd_t pgd;</span>
<span class="p_del">-</span>
<span class="p_del">-		if (IS_ENABLED(CONFIG_X86_5LEVEL)) {</span>
<span class="p_del">-			p4d_p = ppd-&gt;pgtable_area;</span>
<span class="p_del">-			memset(p4d_p, 0, sizeof(*p4d_p) * PTRS_PER_P4D);</span>
<span class="p_del">-			ppd-&gt;pgtable_area += sizeof(*p4d_p) * PTRS_PER_P4D;</span>
<span class="p_del">-</span>
<span class="p_del">-			pgd = native_make_pgd((pgdval_t)p4d_p + PGD_FLAGS);</span>
<span class="p_del">-		} else {</span>
<span class="p_del">-			pud_p = ppd-&gt;pgtable_area;</span>
<span class="p_del">-			memset(pud_p, 0, sizeof(*pud_p) * PTRS_PER_PUD);</span>
<span class="p_del">-			ppd-&gt;pgtable_area += sizeof(*pud_p) * PTRS_PER_PUD;</span>
<span class="p_del">-</span>
<span class="p_del">-			pgd = native_make_pgd((pgdval_t)pud_p + PGD_FLAGS);</span>
<span class="p_del">-		}</span>
<span class="p_del">-		native_set_pgd(pgd_p, pgd);</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	if (IS_ENABLED(CONFIG_X86_5LEVEL)) {</span>
<span class="p_del">-		p4d_p += p4d_index(ppd-&gt;vaddr);</span>
<span class="p_del">-		if (native_p4d_val(*p4d_p)) {</span>
<span class="p_del">-			pud_p = (pud_t *)(native_p4d_val(*p4d_p) &amp; ~PTE_FLAGS_MASK);</span>
<span class="p_del">-		} else {</span>
<span class="p_del">-			p4d_t p4d;</span>
<span class="p_del">-</span>
<span class="p_del">-			pud_p = ppd-&gt;pgtable_area;</span>
<span class="p_del">-			memset(pud_p, 0, sizeof(*pud_p) * PTRS_PER_PUD);</span>
<span class="p_del">-			ppd-&gt;pgtable_area += sizeof(*pud_p) * PTRS_PER_PUD;</span>
<span class="p_del">-</span>
<span class="p_del">-			p4d = native_make_p4d((pudval_t)pud_p + P4D_FLAGS);</span>
<span class="p_del">-			native_set_p4d(p4d_p, p4d);</span>
<span class="p_del">-		}</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	pud_p += pud_index(ppd-&gt;vaddr);</span>
<span class="p_del">-	if (native_pud_val(*pud_p)) {</span>
<span class="p_del">-		if (native_pud_val(*pud_p) &amp; _PAGE_PSE)</span>
<span class="p_del">-			return NULL;</span>
<span class="p_del">-</span>
<span class="p_del">-		pmd_p = (pmd_t *)(native_pud_val(*pud_p) &amp; ~PTE_FLAGS_MASK);</span>
<span class="p_del">-	} else {</span>
<span class="p_del">-		pud_t pud;</span>
<span class="p_del">-</span>
<span class="p_del">-		pmd_p = ppd-&gt;pgtable_area;</span>
<span class="p_del">-		memset(pmd_p, 0, sizeof(*pmd_p) * PTRS_PER_PMD);</span>
<span class="p_del">-		ppd-&gt;pgtable_area += sizeof(*pmd_p) * PTRS_PER_PMD;</span>
<span class="p_del">-</span>
<span class="p_del">-		pud = native_make_pud((pmdval_t)pmd_p + PUD_FLAGS);</span>
<span class="p_del">-		native_set_pud(pud_p, pud);</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	return pmd_p;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static void __init sme_populate_pgd_large(struct sme_populate_pgd_data *ppd)</span>
<span class="p_del">-{</span>
<span class="p_del">-	pmd_t *pmd_p;</span>
<span class="p_del">-</span>
<span class="p_del">-	pmd_p = sme_prepare_pgd(ppd);</span>
<span class="p_del">-	if (!pmd_p)</span>
<span class="p_del">-		return;</span>
<span class="p_del">-</span>
<span class="p_del">-	pmd_p += pmd_index(ppd-&gt;vaddr);</span>
<span class="p_del">-	if (!native_pmd_val(*pmd_p) || !(native_pmd_val(*pmd_p) &amp; _PAGE_PSE))</span>
<span class="p_del">-		native_set_pmd(pmd_p, native_make_pmd(ppd-&gt;paddr | ppd-&gt;pmd_flags));</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static void __init sme_populate_pgd(struct sme_populate_pgd_data *ppd)</span>
<span class="p_del">-{</span>
<span class="p_del">-	pmd_t *pmd_p;</span>
<span class="p_del">-	pte_t *pte_p;</span>
<span class="p_del">-</span>
<span class="p_del">-	pmd_p = sme_prepare_pgd(ppd);</span>
<span class="p_del">-	if (!pmd_p)</span>
<span class="p_del">-		return;</span>
<span class="p_del">-</span>
<span class="p_del">-	pmd_p += pmd_index(ppd-&gt;vaddr);</span>
<span class="p_del">-	if (native_pmd_val(*pmd_p)) {</span>
<span class="p_del">-		if (native_pmd_val(*pmd_p) &amp; _PAGE_PSE)</span>
<span class="p_del">-			return;</span>
<span class="p_del">-</span>
<span class="p_del">-		pte_p = (pte_t *)(native_pmd_val(*pmd_p) &amp; ~PTE_FLAGS_MASK);</span>
<span class="p_del">-	} else {</span>
<span class="p_del">-		pmd_t pmd;</span>
<span class="p_del">-</span>
<span class="p_del">-		pte_p = ppd-&gt;pgtable_area;</span>
<span class="p_del">-		memset(pte_p, 0, sizeof(*pte_p) * PTRS_PER_PTE);</span>
<span class="p_del">-		ppd-&gt;pgtable_area += sizeof(*pte_p) * PTRS_PER_PTE;</span>
<span class="p_del">-</span>
<span class="p_del">-		pmd = native_make_pmd((pteval_t)pte_p + PMD_FLAGS);</span>
<span class="p_del">-		native_set_pmd(pmd_p, pmd);</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	pte_p += pte_index(ppd-&gt;vaddr);</span>
<span class="p_del">-	if (!native_pte_val(*pte_p))</span>
<span class="p_del">-		native_set_pte(pte_p, native_make_pte(ppd-&gt;paddr | ppd-&gt;pte_flags));</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static void __init __sme_map_range_pmd(struct sme_populate_pgd_data *ppd)</span>
<span class="p_del">-{</span>
<span class="p_del">-	while (ppd-&gt;vaddr &lt; ppd-&gt;vaddr_end) {</span>
<span class="p_del">-		sme_populate_pgd_large(ppd);</span>
<span class="p_del">-</span>
<span class="p_del">-		ppd-&gt;vaddr += PMD_PAGE_SIZE;</span>
<span class="p_del">-		ppd-&gt;paddr += PMD_PAGE_SIZE;</span>
<span class="p_del">-	}</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static void __init __sme_map_range_pte(struct sme_populate_pgd_data *ppd)</span>
<span class="p_del">-{</span>
<span class="p_del">-	while (ppd-&gt;vaddr &lt; ppd-&gt;vaddr_end) {</span>
<span class="p_del">-		sme_populate_pgd(ppd);</span>
<span class="p_del">-</span>
<span class="p_del">-		ppd-&gt;vaddr += PAGE_SIZE;</span>
<span class="p_del">-		ppd-&gt;paddr += PAGE_SIZE;</span>
<span class="p_del">-	}</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static void __init __sme_map_range(struct sme_populate_pgd_data *ppd,</span>
<span class="p_del">-				   pmdval_t pmd_flags, pteval_t pte_flags)</span>
<span class="p_del">-{</span>
<span class="p_del">-	unsigned long vaddr_end;</span>
<span class="p_del">-</span>
<span class="p_del">-	ppd-&gt;pmd_flags = pmd_flags;</span>
<span class="p_del">-	ppd-&gt;pte_flags = pte_flags;</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Save original end value since we modify the struct value */</span>
<span class="p_del">-	vaddr_end = ppd-&gt;vaddr_end;</span>
<span class="p_del">-</span>
<span class="p_del">-	/* If start is not 2MB aligned, create PTE entries */</span>
<span class="p_del">-	ppd-&gt;vaddr_end = ALIGN(ppd-&gt;vaddr, PMD_PAGE_SIZE);</span>
<span class="p_del">-	__sme_map_range_pte(ppd);</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Create PMD entries */</span>
<span class="p_del">-	ppd-&gt;vaddr_end = vaddr_end &amp; PMD_PAGE_MASK;</span>
<span class="p_del">-	__sme_map_range_pmd(ppd);</span>
<span class="p_del">-</span>
<span class="p_del">-	/* If end is not 2MB aligned, create PTE entries */</span>
<span class="p_del">-	ppd-&gt;vaddr_end = vaddr_end;</span>
<span class="p_del">-	__sme_map_range_pte(ppd);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static void __init sme_map_range_encrypted(struct sme_populate_pgd_data *ppd)</span>
<span class="p_del">-{</span>
<span class="p_del">-	__sme_map_range(ppd, PMD_FLAGS_ENC, PTE_FLAGS_ENC);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static void __init sme_map_range_decrypted(struct sme_populate_pgd_data *ppd)</span>
<span class="p_del">-{</span>
<span class="p_del">-	__sme_map_range(ppd, PMD_FLAGS_DEC, PTE_FLAGS_DEC);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static void __init sme_map_range_decrypted_wp(struct sme_populate_pgd_data *ppd)</span>
<span class="p_del">-{</span>
<span class="p_del">-	__sme_map_range(ppd, PMD_FLAGS_DEC_WP, PTE_FLAGS_DEC_WP);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static unsigned long __init sme_pgtable_calc(unsigned long len)</span>
<span class="p_del">-{</span>
<span class="p_del">-	unsigned long p4d_size, pud_size, pmd_size, pte_size;</span>
<span class="p_del">-	unsigned long total;</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Perform a relatively simplistic calculation of the pagetable</span>
<span class="p_del">-	 * entries that are needed. Those mappings will be covered mostly</span>
<span class="p_del">-	 * by 2MB PMD entries so we can conservatively calculate the required</span>
<span class="p_del">-	 * number of P4D, PUD and PMD structures needed to perform the</span>
<span class="p_del">-	 * mappings.  For mappings that are not 2MB aligned, PTE mappings</span>
<span class="p_del">-	 * would be needed for the start and end portion of the address range</span>
<span class="p_del">-	 * that fall outside of the 2MB alignment.  This results in, at most,</span>
<span class="p_del">-	 * two extra pages to hold PTE entries for each range that is mapped.</span>
<span class="p_del">-	 * Incrementing the count for each covers the case where the addresses</span>
<span class="p_del">-	 * cross entries.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	if (IS_ENABLED(CONFIG_X86_5LEVEL)) {</span>
<span class="p_del">-		p4d_size = (ALIGN(len, PGDIR_SIZE) / PGDIR_SIZE) + 1;</span>
<span class="p_del">-		p4d_size *= sizeof(p4d_t) * PTRS_PER_P4D;</span>
<span class="p_del">-		pud_size = (ALIGN(len, P4D_SIZE) / P4D_SIZE) + 1;</span>
<span class="p_del">-		pud_size *= sizeof(pud_t) * PTRS_PER_PUD;</span>
<span class="p_del">-	} else {</span>
<span class="p_del">-		p4d_size = 0;</span>
<span class="p_del">-		pud_size = (ALIGN(len, PGDIR_SIZE) / PGDIR_SIZE) + 1;</span>
<span class="p_del">-		pud_size *= sizeof(pud_t) * PTRS_PER_PUD;</span>
<span class="p_del">-	}</span>
<span class="p_del">-	pmd_size = (ALIGN(len, PUD_SIZE) / PUD_SIZE) + 1;</span>
<span class="p_del">-	pmd_size *= sizeof(pmd_t) * PTRS_PER_PMD;</span>
<span class="p_del">-	pte_size = 2 * sizeof(pte_t) * PTRS_PER_PTE;</span>
<span class="p_del">-</span>
<span class="p_del">-	total = p4d_size + pud_size + pmd_size + pte_size;</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Now calculate the added pagetable structures needed to populate</span>
<span class="p_del">-	 * the new pagetables.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	if (IS_ENABLED(CONFIG_X86_5LEVEL)) {</span>
<span class="p_del">-		p4d_size = ALIGN(total, PGDIR_SIZE) / PGDIR_SIZE;</span>
<span class="p_del">-		p4d_size *= sizeof(p4d_t) * PTRS_PER_P4D;</span>
<span class="p_del">-		pud_size = ALIGN(total, P4D_SIZE) / P4D_SIZE;</span>
<span class="p_del">-		pud_size *= sizeof(pud_t) * PTRS_PER_PUD;</span>
<span class="p_del">-	} else {</span>
<span class="p_del">-		p4d_size = 0;</span>
<span class="p_del">-		pud_size = ALIGN(total, PGDIR_SIZE) / PGDIR_SIZE;</span>
<span class="p_del">-		pud_size *= sizeof(pud_t) * PTRS_PER_PUD;</span>
<span class="p_del">-	}</span>
<span class="p_del">-	pmd_size = ALIGN(total, PUD_SIZE) / PUD_SIZE;</span>
<span class="p_del">-	pmd_size *= sizeof(pmd_t) * PTRS_PER_PMD;</span>
<span class="p_del">-</span>
<span class="p_del">-	total += p4d_size + pud_size + pmd_size;</span>
<span class="p_del">-</span>
<span class="p_del">-	return total;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-void __init __nostackprotector sme_encrypt_kernel(struct boot_params *bp)</span>
<span class="p_del">-{</span>
<span class="p_del">-	unsigned long workarea_start, workarea_end, workarea_len;</span>
<span class="p_del">-	unsigned long execute_start, execute_end, execute_len;</span>
<span class="p_del">-	unsigned long kernel_start, kernel_end, kernel_len;</span>
<span class="p_del">-	unsigned long initrd_start, initrd_end, initrd_len;</span>
<span class="p_del">-	struct sme_populate_pgd_data ppd;</span>
<span class="p_del">-	unsigned long pgtable_area_len;</span>
<span class="p_del">-	unsigned long decrypted_base;</span>
<span class="p_del">-</span>
<span class="p_del">-	if (!sme_active())</span>
<span class="p_del">-		return;</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Prepare for encrypting the kernel and initrd by building new</span>
<span class="p_del">-	 * pagetables with the necessary attributes needed to encrypt the</span>
<span class="p_del">-	 * kernel in place.</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 *   One range of virtual addresses will map the memory occupied</span>
<span class="p_del">-	 *   by the kernel and initrd as encrypted.</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 *   Another range of virtual addresses will map the memory occupied</span>
<span class="p_del">-	 *   by the kernel and initrd as decrypted and write-protected.</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 *     The use of write-protect attribute will prevent any of the</span>
<span class="p_del">-	 *     memory from being cached.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Physical addresses gives us the identity mapped virtual addresses */</span>
<span class="p_del">-	kernel_start = __pa_symbol(_text);</span>
<span class="p_del">-	kernel_end = ALIGN(__pa_symbol(_end), PMD_PAGE_SIZE);</span>
<span class="p_del">-	kernel_len = kernel_end - kernel_start;</span>
<span class="p_del">-</span>
<span class="p_del">-	initrd_start = 0;</span>
<span class="p_del">-	initrd_end = 0;</span>
<span class="p_del">-	initrd_len = 0;</span>
<span class="p_del">-#ifdef CONFIG_BLK_DEV_INITRD</span>
<span class="p_del">-	initrd_len = (unsigned long)bp-&gt;hdr.ramdisk_size |</span>
<span class="p_del">-		     ((unsigned long)bp-&gt;ext_ramdisk_size &lt;&lt; 32);</span>
<span class="p_del">-	if (initrd_len) {</span>
<span class="p_del">-		initrd_start = (unsigned long)bp-&gt;hdr.ramdisk_image |</span>
<span class="p_del">-			       ((unsigned long)bp-&gt;ext_ramdisk_image &lt;&lt; 32);</span>
<span class="p_del">-		initrd_end = PAGE_ALIGN(initrd_start + initrd_len);</span>
<span class="p_del">-		initrd_len = initrd_end - initrd_start;</span>
<span class="p_del">-	}</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Set the encryption workarea to be immediately after the kernel */</span>
<span class="p_del">-	workarea_start = kernel_end;</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Calculate required number of workarea bytes needed:</span>
<span class="p_del">-	 *   executable encryption area size:</span>
<span class="p_del">-	 *     stack page (PAGE_SIZE)</span>
<span class="p_del">-	 *     encryption routine page (PAGE_SIZE)</span>
<span class="p_del">-	 *     intermediate copy buffer (PMD_PAGE_SIZE)</span>
<span class="p_del">-	 *   pagetable structures for the encryption of the kernel</span>
<span class="p_del">-	 *   pagetable structures for workarea (in case not currently mapped)</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	execute_start = workarea_start;</span>
<span class="p_del">-	execute_end = execute_start + (PAGE_SIZE * 2) + PMD_PAGE_SIZE;</span>
<span class="p_del">-	execute_len = execute_end - execute_start;</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * One PGD for both encrypted and decrypted mappings and a set of</span>
<span class="p_del">-	 * PUDs and PMDs for each of the encrypted and decrypted mappings.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	pgtable_area_len = sizeof(pgd_t) * PTRS_PER_PGD;</span>
<span class="p_del">-	pgtable_area_len += sme_pgtable_calc(execute_end - kernel_start) * 2;</span>
<span class="p_del">-	if (initrd_len)</span>
<span class="p_del">-		pgtable_area_len += sme_pgtable_calc(initrd_len) * 2;</span>
<span class="p_del">-</span>
<span class="p_del">-	/* PUDs and PMDs needed in the current pagetables for the workarea */</span>
<span class="p_del">-	pgtable_area_len += sme_pgtable_calc(execute_len + pgtable_area_len);</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * The total workarea includes the executable encryption area and</span>
<span class="p_del">-	 * the pagetable area. The start of the workarea is already 2MB</span>
<span class="p_del">-	 * aligned, align the end of the workarea on a 2MB boundary so that</span>
<span class="p_del">-	 * we don&#39;t try to create/allocate PTE entries from the workarea</span>
<span class="p_del">-	 * before it is mapped.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	workarea_len = execute_len + pgtable_area_len;</span>
<span class="p_del">-	workarea_end = ALIGN(workarea_start + workarea_len, PMD_PAGE_SIZE);</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Set the address to the start of where newly created pagetable</span>
<span class="p_del">-	 * structures (PGDs, PUDs and PMDs) will be allocated. New pagetable</span>
<span class="p_del">-	 * structures are created when the workarea is added to the current</span>
<span class="p_del">-	 * pagetables and when the new encrypted and decrypted kernel</span>
<span class="p_del">-	 * mappings are populated.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	ppd.pgtable_area = (void *)execute_end;</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Make sure the current pagetable structure has entries for</span>
<span class="p_del">-	 * addressing the workarea.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	ppd.pgd = (pgd_t *)native_read_cr3_pa();</span>
<span class="p_del">-	ppd.paddr = workarea_start;</span>
<span class="p_del">-	ppd.vaddr = workarea_start;</span>
<span class="p_del">-	ppd.vaddr_end = workarea_end;</span>
<span class="p_del">-	sme_map_range_decrypted(&amp;ppd);</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Flush the TLB - no globals so cr3 is enough */</span>
<span class="p_del">-	native_write_cr3(__native_read_cr3());</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * A new pagetable structure is being built to allow for the kernel</span>
<span class="p_del">-	 * and initrd to be encrypted. It starts with an empty PGD that will</span>
<span class="p_del">-	 * then be populated with new PUDs and PMDs as the encrypted and</span>
<span class="p_del">-	 * decrypted kernel mappings are created.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	ppd.pgd = ppd.pgtable_area;</span>
<span class="p_del">-	memset(ppd.pgd, 0, sizeof(pgd_t) * PTRS_PER_PGD);</span>
<span class="p_del">-	ppd.pgtable_area += sizeof(pgd_t) * PTRS_PER_PGD;</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * A different PGD index/entry must be used to get different</span>
<span class="p_del">-	 * pagetable entries for the decrypted mapping. Choose the next</span>
<span class="p_del">-	 * PGD index and convert it to a virtual address to be used as</span>
<span class="p_del">-	 * the base of the mapping.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	decrypted_base = (pgd_index(workarea_end) + 1) &amp; (PTRS_PER_PGD - 1);</span>
<span class="p_del">-	if (initrd_len) {</span>
<span class="p_del">-		unsigned long check_base;</span>
<span class="p_del">-</span>
<span class="p_del">-		check_base = (pgd_index(initrd_end) + 1) &amp; (PTRS_PER_PGD - 1);</span>
<span class="p_del">-		decrypted_base = max(decrypted_base, check_base);</span>
<span class="p_del">-	}</span>
<span class="p_del">-	decrypted_base &lt;&lt;= PGDIR_SHIFT;</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Add encrypted kernel (identity) mappings */</span>
<span class="p_del">-	ppd.paddr = kernel_start;</span>
<span class="p_del">-	ppd.vaddr = kernel_start;</span>
<span class="p_del">-	ppd.vaddr_end = kernel_end;</span>
<span class="p_del">-	sme_map_range_encrypted(&amp;ppd);</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Add decrypted, write-protected kernel (non-identity) mappings */</span>
<span class="p_del">-	ppd.paddr = kernel_start;</span>
<span class="p_del">-	ppd.vaddr = kernel_start + decrypted_base;</span>
<span class="p_del">-	ppd.vaddr_end = kernel_end + decrypted_base;</span>
<span class="p_del">-	sme_map_range_decrypted_wp(&amp;ppd);</span>
<span class="p_del">-</span>
<span class="p_del">-	if (initrd_len) {</span>
<span class="p_del">-		/* Add encrypted initrd (identity) mappings */</span>
<span class="p_del">-		ppd.paddr = initrd_start;</span>
<span class="p_del">-		ppd.vaddr = initrd_start;</span>
<span class="p_del">-		ppd.vaddr_end = initrd_end;</span>
<span class="p_del">-		sme_map_range_encrypted(&amp;ppd);</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * Add decrypted, write-protected initrd (non-identity) mappings</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		ppd.paddr = initrd_start;</span>
<span class="p_del">-		ppd.vaddr = initrd_start + decrypted_base;</span>
<span class="p_del">-		ppd.vaddr_end = initrd_end + decrypted_base;</span>
<span class="p_del">-		sme_map_range_decrypted_wp(&amp;ppd);</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Add decrypted workarea mappings to both kernel mappings */</span>
<span class="p_del">-	ppd.paddr = workarea_start;</span>
<span class="p_del">-	ppd.vaddr = workarea_start;</span>
<span class="p_del">-	ppd.vaddr_end = workarea_end;</span>
<span class="p_del">-	sme_map_range_decrypted(&amp;ppd);</span>
<span class="p_del">-</span>
<span class="p_del">-	ppd.paddr = workarea_start;</span>
<span class="p_del">-	ppd.vaddr = workarea_start + decrypted_base;</span>
<span class="p_del">-	ppd.vaddr_end = workarea_end + decrypted_base;</span>
<span class="p_del">-	sme_map_range_decrypted(&amp;ppd);</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Perform the encryption */</span>
<span class="p_del">-	sme_encrypt_execute(kernel_start, kernel_start + decrypted_base,</span>
<span class="p_del">-			    kernel_len, workarea_start, (unsigned long)ppd.pgd);</span>
<span class="p_del">-</span>
<span class="p_del">-	if (initrd_len)</span>
<span class="p_del">-		sme_encrypt_execute(initrd_start, initrd_start + decrypted_base,</span>
<span class="p_del">-				    initrd_len, workarea_start,</span>
<span class="p_del">-				    (unsigned long)ppd.pgd);</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * At this point we are running encrypted.  Remove the mappings for</span>
<span class="p_del">-	 * the decrypted areas - all that is needed for this is to remove</span>
<span class="p_del">-	 * the PGD entry/entries.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	ppd.vaddr = kernel_start + decrypted_base;</span>
<span class="p_del">-	ppd.vaddr_end = kernel_end + decrypted_base;</span>
<span class="p_del">-	sme_clear_pgd(&amp;ppd);</span>
<span class="p_del">-</span>
<span class="p_del">-	if (initrd_len) {</span>
<span class="p_del">-		ppd.vaddr = initrd_start + decrypted_base;</span>
<span class="p_del">-		ppd.vaddr_end = initrd_end + decrypted_base;</span>
<span class="p_del">-		sme_clear_pgd(&amp;ppd);</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	ppd.vaddr = workarea_start + decrypted_base;</span>
<span class="p_del">-	ppd.vaddr_end = workarea_end + decrypted_base;</span>
<span class="p_del">-	sme_clear_pgd(&amp;ppd);</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Flush the TLB - no globals so cr3 is enough */</span>
<span class="p_del">-	native_write_cr3(__native_read_cr3());</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-void __init __nostackprotector sme_enable(struct boot_params *bp)</span>
<span class="p_del">-{</span>
<span class="p_del">-	const char *cmdline_ptr, *cmdline_arg, *cmdline_on, *cmdline_off;</span>
<span class="p_del">-	unsigned int eax, ebx, ecx, edx;</span>
<span class="p_del">-	unsigned long feature_mask;</span>
<span class="p_del">-	bool active_by_default;</span>
<span class="p_del">-	unsigned long me_mask;</span>
<span class="p_del">-	char buffer[16];</span>
<span class="p_del">-	u64 msr;</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Check for the SME/SEV support leaf */</span>
<span class="p_del">-	eax = 0x80000000;</span>
<span class="p_del">-	ecx = 0;</span>
<span class="p_del">-	native_cpuid(&amp;eax, &amp;ebx, &amp;ecx, &amp;edx);</span>
<span class="p_del">-	if (eax &lt; 0x8000001f)</span>
<span class="p_del">-		return;</span>
<span class="p_del">-</span>
<span class="p_del">-#define AMD_SME_BIT	BIT(0)</span>
<span class="p_del">-#define AMD_SEV_BIT	BIT(1)</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Set the feature mask (SME or SEV) based on whether we are</span>
<span class="p_del">-	 * running under a hypervisor.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	eax = 1;</span>
<span class="p_del">-	ecx = 0;</span>
<span class="p_del">-	native_cpuid(&amp;eax, &amp;ebx, &amp;ecx, &amp;edx);</span>
<span class="p_del">-	feature_mask = (ecx &amp; BIT(31)) ? AMD_SEV_BIT : AMD_SME_BIT;</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Check for the SME/SEV feature:</span>
<span class="p_del">-	 *   CPUID Fn8000_001F[EAX]</span>
<span class="p_del">-	 *   - Bit 0 - Secure Memory Encryption support</span>
<span class="p_del">-	 *   - Bit 1 - Secure Encrypted Virtualization support</span>
<span class="p_del">-	 *   CPUID Fn8000_001F[EBX]</span>
<span class="p_del">-	 *   - Bits 5:0 - Pagetable bit position used to indicate encryption</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	eax = 0x8000001f;</span>
<span class="p_del">-	ecx = 0;</span>
<span class="p_del">-	native_cpuid(&amp;eax, &amp;ebx, &amp;ecx, &amp;edx);</span>
<span class="p_del">-	if (!(eax &amp; feature_mask))</span>
<span class="p_del">-		return;</span>
<span class="p_del">-</span>
<span class="p_del">-	me_mask = 1UL &lt;&lt; (ebx &amp; 0x3f);</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Check if memory encryption is enabled */</span>
<span class="p_del">-	if (feature_mask == AMD_SME_BIT) {</span>
<span class="p_del">-		/* For SME, check the SYSCFG MSR */</span>
<span class="p_del">-		msr = __rdmsr(MSR_K8_SYSCFG);</span>
<span class="p_del">-		if (!(msr &amp; MSR_K8_SYSCFG_MEM_ENCRYPT))</span>
<span class="p_del">-			return;</span>
<span class="p_del">-	} else {</span>
<span class="p_del">-		/* For SEV, check the SEV MSR */</span>
<span class="p_del">-		msr = __rdmsr(MSR_AMD64_SEV);</span>
<span class="p_del">-		if (!(msr &amp; MSR_AMD64_SEV_ENABLED))</span>
<span class="p_del">-			return;</span>
<span class="p_del">-</span>
<span class="p_del">-		/* SEV state cannot be controlled by a command line option */</span>
<span class="p_del">-		sme_me_mask = me_mask;</span>
<span class="p_del">-		sev_enabled = true;</span>
<span class="p_del">-		return;</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Fixups have not been applied to phys_base yet and we&#39;re running</span>
<span class="p_del">-	 * identity mapped, so we must obtain the address to the SME command</span>
<span class="p_del">-	 * line argument data using rip-relative addressing.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	asm (&quot;lea sme_cmdline_arg(%%rip), %0&quot;</span>
<span class="p_del">-	     : &quot;=r&quot; (cmdline_arg)</span>
<span class="p_del">-	     : &quot;p&quot; (sme_cmdline_arg));</span>
<span class="p_del">-	asm (&quot;lea sme_cmdline_on(%%rip), %0&quot;</span>
<span class="p_del">-	     : &quot;=r&quot; (cmdline_on)</span>
<span class="p_del">-	     : &quot;p&quot; (sme_cmdline_on));</span>
<span class="p_del">-	asm (&quot;lea sme_cmdline_off(%%rip), %0&quot;</span>
<span class="p_del">-	     : &quot;=r&quot; (cmdline_off)</span>
<span class="p_del">-	     : &quot;p&quot; (sme_cmdline_off));</span>
<span class="p_del">-</span>
<span class="p_del">-	if (IS_ENABLED(CONFIG_AMD_MEM_ENCRYPT_ACTIVE_BY_DEFAULT))</span>
<span class="p_del">-		active_by_default = true;</span>
<span class="p_del">-	else</span>
<span class="p_del">-		active_by_default = false;</span>
<span class="p_del">-</span>
<span class="p_del">-	cmdline_ptr = (const char *)((u64)bp-&gt;hdr.cmd_line_ptr |</span>
<span class="p_del">-				     ((u64)bp-&gt;ext_cmd_line_ptr &lt;&lt; 32));</span>
<span class="p_del">-</span>
<span class="p_del">-	cmdline_find_option(cmdline_ptr, cmdline_arg, buffer, sizeof(buffer));</span>
<span class="p_del">-</span>
<span class="p_del">-	if (!strncmp(buffer, cmdline_on, sizeof(buffer)))</span>
<span class="p_del">-		sme_me_mask = me_mask;</span>
<span class="p_del">-	else if (!strncmp(buffer, cmdline_off, sizeof(buffer)))</span>
<span class="p_del">-		sme_me_mask = 0;</span>
<span class="p_del">-	else</span>
<span class="p_del">-		sme_me_mask = active_by_default ? me_mask : 0;</span>
<span class="p_del">-}</span>
<span class="p_header">diff --git a/arch/x86/mm/mem_encrypt.c b/arch/x86/mm/mem_encrypt_identity.c</span>
similarity index 59%
copy from arch/x86/mm/mem_encrypt.c
copy to arch/x86/mm/mem_encrypt_identity.c
<span class="p_header">index 1a53071..a28978a 100644</span>
<span class="p_header">--- a/arch/x86/mm/mem_encrypt.c</span>
<span class="p_header">+++ b/arch/x86/mm/mem_encrypt_identity.c</span>
<span class="p_chunk">@@ -12,461 +12,39 @@</span> <span class="p_context"></span>
 
 #define DISABLE_BRANCH_PROFILING
 
<span class="p_del">-#include &lt;linux/linkage.h&gt;</span>
<span class="p_del">-#include &lt;linux/init.h&gt;</span>
 #include &lt;linux/mm.h&gt;
<span class="p_del">-#include &lt;linux/dma-direct.h&gt;</span>
<span class="p_del">-#include &lt;linux/swiotlb.h&gt;</span>
 #include &lt;linux/mem_encrypt.h&gt;
 
<span class="p_del">-#include &lt;asm/tlbflush.h&gt;</span>
<span class="p_del">-#include &lt;asm/fixmap.h&gt;</span>
 #include &lt;asm/setup.h&gt;
<span class="p_del">-#include &lt;asm/bootparam.h&gt;</span>
<span class="p_del">-#include &lt;asm/set_memory.h&gt;</span>
<span class="p_del">-#include &lt;asm/cacheflush.h&gt;</span>
 #include &lt;asm/sections.h&gt;
<span class="p_del">-#include &lt;asm/processor-flags.h&gt;</span>
<span class="p_del">-#include &lt;asm/msr.h&gt;</span>
 #include &lt;asm/cmdline.h&gt;
 
 #include &quot;mm_internal.h&quot;
 
<span class="p_del">-static char sme_cmdline_arg[] __initdata = &quot;mem_encrypt&quot;;</span>
<span class="p_del">-static char sme_cmdline_on[]  __initdata = &quot;on&quot;;</span>
<span class="p_del">-static char sme_cmdline_off[] __initdata = &quot;off&quot;;</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
<span class="p_del">- * Since SME related variables are set early in the boot process they must</span>
<span class="p_del">- * reside in the .data section so as not to be zeroed out when the .bss</span>
<span class="p_del">- * section is later cleared.</span>
<span class="p_del">- */</span>
<span class="p_del">-u64 sme_me_mask __section(.data) = 0;</span>
<span class="p_del">-EXPORT_SYMBOL(sme_me_mask);</span>
<span class="p_del">-DEFINE_STATIC_KEY_FALSE(sev_enable_key);</span>
<span class="p_del">-EXPORT_SYMBOL_GPL(sev_enable_key);</span>
<span class="p_del">-</span>
<span class="p_del">-static bool sev_enabled __section(.data);</span>
<span class="p_del">-</span>
<span class="p_del">-/* Buffer used for early in-place encryption by BSP, no locking needed */</span>
<span class="p_del">-static char sme_early_buffer[PAGE_SIZE] __aligned(PAGE_SIZE);</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
<span class="p_del">- * This routine does not change the underlying encryption setting of the</span>
<span class="p_del">- * page(s) that map this memory. It assumes that eventually the memory is</span>
<span class="p_del">- * meant to be accessed as either encrypted or decrypted but the contents</span>
<span class="p_del">- * are currently not in the desired state.</span>
<span class="p_del">- *</span>
<span class="p_del">- * This routine follows the steps outlined in the AMD64 Architecture</span>
<span class="p_del">- * Programmer&#39;s Manual Volume 2, Section 7.10.8 Encrypt-in-Place.</span>
<span class="p_del">- */</span>
<span class="p_del">-static void __init __sme_early_enc_dec(resource_size_t paddr,</span>
<span class="p_del">-				       unsigned long size, bool enc)</span>
<span class="p_del">-{</span>
<span class="p_del">-	void *src, *dst;</span>
<span class="p_del">-	size_t len;</span>
<span class="p_del">-</span>
<span class="p_del">-	if (!sme_me_mask)</span>
<span class="p_del">-		return;</span>
<span class="p_del">-</span>
<span class="p_del">-	wbinvd();</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * There are limited number of early mapping slots, so map (at most)</span>
<span class="p_del">-	 * one page at time.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	while (size) {</span>
<span class="p_del">-		len = min_t(size_t, sizeof(sme_early_buffer), size);</span>
<span class="p_del">-</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * Create mappings for the current and desired format of</span>
<span class="p_del">-		 * the memory. Use a write-protected mapping for the source.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		src = enc ? early_memremap_decrypted_wp(paddr, len) :</span>
<span class="p_del">-			    early_memremap_encrypted_wp(paddr, len);</span>
<span class="p_del">-</span>
<span class="p_del">-		dst = enc ? early_memremap_encrypted(paddr, len) :</span>
<span class="p_del">-			    early_memremap_decrypted(paddr, len);</span>
<span class="p_del">-</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * If a mapping can&#39;t be obtained to perform the operation,</span>
<span class="p_del">-		 * then eventual access of that area in the desired mode</span>
<span class="p_del">-		 * will cause a crash.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		BUG_ON(!src || !dst);</span>
<span class="p_del">-</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * Use a temporary buffer, of cache-line multiple size, to</span>
<span class="p_del">-		 * avoid data corruption as documented in the APM.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		memcpy(sme_early_buffer, src, len);</span>
<span class="p_del">-		memcpy(dst, sme_early_buffer, len);</span>
<span class="p_del">-</span>
<span class="p_del">-		early_memunmap(dst, len);</span>
<span class="p_del">-		early_memunmap(src, len);</span>
<span class="p_del">-</span>
<span class="p_del">-		paddr += len;</span>
<span class="p_del">-		size -= len;</span>
<span class="p_del">-	}</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-void __init sme_early_encrypt(resource_size_t paddr, unsigned long size)</span>
<span class="p_del">-{</span>
<span class="p_del">-	__sme_early_enc_dec(paddr, size, true);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-void __init sme_early_decrypt(resource_size_t paddr, unsigned long size)</span>
<span class="p_del">-{</span>
<span class="p_del">-	__sme_early_enc_dec(paddr, size, false);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static void __init __sme_early_map_unmap_mem(void *vaddr, unsigned long size,</span>
<span class="p_del">-					     bool map)</span>
<span class="p_del">-{</span>
<span class="p_del">-	unsigned long paddr = (unsigned long)vaddr - __PAGE_OFFSET;</span>
<span class="p_del">-	pmdval_t pmd_flags, pmd;</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Use early_pmd_flags but remove the encryption mask */</span>
<span class="p_del">-	pmd_flags = __sme_clr(early_pmd_flags);</span>
<span class="p_del">-</span>
<span class="p_del">-	do {</span>
<span class="p_del">-		pmd = map ? (paddr &amp; PMD_MASK) + pmd_flags : 0;</span>
<span class="p_del">-		__early_make_pgtable((unsigned long)vaddr, pmd);</span>
<span class="p_del">-</span>
<span class="p_del">-		vaddr += PMD_SIZE;</span>
<span class="p_del">-		paddr += PMD_SIZE;</span>
<span class="p_del">-		size = (size &lt;= PMD_SIZE) ? 0 : size - PMD_SIZE;</span>
<span class="p_del">-	} while (size);</span>
<span class="p_del">-</span>
<span class="p_del">-	__native_flush_tlb();</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-void __init sme_unmap_bootdata(char *real_mode_data)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct boot_params *boot_data;</span>
<span class="p_del">-	unsigned long cmdline_paddr;</span>
<span class="p_del">-</span>
<span class="p_del">-	if (!sme_active())</span>
<span class="p_del">-		return;</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Get the command line address before unmapping the real_mode_data */</span>
<span class="p_del">-	boot_data = (struct boot_params *)real_mode_data;</span>
<span class="p_del">-	cmdline_paddr = boot_data-&gt;hdr.cmd_line_ptr | ((u64)boot_data-&gt;ext_cmd_line_ptr &lt;&lt; 32);</span>
<span class="p_del">-</span>
<span class="p_del">-	__sme_early_map_unmap_mem(real_mode_data, sizeof(boot_params), false);</span>
<span class="p_del">-</span>
<span class="p_del">-	if (!cmdline_paddr)</span>
<span class="p_del">-		return;</span>
<span class="p_del">-</span>
<span class="p_del">-	__sme_early_map_unmap_mem(__va(cmdline_paddr), COMMAND_LINE_SIZE, false);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-void __init sme_map_bootdata(char *real_mode_data)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct boot_params *boot_data;</span>
<span class="p_del">-	unsigned long cmdline_paddr;</span>
<span class="p_del">-</span>
<span class="p_del">-	if (!sme_active())</span>
<span class="p_del">-		return;</span>
<span class="p_del">-</span>
<span class="p_del">-	__sme_early_map_unmap_mem(real_mode_data, sizeof(boot_params), true);</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Get the command line address after mapping the real_mode_data */</span>
<span class="p_del">-	boot_data = (struct boot_params *)real_mode_data;</span>
<span class="p_del">-	cmdline_paddr = boot_data-&gt;hdr.cmd_line_ptr | ((u64)boot_data-&gt;ext_cmd_line_ptr &lt;&lt; 32);</span>
<span class="p_del">-</span>
<span class="p_del">-	if (!cmdline_paddr)</span>
<span class="p_del">-		return;</span>
<span class="p_del">-</span>
<span class="p_del">-	__sme_early_map_unmap_mem(__va(cmdline_paddr), COMMAND_LINE_SIZE, true);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-void __init sme_early_init(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	unsigned int i;</span>
<span class="p_del">-</span>
<span class="p_del">-	if (!sme_me_mask)</span>
<span class="p_del">-		return;</span>
<span class="p_del">-</span>
<span class="p_del">-	early_pmd_flags = __sme_set(early_pmd_flags);</span>
<span class="p_del">-</span>
<span class="p_del">-	__supported_pte_mask = __sme_set(__supported_pte_mask);</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Update the protection map with memory encryption mask */</span>
<span class="p_del">-	for (i = 0; i &lt; ARRAY_SIZE(protection_map); i++)</span>
<span class="p_del">-		protection_map[i] = pgprot_encrypted(protection_map[i]);</span>
<span class="p_del">-</span>
<span class="p_del">-	if (sev_active())</span>
<span class="p_del">-		swiotlb_force = SWIOTLB_FORCE;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static void *sev_alloc(struct device *dev, size_t size, dma_addr_t *dma_handle,</span>
<span class="p_del">-		       gfp_t gfp, unsigned long attrs)</span>
<span class="p_del">-{</span>
<span class="p_del">-	unsigned long dma_mask;</span>
<span class="p_del">-	unsigned int order;</span>
<span class="p_del">-	struct page *page;</span>
<span class="p_del">-	void *vaddr = NULL;</span>
<span class="p_del">-</span>
<span class="p_del">-	dma_mask = dma_alloc_coherent_mask(dev, gfp);</span>
<span class="p_del">-	order = get_order(size);</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Memory will be memset to zero after marking decrypted, so don&#39;t</span>
<span class="p_del">-	 * bother clearing it before.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	gfp &amp;= ~__GFP_ZERO;</span>
<span class="p_del">-</span>
<span class="p_del">-	page = alloc_pages_node(dev_to_node(dev), gfp, order);</span>
<span class="p_del">-	if (page) {</span>
<span class="p_del">-		dma_addr_t addr;</span>
<span class="p_del">-</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * Since we will be clearing the encryption bit, check the</span>
<span class="p_del">-		 * mask with it already cleared.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		addr = __sme_clr(phys_to_dma(dev, page_to_phys(page)));</span>
<span class="p_del">-		if ((addr + size) &gt; dma_mask) {</span>
<span class="p_del">-			__free_pages(page, get_order(size));</span>
<span class="p_del">-		} else {</span>
<span class="p_del">-			vaddr = page_address(page);</span>
<span class="p_del">-			*dma_handle = addr;</span>
<span class="p_del">-		}</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	if (!vaddr)</span>
<span class="p_del">-		vaddr = swiotlb_alloc_coherent(dev, size, dma_handle, gfp);</span>
<span class="p_del">-</span>
<span class="p_del">-	if (!vaddr)</span>
<span class="p_del">-		return NULL;</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Clear the SME encryption bit for DMA use if not swiotlb area */</span>
<span class="p_del">-	if (!is_swiotlb_buffer(dma_to_phys(dev, *dma_handle))) {</span>
<span class="p_del">-		set_memory_decrypted((unsigned long)vaddr, 1 &lt;&lt; order);</span>
<span class="p_del">-		memset(vaddr, 0, PAGE_SIZE &lt;&lt; order);</span>
<span class="p_del">-		*dma_handle = __sme_clr(*dma_handle);</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	return vaddr;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static void sev_free(struct device *dev, size_t size, void *vaddr,</span>
<span class="p_del">-		     dma_addr_t dma_handle, unsigned long attrs)</span>
<span class="p_del">-{</span>
<span class="p_del">-	/* Set the SME encryption bit for re-use if not swiotlb area */</span>
<span class="p_del">-	if (!is_swiotlb_buffer(dma_to_phys(dev, dma_handle)))</span>
<span class="p_del">-		set_memory_encrypted((unsigned long)vaddr,</span>
<span class="p_del">-				     1 &lt;&lt; get_order(size));</span>
<span class="p_del">-</span>
<span class="p_del">-	swiotlb_free_coherent(dev, size, vaddr, dma_handle);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static void __init __set_clr_pte_enc(pte_t *kpte, int level, bool enc)</span>
<span class="p_del">-{</span>
<span class="p_del">-	pgprot_t old_prot, new_prot;</span>
<span class="p_del">-	unsigned long pfn, pa, size;</span>
<span class="p_del">-	pte_t new_pte;</span>
<span class="p_del">-</span>
<span class="p_del">-	switch (level) {</span>
<span class="p_del">-	case PG_LEVEL_4K:</span>
<span class="p_del">-		pfn = pte_pfn(*kpte);</span>
<span class="p_del">-		old_prot = pte_pgprot(*kpte);</span>
<span class="p_del">-		break;</span>
<span class="p_del">-	case PG_LEVEL_2M:</span>
<span class="p_del">-		pfn = pmd_pfn(*(pmd_t *)kpte);</span>
<span class="p_del">-		old_prot = pmd_pgprot(*(pmd_t *)kpte);</span>
<span class="p_del">-		break;</span>
<span class="p_del">-	case PG_LEVEL_1G:</span>
<span class="p_del">-		pfn = pud_pfn(*(pud_t *)kpte);</span>
<span class="p_del">-		old_prot = pud_pgprot(*(pud_t *)kpte);</span>
<span class="p_del">-		break;</span>
<span class="p_del">-	default:</span>
<span class="p_del">-		return;</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	new_prot = old_prot;</span>
<span class="p_del">-	if (enc)</span>
<span class="p_del">-		pgprot_val(new_prot) |= _PAGE_ENC;</span>
<span class="p_del">-	else</span>
<span class="p_del">-		pgprot_val(new_prot) &amp;= ~_PAGE_ENC;</span>
<span class="p_del">-</span>
<span class="p_del">-	/* If prot is same then do nothing. */</span>
<span class="p_del">-	if (pgprot_val(old_prot) == pgprot_val(new_prot))</span>
<span class="p_del">-		return;</span>
<span class="p_del">-</span>
<span class="p_del">-	pa = pfn &lt;&lt; page_level_shift(level);</span>
<span class="p_del">-	size = page_level_size(level);</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * We are going to perform in-place en-/decryption and change the</span>
<span class="p_del">-	 * physical page attribute from C=1 to C=0 or vice versa. Flush the</span>
<span class="p_del">-	 * caches to ensure that data gets accessed with the correct C-bit.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	clflush_cache_range(__va(pa), size);</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Encrypt/decrypt the contents in-place */</span>
<span class="p_del">-	if (enc)</span>
<span class="p_del">-		sme_early_encrypt(pa, size);</span>
<span class="p_del">-	else</span>
<span class="p_del">-		sme_early_decrypt(pa, size);</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Change the page encryption mask. */</span>
<span class="p_del">-	new_pte = pfn_pte(pfn, new_prot);</span>
<span class="p_del">-	set_pte_atomic(kpte, new_pte);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static int __init early_set_memory_enc_dec(unsigned long vaddr,</span>
<span class="p_del">-					   unsigned long size, bool enc)</span>
<span class="p_del">-{</span>
<span class="p_del">-	unsigned long vaddr_end, vaddr_next;</span>
<span class="p_del">-	unsigned long psize, pmask;</span>
<span class="p_del">-	int split_page_size_mask;</span>
<span class="p_del">-	int level, ret;</span>
<span class="p_del">-	pte_t *kpte;</span>
<span class="p_del">-</span>
<span class="p_del">-	vaddr_next = vaddr;</span>
<span class="p_del">-	vaddr_end = vaddr + size;</span>
<span class="p_del">-</span>
<span class="p_del">-	for (; vaddr &lt; vaddr_end; vaddr = vaddr_next) {</span>
<span class="p_del">-		kpte = lookup_address(vaddr, &amp;level);</span>
<span class="p_del">-		if (!kpte || pte_none(*kpte)) {</span>
<span class="p_del">-			ret = 1;</span>
<span class="p_del">-			goto out;</span>
<span class="p_del">-		}</span>
<span class="p_del">-</span>
<span class="p_del">-		if (level == PG_LEVEL_4K) {</span>
<span class="p_del">-			__set_clr_pte_enc(kpte, level, enc);</span>
<span class="p_del">-			vaddr_next = (vaddr &amp; PAGE_MASK) + PAGE_SIZE;</span>
<span class="p_del">-			continue;</span>
<span class="p_del">-		}</span>
<span class="p_del">-</span>
<span class="p_del">-		psize = page_level_size(level);</span>
<span class="p_del">-		pmask = page_level_mask(level);</span>
<span class="p_del">-</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * Check whether we can change the large page in one go.</span>
<span class="p_del">-		 * We request a split when the address is not aligned and</span>
<span class="p_del">-		 * the number of pages to set/clear encryption bit is smaller</span>
<span class="p_del">-		 * than the number of pages in the large page.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		if (vaddr == (vaddr &amp; pmask) &amp;&amp;</span>
<span class="p_del">-		    ((vaddr_end - vaddr) &gt;= psize)) {</span>
<span class="p_del">-			__set_clr_pte_enc(kpte, level, enc);</span>
<span class="p_del">-			vaddr_next = (vaddr &amp; pmask) + psize;</span>
<span class="p_del">-			continue;</span>
<span class="p_del">-		}</span>
<span class="p_del">-</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * The virtual address is part of a larger page, create the next</span>
<span class="p_del">-		 * level page table mapping (4K or 2M). If it is part of a 2M</span>
<span class="p_del">-		 * page then we request a split of the large page into 4K</span>
<span class="p_del">-		 * chunks. A 1GB large page is split into 2M pages, resp.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		if (level == PG_LEVEL_2M)</span>
<span class="p_del">-			split_page_size_mask = 0;</span>
<span class="p_del">-		else</span>
<span class="p_del">-			split_page_size_mask = 1 &lt;&lt; PG_LEVEL_2M;</span>
<span class="p_del">-</span>
<span class="p_del">-		kernel_physical_mapping_init(__pa(vaddr &amp; pmask),</span>
<span class="p_del">-					     __pa((vaddr_end &amp; pmask) + psize),</span>
<span class="p_del">-					     split_page_size_mask);</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	ret = 0;</span>
<span class="p_del">-</span>
<span class="p_del">-out:</span>
<span class="p_del">-	__flush_tlb_all();</span>
<span class="p_del">-	return ret;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-int __init early_set_memory_decrypted(unsigned long vaddr, unsigned long size)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return early_set_memory_enc_dec(vaddr, size, false);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-int __init early_set_memory_encrypted(unsigned long vaddr, unsigned long size)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return early_set_memory_enc_dec(vaddr, size, true);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
<span class="p_del">- * SME and SEV are very similar but they are not the same, so there are</span>
<span class="p_del">- * times that the kernel will need to distinguish between SME and SEV. The</span>
<span class="p_del">- * sme_active() and sev_active() functions are used for this.  When a</span>
<span class="p_del">- * distinction isn&#39;t needed, the mem_encrypt_active() function can be used.</span>
<span class="p_del">- *</span>
<span class="p_del">- * The trampoline code is a good example for this requirement.  Before</span>
<span class="p_del">- * paging is activated, SME will access all memory as decrypted, but SEV</span>
<span class="p_del">- * will access all memory as encrypted.  So, when APs are being brought</span>
<span class="p_del">- * up under SME the trampoline area cannot be encrypted, whereas under SEV</span>
<span class="p_del">- * the trampoline area must be encrypted.</span>
<span class="p_del">- */</span>
<span class="p_del">-bool sme_active(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return sme_me_mask &amp;&amp; !sev_enabled;</span>
<span class="p_del">-}</span>
<span class="p_del">-EXPORT_SYMBOL(sme_active);</span>
<span class="p_del">-</span>
<span class="p_del">-bool sev_active(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return sme_me_mask &amp;&amp; sev_enabled;</span>
<span class="p_del">-}</span>
<span class="p_del">-EXPORT_SYMBOL(sev_active);</span>
<span class="p_del">-</span>
<span class="p_del">-static const struct dma_map_ops sev_dma_ops = {</span>
<span class="p_del">-	.alloc                  = sev_alloc,</span>
<span class="p_del">-	.free                   = sev_free,</span>
<span class="p_del">-	.map_page               = swiotlb_map_page,</span>
<span class="p_del">-	.unmap_page             = swiotlb_unmap_page,</span>
<span class="p_del">-	.map_sg                 = swiotlb_map_sg_attrs,</span>
<span class="p_del">-	.unmap_sg               = swiotlb_unmap_sg_attrs,</span>
<span class="p_del">-	.sync_single_for_cpu    = swiotlb_sync_single_for_cpu,</span>
<span class="p_del">-	.sync_single_for_device = swiotlb_sync_single_for_device,</span>
<span class="p_del">-	.sync_sg_for_cpu        = swiotlb_sync_sg_for_cpu,</span>
<span class="p_del">-	.sync_sg_for_device     = swiotlb_sync_sg_for_device,</span>
<span class="p_del">-	.mapping_error          = swiotlb_dma_mapping_error,</span>
<span class="p_del">-};</span>
<span class="p_del">-</span>
<span class="p_del">-/* Architecture __weak replacement functions */</span>
<span class="p_del">-void __init mem_encrypt_init(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	if (!sme_me_mask)</span>
<span class="p_del">-		return;</span>
<span class="p_add">+#define PGD_FLAGS		_KERNPG_TABLE_NOENC</span>
<span class="p_add">+#define P4D_FLAGS		_KERNPG_TABLE_NOENC</span>
<span class="p_add">+#define PUD_FLAGS		_KERNPG_TABLE_NOENC</span>
<span class="p_add">+#define PMD_FLAGS		_KERNPG_TABLE_NOENC</span>
 
<span class="p_del">-	/* Call into SWIOTLB to update the SWIOTLB DMA buffers */</span>
<span class="p_del">-	swiotlb_update_mem_attributes();</span>
<span class="p_add">+#define PMD_FLAGS_LARGE		(__PAGE_KERNEL_LARGE_EXEC &amp; ~_PAGE_GLOBAL)</span>
 
<span class="p_del">-	/*</span>
<span class="p_del">-	 * With SEV, DMA operations cannot use encryption. New DMA ops</span>
<span class="p_del">-	 * are required in order to mark the DMA areas as decrypted or</span>
<span class="p_del">-	 * to use bounce buffers.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	if (sev_active())</span>
<span class="p_del">-		dma_ops = &amp;sev_dma_ops;</span>
<span class="p_add">+#define PMD_FLAGS_DEC		PMD_FLAGS_LARGE</span>
<span class="p_add">+#define PMD_FLAGS_DEC_WP	((PMD_FLAGS_DEC &amp; ~_PAGE_CACHE_MASK) | \</span>
<span class="p_add">+				 (_PAGE_PAT | _PAGE_PWT))</span>
 
<span class="p_del">-	/*</span>
<span class="p_del">-	 * With SEV, we need to unroll the rep string I/O instructions.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	if (sev_active())</span>
<span class="p_del">-		static_branch_enable(&amp;sev_enable_key);</span>
<span class="p_add">+#define PMD_FLAGS_ENC		(PMD_FLAGS_LARGE | _PAGE_ENC)</span>
 
<span class="p_del">-	pr_info(&quot;AMD %s active\n&quot;,</span>
<span class="p_del">-		sev_active() ? &quot;Secure Encrypted Virtualization (SEV)&quot;</span>
<span class="p_del">-			     : &quot;Secure Memory Encryption (SME)&quot;);</span>
<span class="p_del">-}</span>
<span class="p_add">+#define PTE_FLAGS		(__PAGE_KERNEL_EXEC &amp; ~_PAGE_GLOBAL)</span>
 
<span class="p_del">-void swiotlb_set_mem_attributes(void *vaddr, unsigned long size)</span>
<span class="p_del">-{</span>
<span class="p_del">-	WARN(PAGE_ALIGN(size) != size,</span>
<span class="p_del">-	     &quot;size is not page-aligned (%#lx)\n&quot;, size);</span>
<span class="p_add">+#define PTE_FLAGS_DEC		PTE_FLAGS</span>
<span class="p_add">+#define PTE_FLAGS_DEC_WP	((PTE_FLAGS_DEC &amp; ~_PAGE_CACHE_MASK) | \</span>
<span class="p_add">+				 (_PAGE_PAT | _PAGE_PWT))</span>
 
<span class="p_del">-	/* Make the SWIOTLB buffer area decrypted */</span>
<span class="p_del">-	set_memory_decrypted((unsigned long)vaddr, size &gt;&gt; PAGE_SHIFT);</span>
<span class="p_del">-}</span>
<span class="p_add">+#define PTE_FLAGS_ENC		(PTE_FLAGS | _PAGE_ENC)</span>
 
 struct sme_populate_pgd_data {
<span class="p_del">-	void	*pgtable_area;</span>
<span class="p_del">-	pgd_t	*pgd;</span>
<span class="p_add">+	void    *pgtable_area;</span>
<span class="p_add">+	pgd_t   *pgd;</span>
 
 	pmdval_t pmd_flags;
 	pteval_t pte_flags;
<span class="p_chunk">@@ -476,6 +54,10 @@</span> <span class="p_context"> struct sme_populate_pgd_data {</span>
 	unsigned long vaddr_end;
 };
 
<span class="p_add">+static char sme_cmdline_arg[] __initdata = &quot;mem_encrypt&quot;;</span>
<span class="p_add">+static char sme_cmdline_on[]  __initdata = &quot;on&quot;;</span>
<span class="p_add">+static char sme_cmdline_off[] __initdata = &quot;off&quot;;</span>
<span class="p_add">+</span>
 static void __init sme_clear_pgd(struct sme_populate_pgd_data *ppd)
 {
 	unsigned long pgd_start, pgd_end, pgd_size;
<span class="p_chunk">@@ -491,27 +73,6 @@</span> <span class="p_context"> static void __init sme_clear_pgd(struct sme_populate_pgd_data *ppd)</span>
 	memset(pgd_p, 0, pgd_size);
 }
 
<span class="p_del">-#define PGD_FLAGS		_KERNPG_TABLE_NOENC</span>
<span class="p_del">-#define P4D_FLAGS		_KERNPG_TABLE_NOENC</span>
<span class="p_del">-#define PUD_FLAGS		_KERNPG_TABLE_NOENC</span>
<span class="p_del">-#define PMD_FLAGS		_KERNPG_TABLE_NOENC</span>
<span class="p_del">-</span>
<span class="p_del">-#define PMD_FLAGS_LARGE		(__PAGE_KERNEL_LARGE_EXEC &amp; ~_PAGE_GLOBAL)</span>
<span class="p_del">-</span>
<span class="p_del">-#define PMD_FLAGS_DEC		PMD_FLAGS_LARGE</span>
<span class="p_del">-#define PMD_FLAGS_DEC_WP	((PMD_FLAGS_DEC &amp; ~_PAGE_CACHE_MASK) | \</span>
<span class="p_del">-				 (_PAGE_PAT | _PAGE_PWT))</span>
<span class="p_del">-</span>
<span class="p_del">-#define PMD_FLAGS_ENC		(PMD_FLAGS_LARGE | _PAGE_ENC)</span>
<span class="p_del">-</span>
<span class="p_del">-#define PTE_FLAGS		(__PAGE_KERNEL_EXEC &amp; ~_PAGE_GLOBAL)</span>
<span class="p_del">-</span>
<span class="p_del">-#define PTE_FLAGS_DEC		PTE_FLAGS</span>
<span class="p_del">-#define PTE_FLAGS_DEC_WP	((PTE_FLAGS_DEC &amp; ~_PAGE_CACHE_MASK) | \</span>
<span class="p_del">-				 (_PAGE_PAT | _PAGE_PWT))</span>
<span class="p_del">-</span>
<span class="p_del">-#define PTE_FLAGS_ENC		(PTE_FLAGS | _PAGE_ENC)</span>
<span class="p_del">-</span>
 static pmd_t __init *sme_prepare_pgd(struct sme_populate_pgd_data *ppd)
 {
 	pgd_t *pgd_p;

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



