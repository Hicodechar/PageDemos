
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v2,5/5] mm: memory-hotplug: Add memory hot remove support for arm64 - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v2,5/5] mm: memory-hotplug: Add memory hot remove support for arm64</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=171743">Andrea Reale</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Nov. 23, 2017, 11:15 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;a7507f1245c957aec8732855a3dfdbe73a1254de.1511433386.git.ar@linux.vnet.ibm.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10072669/mbox/"
   >mbox</a>
|
   <a href="/patch/10072669/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10072669/">/patch/10072669/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	6D5A46056E for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 23 Nov 2017 11:15:56 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 5F4E629F94
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 23 Nov 2017 11:15:56 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 537E429F96; Thu, 23 Nov 2017 11:15:56 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id D036629F94
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 23 Nov 2017 11:15:54 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752691AbdKWLPx (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 23 Nov 2017 06:15:53 -0500
Received: from mx0a-001b2d01.pphosted.com ([148.163.156.1]:37170 &quot;EHLO
	mx0a-001b2d01.pphosted.com&quot; rhost-flags-OK-OK-OK-OK)
	by vger.kernel.org with ESMTP id S1751584AbdKWLPv (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 23 Nov 2017 06:15:51 -0500
Received: from pps.filterd (m0098410.ppops.net [127.0.0.1])
	by mx0a-001b2d01.pphosted.com (8.16.0.21/8.16.0.21) with SMTP id
	vANBFSbp026845
	for &lt;linux-kernel@vger.kernel.org&gt;; Thu, 23 Nov 2017 06:15:51 -0500
Received: from e06smtp14.uk.ibm.com (e06smtp14.uk.ibm.com [195.75.94.110])
	by mx0a-001b2d01.pphosted.com with ESMTP id 2edw7cgdnf-1
	(version=TLSv1.2 cipher=AES256-SHA bits=256 verify=NOT)
	for &lt;linux-kernel@vger.kernel.org&gt;; Thu, 23 Nov 2017 06:15:50 -0500
Received: from localhost
	by e06smtp14.uk.ibm.com with IBM ESMTP SMTP Gateway: Authorized Use
	Only! Violators will be prosecuted
	for &lt;linux-kernel@vger.kernel.org&gt; from &lt;ar@linux.vnet.ibm.com&gt;;
	Thu, 23 Nov 2017 11:15:48 -0000
Received: from b06cxnps4075.portsmouth.uk.ibm.com (9.149.109.197)
	by e06smtp14.uk.ibm.com (192.168.101.144) with IBM ESMTP SMTP
	Gateway: Authorized Use Only! Violators will be prosecuted; 
	Thu, 23 Nov 2017 11:15:44 -0000
Received: from d06av23.portsmouth.uk.ibm.com (d06av23.portsmouth.uk.ibm.com
	[9.149.105.59])
	by b06cxnps4075.portsmouth.uk.ibm.com (8.14.9/8.14.9/NCO v10.0) with
	ESMTP id vANBFixX24707104; Thu, 23 Nov 2017 11:15:44 GMT
Received: from d06av23.portsmouth.uk.ibm.com (unknown [127.0.0.1])
	by IMSVA (Postfix) with ESMTP id 3F0A3A4053;
	Thu, 23 Nov 2017 11:10:23 +0000 (GMT)
Received: from d06av23.portsmouth.uk.ibm.com (unknown [127.0.0.1])
	by IMSVA (Postfix) with ESMTP id B8A9BA404D;
	Thu, 23 Nov 2017 11:10:22 +0000 (GMT)
Received: from samekh (unknown [9.162.48.51])
	by d06av23.portsmouth.uk.ibm.com (Postfix) with ESMTPS;
	Thu, 23 Nov 2017 11:10:22 +0000 (GMT)
Date: Thu, 23 Nov 2017 11:15:42 +0000
From: Andrea Reale &lt;ar@linux.vnet.ibm.com&gt;
To: linux-arm-kernel@lists.infradead.org
Cc: linux-kernel@vger.kernel.org, linux-mm@kvack.org,
	m.bielski@virtualopensystems.com, arunks@qti.qualcomm.com,
	mark.rutland@arm.com, scott.branden@broadcom.com,
	will.deacon@arm.com, qiuxishi@huawei.com, catalin.marinas@arm.com,
	mhocko@suse.com, realean2@ie.ibm.com
Subject: [PATCH v2 5/5] mm: memory-hotplug: Add memory hot remove support for
	arm64
References: &lt;cover.1511433386.git.ar@linux.vnet.ibm.com&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=utf-8
Content-Disposition: inline
In-Reply-To: &lt;cover.1511433386.git.ar@linux.vnet.ibm.com&gt;
User-Agent: Mutt/1.5.24 (2015-08-30)
X-TM-AS-GCONF: 00
x-cbid: 17112311-0016-0000-0000-00000504C3C0
X-IBM-AV-DETECTION: SAVI=unused REMOTE=unused XFE=unused
x-cbparentid: 17112311-0017-0000-0000-0000284097DF
Message-Id: &lt;a7507f1245c957aec8732855a3dfdbe73a1254de.1511433386.git.ar@linux.vnet.ibm.com&gt;
X-Proofpoint-Virus-Version: vendor=fsecure engine=2.50.10432:, ,
	definitions=2017-11-23_04:, , signatures=0
X-Proofpoint-Spam-Details: rule=outbound_notspam policy=outbound score=0
	priorityscore=1501
	malwarescore=0 suspectscore=3 phishscore=0 bulkscore=0 spamscore=0
	clxscore=1015 lowpriorityscore=0 impostorscore=0 adultscore=0
	classifier=spam adjust=0 reason=mlx scancount=1
	engine=8.0.1-1709140000
	definitions=main-1711230157
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=171743">Andrea Reale</a> - Nov. 23, 2017, 11:15 a.m.</div>
<pre class="content">
Implementation of pagetable cleanup routines for arm64 memory hot remove.

How to offline:
 1. Logical Hot remove (offline)
 - # echo offline &gt; /sys/devices/system/memory/memoryXX/state
 2. Physical Hot remove (offline)
 - (if offline is successful)
 - # echo $section_phy_address &gt; /sys/devices/system/memory/remove

Changes v1-&gt;v2:
- introduced check on offlining state before hot remove:
  in x86 (and possibly other architectures), offlining of pages and hot
  remove of physical memory happen in a single step, i.e., via an acpi
  event. In this patchset we are introducing a &quot;remove&quot; sysfs handle
  that triggers the physical hot-remove process after manual offlining.

- new memblock flag used to mark partially unused vmemmap pages, avoiding
  the nasty 0xFD hack used in the prev rev (and in x86 hot remove code):
  the hot remove process needs to take care of freeing vmemmap pages
  and mappings for the memory being removed. Sometimes, it might be not
  possible to free fully a vmemmap page (because it is being used for
  other mappings); in such a case we mark part of that page as unused and
  we free it only when it is fully unused. In the previous version, in
  symmetry to x86 hot remove code, we were doing this marking by filling
  the unused parts of the page with an aribitrary 0xFD constant. In this
  version, we are using a new memblock flag for the same purpose.

- proper cleaning sequence for p[um]ds,ptes and related TLB management:
  i) clear the page table, ii) flush tlb, iii) free the pagetable page

- Removed macros that changed hot remove behavior based on number
  of pgtable levels. Now this is hidden in the pgtable traversal macros.

- Check on the corner case where P[UM]Ds would have to be split during
  hot remove: now this is forbidden.
  Hot addition and removal is done at SECTION_SIZE_BITS granularity
  (currently 1GB).  The only case when we would have to split a P[UM]D
  is when SECTION_SIZE_BITS is smaller than a P[UM]D mapped area (never
  by default), AND when we are removing some P[UM]D-mapped memory that
  was never hot-added (there since boot).  If the above conditions hold,
  we avoid splitting the P[UM]Ds and, instead, we forbid hot removal.

- Minor fixes and refactoring.
<span class="signed-off-by">
Signed-off-by: Andrea Reale &lt;ar@linux.vnet.ibm.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Maciej Bielski &lt;m.bielski@virtualopensystems.com&gt;</span>
---
 arch/arm64/Kconfig           |   3 +
 arch/arm64/configs/defconfig |   1 +
 arch/arm64/include/asm/mmu.h |   4 +
 arch/arm64/mm/init.c         |  29 +++
 arch/arm64/mm/mmu.c          | 572 ++++++++++++++++++++++++++++++++++++++++++-
 5 files changed, 601 insertions(+), 8 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/arm64/Kconfig b/arch/arm64/Kconfig</span>
<span class="p_header">index c736bba..c362ddf 100644</span>
<span class="p_header">--- a/arch/arm64/Kconfig</span>
<span class="p_header">+++ b/arch/arm64/Kconfig</span>
<span class="p_chunk">@@ -649,6 +649,9 @@</span> <span class="p_context"> config ARCH_ENABLE_MEMORY_HOTPLUG</span>
 	def_bool y
     depends on !NUMA
 
<span class="p_add">+config ARCH_ENABLE_MEMORY_HOTREMOVE</span>
<span class="p_add">+	def_bool y</span>
<span class="p_add">+</span>
 # Common NUMA Features
 config NUMA
 	bool &quot;Numa Memory Allocation and Scheduler Support&quot;
<span class="p_header">diff --git a/arch/arm64/configs/defconfig b/arch/arm64/configs/defconfig</span>
<span class="p_header">index 5fc5656..cdac3b8 100644</span>
<span class="p_header">--- a/arch/arm64/configs/defconfig</span>
<span class="p_header">+++ b/arch/arm64/configs/defconfig</span>
<span class="p_chunk">@@ -81,6 +81,7 @@</span> <span class="p_context"> CONFIG_SCHED_MC=y</span>
 CONFIG_NUMA=y
 CONFIG_PREEMPT=y
 CONFIG_MEMORY_HOTPLUG=y
<span class="p_add">+CONFIG_MEMORY_HOTREMOVE=y</span>
 CONFIG_KSM=y
 CONFIG_TRANSPARENT_HUGEPAGE=y
 CONFIG_CMA=y
<span class="p_header">diff --git a/arch/arm64/include/asm/mmu.h b/arch/arm64/include/asm/mmu.h</span>
<span class="p_header">index 2b3fa4d..ca11567 100644</span>
<span class="p_header">--- a/arch/arm64/include/asm/mmu.h</span>
<span class="p_header">+++ b/arch/arm64/include/asm/mmu.h</span>
<span class="p_chunk">@@ -42,6 +42,10 @@</span> <span class="p_context"> extern void *fixmap_remap_fdt(phys_addr_t dt_phys);</span>
 extern void mark_linear_text_alias_ro(void);
 #ifdef CONFIG_MEMORY_HOTPLUG
 extern void hotplug_paging(phys_addr_t start, phys_addr_t size);
<span class="p_add">+#ifdef CONFIG_MEMORY_HOTREMOVE</span>
<span class="p_add">+extern int remove_pagetable(unsigned long start,</span>
<span class="p_add">+	unsigned long end, bool linear_map, bool check_split);</span>
<span class="p_add">+#endif</span>
 #endif
 
 #endif
<span class="p_header">diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c</span>
<span class="p_header">index e96e7d3..406b378 100644</span>
<span class="p_header">--- a/arch/arm64/mm/init.c</span>
<span class="p_header">+++ b/arch/arm64/mm/init.c</span>
<span class="p_chunk">@@ -808,4 +808,33 @@</span> <span class="p_context"> int arch_add_memory(int nid, u64 start, u64 size, bool want_memblock)</span>
 	return ret;
 }
 
<span class="p_add">+#ifdef CONFIG_MEMORY_HOTREMOVE</span>
<span class="p_add">+int arch_remove_memory(u64 start, u64 size)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long start_pfn = start &gt;&gt; PAGE_SHIFT;</span>
<span class="p_add">+	unsigned long nr_pages = size &gt;&gt; PAGE_SHIFT;</span>
<span class="p_add">+	unsigned long va_start = (unsigned long) __va(start);</span>
<span class="p_add">+	unsigned long va_end = (unsigned long)__va(start + size);</span>
<span class="p_add">+	struct page *page = pfn_to_page(start_pfn);</span>
<span class="p_add">+	struct zone *zone;</span>
<span class="p_add">+	int ret = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Check if mem can be removed without splitting</span>
<span class="p_add">+	 * PUD/PMD mappings.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	ret = remove_pagetable(va_start, va_end, true, true);</span>
<span class="p_add">+	if (!ret) {</span>
<span class="p_add">+		zone = page_zone(page);</span>
<span class="p_add">+		ret = __remove_pages(zone, start_pfn, nr_pages);</span>
<span class="p_add">+		WARN_ON_ONCE(ret);</span>
<span class="p_add">+</span>
<span class="p_add">+		/* Actually remove the mapping */</span>
<span class="p_add">+		remove_pagetable(va_start, va_end, true, false);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* CONFIG_MEMORY_HOTREMOVE */</span>
 #endif /* CONFIG_MEMORY_HOTPLUG */
<span class="p_header">diff --git a/arch/arm64/mm/mmu.c b/arch/arm64/mm/mmu.c</span>
<span class="p_header">index d93043d..e6f8c91 100644</span>
<span class="p_header">--- a/arch/arm64/mm/mmu.c</span>
<span class="p_header">+++ b/arch/arm64/mm/mmu.c</span>
<span class="p_chunk">@@ -25,6 +25,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/ioport.h&gt;
 #include &lt;linux/kexec.h&gt;
 #include &lt;linux/libfdt.h&gt;
<span class="p_add">+#include &lt;linux/memremap.h&gt;</span>
 #include &lt;linux/mman.h&gt;
 #include &lt;linux/nodemask.h&gt;
 #include &lt;linux/memblock.h&gt;
<span class="p_chunk">@@ -652,12 +653,532 @@</span> <span class="p_context"> inline void hotplug_paging(phys_addr_t start, phys_addr_t size)</span>
 
 	stop_machine(__hotplug_paging, &amp;section, NULL);
 }
<span class="p_del">-#endif /* CONFIG_MEMORY_HOTPLUG */</span>
<span class="p_add">+#ifdef CONFIG_MEMORY_HOTREMOVE</span>
<span class="p_add">+</span>
<span class="p_add">+static void  free_pagetable(struct page *page, int order, bool linear_map)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long magic;</span>
<span class="p_add">+	unsigned int nr_pages = 1 &lt;&lt; order;</span>
<span class="p_add">+	struct vmem_altmap *altmap = to_vmem_altmap((unsigned long) page);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (altmap) {</span>
<span class="p_add">+		vmem_altmap_free(altmap, nr_pages);</span>
<span class="p_add">+		return;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/* bootmem page has reserved flag */</span>
<span class="p_add">+	if (PageReserved(page)) {</span>
<span class="p_add">+		__ClearPageReserved(page);</span>
<span class="p_add">+</span>
<span class="p_add">+		magic = (unsigned long)page-&gt;lru.next;</span>
<span class="p_add">+		if (magic == SECTION_INFO || magic == MIX_SECTION_INFO) {</span>
<span class="p_add">+			while (nr_pages--)</span>
<span class="p_add">+				put_page_bootmem(page++);</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			while (nr_pages--)</span>
<span class="p_add">+				free_reserved_page(page++);</span>
<span class="p_add">+		}</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Only linear_map pagetable allocation (those allocated via</span>
<span class="p_add">+		 * hotplug) call the pgtable_page_ctor; vmemmap pgtable</span>
<span class="p_add">+		 * allocations don&#39;t.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (linear_map)</span>
<span class="p_add">+			pgtable_page_dtor(page);</span>
<span class="p_add">+</span>
<span class="p_add">+		free_pages((unsigned long)page_address(page), order);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void free_pte_table(unsigned long addr, pmd_t *pmd, bool linear_map)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pte_t *pte;</span>
<span class="p_add">+	struct page *page;</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+</span>
<span class="p_add">+	pte =  pte_offset_kernel(pmd, 0L);</span>
<span class="p_add">+	/* Check if there is no valid entry in the PMD */</span>
<span class="p_add">+	for (i = 0; i &lt; PTRS_PER_PTE; i++, pte++) {</span>
<span class="p_add">+		if (!pte_none(*pte))</span>
<span class="p_add">+			return;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	page = pmd_page(*pmd);</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * This spin lock could be only taken in _pte_aloc_kernel</span>
<span class="p_add">+	 * in mm/memory.c and nowhere else (for arm64). Not sure if</span>
<span class="p_add">+	 * the function above can be called concurrently. In doubt,</span>
<span class="p_add">+	 * I am living it here for now, but it probably can be removed</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	spin_lock(&amp;init_mm.page_table_lock);</span>
<span class="p_add">+	pmd_clear(pmd);</span>
<span class="p_add">+	spin_unlock(&amp;init_mm.page_table_lock);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Make sure addr is aligned with first address of the PMD*/</span>
<span class="p_add">+	addr &amp;= PMD_MASK;</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Invalidate TLB walk caches to PTE</span>
<span class="p_add">+	 * Not sure what is the index of the TLB walk caches.</span>
<span class="p_add">+	 * i.e., if it is indexed just by addr &amp; PMD_MASK or it can be</span>
<span class="p_add">+	 * indexed by any address. Flushing everything to stay on the safe</span>
<span class="p_add">+	 * side.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	flush_tlb_kernel_range(addr, addr + PMD_SIZE);</span>
<span class="p_add">+</span>
<span class="p_add">+	free_pagetable(page, 0, linear_map);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void free_pmd_table(unsigned long addr, pud_t *pud, bool linear_map)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pmd_t *pmd;</span>
<span class="p_add">+	struct page *page;</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+</span>
<span class="p_add">+	pmd = pmd_offset(pud, 0L);</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If PMD is folded onto PUD, cleanup was already performed</span>
<span class="p_add">+	 * up in the call stack. No more work needs to be done.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if ((pud_t *) pmd == pud)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Check if there is no valid entry in the PMD */</span>
<span class="p_add">+	for (i = 0; i &lt; PTRS_PER_PMD; i++, pmd++) {</span>
<span class="p_add">+		if (!pmd_none(*pmd))</span>
<span class="p_add">+			return;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	page = pud_page(*pud);</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * This spin lock could be only taken in _pte_aloc_kernel</span>
<span class="p_add">+	 * in mm/memory.c and nowhere else (for arm64). Not sure if</span>
<span class="p_add">+	 * the function above can be called concurrently. In doubt,</span>
<span class="p_add">+	 * I am living it here for now, but it probably can be removed</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	spin_lock(&amp;init_mm.page_table_lock);</span>
<span class="p_add">+	pud_clear(pud);</span>
<span class="p_add">+	spin_unlock(&amp;init_mm.page_table_lock);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Make sure addr is aligned with first address of the PMD*/</span>
<span class="p_add">+	addr &amp;= PUD_MASK;</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Invalidate TLB walk caches to PMD</span>
<span class="p_add">+	 * Not sure what is the index of the TLB walk caches.</span>
<span class="p_add">+	 * i.e., if it is indexed just by addr &amp; PUD_MASK or it can be</span>
<span class="p_add">+	 * indexed by any address. Flushing everything to stay on the safe</span>
<span class="p_add">+	 * side.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	flush_tlb_kernel_range(addr, addr + PUD_SIZE);</span>
<span class="p_add">+</span>
<span class="p_add">+	free_pagetable(page, 0, linear_map);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void free_pud_table(unsigned long addr, pgd_t *pgd, bool linear_map)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pud_t *pud;</span>
<span class="p_add">+	struct page *page;</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+</span>
<span class="p_add">+	pud = pud_offset(pgd, 0L);</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If PUD is folded onto PGD, cleanup was already performed</span>
<span class="p_add">+	 * up in the call stack. No more work needs to be done.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if ((pgd_t *)pud == pgd)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Check if there is no valid entry in the PUD */</span>
<span class="p_add">+	for (i = 0; i &lt; PTRS_PER_PUD; i++, pud++) {</span>
<span class="p_add">+		if (!pud_none(*pud))</span>
<span class="p_add">+			return;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	page = pgd_page(*pgd);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * This spin lock could be only</span>
<span class="p_add">+	 * taken in _pte_aloc_kernel in</span>
<span class="p_add">+	 * mm/memory.c and nowhere else</span>
<span class="p_add">+	 * (for arm64). Not sure if the</span>
<span class="p_add">+	 * function above can be called</span>
<span class="p_add">+	 * concurrently. In doubt,</span>
<span class="p_add">+	 * I am living it here for now,</span>
<span class="p_add">+	 * but it probably can be removed.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	spin_lock(&amp;init_mm.page_table_lock);</span>
<span class="p_add">+	pgd_clear(pgd);</span>
<span class="p_add">+	spin_unlock(&amp;init_mm.page_table_lock);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Make sure addr is aligned with first address of the PUD*/</span>
<span class="p_add">+	addr &amp;= PGDIR_MASK;</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Invalidate TLB walk caches to PUD</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * Not sure what is the index of the TLB walk caches.</span>
<span class="p_add">+	 * i.e., if it is indexed just by addr &amp; PGDIR_MASK or it can be</span>
<span class="p_add">+	 * indexed by any address. Flushing everything to stay on the safe</span>
<span class="p_add">+	 * side</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	flush_tlb_kernel_range(addr, addr + PGD_SIZE);</span>
<span class="p_add">+</span>
<span class="p_add">+	free_pagetable(page, 0, linear_map);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void mark_n_free_pte_vmemmap(pte_t *pte,</span>
<span class="p_add">+		unsigned long addr, unsigned long size)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long page_offset =  (addr &amp; (~PAGE_MASK));</span>
<span class="p_add">+	phys_addr_t page_start = pte_val(*pte) &amp; PHYS_MASK &amp; (s32)PAGE_MASK;</span>
<span class="p_add">+	phys_addr_t pa_start = page_start + page_offset;</span>
<span class="p_add">+</span>
<span class="p_add">+	memblock_mark_unused_vmemmap(pa_start, size);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (memblock_is_vmemmap_unused_range(&amp;memblock.memory,</span>
<span class="p_add">+				page_start, page_start + PAGE_SIZE)) {</span>
<span class="p_add">+</span>
<span class="p_add">+		free_pagetable(pte_page(*pte), 0, false);</span>
<span class="p_add">+		memblock_clear_unused_vmemmap(page_start, PAGE_SIZE);</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * This spin lock could be only</span>
<span class="p_add">+		 * taken in _pte_aloc_kernel in</span>
<span class="p_add">+		 * mm/memory.c and nowhere else</span>
<span class="p_add">+		 * (for arm64). Not sure if the</span>
<span class="p_add">+		 * function above can be called</span>
<span class="p_add">+		 * concurrently. In doubt,</span>
<span class="p_add">+		 * I am living it here for now,</span>
<span class="p_add">+		 * but it probably can be removed.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		spin_lock(&amp;init_mm.page_table_lock);</span>
<span class="p_add">+		pte_clear(&amp;init_mm, addr, pte);</span>
<span class="p_add">+		spin_unlock(&amp;init_mm.page_table_lock);</span>
<span class="p_add">+</span>
<span class="p_add">+		flush_tlb_kernel_range(addr &amp; PAGE_MASK,</span>
<span class="p_add">+				(addr + PAGE_SIZE) &amp; PAGE_MASK);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void mark_n_free_pmd_vmemmap(pmd_t *pmd,</span>
<span class="p_add">+		unsigned long addr, unsigned long size)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long sec_offset =  (addr &amp; (~PMD_MASK));</span>
<span class="p_add">+	phys_addr_t page_start = pmd_page_paddr(*pmd);</span>
<span class="p_add">+	phys_addr_t pa_start = page_start + sec_offset;</span>
<span class="p_add">+</span>
<span class="p_add">+	memblock_mark_unused_vmemmap(pa_start, size);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (memblock_is_vmemmap_unused_range(&amp;memblock.memory,</span>
<span class="p_add">+				page_start, page_start + PMD_SIZE)) {</span>
<span class="p_add">+</span>
<span class="p_add">+		free_pagetable(pmd_page(*pmd),</span>
<span class="p_add">+				get_order(PMD_SIZE), false);</span>
<span class="p_add">+</span>
<span class="p_add">+		memblock_clear_unused_vmemmap(page_start, PMD_SIZE);</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * This spin lock could be only</span>
<span class="p_add">+		 * taken in _pte_aloc_kernel in</span>
<span class="p_add">+		 * mm/memory.c and nowhere else</span>
<span class="p_add">+		 * (for arm64). Not sure if the</span>
<span class="p_add">+		 * function above can be called</span>
<span class="p_add">+		 * concurrently. In doubt,</span>
<span class="p_add">+		 * I am living it here for now,</span>
<span class="p_add">+		 * but it probably can be removed.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		spin_lock(&amp;init_mm.page_table_lock);</span>
<span class="p_add">+		pmd_clear(pmd);</span>
<span class="p_add">+		spin_unlock(&amp;init_mm.page_table_lock);</span>
<span class="p_add">+</span>
<span class="p_add">+		flush_tlb_kernel_range(addr &amp; PMD_MASK,</span>
<span class="p_add">+				(addr + PMD_SIZE) &amp; PMD_MASK);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void rm_pte_mapping(pte_t *pte, unsigned long addr,</span>
<span class="p_add">+		unsigned long next, bool linear_map)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Linear map pages were already freed when offlining.</span>
<span class="p_add">+	 * We aonly need to free vmemmap pages.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (!linear_map)</span>
<span class="p_add">+		free_pagetable(pte_page(*pte), 0, false);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * This spin lock could be only</span>
<span class="p_add">+	 * taken in _pte_aloc_kernel in</span>
<span class="p_add">+	 * mm/memory.c and nowhere else</span>
<span class="p_add">+	 * (for arm64). Not sure if the</span>
<span class="p_add">+	 * function above can be called</span>
<span class="p_add">+	 * concurrently. In doubt,</span>
<span class="p_add">+	 * I am living it here for now,</span>
<span class="p_add">+	 * but it probably can be removed.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	spin_lock(&amp;init_mm.page_table_lock);</span>
<span class="p_add">+	pte_clear(&amp;init_mm, addr, pte);</span>
<span class="p_add">+	spin_unlock(&amp;init_mm.page_table_lock);</span>
<span class="p_add">+</span>
<span class="p_add">+	flush_tlb_kernel_range(addr, next);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void rm_pmd_mapping(pmd_t *pmd, unsigned long addr,</span>
<span class="p_add">+		unsigned long next, bool linear_map)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/* Freeing vmemmap pages */</span>
<span class="p_add">+	if (!linear_map)</span>
<span class="p_add">+		free_pagetable(pmd_page(*pmd),</span>
<span class="p_add">+				get_order(PMD_SIZE), false);</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * This spin lock could be only</span>
<span class="p_add">+	 * taken in _pte_aloc_kernel in</span>
<span class="p_add">+	 * mm/memory.c and nowhere else</span>
<span class="p_add">+	 * (for arm64). Not sure if the</span>
<span class="p_add">+	 * function above can be called</span>
<span class="p_add">+	 * concurrently. In doubt,</span>
<span class="p_add">+	 * I am living it here for now,</span>
<span class="p_add">+	 * but it probably can be removed.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	spin_lock(&amp;init_mm.page_table_lock);</span>
<span class="p_add">+	pmd_clear(pmd);</span>
<span class="p_add">+	spin_unlock(&amp;init_mm.page_table_lock);</span>
<span class="p_add">+</span>
<span class="p_add">+	flush_tlb_kernel_range(addr, next);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void rm_pud_mapping(pud_t *pud, unsigned long addr,</span>
<span class="p_add">+		unsigned long next, bool linear_map)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/** We never map vmemmap space on PUDs */</span>
<span class="p_add">+	BUG_ON(!linear_map);</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * This spin lock could be only</span>
<span class="p_add">+	 * taken in _pte_aloc_kernel in</span>
<span class="p_add">+	 * mm/memory.c and nowhere else</span>
<span class="p_add">+	 * (for arm64). Not sure if the</span>
<span class="p_add">+	 * function above can be called</span>
<span class="p_add">+	 * concurrently. In doubt,</span>
<span class="p_add">+	 * I am living it here for now,</span>
<span class="p_add">+	 * but it probably can be removed.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	spin_lock(&amp;init_mm.page_table_lock);</span>
<span class="p_add">+	pud_clear(pud);</span>
<span class="p_add">+	spin_unlock(&amp;init_mm.page_table_lock);</span>
<span class="p_add">+</span>
<span class="p_add">+	flush_tlb_kernel_range(addr, next);</span>
<span class="p_add">+}</span>
 
 /*
<span class="p_del">- * Check whether a kernel address is valid (derived from arch/x86/).</span>
<span class="p_add">+ * Used in hot-remove, cleans up PTE entries from addr to end from the pointed</span>
<span class="p_add">+ * pte table. If linear_map is true, this is used called to remove the tables</span>
<span class="p_add">+ * for the memory being hot-removed. If false, this is called to clean-up the</span>
<span class="p_add">+ * tables (and the memory) that were used for the vmemmap of memory being</span>
<span class="p_add">+ * hot-removed.</span>
  */
<span class="p_del">-int kern_addr_valid(unsigned long addr)</span>
<span class="p_add">+static void remove_pte_table(pte_t *pte, unsigned long addr,</span>
<span class="p_add">+	unsigned long end, bool linear_map)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long next;</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+	for (; addr &lt; end; addr = next, pte++) {</span>
<span class="p_add">+		next = (addr + PAGE_SIZE) &amp; PAGE_MASK;</span>
<span class="p_add">+		if (next &gt; end)</span>
<span class="p_add">+			next = end;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!pte_present(*pte))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (PAGE_ALIGNED(addr) &amp;&amp; PAGE_ALIGNED(next)) {</span>
<span class="p_add">+			rm_pte_mapping(pte, addr, next, linear_map);</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			unsigned long sz = next - addr;</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * If we are here, we are freeing vmemmap pages since</span>
<span class="p_add">+			 * linear_map mapped memory ranges to be freed</span>
<span class="p_add">+			 * are aligned.</span>
<span class="p_add">+			 *</span>
<span class="p_add">+			 * If we are not removing the whole page, it means</span>
<span class="p_add">+			 * other page structs in this page are being used and</span>
<span class="p_add">+			 * we canot remove them. We use memblock to mark these</span>
<span class="p_add">+			 * unused pieces and we only removed when they are fully</span>
<span class="p_add">+			 * unuesed.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			mark_n_free_pte_vmemmap(pte, addr, sz);</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * Used in hot-remove, cleans up PMD entries from addr to end from the pointed</span>
<span class="p_add">+ * pmd table.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * If linear_map is true, this is used called to remove the tables for the</span>
<span class="p_add">+ * memory being hot-removed. If false, this is called to clean-up the tables</span>
<span class="p_add">+ * (and the memory) that were used for the vmemmap of memory being hot-removed.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * If check_split is true, no change is done on the table: the call only</span>
<span class="p_add">+ * checks whether removing the entries would cause a section mapped PMD</span>
<span class="p_add">+ * to be split. In such a case, -EBUSY is returned by the method.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static int remove_pmd_table(pmd_t *pmd, unsigned long addr,</span>
<span class="p_add">+	unsigned long end, bool linear_map, bool check_split)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int err = 0;</span>
<span class="p_add">+	unsigned long next;</span>
<span class="p_add">+	pte_t *pte;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (; !err &amp;&amp; addr &lt; end; addr = next, pmd++) {</span>
<span class="p_add">+		next = pmd_addr_end(addr, end);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!pmd_present(*pmd))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (pmd_sect(*pmd)) {</span>
<span class="p_add">+			if (IS_ALIGNED(addr, PMD_SIZE) &amp;&amp;</span>
<span class="p_add">+					IS_ALIGNED(next, PMD_SIZE)) {</span>
<span class="p_add">+</span>
<span class="p_add">+				if (!check_split)</span>
<span class="p_add">+					rm_pmd_mapping(pmd, addr, next,</span>
<span class="p_add">+							linear_map);</span>
<span class="p_add">+</span>
<span class="p_add">+			} else { /* not aligned to PMD size */</span>
<span class="p_add">+</span>
<span class="p_add">+				/*</span>
<span class="p_add">+				 * This should only occur for vmemap.</span>
<span class="p_add">+				 * If it does happen for linear map,</span>
<span class="p_add">+				 * we do not support splitting PMDs,</span>
<span class="p_add">+				 * so we return error</span>
<span class="p_add">+				 */</span>
<span class="p_add">+				if (linear_map) {</span>
<span class="p_add">+					pr_warn(&quot;Hot-remove failed. Cannot split PMD mapping\n&quot;);</span>
<span class="p_add">+					err = -EBUSY;</span>
<span class="p_add">+				} else if (!check_split) {</span>
<span class="p_add">+					unsigned long sz = next - addr;</span>
<span class="p_add">+					/* Freeing vmemmap pages.*/</span>
<span class="p_add">+					mark_n_free_pmd_vmemmap(pmd, addr, sz);</span>
<span class="p_add">+				}</span>
<span class="p_add">+			}</span>
<span class="p_add">+		} else { /* ! pmd_sect() */</span>
<span class="p_add">+</span>
<span class="p_add">+			BUG_ON(!pmd_table(*pmd));</span>
<span class="p_add">+			if (!check_split) {</span>
<span class="p_add">+				pte = pte_offset_map(pmd, addr);</span>
<span class="p_add">+				remove_pte_table(pte, addr, next, linear_map);</span>
<span class="p_add">+				free_pte_table(addr, pmd, linear_map);</span>
<span class="p_add">+			}</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return err;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * Used in hot-remove, cleans up PUD entries from addr to end from the pointed</span>
<span class="p_add">+ * pmd table.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * If linear_map is true, this is used called to remove the tables for the</span>
<span class="p_add">+ * memory being hot-removed. If false, this is called to clean-up the tables</span>
<span class="p_add">+ * (and the memory) that were used for the vmemmap of memory being hot-removed.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * If check_split is true, no change is done on the table: the call only</span>
<span class="p_add">+ * checks whether removing the entries would cause a section mapped PUD</span>
<span class="p_add">+ * to be split. In such a case, -EBUSY is returned by the method.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static int remove_pud_table(pud_t *pud, unsigned long addr,</span>
<span class="p_add">+	unsigned long end, bool linear_map, bool check_split)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int err = 0;</span>
<span class="p_add">+	unsigned long next;</span>
<span class="p_add">+	pmd_t *pmd;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (; !err &amp;&amp; addr &lt; end; addr = next, pud++) {</span>
<span class="p_add">+		next = pud_addr_end(addr, end);</span>
<span class="p_add">+		if (!pud_present(*pud))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * If we are using 4K granules, check if we are using</span>
<span class="p_add">+		 * 1GB section mapping.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (pud_sect(*pud)) {</span>
<span class="p_add">+			if (IS_ALIGNED(addr, PUD_SIZE) &amp;&amp;</span>
<span class="p_add">+					IS_ALIGNED(next, PUD_SIZE)) {</span>
<span class="p_add">+</span>
<span class="p_add">+				if (!check_split)</span>
<span class="p_add">+					rm_pud_mapping(pud, addr, next,</span>
<span class="p_add">+							linear_map);</span>
<span class="p_add">+</span>
<span class="p_add">+			} else { /* not aligned to PUD size */</span>
<span class="p_add">+				/*</span>
<span class="p_add">+				 * As above, we never map vmemmap</span>
<span class="p_add">+				 * space on PUDs</span>
<span class="p_add">+				 */</span>
<span class="p_add">+				BUG_ON(!linear_map);</span>
<span class="p_add">+				pr_warn(&quot;Hot-remove failed. Cannot split PUD mapping\n&quot;);</span>
<span class="p_add">+				err = -EBUSY;</span>
<span class="p_add">+			}</span>
<span class="p_add">+		} else { /* !pud_sect() */</span>
<span class="p_add">+			BUG_ON(!pud_table(*pud));</span>
<span class="p_add">+</span>
<span class="p_add">+			pmd = pmd_offset(pud, addr);</span>
<span class="p_add">+			err = remove_pmd_table(pmd, addr, next,</span>
<span class="p_add">+					linear_map, check_split);</span>
<span class="p_add">+			if (!check_split)</span>
<span class="p_add">+				free_pmd_table(addr, pud, linear_map);</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return err;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * Used in hot-remove, cleans up kernel page tables from addr to end.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * If linear_map is true, this is used called to remove the tables for the</span>
<span class="p_add">+ * memory being hot-removed. If false, this is called to clean-up the tables</span>
<span class="p_add">+ * (and the memory) that were used for the vmemmap of memory being hot-removed.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * If check_split is true, no change is done on the table: the call only</span>
<span class="p_add">+ * checks whether removing the entries would cause a section mapped PUD</span>
<span class="p_add">+ * to be split. In such a case, -EBUSY is returned by the method.</span>
<span class="p_add">+ */</span>
<span class="p_add">+int remove_pagetable(unsigned long start, unsigned long end,</span>
<span class="p_add">+		bool linear_map, bool check_split)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int err;</span>
<span class="p_add">+	unsigned long next;</span>
<span class="p_add">+	unsigned long addr;</span>
<span class="p_add">+	pgd_t *pgd;</span>
<span class="p_add">+	pud_t *pud;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (addr = start; addr &lt; end; addr = next) {</span>
<span class="p_add">+		next = pgd_addr_end(addr, end);</span>
<span class="p_add">+</span>
<span class="p_add">+		pgd = pgd_offset_k(addr);</span>
<span class="p_add">+		if (pgd_none(*pgd))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		pud = pud_offset(pgd, addr);</span>
<span class="p_add">+		err = remove_pud_table(pud, addr, next,</span>
<span class="p_add">+				linear_map, check_split);</span>
<span class="p_add">+		if (err)</span>
<span class="p_add">+			break;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!check_split)</span>
<span class="p_add">+			free_pud_table(addr, pgd, linear_map);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!check_split)</span>
<span class="p_add">+		flush_tlb_all();</span>
<span class="p_add">+</span>
<span class="p_add">+	return err;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* CONFIG_MEMORY_HOTREMOVE */</span>
<span class="p_add">+#endif /* CONFIG_MEMORY_HOTPLUG */</span>
<span class="p_add">+</span>
<span class="p_add">+static unsigned long walk_kern_pgtable(unsigned long addr)</span>
 {
 	pgd_t *pgd;
 	pud_t *pud;
<span class="p_chunk">@@ -676,26 +1197,51 @@</span> <span class="p_context"> int kern_addr_valid(unsigned long addr)</span>
 		return 0;
 
 	if (pud_sect(*pud))
<span class="p_del">-		return pfn_valid(pud_pfn(*pud));</span>
<span class="p_add">+		return pud_pfn(*pud);</span>
 
 	pmd = pmd_offset(pud, addr);
 	if (pmd_none(*pmd))
 		return 0;
 
 	if (pmd_sect(*pmd))
<span class="p_del">-		return pfn_valid(pmd_pfn(*pmd));</span>
<span class="p_add">+		return pmd_pfn(*pmd);</span>
 
 	pte = pte_offset_kernel(pmd, addr);
 	if (pte_none(*pte))
 		return 0;
 
<span class="p_del">-	return pfn_valid(pte_pfn(*pte));</span>
<span class="p_add">+	return pte_pfn(*pte);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Check whether a kernel address is valid (derived from arch/x86/).</span>
<span class="p_add">+ */</span>
<span class="p_add">+int kern_addr_valid(unsigned long addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return pfn_valid(walk_kern_pgtable(addr));</span>
 }
<span class="p_add">+</span>
 #ifdef CONFIG_SPARSEMEM_VMEMMAP
 #if !ARM64_SWAPPER_USES_SECTION_MAPS
 int __meminit vmemmap_populate(unsigned long start, unsigned long end, int node)
 {
<span class="p_del">-	return vmemmap_populate_basepages(start, end, node);</span>
<span class="p_add">+	int err;</span>
<span class="p_add">+</span>
<span class="p_add">+	err = vmemmap_populate_basepages(start, end, node);</span>
<span class="p_add">+#ifdef CONFIG_MEMORY_HOTREMOVE</span>
<span class="p_add">+    /*</span>
<span class="p_add">+     * A bit inefficient (restarting from PGD every time) but saves</span>
<span class="p_add">+     * from lots of duplicated code. Also, this is only called</span>
<span class="p_add">+     * at hot-add time, which should not be a frequent operation</span>
<span class="p_add">+     */</span>
<span class="p_add">+	for (; start &lt; end; start += PAGE_SIZE) {</span>
<span class="p_add">+		unsigned long pfn = walk_kern_pgtable(start);</span>
<span class="p_add">+		phys_addr_t pa_start = ((phys_addr_t)pfn) &lt;&lt; PAGE_SHIFT;</span>
<span class="p_add">+</span>
<span class="p_add">+		memblock_clear_unused_vmemmap(pa_start, PAGE_SIZE);</span>
<span class="p_add">+	}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	return err;</span>
 }
 #else	/* !ARM64_SWAPPER_USES_SECTION_MAPS */
 int __meminit vmemmap_populate(unsigned long start, unsigned long end, int node)
<span class="p_chunk">@@ -726,8 +1272,15 @@</span> <span class="p_context"> int __meminit vmemmap_populate(unsigned long start, unsigned long end, int node)</span>
 				return -ENOMEM;
 
 			set_pmd(pmd, __pmd(__pa(p) | PROT_SECT_NORMAL));
<span class="p_del">-		} else</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			unsigned long sec_offset =  (addr &amp; (~PMD_MASK));</span>
<span class="p_add">+			phys_addr_t pa_start =</span>
<span class="p_add">+				pmd_page_paddr(*pmd) + sec_offset;</span>
 			vmemmap_verify((pte_t *)pmd, node, addr, next);
<span class="p_add">+#ifdef CONFIG_MEMORY_HOTREMOVE</span>
<span class="p_add">+			memblock_clear_unused_vmemmap(pa_start, next - addr);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+		}</span>
 	} while (addr = next, addr != end);
 
 	return 0;
<span class="p_chunk">@@ -735,6 +1288,9 @@</span> <span class="p_context"> int __meminit vmemmap_populate(unsigned long start, unsigned long end, int node)</span>
 #endif	/* CONFIG_ARM64_64K_PAGES */
 void vmemmap_free(unsigned long start, unsigned long end)
 {
<span class="p_add">+#ifdef CONFIG_MEMORY_HOTREMOVE</span>
<span class="p_add">+	remove_pagetable(start, end, false, false);</span>
<span class="p_add">+#endif</span>
 }
 #endif	/* CONFIG_SPARSEMEM_VMEMMAP */
 

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



