
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[2/9] RISC-V: Atomic and Locking Code - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [2/9] RISC-V: Atomic and Locking Code</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=90941">Palmer Dabbelt</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>July 4, 2017, 7:50 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20170704195102.3974-3-palmer@dabbelt.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9825553/mbox/"
   >mbox</a>
|
   <a href="/patch/9825553/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9825553/">/patch/9825553/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	F15FF60237 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  4 Jul 2017 19:53:48 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id ABC37262FF
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  4 Jul 2017 19:53:48 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 9F57B265B9; Tue,  4 Jul 2017 19:53:48 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.4 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	DKIM_VALID, RCVD_IN_DNSWL_HI,
	RCVD_IN_SORBS_SPAM autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 361A6262FF
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  4 Jul 2017 19:53:46 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752631AbdGDTxn (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 4 Jul 2017 15:53:43 -0400
Received: from mail-pg0-f68.google.com ([74.125.83.68]:33992 &quot;EHLO
	mail-pg0-f68.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1752312AbdGDTv0 (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 4 Jul 2017 15:51:26 -0400
Received: by mail-pg0-f68.google.com with SMTP id j186so28483442pge.1
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Tue, 04 Jul 2017 12:51:26 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=dabbelt-com.20150623.gappssmtp.com; s=20150623;
	h=from:to:to:to:to:to:to:to:to:to:to:to:to:to:to:to:to:to:cc:subject
	:date:message-id:in-reply-to:references;
	bh=NL7Q/GGglAfEh9uHQUdJRDGwtw5DEKjZ362wy5WO2ck=;
	b=YJQnwSF5kNsLbFWNoxb4kx/82oszor+v3LZRyxh9BpTS84INp2d1kFtb48VG0rj6/M
	e9lPJzGVvQGso5v8/WgeikN6AUCBXdVr7B2FFiqoWwnPnqjvnxb81gkx80ayUI7Q8unL
	aDYVwXkEWUIffMm9hNRSqN7d2AZRoYuzkc0ipbpxxeM1cEqOhaQqOcqj65+cOIllUe9o
	modrji10+2ZjiZ2gDvkr+WIJ542aJxy/Di5ljDjio/JGJ17sUpDi3V9rWc1u2YLB6HLx
	6eFsrzRVJry5+E/nGamNYnKWzKEH3JKPa4blZA2mIV46rYrv7BV4S/Z3laW88GWNpwkc
	UaTg==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20161025;
	h=x-gm-message-state:from:to:to:to:to:to:to:to:to:to:to:to:to:to:to
	:to:to:to:cc:subject:date:message-id:in-reply-to:references;
	bh=NL7Q/GGglAfEh9uHQUdJRDGwtw5DEKjZ362wy5WO2ck=;
	b=nOSrmgpGdkDGk6QKXkfygFSfmBDIoWblfYqTg4zMWNMoRpnYIcdJIiaZ65MwamkOmE
	U7VqhU9iZfcn0mBtGoR8qYQP0Q1saySRiEKeuceF70Ww1kBVrlzXzVfekBVGGLLm+Ejf
	xnFWO64XKZvGGjWdPbyeBsgsZo1dNuwcQaJMjWgh+6nZwVXujKrDkBNzfEIJynJstl0m
	Mm5uvOmTUhjvVFtT0R5L1qXdXIMuVy4+7Wy4K9NGSmrbqorgfWnxVy+Po7NOFaX6iTtD
	AM0ylpaux8NtQw6WWTJDu724vPKbY88p5xDEjaOMZmTH+lXPVYS1yErWyj6+5nllFAwh
	RO+w==
X-Gm-Message-State: AIVw112BjFm17G8wReP9v0tiSqGn8KQmqf2ipxeMwVDkva7KQkvmOjbi
	P5w10kYtwPY3M2eQ
X-Received: by 10.99.138.76 with SMTP id y73mr17470629pgd.203.1499197885176; 
	Tue, 04 Jul 2017 12:51:25 -0700 (PDT)
Received: from localhost ([2600:8806:8200:2700::3])
	by smtp.gmail.com with ESMTPSA id
	z82sm49336161pfk.1.2017.07.04.12.51.24
	(version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
	Tue, 04 Jul 2017 12:51:24 -0700 (PDT)
From: Palmer Dabbelt &lt;palmer@dabbelt.com&gt;
To: peterz@infradead.org
To: mingo@redhat.com
To: mcgrof@kernel.org
To: viro@zeniv.linux.org.uk
To: sfr@canb.auug.org.au
To: nicolas.dichtel@6wind.com
To: rmk+kernel@armlinux.org.uk
To: msalter@redhat.com
To: tklauser@distanz.ch
To: will.deacon@arm.com
To: james.hogan@imgtec.com
To: paul.gortmaker@windriver.com
To: linux@roeck-us.net
To: linux-kernel@vger.kernel.org
To: linux-arch@vger.kernel.org
To: albert@sifive.com
To: patches@groups.riscv.org
Cc: Palmer Dabbelt &lt;palmer@dabbelt.com&gt;
Subject: [PATCH 2/9] RISC-V: Atomic and Locking Code
Date: Tue,  4 Jul 2017 12:50:55 -0700
Message-Id: &lt;20170704195102.3974-3-palmer@dabbelt.com&gt;
X-Mailer: git-send-email 2.13.0
In-Reply-To: &lt;20170704195102.3974-1-palmer@dabbelt.com&gt;
References: &lt;20170704195102.3974-1-palmer@dabbelt.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=90941">Palmer Dabbelt</a> - July 4, 2017, 7:50 p.m.</div>
<pre class="content">
This contains all the code that directly interfaces with the RISC-V
memory model.  While this code corforms to the current RISC-V ISA
specifications (user 2.2 and priv 1.10), the memory model is somewhat
underspecified in those documents.  There is a working group that hopes
to produce a formal memory model by the end of the year, but my
understanding is that the basic definitions we&#39;re relying on here won&#39;t
change significantly.
<span class="signed-off-by">
Signed-off-by: Palmer Dabbelt &lt;palmer@dabbelt.com&gt;</span>
---
 arch/riscv/include/asm/atomic.h         | 337 ++++++++++++++++++++++++++++++++
 arch/riscv/include/asm/barrier.h        | 143 ++++++++++++++
 arch/riscv/include/asm/bitops.h         | 228 +++++++++++++++++++++
 arch/riscv/include/asm/cacheflush.h     |  39 ++++
 arch/riscv/include/asm/cmpxchg.h        | 138 +++++++++++++
 arch/riscv/include/asm/io.h             | 180 +++++++++++++++++
 arch/riscv/include/asm/spinlock.h       | 167 ++++++++++++++++
 arch/riscv/include/asm/spinlock_types.h |  33 ++++
 arch/riscv/include/asm/tlb.h            |  24 +++
 arch/riscv/include/asm/tlbflush.h       |  64 ++++++
 10 files changed, 1353 insertions(+)
 create mode 100644 arch/riscv/include/asm/atomic.h
 create mode 100644 arch/riscv/include/asm/barrier.h
 create mode 100644 arch/riscv/include/asm/bitops.h
 create mode 100644 arch/riscv/include/asm/cacheflush.h
 create mode 100644 arch/riscv/include/asm/cmpxchg.h
 create mode 100644 arch/riscv/include/asm/io.h
 create mode 100644 arch/riscv/include/asm/spinlock.h
 create mode 100644 arch/riscv/include/asm/spinlock_types.h
 create mode 100644 arch/riscv/include/asm/tlb.h
 create mode 100644 arch/riscv/include/asm/tlbflush.h
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - July 5, 2017, 8:43 a.m.</div>
<pre class="content">
On Tue, Jul 04, 2017 at 12:50:55PM -0700, Palmer Dabbelt wrote:
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * FIXME: I could only find documentation that atomic_{add,sub,inc,dec} are</span>
<span class="quote">&gt; + * barrier-free.  I&#39;m assuming that and/or/xor have the same constraints as the</span>
<span class="quote">&gt; + * others.</span>
<span class="quote">&gt; + */</span>

Yes.. we have new documentation in the work to which I would post a link
but for some reason copy/paste stopped working again (Konsole does that
at times and is #$%#$%#4# annoying).

Ha, found it using google...

  https://marc.info/?l=linux-kernel&amp;m=14972790112580
<span class="quote">

&gt; +</span>
<span class="quote">
&gt; +/*</span>
<span class="quote">&gt; + * atomic_{cmp,}xchg is required to have exactly the same ordering semantics as</span>
<span class="quote">&gt; + * {cmp,}xchg and the operations that return, so they need a barrier.  We just</span>
<span class="quote">&gt; + * use the other implementations directly.</span>
<span class="quote">&gt; + */</span>

cmpxchg triggers an extra rule; all conditional operations only need to
imply barriers on success. So a cmpxchg that fails, need not imply any
ordering what so ever.
<span class="quote">
&gt; +/*</span>
<span class="quote">&gt; + * Our atomic operations set the AQ and RL bits and therefor we don&#39;t need to</span>
<span class="quote">&gt; + * fence around atomics.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +#define __smb_mb__before_atomic()	barrier()</span>
<span class="quote">&gt; +#define __smb_mb__after_atomic()	barrier()</span>

Ah, not quite... you need full barriers here. Because your regular
atomic ops imply no ordering what so ever.
<span class="quote">
&gt; +/*</span>
<span class="quote">&gt; + * These barries are meant to prevent memory operations inside a spinlock from</span>
<span class="quote">&gt; + * moving outside of that spinlock.  Since we set the AQ and RL bits when</span>
<span class="quote">&gt; + * entering or leaving spinlocks, no additional fence needs to be performed.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +#define smb_mb__before_spinlock()	barrier()</span>
<span class="quote">&gt; +#define smb_mb__after_spinlock()	barrier()</span>

Also probably not true. I _think_ you want a full barrier here, but
given the total lack of documentation on your end and the fact I&#39;ve not
yet read the spinlock (which I suppose is below) I cannot yet state
more.
<span class="quote">

&gt; +</span>
<span class="quote">&gt; +/* FIXME: I don&#39;t think RISC-V is allowed to perform a speculative load. */</span>
<span class="quote">&gt; +#define smp_acquire__after_ctrl_dep()	barrier()</span>

That would be a very weird thing to disallow... speculative loads are
teh awesome ;-) Note you can get the very same effect from caches when
your stores are not globally atomic.
<span class="quote">
&gt; +/*</span>
<span class="quote">&gt; + * The RISC-V ISA doesn&#39;t support byte or half-word AMOs, so we fall back to a</span>
<span class="quote">&gt; + * regular store and a fence here.  Otherwise we emit an AMO with an AQ or RL</span>
<span class="quote">&gt; + * bit set and allow the microarchitecture to avoid the other half of the AMO.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +#define __smp_store_release(p, v)					\</span>
<span class="quote">&gt; +do {									\</span>
<span class="quote">&gt; +	union { typeof(*p) __val; char __c[1]; } __u =			\</span>
<span class="quote">&gt; +		{ .__val = (__force typeof(*p)) (v) };			\</span>
<span class="quote">&gt; +	compiletime_assert_atomic_type(*p);				\</span>
<span class="quote">&gt; +	switch (sizeof(*p)) {						\</span>
<span class="quote">&gt; +	case 1:								\</span>
<span class="quote">&gt; +	case 2:								\</span>
<span class="quote">&gt; +		smb_mb();						\</span>
<span class="quote">&gt; +		WRITE_ONCE(*p, __u.__val);				\</span>
<span class="quote">&gt; +		break;							\</span>
<span class="quote">&gt; +	case 4:								\</span>
<span class="quote">&gt; +		__asm__ __volatile__ (					\</span>
<span class="quote">&gt; +			&quot;amoswap.w.rl zero, %1, %0&quot;			\</span>
<span class="quote">&gt; +			: &quot;+A&quot; (*p), &quot;r&quot; (__u.__val)			\</span>
<span class="quote">&gt; +			: 						\</span>
<span class="quote">&gt; +			: &quot;memory&quot;);					\</span>
<span class="quote">&gt; +		break;							\</span>
<span class="quote">&gt; +	case 8:								\</span>
<span class="quote">&gt; +		__asm__ __volatile__ (					\</span>
<span class="quote">&gt; +			&quot;amoswap.d.rl zero, %1, %0&quot;			\</span>
<span class="quote">&gt; +			: &quot;+A&quot; (*p), &quot;r&quot; (__u.__val)			\</span>
<span class="quote">&gt; +			: 						\</span>
<span class="quote">&gt; +			: &quot;memory&quot;);					\</span>
<span class="quote">&gt; +		break;							\</span>
<span class="quote">&gt; +	}								\</span>
<span class="quote">&gt; +} while (0)</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define __smp_load_acquire(p)						\</span>
<span class="quote">&gt; +do {									\</span>
<span class="quote">&gt; +	union { typeof(*p) __val; char __c[1]; } __u =			\</span>
<span class="quote">&gt; +		{ .__val = (__force typeof(*p)) (v) };			\</span>
<span class="quote">&gt; +	compiletime_assert_atomic_type(*p);				\</span>
<span class="quote">&gt; +	switch (sizeof(*p)) {						\</span>
<span class="quote">&gt; +	case 1:								\</span>
<span class="quote">&gt; +	case 2:								\</span>
<span class="quote">&gt; +		__u.__val = READ_ONCE(*p);				\</span>
<span class="quote">&gt; +		smb_mb();						\</span>
<span class="quote">&gt; +		break;							\</span>
<span class="quote">&gt; +	case 4:								\</span>
<span class="quote">&gt; +		__asm__ __volatile__ (					\</span>
<span class="quote">&gt; +			&quot;amoor.w.aq %1, zero, %0&quot;			\</span>
<span class="quote">&gt; +			: &quot;+A&quot; (*p)					\</span>
<span class="quote">&gt; +			: &quot;=r&quot; (__u.__val)				\</span>
<span class="quote">&gt; +			: &quot;memory&quot;);					\</span>
<span class="quote">&gt; +		break;							\</span>
<span class="quote">&gt; +	case 8:								\</span>
<span class="quote">&gt; +		__asm__ __volatile__ (					\</span>
<span class="quote">&gt; +			&quot;amoor.d.aq %1, zero, %0&quot;			\</span>
<span class="quote">&gt; +			: &quot;+A&quot; (*p)					\</span>
<span class="quote">&gt; +			: &quot;=r&quot; (__u.__val)				\</span>
<span class="quote">&gt; +			: &quot;memory&quot;);					\</span>
<span class="quote">&gt; +		break;							\</span>
<span class="quote">&gt; +	}								\</span>
<span class="quote">&gt; +	__u.__val;							\</span>
<span class="quote">&gt; +} while (0)</span>

&#39;creative&#39; use of amoswap and amoor :-)

You should really look at a normal load with ordering instruction
though, that amoor.aq is a rmw and will promote the cacheline to
exclusive (and dirty it).
<span class="quote">
&gt; +/*</span>
<span class="quote">&gt; + * Simple spin lock operations.  These provide no fairness guarantees.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/* FIXME: Replace this with a ticket lock, like MIPS. */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define arch_spin_lock_flags(lock, flags) arch_spin_lock(lock)</span>
<span class="quote">&gt; +#define arch_spin_is_locked(x)	((x)-&gt;lock != 0)</span>
<span class="quote">&gt; +#define arch_spin_unlock_wait(x) \</span>
<span class="quote">&gt; +		do { cpu_relax(); } while ((x)-&gt;lock)</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline void arch_spin_unlock(arch_spinlock_t *lock)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	__asm__ __volatile__ (</span>
<span class="quote">&gt; +		&quot;amoswap.w.rl x0, x0, %0&quot;</span>
<span class="quote">&gt; +		: &quot;=A&quot; (lock-&gt;lock)</span>
<span class="quote">&gt; +		:: &quot;memory&quot;);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline int arch_spin_trylock(arch_spinlock_t *lock)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	int tmp = 1, busy;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	__asm__ __volatile__ (</span>
<span class="quote">&gt; +		&quot;amoswap.w.aq %0, %2, %1&quot;</span>
<span class="quote">&gt; +		: &quot;=r&quot; (busy), &quot;+A&quot; (lock-&gt;lock)</span>
<span class="quote">&gt; +		: &quot;r&quot; (tmp)</span>
<span class="quote">&gt; +		: &quot;memory&quot;);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return !busy;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline void arch_spin_lock(arch_spinlock_t *lock)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	while (1) {</span>
<span class="quote">&gt; +		if (arch_spin_is_locked(lock))</span>
<span class="quote">&gt; +			continue;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		if (arch_spin_trylock(lock))</span>
<span class="quote">&gt; +			break;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +}</span>

OK, so back to smp_mb__{before,after}_spinlock(), that wants to order
things like:

	wakeup:					block:

	COND = 1;				p-&gt;state = UNINTERRUPTIBLE;
						smp_mb();
	smp_mb__before_spinlock();
	spin_lock(&amp;lock);			if (!COND)
						  schedule()
	if (p-&gt;state &amp; state)
		goto out;


And here it is important that the COND store not happen _after_ the
p-&gt;state load.

Now, your spin_lock() only implies the AQ thing, which should only
constraint later load/stores but does nothing for the prior load/stores.
So our COND store can drop into the lock and even happen after the
p-&gt;state load.

So you very much want your smp_mb__{before,after}_spinlock thingies to
be full barriers.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - July 6, 2017, 7:26 a.m.</div>
<pre class="content">
On Thu, Jul 06, 2017 at 07:08:33PM +0800, Boqun Feng wrote:
<span class="quote">&gt; On Wed, Jul 05, 2017 at 10:43:21AM +0200, Peter Zijlstra wrote:</span>
<span class="quote">&gt; &gt; On Tue, Jul 04, 2017 at 12:50:55PM -0700, Palmer Dabbelt wrote:</span>
<span class="quote">&gt; &gt; &gt; +/*</span>
<span class="quote">&gt; &gt; &gt; + * FIXME: I could only find documentation that atomic_{add,sub,inc,dec} are</span>
<span class="quote">&gt; &gt; &gt; + * barrier-free.  I&#39;m assuming that and/or/xor have the same constraints as the</span>
<span class="quote">&gt; &gt; &gt; + * others.</span>
<span class="quote">&gt; &gt; &gt; + */</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Yes.. we have new documentation in the work to which I would post a link</span>
<span class="quote">&gt; &gt; but for some reason copy/paste stopped working again (Konsole does that</span>
<span class="quote">&gt; &gt; at times and is #$%#$%#4# annoying).</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Ha, found it using google...</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt;   https://marc.info/?l=linux-kernel&amp;m=14972790112580</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The link is broken, you miss a tailing 1 ;-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	https://marc.info/?l=linux-kernel&amp;m=149727901125801</span>
<span class="quote">&gt; </span>

Argh... thanks for fixing that!
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=124171">Boqun Feng</a> - July 6, 2017, 10:33 a.m.</div>
<pre class="content">
On Tue, Jul 04, 2017 at 12:50:55PM -0700, Palmer Dabbelt wrote:
[...]
<span class="quote">&gt; diff --git a/arch/riscv/include/asm/cmpxchg.h b/arch/riscv/include/asm/cmpxchg.h</span>
<span class="quote">&gt; new file mode 100644</span>
<span class="quote">&gt; index 000000000000..81025c056412</span>
<span class="quote">&gt; --- /dev/null</span>
<span class="quote">&gt; +++ b/arch/riscv/include/asm/cmpxchg.h</span>
<span class="quote">&gt; @@ -0,0 +1,138 @@</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * Copyright (C) 2014 Regents of the University of California</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + *   This program is free software; you can redistribute it and/or</span>
<span class="quote">&gt; + *   modify it under the terms of the GNU General Public License</span>
<span class="quote">&gt; + *   as published by the Free Software Foundation, version 2.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + *   This program is distributed in the hope that it will be useful,</span>
<span class="quote">&gt; + *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="quote">&gt; + *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="quote">&gt; + *   GNU General Public License for more details.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#ifndef _ASM_RISCV_CMPXCHG_H</span>
<span class="quote">&gt; +#define _ASM_RISCV_CMPXCHG_H</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#include &lt;linux/bug.h&gt;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#ifdef CONFIG_ISA_A</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#include &lt;asm/barrier.h&gt;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define __xchg(new, ptr, size, asm_or)				\</span>
<span class="quote">&gt; +({								\</span>
<span class="quote">&gt; +	__typeof__(ptr) __ptr = (ptr);				\</span>
<span class="quote">&gt; +	__typeof__(new) __new = (new);				\</span>
<span class="quote">&gt; +	__typeof__(*(ptr)) __ret;				\</span>
<span class="quote">&gt; +	switch (size) {						\</span>
<span class="quote">&gt; +	case 4:							\</span>
<span class="quote">&gt; +		__asm__ __volatile__ (				\</span>
<span class="quote">&gt; +			&quot;amoswap.w&quot; #asm_or &quot; %0, %2, %1&quot;	\</span>
<span class="quote">&gt; +			: &quot;=r&quot; (__ret), &quot;+A&quot; (*__ptr)		\</span>
<span class="quote">&gt; +			: &quot;r&quot; (__new));				\</span>

It seems that you miss the &quot;memmory&quot; clobber here, so as for cmpxchg(),
did you do this on purpose? AFAIK, without this clobber, compilers are
within their right to reorder operations preceding and following this
operation, which makes it unordered.
<span class="quote">
&gt; +		break;						\</span>
<span class="quote">&gt; +	case 8:							\</span>
<span class="quote">&gt; +		__asm__ __volatile__ (				\</span>
<span class="quote">&gt; +			&quot;amoswap.d&quot; #asm_or &quot; %0, %2, %1&quot;	\</span>
<span class="quote">&gt; +			: &quot;=r&quot; (__ret), &quot;+A&quot; (*__ptr)		\</span>
<span class="quote">&gt; +			: &quot;r&quot; (__new));				\</span>
<span class="quote">&gt; +		break;						\</span>
<span class="quote">&gt; +	default:						\</span>
<span class="quote">&gt; +		BUILD_BUG();					\</span>
<span class="quote">&gt; +	}							\</span>
<span class="quote">&gt; +	__ret;							\</span>
<span class="quote">&gt; +})</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define xchg(ptr, x)    (__xchg((x), (ptr), sizeof(*(ptr)), .aqrl))</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define xchg32(ptr, x)				\</span>
<span class="quote">&gt; +({						\</span>
<span class="quote">&gt; +	BUILD_BUG_ON(sizeof(*(ptr)) != 4);	\</span>
<span class="quote">&gt; +	xchg((ptr), (x));			\</span>
<span class="quote">&gt; +})</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define xchg64(ptr, x)				\</span>
<span class="quote">&gt; +({						\</span>
<span class="quote">&gt; +	BUILD_BUG_ON(sizeof(*(ptr)) != 8);	\</span>
<span class="quote">&gt; +	xchg((ptr), (x));			\</span>
<span class="quote">&gt; +})</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * Atomic compare and exchange.  Compare OLD with MEM, if identical,</span>
<span class="quote">&gt; + * store NEW in MEM.  Return the initial value in MEM.  Success is</span>
<span class="quote">&gt; + * indicated by comparing RETURN with OLD.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +#define __cmpxchg(ptr, old, new, size, lrb, scb)			\</span>
<span class="quote">&gt; +({									\</span>
<span class="quote">&gt; +	__typeof__(ptr) __ptr = (ptr);					\</span>
<span class="quote">&gt; +	__typeof__(old) __old = (old);					\</span>
<span class="quote">&gt; +	__typeof__(new) __new = (new);					\</span>

Better write those two lines as:

	__typeof__(*(ptr)) __old = (old);					\
	__typeof__(*(ptr)) __new = (new);					\

? I&#39;m thinking the case where @old and @new are int and ptr is &quot;long *&quot;,
could the asm below do the implicitly converting right, i.e. keep the
sign bit?

Regards,
Boqun
<span class="quote">
&gt; +	__typeof__(*(ptr)) __ret;					\</span>
<span class="quote">&gt; +	register unsigned int __rc;					\</span>
<span class="quote">&gt; +	switch (size) {							\</span>
<span class="quote">&gt; +	case 4:								\</span>
<span class="quote">&gt; +		__asm__ __volatile__ (					\</span>
<span class="quote">&gt; +		&quot;0:&quot;							\</span>
<span class="quote">&gt; +			&quot;lr.w&quot; #scb &quot; %0, %2\n&quot;				\</span>
<span class="quote">&gt; +			&quot;bne         %0, %z3, 1f\n&quot;			\</span>
<span class="quote">&gt; +			&quot;sc.w&quot; #lrb &quot; %1, %z4, %2\n&quot;			\</span>
<span class="quote">&gt; +			&quot;bnez        %1, 0b\n&quot;				\</span>
<span class="quote">&gt; +		&quot;1:&quot;							\</span>
<span class="quote">&gt; +			: &quot;=&amp;r&quot; (__ret), &quot;=&amp;r&quot; (__rc), &quot;+A&quot; (*__ptr)	\</span>
<span class="quote">&gt; +			: &quot;rJ&quot; (__old), &quot;rJ&quot; (__new));			\</span>
<span class="quote">&gt; +		break;							\</span>
<span class="quote">&gt; +	case 8:								\</span>
<span class="quote">&gt; +		__asm__ __volatile__ (					\</span>
<span class="quote">&gt; +		&quot;0:&quot;							\</span>
<span class="quote">&gt; +			&quot;lr.d&quot; #scb &quot; %0, %2\n&quot;				\</span>
<span class="quote">&gt; +			&quot;bne         %0, %z3, 1f\n&quot;			\</span>
<span class="quote">&gt; +			&quot;sc.d&quot; #lrb &quot; %1, %z4, %2\n&quot;			\</span>
<span class="quote">&gt; +			&quot;bnez        %1, 0b\n&quot;				\</span>
<span class="quote">&gt; +		&quot;1:&quot;							\</span>
<span class="quote">&gt; +			: &quot;=&amp;r&quot; (__ret), &quot;=&amp;r&quot; (__rc), &quot;+A&quot; (*__ptr)	\</span>
<span class="quote">&gt; +			: &quot;rJ&quot; (__old), &quot;rJ&quot; (__new));			\</span>
<span class="quote">&gt; +		break;							\</span>
<span class="quote">&gt; +	default:							\</span>
<span class="quote">&gt; +		BUILD_BUG();						\</span>
<span class="quote">&gt; +	}								\</span>
<span class="quote">&gt; +	__ret;								\</span>
<span class="quote">&gt; +})</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define cmpxchg(ptr, o, n) \</span>
<span class="quote">&gt; +	(__cmpxchg((ptr), (o), (n), sizeof(*(ptr)), .aqrl, .aqrl))</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define cmpxchg_local(ptr, o, n) \</span>
<span class="quote">&gt; +	(__cmpxchg((ptr), (o), (n), sizeof(*(ptr)), , ))</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define cmpxchg32(ptr, o, n)			\</span>
<span class="quote">&gt; +({						\</span>
<span class="quote">&gt; +	BUILD_BUG_ON(sizeof(*(ptr)) != 4);	\</span>
<span class="quote">&gt; +	cmpxchg((ptr), (o), (n));		\</span>
<span class="quote">&gt; +})</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define cmpxchg32_local(ptr, o, n)		\</span>
<span class="quote">&gt; +({						\</span>
<span class="quote">&gt; +	BUILD_BUG_ON(sizeof(*(ptr)) != 4);	\</span>
<span class="quote">&gt; +	cmpxchg_local((ptr), (o), (n));		\</span>
<span class="quote">&gt; +})</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define cmpxchg64(ptr, o, n)			\</span>
<span class="quote">&gt; +({						\</span>
<span class="quote">&gt; +	BUILD_BUG_ON(sizeof(*(ptr)) != 8);	\</span>
<span class="quote">&gt; +	cmpxchg((ptr), (o), (n));		\</span>
<span class="quote">&gt; +})</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define cmpxchg64_local(ptr, o, n)		\</span>
<span class="quote">&gt; +({						\</span>
<span class="quote">&gt; +	BUILD_BUG_ON(sizeof(*(ptr)) != 8);	\</span>
<span class="quote">&gt; +	cmpxchg_local((ptr), (o), (n));		\</span>
<span class="quote">&gt; +})</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#else /* !CONFIG_ISA_A */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#include &lt;asm-generic/cmpxchg.h&gt;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#endif /* CONFIG_ISA_A */</span>
<span class="quote">&gt; +</span>

[...]
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=124171">Boqun Feng</a> - July 6, 2017, 11:08 a.m.</div>
<pre class="content">
On Wed, Jul 05, 2017 at 10:43:21AM +0200, Peter Zijlstra wrote:
<span class="quote">&gt; On Tue, Jul 04, 2017 at 12:50:55PM -0700, Palmer Dabbelt wrote:</span>
<span class="quote">&gt; &gt; +/*</span>
<span class="quote">&gt; &gt; + * FIXME: I could only find documentation that atomic_{add,sub,inc,dec} are</span>
<span class="quote">&gt; &gt; + * barrier-free.  I&#39;m assuming that and/or/xor have the same constraints as the</span>
<span class="quote">&gt; &gt; + * others.</span>
<span class="quote">&gt; &gt; + */</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Yes.. we have new documentation in the work to which I would post a link</span>
<span class="quote">&gt; but for some reason copy/paste stopped working again (Konsole does that</span>
<span class="quote">&gt; at times and is #$%#$%#4# annoying).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Ha, found it using google...</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;   https://marc.info/?l=linux-kernel&amp;m=14972790112580</span>
<span class="quote">&gt; </span>

The link is broken, you miss a tailing 1 ;-)

	https://marc.info/?l=linux-kernel&amp;m=149727901125801

Regards,
Boqun
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=90941">Palmer Dabbelt</a> - July 7, 2017, 1:04 a.m.</div>
<pre class="content">
On Wed, 05 Jul 2017 01:43:21 PDT (-0700), peterz@infradead.org wrote:
<span class="quote">&gt; On Tue, Jul 04, 2017 at 12:50:55PM -0700, Palmer Dabbelt wrote:</span>
<span class="quote">&gt;&gt; +/*</span>
<span class="quote">&gt;&gt; + * FIXME: I could only find documentation that atomic_{add,sub,inc,dec} are</span>
<span class="quote">&gt;&gt; + * barrier-free.  I&#39;m assuming that and/or/xor have the same constraints as the</span>
<span class="quote">&gt;&gt; + * others.</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Yes.. we have new documentation in the work to which I would post a link</span>
<span class="quote">&gt; but for some reason copy/paste stopped working again (Konsole does that</span>
<span class="quote">&gt; at times and is #$%#$%#4# annoying).</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Ha, found it using google...</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;   https://marc.info/?l=linux-kernel&amp;m=149727901125801</span>

Thanks.
<span class="quote">
&gt;&gt; +/*</span>
<span class="quote">&gt;&gt; + * atomic_{cmp,}xchg is required to have exactly the same ordering semantics as</span>
<span class="quote">&gt;&gt; + * {cmp,}xchg and the operations that return, so they need a barrier.  We just</span>
<span class="quote">&gt;&gt; + * use the other implementations directly.</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; cmpxchg triggers an extra rule; all conditional operations only need to</span>
<span class="quote">&gt; imply barriers on success. So a cmpxchg that fails, need not imply any</span>
<span class="quote">&gt; ordering what so ever.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; +/*</span>
<span class="quote">&gt;&gt; + * Our atomic operations set the AQ and RL bits and therefor we don&#39;t need to</span>
<span class="quote">&gt;&gt; + * fence around atomics.</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;&gt; +#define __smb_mb__before_atomic()	barrier()</span>
<span class="quote">&gt;&gt; +#define __smb_mb__after_atomic()	barrier()</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Ah, not quite... you need full barriers here. Because your regular</span>
<span class="quote">&gt; atomic ops imply no ordering what so ever.</span>

The new documentation helps here, too.  Thanks!

  diff --git a/arch/riscv/include/asm/barrier.h b/arch/riscv/include/asm/barrier.h
  index 82a0092a86d0..a480c0fb85e5 100644
  --- a/arch/riscv/include/asm/barrier.h
  +++ b/arch/riscv/include/asm/barrier.h
  @@ -39,11 +39,19 @@
   #define smp_wmb()      RISCV_FENCE(w,w)

   /*
  - * Our atomic operations set the AQ and RL bits and therefor we don&#39;t need to
  - * fence around atomics.
  + * These fences exist to enforce ordering around the relaxed AMOs.  The
  + * documentation defines that
  + * &quot;
  + *     atomic_fetch_add();
  + *   is equivalent to:
  + *     smp_mb__before_atomic();
  + *     atomic_fetch_add_relaxed();
  + *     smp_mb__after_atomic();
  + * &quot;
  + * So we emit full fences on both sides.
    */
  -#define __smb_mb__before_atomic()      barrier()
  -#define __smb_mb__after_atomic()       barrier()
  +#define __smb_mb__before_atomic()      smp_mb()
  +#define __smb_mb__after_atomic()       smp_mb()

   /*
    * These barries are meant to prevent memory operations inside a spinlock from
<span class="quote">
&gt;&gt; +/*</span>
<span class="quote">&gt;&gt; + * These barries are meant to prevent memory operations inside a spinlock from</span>
<span class="quote">&gt;&gt; + * moving outside of that spinlock.  Since we set the AQ and RL bits when</span>
<span class="quote">&gt;&gt; + * entering or leaving spinlocks, no additional fence needs to be performed.</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;&gt; +#define smb_mb__before_spinlock()	barrier()</span>
<span class="quote">&gt;&gt; +#define smb_mb__after_spinlock()	barrier()</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Also probably not true. I _think_ you want a full barrier here, but</span>
<span class="quote">&gt; given the total lack of documentation on your end and the fact I&#39;ve not</span>
<span class="quote">&gt; yet read the spinlock (which I suppose is below) I cannot yet state</span>
<span class="quote">&gt; more.</span>

Ya, sorry about that -- we&#39;re waiting on a proper memory model spec.  Is there
any other documentation I should produce?

More below.
<span class="quote">
&gt;&gt; +</span>
<span class="quote">&gt;&gt; +/* FIXME: I don&#39;t think RISC-V is allowed to perform a speculative load. */</span>
<span class="quote">&gt;&gt; +#define smp_acquire__after_ctrl_dep()	barrier()</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; That would be a very weird thing to disallow... speculative loads are</span>
<span class="quote">&gt; teh awesome ;-) Note you can get the very same effect from caches when</span>
<span class="quote">&gt; your stores are not globally atomic.</span>

OK -- I guess generally the user ISA spec is written disregarding
microarchitecture, so I assumed this would be illegal.  We&#39;ll wait for the
memory model spec.

  diff --git a/arch/riscv/include/asm/barrier.h b/arch/riscv/include/asm/barrier.h
  index a4e54f4c17eb..c039333d4a5d 100644
  --- a/arch/riscv/include/asm/barrier.h
  +++ b/arch/riscv/include/asm/barrier.h
  @@ -61,8 +61,12 @@
   #define smb_mb__before_spinlock()      smp_mb()
   #define smb_mb__after_spinlock()       smp_mb()

  -/* FIXME: I don&#39;t think RISC-V is allowed to perform a speculative load. */
  -#define smp_acquire__after_ctrl_dep()  barrier()
  +/*
  + * TODO_RISCV_MEMORY_MODEL: I don&#39;t think RISC-V is allowed to perform a
  + * speculative load, but we&#39;re going to wait on a formal memory model in order
  + * to ensure this is safe to elide.
  + */
  +#define smp_acquire__after_ctrl_dep()  smp_mb()

   /*
    * The RISC-V ISA doesn&#39;t support byte or half-word AMOs, so we fall back to a
  @@ -137,24 +141,6 @@
          __u.__val;                                                      \
   })

  -/*
  - * The default implementation of this uses READ_ONCE and
  - * smp_acquire__after_ctrl_dep, but since we can directly do an ACQUIRE load we
  - * can avoid the extra barrier.
  - */
  -#define smp_cond_load_acquire(ptr, cond_expr) ({                       \
  -       typeof(ptr) __PTR = (ptr);                                      \
  -       typeof(*ptr) VAL;                                               \
  -       for (;;) {                                                      \
  -               VAL = __smp_load_acquire(__PTR);                        \
  -               if (cond_expr)                                          \
  -                       break;                                          \
  -               cpu_relax();                                            \
  -       }                                                               \
  -       smp_acquire__after_ctrl_dep();                                  \
  -       VAL;                                                            \
  -})
  -
   #include &lt;asm-generic/barrier.h&gt;

   #endif /* __ASSEMBLY__ */
<span class="quote">
&gt;&gt; +/*</span>
<span class="quote">&gt;&gt; + * The RISC-V ISA doesn&#39;t support byte or half-word AMOs, so we fall back to a</span>
<span class="quote">&gt;&gt; + * regular store and a fence here.  Otherwise we emit an AMO with an AQ or RL</span>
<span class="quote">&gt;&gt; + * bit set and allow the microarchitecture to avoid the other half of the AMO.</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;&gt; +#define __smp_store_release(p, v)					\</span>
<span class="quote">&gt;&gt; +do {									\</span>
<span class="quote">&gt;&gt; +	union { typeof(*p) __val; char __c[1]; } __u =			\</span>
<span class="quote">&gt;&gt; +		{ .__val = (__force typeof(*p)) (v) };			\</span>
<span class="quote">&gt;&gt; +	compiletime_assert_atomic_type(*p);				\</span>
<span class="quote">&gt;&gt; +	switch (sizeof(*p)) {						\</span>
<span class="quote">&gt;&gt; +	case 1:								\</span>
<span class="quote">&gt;&gt; +	case 2:								\</span>
<span class="quote">&gt;&gt; +		smb_mb();						\</span>
<span class="quote">&gt;&gt; +		WRITE_ONCE(*p, __u.__val);				\</span>
<span class="quote">&gt;&gt; +		break;							\</span>
<span class="quote">&gt;&gt; +	case 4:								\</span>
<span class="quote">&gt;&gt; +		__asm__ __volatile__ (					\</span>
<span class="quote">&gt;&gt; +			&quot;amoswap.w.rl zero, %1, %0&quot;			\</span>
<span class="quote">&gt;&gt; +			: &quot;+A&quot; (*p), &quot;r&quot; (__u.__val)			\</span>
<span class="quote">&gt;&gt; +			: 						\</span>
<span class="quote">&gt;&gt; +			: &quot;memory&quot;);					\</span>
<span class="quote">&gt;&gt; +		break;							\</span>
<span class="quote">&gt;&gt; +	case 8:								\</span>
<span class="quote">&gt;&gt; +		__asm__ __volatile__ (					\</span>
<span class="quote">&gt;&gt; +			&quot;amoswap.d.rl zero, %1, %0&quot;			\</span>
<span class="quote">&gt;&gt; +			: &quot;+A&quot; (*p), &quot;r&quot; (__u.__val)			\</span>
<span class="quote">&gt;&gt; +			: 						\</span>
<span class="quote">&gt;&gt; +			: &quot;memory&quot;);					\</span>
<span class="quote">&gt;&gt; +		break;							\</span>
<span class="quote">&gt;&gt; +	}								\</span>
<span class="quote">&gt;&gt; +} while (0)</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#define __smp_load_acquire(p)						\</span>
<span class="quote">&gt;&gt; +do {									\</span>
<span class="quote">&gt;&gt; +	union { typeof(*p) __val; char __c[1]; } __u =			\</span>
<span class="quote">&gt;&gt; +		{ .__val = (__force typeof(*p)) (v) };			\</span>
<span class="quote">&gt;&gt; +	compiletime_assert_atomic_type(*p);				\</span>
<span class="quote">&gt;&gt; +	switch (sizeof(*p)) {						\</span>
<span class="quote">&gt;&gt; +	case 1:								\</span>
<span class="quote">&gt;&gt; +	case 2:								\</span>
<span class="quote">&gt;&gt; +		__u.__val = READ_ONCE(*p);				\</span>
<span class="quote">&gt;&gt; +		smb_mb();						\</span>
<span class="quote">&gt;&gt; +		break;							\</span>
<span class="quote">&gt;&gt; +	case 4:								\</span>
<span class="quote">&gt;&gt; +		__asm__ __volatile__ (					\</span>
<span class="quote">&gt;&gt; +			&quot;amoor.w.aq %1, zero, %0&quot;			\</span>
<span class="quote">&gt;&gt; +			: &quot;+A&quot; (*p)					\</span>
<span class="quote">&gt;&gt; +			: &quot;=r&quot; (__u.__val)				\</span>
<span class="quote">&gt;&gt; +			: &quot;memory&quot;);					\</span>
<span class="quote">&gt;&gt; +		break;							\</span>
<span class="quote">&gt;&gt; +	case 8:								\</span>
<span class="quote">&gt;&gt; +		__asm__ __volatile__ (					\</span>
<span class="quote">&gt;&gt; +			&quot;amoor.d.aq %1, zero, %0&quot;			\</span>
<span class="quote">&gt;&gt; +			: &quot;+A&quot; (*p)					\</span>
<span class="quote">&gt;&gt; +			: &quot;=r&quot; (__u.__val)				\</span>
<span class="quote">&gt;&gt; +			: &quot;memory&quot;);					\</span>
<span class="quote">&gt;&gt; +		break;							\</span>
<span class="quote">&gt;&gt; +	}								\</span>
<span class="quote">&gt;&gt; +	__u.__val;							\</span>
<span class="quote">&gt;&gt; +} while (0)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; &#39;creative&#39; use of amoswap and amoor :-)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; You should really look at a normal load with ordering instruction</span>
<span class="quote">&gt; though, that amoor.aq is a rmw and will promote the cacheline to</span>
<span class="quote">&gt; exclusive (and dirty it).</span>

The thought here was that implementations could elide the MW by pattern
matching the &quot;zero&quot; (x0, the architectural zero register) forms of AMOs where
it&#39;s interesting.  I talked to one of our microarchitecture guys, and while he
agrees that&#39;s easy he points out that eliding half the AMO may wreak havoc on
the consistency model.  Since we&#39;re not sure what the memory model is actually
going to look like, we thought it&#39;d be best to just write the simplest code
here

  /*
   * TODO_RISCV_MEMORY_MODEL: While we could emit AMOs for the W and D sized
   * accesses here, it&#39;s questionable if that actually helps or not: the lack of
   * offsets in the AMOs means they&#39;re usually preceded by an addi, so they
   * probably won&#39;t save code space.  For now we&#39;ll just emit the fence.
   */
  #define __smp_store_release(p, v)                                       \
  ({                                                                      \
          compiletime_assert_atomic_type(*p);                             \
          smp_mb();                                                       \
          WRITE_ONCE(*p, v);                                              \
  })

  #define __smp_load_acquire(p)                                           \
  ({                                                                      \
          union{typeof(*p) __p; long __l;} __u;                           \
          compiletime_assert_atomic_type(*p);                             \
          __u.__l = READ_ONCE(*p);                                        \
          smp_mb();                                                       \
          __u.__p;                                                        \
  })
<span class="quote">
&gt;&gt; +/*</span>
<span class="quote">&gt;&gt; + * Simple spin lock operations.  These provide no fairness guarantees.</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +/* FIXME: Replace this with a ticket lock, like MIPS. */</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#define arch_spin_lock_flags(lock, flags) arch_spin_lock(lock)</span>
<span class="quote">&gt;&gt; +#define arch_spin_is_locked(x)	((x)-&gt;lock != 0)</span>
<span class="quote">&gt;&gt; +#define arch_spin_unlock_wait(x) \</span>
<span class="quote">&gt;&gt; +		do { cpu_relax(); } while ((x)-&gt;lock)</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +static inline void arch_spin_unlock(arch_spinlock_t *lock)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	__asm__ __volatile__ (</span>
<span class="quote">&gt;&gt; +		&quot;amoswap.w.rl x0, x0, %0&quot;</span>
<span class="quote">&gt;&gt; +		: &quot;=A&quot; (lock-&gt;lock)</span>
<span class="quote">&gt;&gt; +		:: &quot;memory&quot;);</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +static inline int arch_spin_trylock(arch_spinlock_t *lock)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	int tmp = 1, busy;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	__asm__ __volatile__ (</span>
<span class="quote">&gt;&gt; +		&quot;amoswap.w.aq %0, %2, %1&quot;</span>
<span class="quote">&gt;&gt; +		: &quot;=r&quot; (busy), &quot;+A&quot; (lock-&gt;lock)</span>
<span class="quote">&gt;&gt; +		: &quot;r&quot; (tmp)</span>
<span class="quote">&gt;&gt; +		: &quot;memory&quot;);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	return !busy;</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +static inline void arch_spin_lock(arch_spinlock_t *lock)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	while (1) {</span>
<span class="quote">&gt;&gt; +		if (arch_spin_is_locked(lock))</span>
<span class="quote">&gt;&gt; +			continue;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +		if (arch_spin_trylock(lock))</span>
<span class="quote">&gt;&gt; +			break;</span>
<span class="quote">&gt;&gt; +	}</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; OK, so back to smp_mb__{before,after}_spinlock(), that wants to order</span>
<span class="quote">&gt; things like:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; 	wakeup:					block:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; 	COND = 1;				p-&gt;state = UNINTERRUPTIBLE;</span>
<span class="quote">&gt; 						smp_mb();</span>
<span class="quote">&gt; 	smp_mb__before_spinlock();</span>
<span class="quote">&gt; 	spin_lock(&amp;lock);			if (!COND)</span>
<span class="quote">&gt; 						  schedule()</span>
<span class="quote">&gt; 	if (p-&gt;state &amp; state)</span>
<span class="quote">&gt; 		goto out;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; And here it is important that the COND store not happen _after_ the</span>
<span class="quote">&gt; p-&gt;state load.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Now, your spin_lock() only implies the AQ thing, which should only</span>
<span class="quote">&gt; constraint later load/stores but does nothing for the prior load/stores.</span>
<span class="quote">&gt; So our COND store can drop into the lock and even happen after the</span>
<span class="quote">&gt; p-&gt;state load.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; So you very much want your smp_mb__{before,after}_spinlock thingies to</span>
<span class="quote">&gt; be full barriers.</span>

OK, thanks!  I just had the movement direction backwards.  This makes much more
sense.

  diff --git a/arch/riscv/include/asm/barrier.h b/arch/riscv/include/asm/barrier.h
  index a480c0fb85e5..a4e54f4c17eb 100644
  --- a/arch/riscv/include/asm/barrier.h
  +++ b/arch/riscv/include/asm/barrier.h
  @@ -54,12 +54,12 @@
   #define __smb_mb__after_atomic()       smp_mb()

   /*
  - * These barries are meant to prevent memory operations inside a spinlock from
  - * moving outside of that spinlock.  Since we set the AQ and RL bits when
  - * entering or leaving spinlocks, no additional fence needs to be performed.
  + * These barries prevent accesses performed outside a spinlock from being moved
  + * inside a spinlock.  Since RISC-V sets the aq/rl bits on our spinlock only
  + * enforce release consistency, we need full fences here.
    */
  -#define smb_mb__before_spinlock()      barrier()
  -#define smb_mb__after_spinlock()       barrier()
  +#define smb_mb__before_spinlock()      smp_mb()
  +#define smb_mb__after_spinlock()       smp_mb()

   /* FIXME: I don&#39;t think RISC-V is allowed to perform a speculative load. */
   #define smp_acquire__after_ctrl_dep()  barrier()

Thanks for all the help!
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=124171">Boqun Feng</a> - July 7, 2017, 2:14 a.m.</div>
<pre class="content">
On Thu, Jul 06, 2017 at 06:04:13PM -0700, Palmer Dabbelt wrote:
[...]
<span class="quote">&gt; &gt;&gt; +#define __smp_load_acquire(p)						\</span>
<span class="quote">&gt; &gt;&gt; +do {									\</span>
<span class="quote">&gt; &gt;&gt; +	union { typeof(*p) __val; char __c[1]; } __u =			\</span>
<span class="quote">&gt; &gt;&gt; +		{ .__val = (__force typeof(*p)) (v) };			\</span>
<span class="quote">&gt; &gt;&gt; +	compiletime_assert_atomic_type(*p);				\</span>
<span class="quote">&gt; &gt;&gt; +	switch (sizeof(*p)) {						\</span>
<span class="quote">&gt; &gt;&gt; +	case 1:								\</span>
<span class="quote">&gt; &gt;&gt; +	case 2:								\</span>
<span class="quote">&gt; &gt;&gt; +		__u.__val = READ_ONCE(*p);				\</span>
<span class="quote">&gt; &gt;&gt; +		smb_mb();						\</span>
<span class="quote">&gt; &gt;&gt; +		break;							\</span>
<span class="quote">&gt; &gt;&gt; +	case 4:								\</span>
<span class="quote">&gt; &gt;&gt; +		__asm__ __volatile__ (					\</span>
<span class="quote">&gt; &gt;&gt; +			&quot;amoor.w.aq %1, zero, %0&quot;			\</span>
<span class="quote">&gt; &gt;&gt; +			: &quot;+A&quot; (*p)					\</span>
<span class="quote">&gt; &gt;&gt; +			: &quot;=r&quot; (__u.__val)				\</span>
<span class="quote">&gt; &gt;&gt; +			: &quot;memory&quot;);					\</span>
<span class="quote">&gt; &gt;&gt; +		break;							\</span>
<span class="quote">&gt; &gt;&gt; +	case 8:								\</span>
<span class="quote">&gt; &gt;&gt; +		__asm__ __volatile__ (					\</span>
<span class="quote">&gt; &gt;&gt; +			&quot;amoor.d.aq %1, zero, %0&quot;			\</span>
<span class="quote">&gt; &gt;&gt; +			: &quot;+A&quot; (*p)					\</span>
<span class="quote">&gt; &gt;&gt; +			: &quot;=r&quot; (__u.__val)				\</span>
<span class="quote">&gt; &gt;&gt; +			: &quot;memory&quot;);					\</span>
<span class="quote">&gt; &gt;&gt; +		break;							\</span>
<span class="quote">&gt; &gt;&gt; +	}								\</span>
<span class="quote">&gt; &gt;&gt; +	__u.__val;							\</span>
<span class="quote">&gt; &gt;&gt; +} while (0)</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; &#39;creative&#39; use of amoswap and amoor :-)</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; You should really look at a normal load with ordering instruction</span>
<span class="quote">&gt; &gt; though, that amoor.aq is a rmw and will promote the cacheline to</span>
<span class="quote">&gt; &gt; exclusive (and dirty it).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The thought here was that implementations could elide the MW by pattern</span>
<span class="quote">&gt; matching the &quot;zero&quot; (x0, the architectural zero register) forms of AMOs where</span>
<span class="quote">&gt; it&#39;s interesting.  I talked to one of our microarchitecture guys, and while he</span>
<span class="quote">&gt; agrees that&#39;s easy he points out that eliding half the AMO may wreak havoc on</span>
<span class="quote">&gt; the consistency model.  Since we&#39;re not sure what the memory model is actually</span>
<span class="quote">&gt; going to look like, we thought it&#39;d be best to just write the simplest code</span>
<span class="quote">&gt; here</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;   /*</span>
<span class="quote">&gt;    * TODO_RISCV_MEMORY_MODEL: While we could emit AMOs for the W and D sized</span>
<span class="quote">&gt;    * accesses here, it&#39;s questionable if that actually helps or not: the lack of</span>
<span class="quote">&gt;    * offsets in the AMOs means they&#39;re usually preceded by an addi, so they</span>
<span class="quote">&gt;    * probably won&#39;t save code space.  For now we&#39;ll just emit the fence.</span>
<span class="quote">&gt;    */</span>
<span class="quote">&gt;   #define __smp_store_release(p, v)                                       \</span>
<span class="quote">&gt;   ({                                                                      \</span>
<span class="quote">&gt;           compiletime_assert_atomic_type(*p);                             \</span>
<span class="quote">&gt;           smp_mb();                                                       \</span>
<span class="quote">&gt;           WRITE_ONCE(*p, v);                                              \</span>
<span class="quote">&gt;   })</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;   #define __smp_load_acquire(p)                                           \</span>
<span class="quote">&gt;   ({                                                                      \</span>
<span class="quote">&gt;           union{typeof(*p) __p; long __l;} __u;                           \</span>

AFAICT, there seems to be an endian issue if you do this. No?

Let us assume typeof(*p) is char and *p == 1, and on a big endian 32bit
platform:
<span class="quote">
&gt;           compiletime_assert_atomic_type(*p);                             \</span>
<span class="quote">&gt;           __u.__l = READ_ONCE(*p);                                        \</span>

	READ_ONCE(*p) is 1 so
	__u.__l is 0x00 00 00 01 now
<span class="quote">
&gt;           smp_mb();                                                       \</span>
<span class="quote">&gt;           __u.__p;                                                        \</span>

	__u.__p is then 0x00.

Am I missing something here?

Even so why not use the simple definition as in include/asm-generic/barrier.h?

Regards,
Boqun
<span class="quote">


&gt;   })</span>
<span class="quote">&gt; </span>
[...]
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - July 7, 2017, 8:08 a.m.</div>
<pre class="content">
On Thu, Jul 06, 2017 at 06:04:13PM -0700, Palmer Dabbelt wrote:
<span class="quote">&gt; &gt; Also probably not true. I _think_ you want a full barrier here, but</span>
<span class="quote">&gt; &gt; given the total lack of documentation on your end and the fact I&#39;ve not</span>
<span class="quote">&gt; &gt; yet read the spinlock (which I suppose is below) I cannot yet state</span>
<span class="quote">&gt; &gt; more.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Ya, sorry about that -- we&#39;re waiting on a proper memory model spec.  Is there</span>
<span class="quote">&gt; any other documentation I should produce?</span>

Nah, I&#39;ll wait for your shiny new document.
<span class="quote">

&gt;   +/*</span>
<span class="quote">&gt;   + * TODO_RISCV_MEMORY_MODEL: I don&#39;t think RISC-V is allowed to perform a</span>
<span class="quote">&gt;   + * speculative load, but we&#39;re going to wait on a formal memory model in order</span>
<span class="quote">&gt;   + * to ensure this is safe to elide.</span>
<span class="quote">&gt;   + */</span>
<span class="quote">&gt;   +#define smp_acquire__after_ctrl_dep()  smp_mb()</span>

So typically a control dependency already provides read-&gt;write ordering,
by virtue of speculative writes being BAD.

So a control dependency only needs to provide read-&gt;read ordering in
addition to the existing read-&gt;write ordering and hence this barrier is
typically a smp_rmb().

See the definition in asm-generic/barrier.h.

Having to use a full barrier here would imply your architecture does not
respect control dependencies, which would be BAD because we actually
rely on them.

So either the normal definition is good and you don&#39;t need to do
anything, or you prohibit read speculation in which case you have a
special case like TILE does.
<span class="quote">


&gt; &gt;&gt; +#define __smp_load_acquire(p)						\</span>
<span class="quote">&gt; &gt;&gt; +do {									\</span>
<span class="quote">&gt; &gt;&gt; +	union { typeof(*p) __val; char __c[1]; } __u =			\</span>
<span class="quote">&gt; &gt;&gt; +		{ .__val = (__force typeof(*p)) (v) };			\</span>
<span class="quote">&gt; &gt;&gt; +	compiletime_assert_atomic_type(*p);				\</span>
<span class="quote">&gt; &gt;&gt; +	switch (sizeof(*p)) {						\</span>
<span class="quote">&gt; &gt;&gt; +	case 1:								\</span>
<span class="quote">&gt; &gt;&gt; +	case 2:								\</span>
<span class="quote">&gt; &gt;&gt; +		__u.__val = READ_ONCE(*p);				\</span>
<span class="quote">&gt; &gt;&gt; +		smb_mb();						\</span>
<span class="quote">&gt; &gt;&gt; +		break;							\</span>
<span class="quote">&gt; &gt;&gt; +	case 4:								\</span>
<span class="quote">&gt; &gt;&gt; +		__asm__ __volatile__ (					\</span>
<span class="quote">&gt; &gt;&gt; +			&quot;amoor.w.aq %1, zero, %0&quot;			\</span>
<span class="quote">&gt; &gt;&gt; +			: &quot;+A&quot; (*p)					\</span>
<span class="quote">&gt; &gt;&gt; +			: &quot;=r&quot; (__u.__val)				\</span>
<span class="quote">&gt; &gt;&gt; +			: &quot;memory&quot;);					\</span>
<span class="quote">&gt; &gt;&gt; +		break;							\</span>
<span class="quote">&gt; &gt;&gt; +	case 8:								\</span>
<span class="quote">&gt; &gt;&gt; +		__asm__ __volatile__ (					\</span>
<span class="quote">&gt; &gt;&gt; +			&quot;amoor.d.aq %1, zero, %0&quot;			\</span>
<span class="quote">&gt; &gt;&gt; +			: &quot;+A&quot; (*p)					\</span>
<span class="quote">&gt; &gt;&gt; +			: &quot;=r&quot; (__u.__val)				\</span>
<span class="quote">&gt; &gt;&gt; +			: &quot;memory&quot;);					\</span>
<span class="quote">&gt; &gt;&gt; +		break;							\</span>
<span class="quote">&gt; &gt;&gt; +	}								\</span>
<span class="quote">&gt; &gt;&gt; +	__u.__val;							\</span>
<span class="quote">&gt; &gt;&gt; +} while (0)</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; &#39;creative&#39; use of amoswap and amoor :-)</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; You should really look at a normal load with ordering instruction</span>
<span class="quote">&gt; &gt; though, that amoor.aq is a rmw and will promote the cacheline to</span>
<span class="quote">&gt; &gt; exclusive (and dirty it).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The thought here was that implementations could elide the MW by pattern</span>
<span class="quote">&gt; matching the &quot;zero&quot; (x0, the architectural zero register) forms of AMOs where</span>
<span class="quote">&gt; it&#39;s interesting.  I talked to one of our microarchitecture guys, and while he</span>
<span class="quote">&gt; agrees that&#39;s easy he points out that eliding half the AMO may wreak havoc on</span>
<span class="quote">&gt; the consistency model.  Since we&#39;re not sure what the memory model is actually</span>
<span class="quote">&gt; going to look like, we thought it&#39;d be best to just write the simplest code</span>
<span class="quote">&gt; here</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;   /*</span>
<span class="quote">&gt;    * TODO_RISCV_MEMORY_MODEL: While we could emit AMOs for the W and D sized</span>
<span class="quote">&gt;    * accesses here, it&#39;s questionable if that actually helps or not: the lack of</span>
<span class="quote">&gt;    * offsets in the AMOs means they&#39;re usually preceded by an addi, so they</span>
<span class="quote">&gt;    * probably won&#39;t save code space.  For now we&#39;ll just emit the fence.</span>
<span class="quote">&gt;    */</span>
<span class="quote">&gt;   #define __smp_store_release(p, v)                                       \</span>
<span class="quote">&gt;   ({                                                                      \</span>
<span class="quote">&gt;           compiletime_assert_atomic_type(*p);                             \</span>
<span class="quote">&gt;           smp_mb();                                                       \</span>
<span class="quote">&gt;           WRITE_ONCE(*p, v);                                              \</span>
<span class="quote">&gt;   })</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;   #define __smp_load_acquire(p)                                           \</span>
<span class="quote">&gt;   ({                                                                      \</span>
<span class="quote">&gt;           union{typeof(*p) __p; long __l;} __u;                           \</span>
<span class="quote">&gt;           compiletime_assert_atomic_type(*p);                             \</span>
<span class="quote">&gt;           __u.__l = READ_ONCE(*p);                                        \</span>
<span class="quote">&gt;           smp_mb();                                                       \</span>
<span class="quote">&gt;           __u.__p;                                                        \</span>
<span class="quote">&gt;   })</span>

Fair enough, that works.
<span class="quote">
&gt; &gt; OK, so back to smp_mb__{before,after}_spinlock(), that wants to order</span>
<span class="quote">&gt; &gt; things like:</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; 	wakeup:					block:</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; 	COND = 1;				p-&gt;state = UNINTERRUPTIBLE;</span>
<span class="quote">&gt; &gt; 						smp_mb();</span>
<span class="quote">&gt; &gt; 	smp_mb__before_spinlock();</span>
<span class="quote">&gt; &gt; 	spin_lock(&amp;lock);			if (!COND)</span>
<span class="quote">&gt; &gt; 						  schedule()</span>
<span class="quote">&gt; &gt; 	if (p-&gt;state &amp; state)</span>
<span class="quote">&gt; &gt; 		goto out;</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; And here it is important that the COND store not happen _after_ the</span>
<span class="quote">&gt; &gt; p-&gt;state load.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Now, your spin_lock() only implies the AQ thing, which should only</span>
<span class="quote">&gt; &gt; constraint later load/stores but does nothing for the prior load/stores.</span>
<span class="quote">&gt; &gt; So our COND store can drop into the lock and even happen after the</span>
<span class="quote">&gt; &gt; p-&gt;state load.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; So you very much want your smp_mb__{before,after}_spinlock thingies to</span>
<span class="quote">&gt; &gt; be full barriers.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; OK, thanks!  I just had the movement direction backwards.  This makes much more</span>
<span class="quote">&gt; sense.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;   diff --git a/arch/riscv/include/asm/barrier.h b/arch/riscv/include/asm/barrier.h</span>
<span class="quote">&gt;   index a480c0fb85e5..a4e54f4c17eb 100644</span>
<span class="quote">&gt;   --- a/arch/riscv/include/asm/barrier.h</span>
<span class="quote">&gt;   +++ b/arch/riscv/include/asm/barrier.h</span>
<span class="quote">&gt;   @@ -54,12 +54,12 @@</span>
<span class="quote">&gt;    #define __smb_mb__after_atomic()       smp_mb()</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;    /*</span>
<span class="quote">&gt;   - * These barries are meant to prevent memory operations inside a spinlock from</span>
<span class="quote">&gt;   - * moving outside of that spinlock.  Since we set the AQ and RL bits when</span>
<span class="quote">&gt;   - * entering or leaving spinlocks, no additional fence needs to be performed.</span>
<span class="quote">&gt;   + * These barries prevent accesses performed outside a spinlock from being moved</span>
<span class="quote">&gt;   + * inside a spinlock.  Since RISC-V sets the aq/rl bits on our spinlock only</span>
<span class="quote">&gt;   + * enforce release consistency, we need full fences here.</span>
<span class="quote">&gt;     */</span>
<span class="quote">&gt;   -#define smb_mb__before_spinlock()      barrier()</span>
<span class="quote">&gt;   -#define smb_mb__after_spinlock()       barrier()</span>
<span class="quote">&gt;   +#define smb_mb__before_spinlock()      smp_mb()</span>
<span class="quote">&gt;   +#define smb_mb__after_spinlock()       smp_mb()</span>
<span class="quote">&gt; </span>

Most excellent. Thanks!
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=14812">Jonathan Neuschäfer</a> - July 7, 2017, 1:16 p.m.</div>
<pre class="content">
On Tue, Jul 04, 2017 at 12:50:55PM -0700, Palmer Dabbelt wrote:
[...]
<span class="quote">&gt; +/* These barries need to enforce ordering on both devices or memory. */</span>

Very minor nit: s/barries/barriers/ (in several places)


Jonathan Neuschäfer
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=90941">Palmer Dabbelt</a> - July 10, 2017, 8:39 p.m.</div>
<pre class="content">
On Thu, 06 Jul 2017 19:14:25 PDT (-0700), boqun.feng@gmail.com wrote:
<span class="quote">&gt; On Thu, Jul 06, 2017 at 06:04:13PM -0700, Palmer Dabbelt wrote:</span>
<span class="quote">&gt; [...]</span>
<span class="quote">&gt;&gt; &gt;&gt; +#define __smp_load_acquire(p)						\</span>
<span class="quote">&gt;&gt; &gt;&gt; +do {									\</span>
<span class="quote">&gt;&gt; &gt;&gt; +	union { typeof(*p) __val; char __c[1]; } __u =			\</span>
<span class="quote">&gt;&gt; &gt;&gt; +		{ .__val = (__force typeof(*p)) (v) };			\</span>
<span class="quote">&gt;&gt; &gt;&gt; +	compiletime_assert_atomic_type(*p);				\</span>
<span class="quote">&gt;&gt; &gt;&gt; +	switch (sizeof(*p)) {						\</span>
<span class="quote">&gt;&gt; &gt;&gt; +	case 1:								\</span>
<span class="quote">&gt;&gt; &gt;&gt; +	case 2:								\</span>
<span class="quote">&gt;&gt; &gt;&gt; +		__u.__val = READ_ONCE(*p);				\</span>
<span class="quote">&gt;&gt; &gt;&gt; +		smb_mb();						\</span>
<span class="quote">&gt;&gt; &gt;&gt; +		break;							\</span>
<span class="quote">&gt;&gt; &gt;&gt; +	case 4:								\</span>
<span class="quote">&gt;&gt; &gt;&gt; +		__asm__ __volatile__ (					\</span>
<span class="quote">&gt;&gt; &gt;&gt; +			&quot;amoor.w.aq %1, zero, %0&quot;			\</span>
<span class="quote">&gt;&gt; &gt;&gt; +			: &quot;+A&quot; (*p)					\</span>
<span class="quote">&gt;&gt; &gt;&gt; +			: &quot;=r&quot; (__u.__val)				\</span>
<span class="quote">&gt;&gt; &gt;&gt; +			: &quot;memory&quot;);					\</span>
<span class="quote">&gt;&gt; &gt;&gt; +		break;							\</span>
<span class="quote">&gt;&gt; &gt;&gt; +	case 8:								\</span>
<span class="quote">&gt;&gt; &gt;&gt; +		__asm__ __volatile__ (					\</span>
<span class="quote">&gt;&gt; &gt;&gt; +			&quot;amoor.d.aq %1, zero, %0&quot;			\</span>
<span class="quote">&gt;&gt; &gt;&gt; +			: &quot;+A&quot; (*p)					\</span>
<span class="quote">&gt;&gt; &gt;&gt; +			: &quot;=r&quot; (__u.__val)				\</span>
<span class="quote">&gt;&gt; &gt;&gt; +			: &quot;memory&quot;);					\</span>
<span class="quote">&gt;&gt; &gt;&gt; +		break;							\</span>
<span class="quote">&gt;&gt; &gt;&gt; +	}								\</span>
<span class="quote">&gt;&gt; &gt;&gt; +	__u.__val;							\</span>
<span class="quote">&gt;&gt; &gt;&gt; +} while (0)</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; &#39;creative&#39; use of amoswap and amoor :-)</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; You should really look at a normal load with ordering instruction</span>
<span class="quote">&gt;&gt; &gt; though, that amoor.aq is a rmw and will promote the cacheline to</span>
<span class="quote">&gt;&gt; &gt; exclusive (and dirty it).</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; The thought here was that implementations could elide the MW by pattern</span>
<span class="quote">&gt;&gt; matching the &quot;zero&quot; (x0, the architectural zero register) forms of AMOs where</span>
<span class="quote">&gt;&gt; it&#39;s interesting.  I talked to one of our microarchitecture guys, and while he</span>
<span class="quote">&gt;&gt; agrees that&#39;s easy he points out that eliding half the AMO may wreak havoc on</span>
<span class="quote">&gt;&gt; the consistency model.  Since we&#39;re not sure what the memory model is actually</span>
<span class="quote">&gt;&gt; going to look like, we thought it&#39;d be best to just write the simplest code</span>
<span class="quote">&gt;&gt; here</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;   /*</span>
<span class="quote">&gt;&gt;    * TODO_RISCV_MEMORY_MODEL: While we could emit AMOs for the W and D sized</span>
<span class="quote">&gt;&gt;    * accesses here, it&#39;s questionable if that actually helps or not: the lack of</span>
<span class="quote">&gt;&gt;    * offsets in the AMOs means they&#39;re usually preceded by an addi, so they</span>
<span class="quote">&gt;&gt;    * probably won&#39;t save code space.  For now we&#39;ll just emit the fence.</span>
<span class="quote">&gt;&gt;    */</span>
<span class="quote">&gt;&gt;   #define __smp_store_release(p, v)                                       \</span>
<span class="quote">&gt;&gt;   ({                                                                      \</span>
<span class="quote">&gt;&gt;           compiletime_assert_atomic_type(*p);                             \</span>
<span class="quote">&gt;&gt;           smp_mb();                                                       \</span>
<span class="quote">&gt;&gt;           WRITE_ONCE(*p, v);                                              \</span>
<span class="quote">&gt;&gt;   })</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;   #define __smp_load_acquire(p)                                           \</span>
<span class="quote">&gt;&gt;   ({                                                                      \</span>
<span class="quote">&gt;&gt;           union{typeof(*p) __p; long __l;} __u;                           \</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; AFAICT, there seems to be an endian issue if you do this. No?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Let us assume typeof(*p) is char and *p == 1, and on a big endian 32bit</span>
<span class="quote">&gt; platform:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt;           compiletime_assert_atomic_type(*p);                             \</span>
<span class="quote">&gt;&gt;           __u.__l = READ_ONCE(*p);                                        \</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; 	READ_ONCE(*p) is 1 so</span>
<span class="quote">&gt; 	__u.__l is 0x00 00 00 01 now</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt;           smp_mb();                                                       \</span>
<span class="quote">&gt;&gt;           __u.__p;                                                        \</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; 	__u.__p is then 0x00.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Am I missing something here?</span>

We&#39;re little endian (though I might have still screwed it up).  I didn&#39;t really
bother looking because...
<span class="quote">
&gt; Even so why not use the simple definition as in include/asm-generic/barrier.h?</span>

...that&#39;s much better -- I forgot there were generic versions, as we used to
have a much more complicated one.

  https://github.com/riscv/riscv-linux/commit/910d2bf4c3c349b670a1d839462e32e122ac70a5

Thanks!
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=90941">Palmer Dabbelt</a> - July 10, 2017, 8:39 p.m.</div>
<pre class="content">
On Fri, 07 Jul 2017 01:08:19 PDT (-0700), peterz@infradead.org wrote:
<span class="quote">&gt; On Thu, Jul 06, 2017 at 06:04:13PM -0700, Palmer Dabbelt wrote:</span>
<span class="quote">&gt;&gt;   +/*</span>
<span class="quote">&gt;&gt;   + * TODO_RISCV_MEMORY_MODEL: I don&#39;t think RISC-V is allowed to perform a</span>
<span class="quote">&gt;&gt;   + * speculative load, but we&#39;re going to wait on a formal memory model in order</span>
<span class="quote">&gt;&gt;   + * to ensure this is safe to elide.</span>
<span class="quote">&gt;&gt;   + */</span>
<span class="quote">&gt;&gt;   +#define smp_acquire__after_ctrl_dep()  smp_mb()</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; So typically a control dependency already provides read-&gt;write ordering,</span>
<span class="quote">&gt; by virtue of speculative writes being BAD.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; So a control dependency only needs to provide read-&gt;read ordering in</span>
<span class="quote">&gt; addition to the existing read-&gt;write ordering and hence this barrier is</span>
<span class="quote">&gt; typically a smp_rmb().</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; See the definition in asm-generic/barrier.h.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Having to use a full barrier here would imply your architecture does not</span>
<span class="quote">&gt; respect control dependencies, which would be BAD because we actually</span>
<span class="quote">&gt; rely on them.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; So either the normal definition is good and you don&#39;t need to do</span>
<span class="quote">&gt; anything, or you prohibit read speculation in which case you have a</span>
<span class="quote">&gt; special case like TILE does.</span>

I&#39;d be very surprised (and very unhappy) if we ended up with speculative
writes, as that would be a huge mess.

Thanks!
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=90941">Palmer Dabbelt</a> - July 10, 2017, 8:39 p.m.</div>
<pre class="content">
On Fri, 07 Jul 2017 06:16:07 PDT (-0700), j.neuschaefer@gmx.net wrote:
<span class="quote">&gt; On Tue, Jul 04, 2017 at 12:50:55PM -0700, Palmer Dabbelt wrote:</span>
<span class="quote">&gt; [...]</span>
<span class="quote">&gt;&gt; +/* These barries need to enforce ordering on both devices or memory. */</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Very minor nit: s/barries/barriers/ (in several places)</span>

I think this should do it

  https://github.com/riscv/riscv-linux/commit/b356e7a2223b5e21df424ea7e9900e5bf408762f

Thanks!
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/riscv/include/asm/atomic.h b/arch/riscv/include/asm/atomic.h</span>
new file mode 100644
<span class="p_header">index 000000000000..e5a12cfa405a</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/atomic.h</span>
<span class="p_chunk">@@ -0,0 +1,337 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2007 Red Hat, Inc. All Rights Reserved.</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ * Copyright (C) 2017 SiFive</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ * modify it under the terms of the GNU General Public Licence</span>
<span class="p_add">+ * as published by the Free Software Foundation; either version</span>
<span class="p_add">+ * 2 of the Licence, or (at your option) any later version.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_ATOMIC_H</span>
<span class="p_add">+#define _ASM_RISCV_ATOMIC_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_GENERIC_ATOMIC64</span>
<span class="p_add">+# include &lt;asm-generic/atomic64.h&gt;</span>
<span class="p_add">+#else</span>
<span class="p_add">+# if (__riscv_xlen &lt; 64)</span>
<span class="p_add">+#  error &quot;64-bit atomics require XLEN to be at least 64&quot;</span>
<span class="p_add">+# endif</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_ISA_A</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/cmpxchg.h&gt;</span>
<span class="p_add">+#include &lt;asm/barrier.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#define ATOMIC_INIT(i)	{ (i) }</span>
<span class="p_add">+static __always_inline int atomic_read(const atomic_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return READ_ONCE(v-&gt;counter);</span>
<span class="p_add">+}</span>
<span class="p_add">+static __always_inline void atomic_set(atomic_t *v, int i)</span>
<span class="p_add">+{</span>
<span class="p_add">+	WRITE_ONCE(v-&gt;counter, i);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef CONFIG_GENERIC_ATOMIC64</span>
<span class="p_add">+#define ATOMIC64_INIT(i) { (i) }</span>
<span class="p_add">+static __always_inline int atomic64_read(const atomic64_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return READ_ONCE(v-&gt;counter);</span>
<span class="p_add">+}</span>
<span class="p_add">+static __always_inline void atomic64_set(atomic64_t *v, int i)</span>
<span class="p_add">+{</span>
<span class="p_add">+	WRITE_ONCE(v-&gt;counter, i);</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * First, the atomic ops that have no ordering constraints and therefor don&#39;t</span>
<span class="p_add">+ * have the AQ or RL bits set.  These don&#39;t return anything, so there&#39;s only</span>
<span class="p_add">+ * one version to worry about.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define ATOMIC_OP(op, asm_op, c_op, I, asm_type, c_type, prefix)				\</span>
<span class="p_add">+static __always_inline void atomic##prefix##_##op(c_type i, atomic##prefix##_t *v)		\</span>
<span class="p_add">+{												\</span>
<span class="p_add">+	__asm__ __volatile__ (									\</span>
<span class="p_add">+		&quot;amo&quot; #asm_op &quot;.&quot; #asm_type &quot; zero, %1, %0&quot;					\</span>
<span class="p_add">+		: &quot;+A&quot; (v-&gt;counter)								\</span>
<span class="p_add">+		: &quot;r&quot; (I));									\</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_GENERIC_ATOMIC64</span>
<span class="p_add">+#define ATOMIC_OPS(op, asm_op, c_op, I)			\</span>
<span class="p_add">+        ATOMIC_OP (op, asm_op, c_op, I, w,  int,   )</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define ATOMIC_OPS(op, asm_op, c_op, I)			\</span>
<span class="p_add">+        ATOMIC_OP (op, asm_op, c_op, I, w,  int,   )	\</span>
<span class="p_add">+        ATOMIC_OP (op, asm_op, c_op, I, d, long, 64)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+ATOMIC_OPS(add, add, +,  i)</span>
<span class="p_add">+ATOMIC_OPS(sub, add, +, -i)</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * FIXME: I could only find documentation that atomic_{add,sub,inc,dec} are</span>
<span class="p_add">+ * barrier-free.  I&#39;m assuming that and/or/xor have the same constraints as the</span>
<span class="p_add">+ * others.</span>
<span class="p_add">+ */</span>
<span class="p_add">+ATOMIC_OPS(and, and, &amp;,  i)</span>
<span class="p_add">+ATOMIC_OPS( or,  or, |,  i)</span>
<span class="p_add">+ATOMIC_OPS(xor, xor, ^,  i)</span>
<span class="p_add">+</span>
<span class="p_add">+#undef ATOMIC_OP</span>
<span class="p_add">+#undef ATOMIC_OPS</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Atomic ops that have ordered, relaxed, acquire, and relese variants.</span>
<span class="p_add">+ * There&#39;s two flavors of these: the arithmatic ops have both fetch and return</span>
<span class="p_add">+ * versions, while the logical ops only have fetch versions.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define ATOMIC_FETCH_OP(op, asm_op, c_op, I, asm_or, c_or, asm_type, c_type, prefix)			\</span>
<span class="p_add">+static __always_inline c_type atomic##prefix##_fetch_##op##c_or(c_type i, atomic##prefix##_t *v)	\</span>
<span class="p_add">+{													\</span>
<span class="p_add">+	register c_type ret;										\</span>
<span class="p_add">+	__asm__ __volatile__ (										\</span>
<span class="p_add">+		&quot;amo&quot; #asm_op &quot;.&quot; #asm_type #asm_or &quot; %2, %1, %0&quot;					\</span>
<span class="p_add">+		: &quot;+A&quot; (v-&gt;counter), &quot;=r&quot; (ret)								\</span>
<span class="p_add">+		: &quot;r&quot; (I));										\</span>
<span class="p_add">+	return ret;											\</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define ATOMIC_OP_RETURN(op, asm_op, c_op, I, asm_or, c_or, asm_type, c_type, prefix)			\</span>
<span class="p_add">+static __always_inline c_type atomic##prefix##_##op##_return##c_or(int i, atomic##prefix##_t *v)	\</span>
<span class="p_add">+{													\</span>
<span class="p_add">+        return atomic##prefix##_fetch_##op##c_or(i, v) c_op I;						\</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_GENERIC_ATOMIC64</span>
<span class="p_add">+#define ATOMIC_OPS(op, asm_op, c_op, I, asm_or, c_or)				\</span>
<span class="p_add">+        ATOMIC_FETCH_OP (op, asm_op, c_op, I, asm_or, c_or, w,  int,   )	\</span>
<span class="p_add">+        ATOMIC_OP_RETURN(op, asm_op, c_op, I, asm_or, c_or, w,  int,   )</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define ATOMIC_OPS(op, asm_op, c_op, I, asm_or, c_or)				\</span>
<span class="p_add">+        ATOMIC_FETCH_OP (op, asm_op, c_op, I, asm_or, c_or, w,  int,   )	\</span>
<span class="p_add">+        ATOMIC_OP_RETURN(op, asm_op, c_op, I, asm_or, c_or, w,  int,   )	\</span>
<span class="p_add">+        ATOMIC_FETCH_OP (op, asm_op, c_op, I, asm_or, c_or, d, long, 64)	\</span>
<span class="p_add">+        ATOMIC_OP_RETURN(op, asm_op, c_op, I, asm_or, c_or, d, long, 64)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+ATOMIC_OPS(add, add, +,  i,      , _relaxed)</span>
<span class="p_add">+ATOMIC_OPS(add, add, +,  i, .aq  , _acquire)</span>
<span class="p_add">+ATOMIC_OPS(add, add, +,  i, .rl  , _release)</span>
<span class="p_add">+ATOMIC_OPS(add, add, +,  i, .aqrl,         )</span>
<span class="p_add">+</span>
<span class="p_add">+ATOMIC_OPS(sub, add, +, -i,      , _relaxed)</span>
<span class="p_add">+ATOMIC_OPS(sub, add, +, -i, .aq  , _acquire)</span>
<span class="p_add">+ATOMIC_OPS(sub, add, +, -i, .rl  , _release)</span>
<span class="p_add">+ATOMIC_OPS(sub, add, +, -i, .aqrl,         )</span>
<span class="p_add">+</span>
<span class="p_add">+#undef ATOMIC_OPS</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_GENERIC_ATOMIC64</span>
<span class="p_add">+#define ATOMIC_OPS(op, asm_op, c_op, I, asm_or, c_or)				\</span>
<span class="p_add">+        ATOMIC_FETCH_OP(op, asm_op, c_op, I, asm_or, c_or, w,  int,   )</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define ATOMIC_OPS(op, asm_op, c_op, I, asm_or, c_or)				\</span>
<span class="p_add">+        ATOMIC_FETCH_OP(op, asm_op, c_op, I, asm_or, c_or, w,  int,   )		\</span>
<span class="p_add">+        ATOMIC_FETCH_OP(op, asm_op, c_op, I, asm_or, c_or, d, long, 64)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+ATOMIC_OPS(and, and, &amp;,  i,      , _relaxed)</span>
<span class="p_add">+ATOMIC_OPS(and, and, &amp;,  i, .aq  , _acquire)</span>
<span class="p_add">+ATOMIC_OPS(and, and, &amp;,  i, .rl  , _release)</span>
<span class="p_add">+ATOMIC_OPS(and, and, &amp;,  i, .aqrl,         )</span>
<span class="p_add">+</span>
<span class="p_add">+ATOMIC_OPS( or,  or, |,  i,      , _relaxed)</span>
<span class="p_add">+ATOMIC_OPS( or,  or, |,  i, .aq  , _acquire)</span>
<span class="p_add">+ATOMIC_OPS( or,  or, |,  i, .rl  , _release)</span>
<span class="p_add">+ATOMIC_OPS( or,  or, |,  i, .aqrl,         )</span>
<span class="p_add">+</span>
<span class="p_add">+ATOMIC_OPS(xor, xor, ^,  i,      , _relaxed)</span>
<span class="p_add">+ATOMIC_OPS(xor, xor, ^,  i, .aq  , _acquire)</span>
<span class="p_add">+ATOMIC_OPS(xor, xor, ^,  i, .rl  , _release)</span>
<span class="p_add">+ATOMIC_OPS(xor, xor, ^,  i, .aqrl,         )</span>
<span class="p_add">+</span>
<span class="p_add">+#undef ATOMIC_OPS</span>
<span class="p_add">+</span>
<span class="p_add">+#undef ATOMIC_FETCH_OP</span>
<span class="p_add">+#undef ATOMIC_OP_RETURN</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * The extra atomic operations that are constructed from one of the core</span>
<span class="p_add">+ * AMO-based operations above (aside from sub, which is easier to fit above).</span>
<span class="p_add">+ * These are required to perform a barrier, but they&#39;re OK this way because</span>
<span class="p_add">+ * atomic_*_return is also required to perform a barrier.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define ATOMIC_OP(op, func_op, comp_op, I, c_type, prefix)			\</span>
<span class="p_add">+static __always_inline bool atomic##prefix##_##op(c_type i, atomic##prefix##_t *v) \</span>
<span class="p_add">+{										\</span>
<span class="p_add">+	return atomic##prefix##_##func_op##_return(i, v) comp_op I;		\</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_GENERIC_ATOMIC64</span>
<span class="p_add">+#define ATOMIC_OPS(op, func_op, comp_op, I)			\</span>
<span class="p_add">+        ATOMIC_OP (op, func_op, comp_op, I,  int,   )</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define ATOMIC_OPS(op, func_op, comp_op, I)			\</span>
<span class="p_add">+        ATOMIC_OP (op, func_op, comp_op, I,  int,   )		\</span>
<span class="p_add">+        ATOMIC_OP (op, func_op, comp_op, I, long, 64)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+ATOMIC_OPS(add_and_test, add, ==, 0)</span>
<span class="p_add">+ATOMIC_OPS(sub_and_test, sub, ==, 0)</span>
<span class="p_add">+ATOMIC_OPS(add_negative, add,  &lt;, 0)</span>
<span class="p_add">+</span>
<span class="p_add">+#undef ATOMIC_OP</span>
<span class="p_add">+#undef ATOMIC_OPS</span>
<span class="p_add">+</span>
<span class="p_add">+#define ATOMIC_OP(op, func_op, c_op, I, prefix)					\</span>
<span class="p_add">+static __always_inline void atomic##prefix##_##op(atomic##prefix##_t *v)	\</span>
<span class="p_add">+{										\</span>
<span class="p_add">+	atomic##prefix##_##func_op(I, v);					\</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define ATOMIC_FETCH_OP(op, func_op, c_op, I, prefix)				\</span>
<span class="p_add">+static __always_inline int atomic##prefix##_fetch_##op(atomic##prefix##_t *v)	\</span>
<span class="p_add">+{										\</span>
<span class="p_add">+	return atomic##prefix##_fetch_##func_op(I, v);				\</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define ATOMIC_OP_RETURN(op, asm_op, c_op, I, prefix)				\</span>
<span class="p_add">+static __always_inline int atomic##prefix##_##op##_return(atomic##prefix##_t *v) \</span>
<span class="p_add">+{										\</span>
<span class="p_add">+        return atomic##prefix##_fetch_##op(v) c_op I;				\</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_GENERIC_ATOMIC64</span>
<span class="p_add">+#define ATOMIC_OPS(op, asm_op, c_op, I)						\</span>
<span class="p_add">+        ATOMIC_OP       (op, asm_op, c_op, I,   )				\</span>
<span class="p_add">+        ATOMIC_FETCH_OP (op, asm_op, c_op, I,   )				\</span>
<span class="p_add">+        ATOMIC_OP_RETURN(op, asm_op, c_op, I,   )</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define ATOMIC_OPS(op, asm_op, c_op, I)						\</span>
<span class="p_add">+        ATOMIC_OP       (op, asm_op, c_op, I,   )				\</span>
<span class="p_add">+        ATOMIC_FETCH_OP (op, asm_op, c_op, I,   )				\</span>
<span class="p_add">+        ATOMIC_OP_RETURN(op, asm_op, c_op, I,   )				\</span>
<span class="p_add">+        ATOMIC_OP       (op, asm_op, c_op, I, 64)				\</span>
<span class="p_add">+        ATOMIC_FETCH_OP (op, asm_op, c_op, I, 64)				\</span>
<span class="p_add">+        ATOMIC_OP_RETURN(op, asm_op, c_op, I, 64)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+ATOMIC_OPS(inc, add, +,  1)</span>
<span class="p_add">+ATOMIC_OPS(dec, add, +, -1)</span>
<span class="p_add">+</span>
<span class="p_add">+#undef ATOMIC_OPS</span>
<span class="p_add">+#undef ATOMIC_OP</span>
<span class="p_add">+#undef ATOMIC_FETCH_OP</span>
<span class="p_add">+#undef ATOMIC_OP_RETURN</span>
<span class="p_add">+</span>
<span class="p_add">+#define ATOMIC_OP(op, func_op, comp_op, I, prefix)				\</span>
<span class="p_add">+static __always_inline bool atomic##prefix##_##op(atomic##prefix##_t *v)	\</span>
<span class="p_add">+{										\</span>
<span class="p_add">+	return atomic##prefix##_##func_op##_return(v) comp_op I;		\</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+ATOMIC_OP(inc_and_test, inc, ==, 0,   )</span>
<span class="p_add">+ATOMIC_OP(dec_and_test, dec, ==, 0,   )</span>
<span class="p_add">+#ifndef CONFIG_GENERIC_ATOMIC64</span>
<span class="p_add">+ATOMIC_OP(inc_and_test, inc, ==, 0, 64)</span>
<span class="p_add">+ATOMIC_OP(dec_and_test, dec, ==, 0, 64)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#undef ATOMIC_OP</span>
<span class="p_add">+</span>
<span class="p_add">+/* This is required to provide a barrier on success. */</span>
<span class="p_add">+static __always_inline int __atomic_add_unless(atomic_t *v, int a, int u)</span>
<span class="p_add">+{</span>
<span class="p_add">+       register int prev, rc;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;0:\n\t&quot;</span>
<span class="p_add">+		&quot;lr.w.aqrl %0, %2\n\t&quot;</span>
<span class="p_add">+		&quot;beq       %0, %4, 1f\n\t&quot;</span>
<span class="p_add">+		&quot;add       %1, %0, %3\n\t&quot;</span>
<span class="p_add">+		&quot;sc.w.aqrl %1, %1, %2\n\t&quot;</span>
<span class="p_add">+		&quot;bnez      %1, 0b\n\t&quot;</span>
<span class="p_add">+		&quot;1:&quot;</span>
<span class="p_add">+		: &quot;=&amp;r&quot; (prev), &quot;=&amp;r&quot; (rc), &quot;+A&quot; (v-&gt;counter)</span>
<span class="p_add">+		: &quot;r&quot; (a), &quot;r&quot; (u));</span>
<span class="p_add">+	return prev;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef CONFIG_GENERIC_ATOMIC64</span>
<span class="p_add">+static __always_inline long atomic64_add_unless(atomic64_t *v, long a, long u)</span>
<span class="p_add">+{</span>
<span class="p_add">+       register long prev, rc;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;0:\n\t&quot;</span>
<span class="p_add">+		&quot;lr.d.aqrl %0, %2\n\t&quot;</span>
<span class="p_add">+		&quot;beq       %0, %4, 1f\n\t&quot;</span>
<span class="p_add">+		&quot;add       %1, %0, %3\n\t&quot;</span>
<span class="p_add">+		&quot;sc.d.aqrl %1, %1, %2\n\t&quot;</span>
<span class="p_add">+		&quot;bnez      %1, 0b\n\t&quot;</span>
<span class="p_add">+		&quot;1:&quot;</span>
<span class="p_add">+		: &quot;=&amp;r&quot; (prev), &quot;=&amp;r&quot; (rc), &quot;+A&quot; (v-&gt;counter)</span>
<span class="p_add">+		: &quot;r&quot; (a), &quot;r&quot; (u));</span>
<span class="p_add">+	return prev;</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * The extra atomic operations that are constructed from one of the core</span>
<span class="p_add">+ * LR/SC-based operations above.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static __always_inline int atomic_inc_not_zero(atomic_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+        return __atomic_add_unless(v, 1, 0);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef CONFIG_GENERIC_ATOMIC64</span>
<span class="p_add">+static __always_inline int atomic64_inc_not_zero(atomic64_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+        return atomic64_add_unless(v, 1, 0);</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * atomic_{cmp,}xchg is required to have exactly the same ordering semantics as</span>
<span class="p_add">+ * {cmp,}xchg and the operations that return, so they need a barrier.  We just</span>
<span class="p_add">+ * use the other implementations directly.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define ATOMIC_OP(c_t, prefix, c_or, size, asm_or)						\</span>
<span class="p_add">+static __always_inline c_t atomic##prefix##_cmpxchg##c_or(atomic##prefix##_t *v, c_t o, c_t n) 	\</span>
<span class="p_add">+{												\</span>
<span class="p_add">+	return __cmpxchg(&amp;(v-&gt;counter), o, n, size, asm_or, asm_or);				\</span>
<span class="p_add">+}												\</span>
<span class="p_add">+static __always_inline c_t atomic##prefix##_xchg##c_or(atomic##prefix##_t *v, c_t n) 		\</span>
<span class="p_add">+{												\</span>
<span class="p_add">+	return __xchg(n, &amp;(v-&gt;counter), size, asm_or);						\</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_GENERIC_ATOMIC64</span>
<span class="p_add">+#define ATOMIC_OPS(c_or, asm_or)			\</span>
<span class="p_add">+	ATOMIC_OP( int,   , c_or, 4, asm_or)</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define ATOMIC_OPS(c_or, asm_or)			\</span>
<span class="p_add">+	ATOMIC_OP( int,   , c_or, 4, asm_or)		\</span>
<span class="p_add">+	ATOMIC_OP(long, 64, c_or, 8, asm_or)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+ATOMIC_OPS(        , .aqrl)</span>
<span class="p_add">+ATOMIC_OPS(_acquire,   .aq)</span>
<span class="p_add">+ATOMIC_OPS(_release,   .rl)</span>
<span class="p_add">+ATOMIC_OPS(_relaxed,      )</span>
<span class="p_add">+</span>
<span class="p_add">+#undef ATOMIC_OPS</span>
<span class="p_add">+#undef ATOMIC_OP</span>
<span class="p_add">+</span>
<span class="p_add">+#else /* !CONFIG_ISA_A */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/atomic.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* CONFIG_ISA_A */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_ATOMIC_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/barrier.h b/arch/riscv/include/asm/barrier.h</span>
new file mode 100644
<span class="p_header">index 000000000000..cbe7bc5c506e</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/barrier.h</span>
<span class="p_chunk">@@ -0,0 +1,143 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Based on arch/arm/include/asm/barrier.h</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Copyright (C) 2012 ARM Ltd.</span>
<span class="p_add">+ * Copyright (C) 2013 Regents of the University of California</span>
<span class="p_add">+ * Copyright (C) 2017 SiFive</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of the GNU General Public License version 2 as</span>
<span class="p_add">+ * published by the Free Software Foundation.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ * GNU General Public License for more details.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * You should have received a copy of the GNU General Public License</span>
<span class="p_add">+ * along with this program.  If not, see &lt;http://www.gnu.org/licenses/&gt;.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_BARRIER_H</span>
<span class="p_add">+#define _ASM_RISCV_BARRIER_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __ASSEMBLY__</span>
<span class="p_add">+</span>
<span class="p_add">+#define nop()		__asm__ __volatile__ (&quot;nop&quot;)</span>
<span class="p_add">+</span>
<span class="p_add">+#define RISCV_FENCE(p, s) \</span>
<span class="p_add">+	__asm__ __volatile__ (&quot;fence &quot; #p &quot;,&quot; #s : : : &quot;memory&quot;)</span>
<span class="p_add">+</span>
<span class="p_add">+/* These barries need to enforce ordering on both devices or memory. */</span>
<span class="p_add">+#define mb()		RISCV_FENCE(iorw,iorw)</span>
<span class="p_add">+#define rmb()		RISCV_FENCE(ir,ir)</span>
<span class="p_add">+#define wmb()		RISCV_FENCE(ow,ow)</span>
<span class="p_add">+</span>
<span class="p_add">+/* These barries do not need to enforce ordering on devices, just memory. */</span>
<span class="p_add">+#define smp_mb()	RISCV_FENCE(rw,rw)</span>
<span class="p_add">+#define smp_rmb()	RISCV_FENCE(r,r)</span>
<span class="p_add">+#define smp_wmb()	RISCV_FENCE(w,w)</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Our atomic operations set the AQ and RL bits and therefor we don&#39;t need to</span>
<span class="p_add">+ * fence around atomics.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define __smb_mb__before_atomic()	barrier()</span>
<span class="p_add">+#define __smb_mb__after_atomic()	barrier()</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * These barries are meant to prevent memory operations inside a spinlock from</span>
<span class="p_add">+ * moving outside of that spinlock.  Since we set the AQ and RL bits when</span>
<span class="p_add">+ * entering or leaving spinlocks, no additional fence needs to be performed.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define smb_mb__before_spinlock()	barrier()</span>
<span class="p_add">+#define smb_mb__after_spinlock()	barrier()</span>
<span class="p_add">+</span>
<span class="p_add">+/* FIXME: I don&#39;t think RISC-V is allowed to perform a speculative load. */</span>
<span class="p_add">+#define smp_acquire__after_ctrl_dep()	barrier()</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * The RISC-V ISA doesn&#39;t support byte or half-word AMOs, so we fall back to a</span>
<span class="p_add">+ * regular store and a fence here.  Otherwise we emit an AMO with an AQ or RL</span>
<span class="p_add">+ * bit set and allow the microarchitecture to avoid the other half of the AMO.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define __smp_store_release(p, v)					\</span>
<span class="p_add">+do {									\</span>
<span class="p_add">+	union { typeof(*p) __val; char __c[1]; } __u =			\</span>
<span class="p_add">+		{ .__val = (__force typeof(*p)) (v) };			\</span>
<span class="p_add">+	compiletime_assert_atomic_type(*p);				\</span>
<span class="p_add">+	switch (sizeof(*p)) {						\</span>
<span class="p_add">+	case 1:								\</span>
<span class="p_add">+	case 2:								\</span>
<span class="p_add">+		smb_mb();						\</span>
<span class="p_add">+		WRITE_ONCE(*p, __u.__val);				\</span>
<span class="p_add">+		break;							\</span>
<span class="p_add">+	case 4:								\</span>
<span class="p_add">+		__asm__ __volatile__ (					\</span>
<span class="p_add">+			&quot;amoswap.w.rl zero, %1, %0&quot;			\</span>
<span class="p_add">+			: &quot;+A&quot; (*p), &quot;r&quot; (__u.__val)			\</span>
<span class="p_add">+			: 						\</span>
<span class="p_add">+			: &quot;memory&quot;);					\</span>
<span class="p_add">+		break;							\</span>
<span class="p_add">+	case 8:								\</span>
<span class="p_add">+		__asm__ __volatile__ (					\</span>
<span class="p_add">+			&quot;amoswap.d.rl zero, %1, %0&quot;			\</span>
<span class="p_add">+			: &quot;+A&quot; (*p), &quot;r&quot; (__u.__val)			\</span>
<span class="p_add">+			: 						\</span>
<span class="p_add">+			: &quot;memory&quot;);					\</span>
<span class="p_add">+		break;							\</span>
<span class="p_add">+	}								\</span>
<span class="p_add">+} while (0)</span>
<span class="p_add">+</span>
<span class="p_add">+#define __smp_load_acquire(p)						\</span>
<span class="p_add">+do {									\</span>
<span class="p_add">+	union { typeof(*p) __val; char __c[1]; } __u =			\</span>
<span class="p_add">+		{ .__val = (__force typeof(*p)) (v) };			\</span>
<span class="p_add">+	compiletime_assert_atomic_type(*p);				\</span>
<span class="p_add">+	switch (sizeof(*p)) {						\</span>
<span class="p_add">+	case 1:								\</span>
<span class="p_add">+	case 2:								\</span>
<span class="p_add">+		__u.__val = READ_ONCE(*p);				\</span>
<span class="p_add">+		smb_mb();						\</span>
<span class="p_add">+		break;							\</span>
<span class="p_add">+	case 4:								\</span>
<span class="p_add">+		__asm__ __volatile__ (					\</span>
<span class="p_add">+			&quot;amoor.w.aq %1, zero, %0&quot;			\</span>
<span class="p_add">+			: &quot;+A&quot; (*p)					\</span>
<span class="p_add">+			: &quot;=r&quot; (__u.__val)				\</span>
<span class="p_add">+			: &quot;memory&quot;);					\</span>
<span class="p_add">+		break;							\</span>
<span class="p_add">+	case 8:								\</span>
<span class="p_add">+		__asm__ __volatile__ (					\</span>
<span class="p_add">+			&quot;amoor.d.aq %1, zero, %0&quot;			\</span>
<span class="p_add">+			: &quot;+A&quot; (*p)					\</span>
<span class="p_add">+			: &quot;=r&quot; (__u.__val)				\</span>
<span class="p_add">+			: &quot;memory&quot;);					\</span>
<span class="p_add">+		break;							\</span>
<span class="p_add">+	}								\</span>
<span class="p_add">+	__u.__val;							\</span>
<span class="p_add">+} while (0)</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * The default implementation of this uses READ_ONCE and</span>
<span class="p_add">+ * smp_acquire__after_ctrl_dep, but since we can directly do an ACQUIRE load we</span>
<span class="p_add">+ * can avoid the extra barrier.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define smp_cond_load_acquire(ptr, cond_expr) ({			\</span>
<span class="p_add">+	typeof(ptr) __PTR = (ptr);					\</span>
<span class="p_add">+	typeof(*ptr) VAL;						\</span>
<span class="p_add">+	for (;;) {							\</span>
<span class="p_add">+		VAL = __smp_load_acquire(__PTR);			\</span>
<span class="p_add">+		if (cond_expr)						\</span>
<span class="p_add">+			break;						\</span>
<span class="p_add">+		cpu_relax();						\</span>
<span class="p_add">+	}								\</span>
<span class="p_add">+	smp_acquire__after_ctrl_dep();					\</span>
<span class="p_add">+	VAL;								\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/barrier.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* __ASSEMBLY__ */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_BARRIER_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/bitops.h b/arch/riscv/include/asm/bitops.h</span>
new file mode 100644
<span class="p_header">index 000000000000..27e47858c6b1</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/bitops.h</span>
<span class="p_chunk">@@ -0,0 +1,228 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_BITOPS_H</span>
<span class="p_add">+#define _ASM_RISCV_BITOPS_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _LINUX_BITOPS_H</span>
<span class="p_add">+#error &quot;Only &lt;linux/bitops.h&gt; can be included directly&quot;</span>
<span class="p_add">+#endif /* _LINUX_BITOPS_H */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef __KERNEL__</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/compiler.h&gt;</span>
<span class="p_add">+#include &lt;linux/irqflags.h&gt;</span>
<span class="p_add">+#include &lt;asm/barrier.h&gt;</span>
<span class="p_add">+#include &lt;asm/bitsperlong.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_ISA_A</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef smp_mb__before_clear_bit</span>
<span class="p_add">+#define smp_mb__before_clear_bit()  smp_mb()</span>
<span class="p_add">+#define smp_mb__after_clear_bit()   smp_mb()</span>
<span class="p_add">+#endif /* smp_mb__before_clear_bit */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/bitops/__ffs.h&gt;</span>
<span class="p_add">+#include &lt;asm-generic/bitops/ffz.h&gt;</span>
<span class="p_add">+#include &lt;asm-generic/bitops/fls.h&gt;</span>
<span class="p_add">+#include &lt;asm-generic/bitops/__fls.h&gt;</span>
<span class="p_add">+#include &lt;asm-generic/bitops/fls64.h&gt;</span>
<span class="p_add">+#include &lt;asm-generic/bitops/find.h&gt;</span>
<span class="p_add">+#include &lt;asm-generic/bitops/sched.h&gt;</span>
<span class="p_add">+#include &lt;asm-generic/bitops/ffs.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/bitops/hweight.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#if (BITS_PER_LONG == 64)</span>
<span class="p_add">+#define __AMO(op)	&quot;amo&quot; #op &quot;.d&quot;</span>
<span class="p_add">+#elif (BITS_PER_LONG == 32)</span>
<span class="p_add">+#define __AMO(op)	&quot;amo&quot; #op &quot;.w&quot;</span>
<span class="p_add">+#else</span>
<span class="p_add">+#error &quot;Unexpected BITS_PER_LONG&quot;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#define __test_and_op_bit(op, mod, nr, addr)			\</span>
<span class="p_add">+({								\</span>
<span class="p_add">+	unsigned long __res, __mask;				\</span>
<span class="p_add">+	__mask = BIT_MASK(nr);					\</span>
<span class="p_add">+	__asm__ __volatile__ (					\</span>
<span class="p_add">+		__AMO(op) &quot; %0, %2, %1&quot;				\</span>
<span class="p_add">+		: &quot;=r&quot; (__res), &quot;+A&quot; (addr[BIT_WORD(nr)])	\</span>
<span class="p_add">+		: &quot;r&quot; (mod(__mask)));				\</span>
<span class="p_add">+	((__res &amp; __mask) != 0);				\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+#define __op_bit(op, mod, nr, addr)				\</span>
<span class="p_add">+	__asm__ __volatile__ (					\</span>
<span class="p_add">+		__AMO(op) &quot; zero, %1, %0&quot;			\</span>
<span class="p_add">+		: &quot;+A&quot; (addr[BIT_WORD(nr)])			\</span>
<span class="p_add">+		: &quot;r&quot; (mod(BIT_MASK(nr))))</span>
<span class="p_add">+</span>
<span class="p_add">+/* Bitmask modifiers */</span>
<span class="p_add">+#define __NOP(x)	(x)</span>
<span class="p_add">+#define __NOT(x)	(~(x))</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * test_and_set_bit - Set a bit and return its old value</span>
<span class="p_add">+ * @nr: Bit to set</span>
<span class="p_add">+ * @addr: Address to count from</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This operation is atomic and cannot be reordered.</span>
<span class="p_add">+ * It may be reordered on other architectures than x86.</span>
<span class="p_add">+ * It also implies a memory barrier.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline int test_and_set_bit(int nr, volatile unsigned long *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __test_and_op_bit(or, __NOP, nr, addr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * test_and_clear_bit - Clear a bit and return its old value</span>
<span class="p_add">+ * @nr: Bit to clear</span>
<span class="p_add">+ * @addr: Address to count from</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This operation is atomic and cannot be reordered.</span>
<span class="p_add">+ * It can be reordered on other architectures other than x86.</span>
<span class="p_add">+ * It also implies a memory barrier.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline int test_and_clear_bit(int nr, volatile unsigned long *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __test_and_op_bit(and, __NOT, nr, addr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * test_and_change_bit - Change a bit and return its old value</span>
<span class="p_add">+ * @nr: Bit to change</span>
<span class="p_add">+ * @addr: Address to count from</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This operation is atomic and cannot be reordered.</span>
<span class="p_add">+ * It also implies a memory barrier.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline int test_and_change_bit(int nr, volatile unsigned long *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __test_and_op_bit(xor, __NOP, nr, addr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * set_bit - Atomically set a bit in memory</span>
<span class="p_add">+ * @nr: the bit to set</span>
<span class="p_add">+ * @addr: the address to start counting from</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This function is atomic and may not be reordered.  See __set_bit()</span>
<span class="p_add">+ * if you do not require the atomic guarantees.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Note: there are no guarantees that this function will not be reordered</span>
<span class="p_add">+ * on non x86 architectures, so if you are writing portable code,</span>
<span class="p_add">+ * make sure not to rely on its reordering guarantees.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Note that @nr may be almost arbitrarily large; this function is not</span>
<span class="p_add">+ * restricted to acting on a single-word quantity.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void set_bit(int nr, volatile unsigned long *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__op_bit(or, __NOP, nr, addr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * clear_bit - Clears a bit in memory</span>
<span class="p_add">+ * @nr: Bit to clear</span>
<span class="p_add">+ * @addr: Address to start counting from</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * clear_bit() is atomic and may not be reordered.  However, it does</span>
<span class="p_add">+ * not contain a memory barrier, so if it is used for locking purposes,</span>
<span class="p_add">+ * you should call smp_mb__before_clear_bit() and/or smp_mb__after_clear_bit()</span>
<span class="p_add">+ * in order to ensure changes are visible on other processors.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void clear_bit(int nr, volatile unsigned long *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__op_bit(and, __NOT, nr, addr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * change_bit - Toggle a bit in memory</span>
<span class="p_add">+ * @nr: Bit to change</span>
<span class="p_add">+ * @addr: Address to start counting from</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * change_bit() is atomic and may not be reordered. It may be</span>
<span class="p_add">+ * reordered on other architectures than x86.</span>
<span class="p_add">+ * Note that @nr may be almost arbitrarily large; this function is not</span>
<span class="p_add">+ * restricted to acting on a single-word quantity.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void change_bit(int nr, volatile unsigned long *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__op_bit(xor, __NOP, nr, addr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * test_and_set_bit_lock - Set a bit and return its old value, for lock</span>
<span class="p_add">+ * @nr: Bit to set</span>
<span class="p_add">+ * @addr: Address to count from</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This operation is atomic and provides acquire barrier semantics.</span>
<span class="p_add">+ * It can be used to implement bit locks.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline int test_and_set_bit_lock(</span>
<span class="p_add">+	unsigned long nr, volatile unsigned long *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return test_and_set_bit(nr, addr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * clear_bit_unlock - Clear a bit in memory, for unlock</span>
<span class="p_add">+ * @nr: the bit to set</span>
<span class="p_add">+ * @addr: the address to start counting from</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This operation is atomic and provides release barrier semantics.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void clear_bit_unlock(</span>
<span class="p_add">+	unsigned long nr, volatile unsigned long *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	clear_bit(nr, addr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * __clear_bit_unlock - Clear a bit in memory, for unlock</span>
<span class="p_add">+ * @nr: the bit to set</span>
<span class="p_add">+ * @addr: the address to start counting from</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This operation is like clear_bit_unlock, however it is not atomic.</span>
<span class="p_add">+ * It does provide release barrier semantics so it can be used to unlock</span>
<span class="p_add">+ * a bit lock, however it would only be used if no other CPU can modify</span>
<span class="p_add">+ * any bits in the memory until the lock is released (a good example is</span>
<span class="p_add">+ * if the bit lock itself protects access to the other bits in the word).</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void __clear_bit_unlock(</span>
<span class="p_add">+	unsigned long nr, volatile unsigned long *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	clear_bit(nr, addr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#undef __test_and_op_bit</span>
<span class="p_add">+#undef __op_bit</span>
<span class="p_add">+#undef __NOP</span>
<span class="p_add">+#undef __NOT</span>
<span class="p_add">+#undef __AMO</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/bitops/non-atomic.h&gt;</span>
<span class="p_add">+#include &lt;asm-generic/bitops/le.h&gt;</span>
<span class="p_add">+#include &lt;asm-generic/bitops/ext2-atomic.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#else /* !CONFIG_ISA_A */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/bitops.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* CONFIG_ISA_A */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* __KERNEL__ */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_BITOPS_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/cacheflush.h b/arch/riscv/include/asm/cacheflush.h</span>
new file mode 100644
<span class="p_header">index 000000000000..0595585013b0</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/cacheflush.h</span>
<span class="p_chunk">@@ -0,0 +1,39 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2015 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_CACHEFLUSH_H</span>
<span class="p_add">+#define _ASM_RISCV_CACHEFLUSH_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/cacheflush.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#undef flush_icache_range</span>
<span class="p_add">+#undef flush_icache_user_range</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void local_flush_icache_all(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	asm volatile (&quot;fence.i&quot; ::: &quot;memory&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef CONFIG_SMP</span>
<span class="p_add">+</span>
<span class="p_add">+#define flush_icache_range(start, end) local_flush_icache_all()</span>
<span class="p_add">+#define flush_icache_user_range(vma, pg, addr, len) local_flush_icache_all()</span>
<span class="p_add">+</span>
<span class="p_add">+#else /* CONFIG_SMP */</span>
<span class="p_add">+</span>
<span class="p_add">+#define flush_icache_range(start, end) sbi_remote_fence_i(0)</span>
<span class="p_add">+#define flush_icache_user_range(vma, pg, addr, len) sbi_remote_fence_i(0)</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* CONFIG_SMP */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_CACHEFLUSH_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/cmpxchg.h b/arch/riscv/include/asm/cmpxchg.h</span>
new file mode 100644
<span class="p_header">index 000000000000..81025c056412</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/cmpxchg.h</span>
<span class="p_chunk">@@ -0,0 +1,138 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2014 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_CMPXCHG_H</span>
<span class="p_add">+#define _ASM_RISCV_CMPXCHG_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/bug.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_ISA_A</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/barrier.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#define __xchg(new, ptr, size, asm_or)				\</span>
<span class="p_add">+({								\</span>
<span class="p_add">+	__typeof__(ptr) __ptr = (ptr);				\</span>
<span class="p_add">+	__typeof__(new) __new = (new);				\</span>
<span class="p_add">+	__typeof__(*(ptr)) __ret;				\</span>
<span class="p_add">+	switch (size) {						\</span>
<span class="p_add">+	case 4:							\</span>
<span class="p_add">+		__asm__ __volatile__ (				\</span>
<span class="p_add">+			&quot;amoswap.w&quot; #asm_or &quot; %0, %2, %1&quot;	\</span>
<span class="p_add">+			: &quot;=r&quot; (__ret), &quot;+A&quot; (*__ptr)		\</span>
<span class="p_add">+			: &quot;r&quot; (__new));				\</span>
<span class="p_add">+		break;						\</span>
<span class="p_add">+	case 8:							\</span>
<span class="p_add">+		__asm__ __volatile__ (				\</span>
<span class="p_add">+			&quot;amoswap.d&quot; #asm_or &quot; %0, %2, %1&quot;	\</span>
<span class="p_add">+			: &quot;=r&quot; (__ret), &quot;+A&quot; (*__ptr)		\</span>
<span class="p_add">+			: &quot;r&quot; (__new));				\</span>
<span class="p_add">+		break;						\</span>
<span class="p_add">+	default:						\</span>
<span class="p_add">+		BUILD_BUG();					\</span>
<span class="p_add">+	}							\</span>
<span class="p_add">+	__ret;							\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+#define xchg(ptr, x)    (__xchg((x), (ptr), sizeof(*(ptr)), .aqrl))</span>
<span class="p_add">+</span>
<span class="p_add">+#define xchg32(ptr, x)				\</span>
<span class="p_add">+({						\</span>
<span class="p_add">+	BUILD_BUG_ON(sizeof(*(ptr)) != 4);	\</span>
<span class="p_add">+	xchg((ptr), (x));			\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+#define xchg64(ptr, x)				\</span>
<span class="p_add">+({						\</span>
<span class="p_add">+	BUILD_BUG_ON(sizeof(*(ptr)) != 8);	\</span>
<span class="p_add">+	xchg((ptr), (x));			\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Atomic compare and exchange.  Compare OLD with MEM, if identical,</span>
<span class="p_add">+ * store NEW in MEM.  Return the initial value in MEM.  Success is</span>
<span class="p_add">+ * indicated by comparing RETURN with OLD.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define __cmpxchg(ptr, old, new, size, lrb, scb)			\</span>
<span class="p_add">+({									\</span>
<span class="p_add">+	__typeof__(ptr) __ptr = (ptr);					\</span>
<span class="p_add">+	__typeof__(old) __old = (old);					\</span>
<span class="p_add">+	__typeof__(new) __new = (new);					\</span>
<span class="p_add">+	__typeof__(*(ptr)) __ret;					\</span>
<span class="p_add">+	register unsigned int __rc;					\</span>
<span class="p_add">+	switch (size) {							\</span>
<span class="p_add">+	case 4:								\</span>
<span class="p_add">+		__asm__ __volatile__ (					\</span>
<span class="p_add">+		&quot;0:&quot;							\</span>
<span class="p_add">+			&quot;lr.w&quot; #scb &quot; %0, %2\n&quot;				\</span>
<span class="p_add">+			&quot;bne         %0, %z3, 1f\n&quot;			\</span>
<span class="p_add">+			&quot;sc.w&quot; #lrb &quot; %1, %z4, %2\n&quot;			\</span>
<span class="p_add">+			&quot;bnez        %1, 0b\n&quot;				\</span>
<span class="p_add">+		&quot;1:&quot;							\</span>
<span class="p_add">+			: &quot;=&amp;r&quot; (__ret), &quot;=&amp;r&quot; (__rc), &quot;+A&quot; (*__ptr)	\</span>
<span class="p_add">+			: &quot;rJ&quot; (__old), &quot;rJ&quot; (__new));			\</span>
<span class="p_add">+		break;							\</span>
<span class="p_add">+	case 8:								\</span>
<span class="p_add">+		__asm__ __volatile__ (					\</span>
<span class="p_add">+		&quot;0:&quot;							\</span>
<span class="p_add">+			&quot;lr.d&quot; #scb &quot; %0, %2\n&quot;				\</span>
<span class="p_add">+			&quot;bne         %0, %z3, 1f\n&quot;			\</span>
<span class="p_add">+			&quot;sc.d&quot; #lrb &quot; %1, %z4, %2\n&quot;			\</span>
<span class="p_add">+			&quot;bnez        %1, 0b\n&quot;				\</span>
<span class="p_add">+		&quot;1:&quot;							\</span>
<span class="p_add">+			: &quot;=&amp;r&quot; (__ret), &quot;=&amp;r&quot; (__rc), &quot;+A&quot; (*__ptr)	\</span>
<span class="p_add">+			: &quot;rJ&quot; (__old), &quot;rJ&quot; (__new));			\</span>
<span class="p_add">+		break;							\</span>
<span class="p_add">+	default:							\</span>
<span class="p_add">+		BUILD_BUG();						\</span>
<span class="p_add">+	}								\</span>
<span class="p_add">+	__ret;								\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+#define cmpxchg(ptr, o, n) \</span>
<span class="p_add">+	(__cmpxchg((ptr), (o), (n), sizeof(*(ptr)), .aqrl, .aqrl))</span>
<span class="p_add">+</span>
<span class="p_add">+#define cmpxchg_local(ptr, o, n) \</span>
<span class="p_add">+	(__cmpxchg((ptr), (o), (n), sizeof(*(ptr)), , ))</span>
<span class="p_add">+</span>
<span class="p_add">+#define cmpxchg32(ptr, o, n)			\</span>
<span class="p_add">+({						\</span>
<span class="p_add">+	BUILD_BUG_ON(sizeof(*(ptr)) != 4);	\</span>
<span class="p_add">+	cmpxchg((ptr), (o), (n));		\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+#define cmpxchg32_local(ptr, o, n)		\</span>
<span class="p_add">+({						\</span>
<span class="p_add">+	BUILD_BUG_ON(sizeof(*(ptr)) != 4);	\</span>
<span class="p_add">+	cmpxchg_local((ptr), (o), (n));		\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+#define cmpxchg64(ptr, o, n)			\</span>
<span class="p_add">+({						\</span>
<span class="p_add">+	BUILD_BUG_ON(sizeof(*(ptr)) != 8);	\</span>
<span class="p_add">+	cmpxchg((ptr), (o), (n));		\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+#define cmpxchg64_local(ptr, o, n)		\</span>
<span class="p_add">+({						\</span>
<span class="p_add">+	BUILD_BUG_ON(sizeof(*(ptr)) != 8);	\</span>
<span class="p_add">+	cmpxchg_local((ptr), (o), (n));		\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+#else /* !CONFIG_ISA_A */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/cmpxchg.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* CONFIG_ISA_A */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_CMPXCHG_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/io.h b/arch/riscv/include/asm/io.h</span>
new file mode 100644
<span class="p_header">index 000000000000..c47177cb6660</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/io.h</span>
<span class="p_chunk">@@ -0,0 +1,180 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * {read,write}{b,w,l,q} based on arch/arm64/include/asm/io.h</span>
<span class="p_add">+ *   which was based on arch/arm/include/io.h</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Copyright (C) 1996-2000 Russell King</span>
<span class="p_add">+ * Copyright (C) 2012 ARM Ltd.</span>
<span class="p_add">+ * Copyright (C) 2014 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_IO_H</span>
<span class="p_add">+#define _ASM_RISCV_IO_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_MMU</span>
<span class="p_add">+</span>
<span class="p_add">+extern void __iomem *ioremap(phys_addr_t offset, unsigned long size);</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * The RISC-V ISA doesn&#39;t yet specify how to query of modify PMAs, so we can&#39;t</span>
<span class="p_add">+ * change the properties of memory regions.  This should be fixed by the</span>
<span class="p_add">+ * upcoming platform spec.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define ioremap_nocache(addr, size) ioremap((addr), (size))</span>
<span class="p_add">+#define ioremap_wc(addr, size) ioremap((addr), (size))</span>
<span class="p_add">+#define ioremap_wt(addr, size) ioremap((addr), (size))</span>
<span class="p_add">+</span>
<span class="p_add">+extern void iounmap(void __iomem *addr);</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* CONFIG_MMU */</span>
<span class="p_add">+</span>
<span class="p_add">+/* Generic IO read/write.  These perform native-endian accesses. */</span>
<span class="p_add">+#define __raw_writeb __raw_writeb</span>
<span class="p_add">+static inline void __raw_writeb(u8 val, volatile void __iomem *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	asm volatile(&quot;sb %0, 0(%1)&quot; : : &quot;r&quot; (val), &quot;r&quot; (addr));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define __raw_writew __raw_writew</span>
<span class="p_add">+static inline void __raw_writew(u16 val, volatile void __iomem *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	asm volatile(&quot;sh %0, 0(%1)&quot; : : &quot;r&quot; (val), &quot;r&quot; (addr));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define __raw_writel __raw_writel</span>
<span class="p_add">+static inline void __raw_writel(u32 val, volatile void __iomem *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	asm volatile(&quot;sw %0, 0(%1)&quot; : : &quot;r&quot; (val), &quot;r&quot; (addr));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef __riscv64</span>
<span class="p_add">+#define __raw_writeq __raw_writeq</span>
<span class="p_add">+static inline void __raw_writeq(u64 val, volatile void __iomem *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	asm volatile(&quot;sd %0, 0(%1)&quot; : : &quot;r&quot; (val), &quot;r&quot; (addr));</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#define __raw_readb __raw_readb</span>
<span class="p_add">+static inline u8 __raw_readb(const volatile void __iomem *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	u8 val;</span>
<span class="p_add">+</span>
<span class="p_add">+	asm volatile(&quot;lb %0, 0(%1)&quot; : &quot;=r&quot; (val) : &quot;r&quot; (addr));</span>
<span class="p_add">+	return val;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define __raw_readw __raw_readw</span>
<span class="p_add">+static inline u16 __raw_readw(const volatile void __iomem *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	u16 val;</span>
<span class="p_add">+</span>
<span class="p_add">+	asm volatile(&quot;lh %0, 0(%1)&quot; : &quot;=r&quot; (val) : &quot;r&quot; (addr));</span>
<span class="p_add">+	return val;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define __raw_readl __raw_readl</span>
<span class="p_add">+static inline u32 __raw_readl(const volatile void __iomem *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	u32 val;</span>
<span class="p_add">+</span>
<span class="p_add">+	asm volatile(&quot;lw %0, 0(%1)&quot; : &quot;=r&quot; (val) : &quot;r&quot; (addr));</span>
<span class="p_add">+	return val;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef __riscv64</span>
<span class="p_add">+#define __raw_readq __raw_readq</span>
<span class="p_add">+static inline u64 __raw_readq(const volatile void __iomem *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	u64 val;</span>
<span class="p_add">+</span>
<span class="p_add">+	asm volatile(&quot;ld %0, 0(%1)&quot; : &quot;=r&quot; (val) : &quot;r&quot; (addr));</span>
<span class="p_add">+	return val;</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * FIXME: I&#39;m flip-flopping on whether or not we should keep this or enforce</span>
<span class="p_add">+ * the ordering with I/O on spinlocks.  The worry is that drivers won&#39;t get</span>
<span class="p_add">+ * this correct, but I also don&#39;t want to introduce a fence into the lock code</span>
<span class="p_add">+ * that otherwise only uses AMOs and LR/SC.   For now I&#39;m leaving this here:</span>
<span class="p_add">+ * &quot;w,o&quot; is sufficient to ensure that all writes to the device has completed</span>
<span class="p_add">+ * before the write to the spinlock is allowed to commit.  I surmised this from</span>
<span class="p_add">+ * reading &quot;ACQUIRES VS I/O ACCESSES&quot; in memory-barries.txt.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define mmiowb()	__asm__ __volatile__ (&quot;fence o,w&quot; : : : &quot;memory&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Unordered I/O memory access primitives.  These are even more relaxed than</span>
<span class="p_add">+ * the relaxed versions, as they don&#39;t even order accesses between successive</span>
<span class="p_add">+ * operations to the I/O regions.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define readb_cpu(c)		({ u8  __r = __raw_readb(c); __r; })</span>
<span class="p_add">+#define readw_cpu(c)		({ u16 __r = le16_to_cpu((__force __le16)__raw_readw(c)); __r; })</span>
<span class="p_add">+#define readl_cpu(c)		({ u32 __r = le32_to_cpu((__force __le32)__raw_readl(c)); __r; })</span>
<span class="p_add">+#define readq_cpu(c)		({ u64 __r = le64_to_cpu((__force __le64)__raw_readq(c)); __r; })</span>
<span class="p_add">+</span>
<span class="p_add">+#define writeb_cpu(v,c)		((void)__raw_writeb((v),(c)))</span>
<span class="p_add">+#define writew_cpu(v,c)		((void)__raw_writew((__force u16)cpu_to_le16(v),(c)))</span>
<span class="p_add">+#define writel_cpu(v,c)		((void)__raw_writel((__force u32)cpu_to_le32(v),(c)))</span>
<span class="p_add">+#define writeq_cpu(v,c)		((void)__raw_writeq((__force u64)cpu_to_le64(v),(c)))</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Relaxed I/O memory access primitives. These follow the Device memory</span>
<span class="p_add">+ * ordering rules but do not guarantee any ordering relative to Normal memory</span>
<span class="p_add">+ * accesses.  The memory barries here are necessary as RISC-V doesn&#39;t define</span>
<span class="p_add">+ * any ordering constraints on accesses to the device I/O space.  These are</span>
<span class="p_add">+ * defined to order the indicated access (either a read or write) with all</span>
<span class="p_add">+ * other I/O memory accesses.</span>
<span class="p_add">+ */</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * FIXME: The platform spec will define the default Linux-capable platform to</span>
<span class="p_add">+ * have some extra IO ordering constraints that will make these fences</span>
<span class="p_add">+ * unnecessary.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define __iorrmb()	__asm__ __volatile__ (&quot;fence i,io&quot; : : : &quot;memory&quot;);</span>
<span class="p_add">+#define __iorwmb()	__asm__ __volatile__ (&quot;fence io,o&quot; : : : &quot;memory&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+#define readb_relaxed(c)	({ u8  __v = readb_cpu(c); __iorrmb(); __v; })</span>
<span class="p_add">+#define readw_relaxed(c)	({ u16 __v = readw_cpu(c); __iorrmb(); __v; })</span>
<span class="p_add">+#define readl_relaxed(c)	({ u32 __v = readl_cpu(c); __iorrmb(); __v; })</span>
<span class="p_add">+#define readq_relaxed(c)	({ u64 __v = readq_cpu(c); __iorrmb(); __v; })</span>
<span class="p_add">+</span>
<span class="p_add">+#define writeb_relaxed(v,c)	({ __iorwmb(); writeb_cpu((v),(c)); })</span>
<span class="p_add">+#define writew_relaxed(v,c)	({ __iorwmb(); writew_cpu((v),(c)); })</span>
<span class="p_add">+#define writel_relaxed(v,c)	({ __iorwmb(); writel_cpu((v),(c)); })</span>
<span class="p_add">+#define writeq_relaxed(v,c)	({ __iorwmb(); writeq_cpu((v),(c)); })</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * I/O memory access primitives. Reads are ordered relative to any</span>
<span class="p_add">+ * following Normal memory access. Writes are ordered relative to any prior</span>
<span class="p_add">+ * Normal memory access.  The memory barriers here are necessary as RISC-V</span>
<span class="p_add">+ * doesn&#39;t define any ordering between the memory space and the I/O space.</span>
<span class="p_add">+ * They may be stronger than necessary (&quot;i,r&quot; and &quot;w,o&quot; might be sufficient),</span>
<span class="p_add">+ * but I feel kind of queasy making these weaker in any manner than the relaxed</span>
<span class="p_add">+ * versions above.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define __iormb()	__asm__ __volatile__ (&quot;fence i,ior&quot; : : : &quot;memory&quot;);</span>
<span class="p_add">+#define __iowmb()	__asm__ __volatile__ (&quot;fence iow,o&quot; : : : &quot;memory&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+#define readb(c)		({ u8  __v = readb_cpu(c); __iormb(); __v; })</span>
<span class="p_add">+#define readw(c)		({ u16 __v = readw_cpu(c); __iormb(); __v; })</span>
<span class="p_add">+#define readl(c)		({ u32 __v = readl_cpu(c); __iormb(); __v; })</span>
<span class="p_add">+#define readq(c)		({ u64 __v = readq_cpu(c); __iormb(); __v; })</span>
<span class="p_add">+</span>
<span class="p_add">+#define writeb(v,c)		({ __iowmb(); writeb_cpu((v),(c)); })</span>
<span class="p_add">+#define writew(v,c)		({ __iowmb(); writew_cpu((v),(c)); })</span>
<span class="p_add">+#define writel(v,c)		({ __iowmb(); writel_cpu((v),(c)); })</span>
<span class="p_add">+#define writeq(v,c)		({ __iowmb(); writeq_cpu((v),(c)); })</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/io.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_IO_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/spinlock.h b/arch/riscv/include/asm/spinlock.h</span>
new file mode 100644
<span class="p_header">index 000000000000..54ed73bfa972</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/spinlock.h</span>
<span class="p_chunk">@@ -0,0 +1,167 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2015 Regents of the University of California</span>
<span class="p_add">+ * Copyright (C) 2017 SiFive</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_SPINLOCK_H</span>
<span class="p_add">+#define _ASM_RISCV_SPINLOCK_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/kernel.h&gt;</span>
<span class="p_add">+#include &lt;asm/current.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Simple spin lock operations.  These provide no fairness guarantees.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+/* FIXME: Replace this with a ticket lock, like MIPS. */</span>
<span class="p_add">+</span>
<span class="p_add">+#define arch_spin_lock_flags(lock, flags) arch_spin_lock(lock)</span>
<span class="p_add">+#define arch_spin_is_locked(x)	((x)-&gt;lock != 0)</span>
<span class="p_add">+#define arch_spin_unlock_wait(x) \</span>
<span class="p_add">+		do { cpu_relax(); } while ((x)-&gt;lock)</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void arch_spin_unlock(arch_spinlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;amoswap.w.rl x0, x0, %0&quot;</span>
<span class="p_add">+		: &quot;=A&quot; (lock-&gt;lock)</span>
<span class="p_add">+		:: &quot;memory&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int arch_spin_trylock(arch_spinlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int tmp = 1, busy;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;amoswap.w.aq %0, %2, %1&quot;</span>
<span class="p_add">+		: &quot;=r&quot; (busy), &quot;+A&quot; (lock-&gt;lock)</span>
<span class="p_add">+		: &quot;r&quot; (tmp)</span>
<span class="p_add">+		: &quot;memory&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+	return !busy;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void arch_spin_lock(arch_spinlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	while (1) {</span>
<span class="p_add">+		if (arch_spin_is_locked(lock))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (arch_spin_trylock(lock))</span>
<span class="p_add">+			break;</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void arch_spin_unlock_wait(arch_spinlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	smp_rmb();</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		cpu_relax();</span>
<span class="p_add">+	} while (arch_spin_is_locked(lock));</span>
<span class="p_add">+	smp_acquire__after_ctrl_dep();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/***********************************************************/</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int arch_read_can_lock(arch_rwlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return lock-&gt;lock &gt;= 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int arch_write_can_lock(arch_rwlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return lock-&gt;lock == 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void arch_read_lock(arch_rwlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int tmp;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__(</span>
<span class="p_add">+		&quot;1:	lr.w	%1, %0\n&quot;</span>
<span class="p_add">+		&quot;	bltz	%1, 1b\n&quot;</span>
<span class="p_add">+		&quot;	addi	%1, %1, 1\n&quot;</span>
<span class="p_add">+		&quot;	sc.w.aq	%1, %1, %0\n&quot;</span>
<span class="p_add">+		&quot;	bnez	%1, 1b\n&quot;</span>
<span class="p_add">+		: &quot;+A&quot; (lock-&gt;lock), &quot;=&amp;r&quot; (tmp)</span>
<span class="p_add">+		:: &quot;memory&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void arch_write_lock(arch_rwlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int tmp;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__(</span>
<span class="p_add">+		&quot;1:	lr.w	%1, %0\n&quot;</span>
<span class="p_add">+		&quot;	bnez	%1, 1b\n&quot;</span>
<span class="p_add">+		&quot;	li	%1, -1\n&quot;</span>
<span class="p_add">+		&quot;	sc.w.aq	%1, %1, %0\n&quot;</span>
<span class="p_add">+		&quot;	bnez	%1, 1b\n&quot;</span>
<span class="p_add">+		: &quot;+A&quot; (lock-&gt;lock), &quot;=&amp;r&quot; (tmp)</span>
<span class="p_add">+		:: &quot;memory&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int arch_read_trylock(arch_rwlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int busy;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__(</span>
<span class="p_add">+		&quot;1:	lr.w	%1, %0\n&quot;</span>
<span class="p_add">+		&quot;	bltz	%1, 1f\n&quot;</span>
<span class="p_add">+		&quot;	addi	%1, %1, 1\n&quot;</span>
<span class="p_add">+		&quot;	sc.w.aq	%1, %1, %0\n&quot;</span>
<span class="p_add">+		&quot;	bnez	%1, 1b\n&quot;</span>
<span class="p_add">+		&quot;1:\n&quot;</span>
<span class="p_add">+		: &quot;+A&quot; (lock-&gt;lock), &quot;=&amp;r&quot; (busy)</span>
<span class="p_add">+		:: &quot;memory&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+	return !busy;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int arch_write_trylock(arch_rwlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int busy;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__(</span>
<span class="p_add">+		&quot;1:	lr.w	%1, %0\n&quot;</span>
<span class="p_add">+		&quot;	bnez	%1, 1f\n&quot;</span>
<span class="p_add">+		&quot;	li	%1, -1\n&quot;</span>
<span class="p_add">+		&quot;	sc.w.aq	%1, %1, %0\n&quot;</span>
<span class="p_add">+		&quot;	bnez	%1, 1b\n&quot;</span>
<span class="p_add">+		&quot;1:\n&quot;</span>
<span class="p_add">+		: &quot;+A&quot; (lock-&gt;lock), &quot;=&amp;r&quot; (busy)</span>
<span class="p_add">+		:: &quot;memory&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+	return !busy;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void arch_read_unlock(arch_rwlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__asm__ __volatile__(</span>
<span class="p_add">+		&quot;amoadd.w.rl x0, %1, %0&quot;</span>
<span class="p_add">+		: &quot;+A&quot; (lock-&gt;lock)</span>
<span class="p_add">+		: &quot;r&quot; (-1)</span>
<span class="p_add">+		: &quot;memory&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void arch_write_unlock(arch_rwlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;amoswap.w.rl x0, x0, %0&quot;</span>
<span class="p_add">+		: &quot;=A&quot; (lock-&gt;lock)</span>
<span class="p_add">+		:: &quot;memory&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define arch_read_lock_flags(lock, flags) arch_read_lock(lock)</span>
<span class="p_add">+#define arch_write_lock_flags(lock, flags) arch_write_lock(lock)</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_SPINLOCK_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/spinlock_types.h b/arch/riscv/include/asm/spinlock_types.h</span>
new file mode 100644
<span class="p_header">index 000000000000..83ac4ac9e2ac</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/spinlock_types.h</span>
<span class="p_chunk">@@ -0,0 +1,33 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2015 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_SPINLOCK_TYPES_H</span>
<span class="p_add">+#define _ASM_RISCV_SPINLOCK_TYPES_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __LINUX_SPINLOCK_TYPES_H</span>
<span class="p_add">+# error &quot;please don&#39;t include this file directly&quot;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+typedef struct {</span>
<span class="p_add">+	volatile unsigned int lock;</span>
<span class="p_add">+} arch_spinlock_t;</span>
<span class="p_add">+</span>
<span class="p_add">+#define __ARCH_SPIN_LOCK_UNLOCKED	{ 0 }</span>
<span class="p_add">+</span>
<span class="p_add">+typedef struct {</span>
<span class="p_add">+	volatile unsigned int lock;</span>
<span class="p_add">+} arch_rwlock_t;</span>
<span class="p_add">+</span>
<span class="p_add">+#define __ARCH_RW_LOCK_UNLOCKED		{ 0 }</span>
<span class="p_add">+</span>
<span class="p_add">+#endif</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/tlb.h b/arch/riscv/include/asm/tlb.h</span>
new file mode 100644
<span class="p_header">index 000000000000..c229509288ea</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/tlb.h</span>
<span class="p_chunk">@@ -0,0 +1,24 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_TLB_H</span>
<span class="p_add">+#define _ASM_RISCV_TLB_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/tlb.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void tlb_flush(struct mmu_gather *tlb)</span>
<span class="p_add">+{</span>
<span class="p_add">+	flush_tlb_mm(tlb-&gt;mm);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_TLB_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/tlbflush.h b/arch/riscv/include/asm/tlbflush.h</span>
new file mode 100644
<span class="p_header">index 000000000000..5ee4ae370b5e</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/tlbflush.h</span>
<span class="p_chunk">@@ -0,0 +1,64 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2009 Chen Liqin &lt;liqin.chen@sunplusct.com&gt;</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_TLBFLUSH_H</span>
<span class="p_add">+#define _ASM_RISCV_TLBFLUSH_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_MMU</span>
<span class="p_add">+</span>
<span class="p_add">+/* Flush entire local TLB */</span>
<span class="p_add">+static inline void local_flush_tlb_all(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__asm__ __volatile__ (&quot;sfence.vma&quot; : : : &quot;memory&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Flush one page from local TLB */</span>
<span class="p_add">+static inline void local_flush_tlb_page(unsigned long addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__asm__ __volatile__ (&quot;sfence.vma %0&quot; : : &quot;r&quot; (addr) : &quot;memory&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef CONFIG_SMP</span>
<span class="p_add">+</span>
<span class="p_add">+#define flush_tlb_all() local_flush_tlb_all()</span>
<span class="p_add">+#define flush_tlb_page(vma, addr) local_flush_tlb_page(addr)</span>
<span class="p_add">+#define flush_tlb_range(vma, start, end) local_flush_tlb_all()</span>
<span class="p_add">+</span>
<span class="p_add">+#else /* CONFIG_SMP */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/sbi.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#define flush_tlb_all() sbi_remote_sfence_vma(0, 0, -1)</span>
<span class="p_add">+#define flush_tlb_page(vma, addr) flush_tlb_range(vma, addr, 0)</span>
<span class="p_add">+#define flush_tlb_range(vma, start, end) \</span>
<span class="p_add">+	sbi_remote_sfence_vma(0, start, (end) - (start))</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* CONFIG_SMP */</span>
<span class="p_add">+</span>
<span class="p_add">+/* Flush the TLB entries of the specified mm context */</span>
<span class="p_add">+static inline void flush_tlb_mm(struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+	flush_tlb_all();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Flush a range of kernel pages */</span>
<span class="p_add">+static inline void flush_tlb_kernel_range(unsigned long start,</span>
<span class="p_add">+	unsigned long end)</span>
<span class="p_add">+{</span>
<span class="p_add">+	flush_tlb_all();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* CONFIG_MMU */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_TLBFLUSH_H */</span>

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



