
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[PATCHv7,01/14] x86/mm/gup: Switch GUP to the generic get_user_page_fast() implementation - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [PATCHv7,01/14] x86/mm/gup: Switch GUP to the generic get_user_page_fast() implementation</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=40781">Kirill A. Shutemov</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>June 6, 2017, 11:31 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20170606113133.22974-2-kirill.shutemov@linux.intel.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9768691/mbox/"
   >mbox</a>
|
   <a href="/patch/9768691/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9768691/">/patch/9768691/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	950B560393 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  6 Jun 2017 11:32:13 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 8498D2841C
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  6 Jun 2017 11:32:13 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 78F8828464; Tue,  6 Jun 2017 11:32:13 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id D0DAF2841C
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  6 Jun 2017 11:32:11 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1751641AbdFFLbs (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 6 Jun 2017 07:31:48 -0400
Received: from mga09.intel.com ([134.134.136.24]:26106 &quot;EHLO mga09.intel.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1751384AbdFFLbn (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 6 Jun 2017 07:31:43 -0400
Received: from fmsmga001.fm.intel.com ([10.253.24.23])
	by orsmga102.jf.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384;
	06 Jun 2017 04:31:41 -0700
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i=&quot;5.39,306,1493708400&quot;; d=&quot;scan&#39;208&quot;;a=&quot;1157120162&quot;
Received: from black.fi.intel.com ([10.237.72.28])
	by fmsmga001.fm.intel.com with ESMTP; 06 Jun 2017 04:31:37 -0700
Received: by black.fi.intel.com (Postfix, from userid 1000)
	id 47254185; Tue,  6 Jun 2017 14:31:35 +0300 (EEST)
From: &quot;Kirill A. Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt;
To: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;, x86@kernel.org,
	Thomas Gleixner &lt;tglx@linutronix.de&gt;, Ingo Molnar &lt;mingo@redhat.com&gt;,
	&quot;H. Peter Anvin&quot; &lt;hpa@zytor.com&gt;
Cc: Andi Kleen &lt;ak@linux.intel.com&gt;, Dave Hansen &lt;dave.hansen@intel.com&gt;,
	Andy Lutomirski &lt;luto@amacapital.net&gt;,
	linux-arch@vger.kernel.org, linux-mm@kvack.org,
	linux-kernel@vger.kernel.org,
	&quot;Kirill A. Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt;
Subject: [PATCHv7 01/14] x86/mm/gup: Switch GUP to the generic
	get_user_page_fast() implementation
Date: Tue,  6 Jun 2017 14:31:20 +0300
Message-Id: &lt;20170606113133.22974-2-kirill.shutemov@linux.intel.com&gt;
X-Mailer: git-send-email 2.11.0
In-Reply-To: &lt;20170606113133.22974-1-kirill.shutemov@linux.intel.com&gt;
References: &lt;20170606113133.22974-1-kirill.shutemov@linux.intel.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=40781">Kirill A. Shutemov</a> - June 6, 2017, 11:31 a.m.</div>
<pre class="content">
This patch provides all required callbacks required by the generic
get_user_pages_fast() code and switches x86 over - and removes
the platform specific implementation.
<span class="signed-off-by">
Signed-off-by: Kirill A. Shutemov &lt;kirill.shutemov@linux.intel.com&gt;</span>
---
 arch/arm/Kconfig                      |   2 +-
 arch/arm64/Kconfig                    |   2 +-
 arch/powerpc/Kconfig                  |   2 +-
 arch/x86/Kconfig                      |   3 +
 arch/x86/include/asm/mmu_context.h    |  12 -
 arch/x86/include/asm/pgtable-3level.h |  47 ++++
 arch/x86/include/asm/pgtable.h        |  53 ++++
 arch/x86/include/asm/pgtable_64.h     |  16 +-
 arch/x86/mm/Makefile                  |   2 +-
 arch/x86/mm/gup.c                     | 496 ----------------------------------
 mm/Kconfig                            |   2 +-
 mm/gup.c                              |  10 +-
 12 files changed, 128 insertions(+), 519 deletions(-)
 delete mode 100644 arch/x86/mm/gup.c
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/arm/Kconfig b/arch/arm/Kconfig</span>
<span class="p_header">index dabcaeb2ee3e..fce9fd34eb33 100644</span>
<span class="p_header">--- a/arch/arm/Kconfig</span>
<span class="p_header">+++ b/arch/arm/Kconfig</span>
<span class="p_chunk">@@ -1637,7 +1637,7 @@</span> <span class="p_context"> config ARCH_SELECT_MEMORY_MODEL</span>
 config HAVE_ARCH_PFN_VALID
 	def_bool ARCH_HAS_HOLES_MEMORYMODEL || !SPARSEMEM
 
<span class="p_del">-config HAVE_GENERIC_RCU_GUP</span>
<span class="p_add">+config HAVE_GENERIC_GUP</span>
 	def_bool y
 	depends on ARM_LPAE
 
<span class="p_header">diff --git a/arch/arm64/Kconfig b/arch/arm64/Kconfig</span>
<span class="p_header">index 3dcd7ec69bca..a7c5f8c3f13d 100644</span>
<span class="p_header">--- a/arch/arm64/Kconfig</span>
<span class="p_header">+++ b/arch/arm64/Kconfig</span>
<span class="p_chunk">@@ -205,7 +205,7 @@</span> <span class="p_context"> config GENERIC_CALIBRATE_DELAY</span>
 config ZONE_DMA
 	def_bool y
 
<span class="p_del">-config HAVE_GENERIC_RCU_GUP</span>
<span class="p_add">+config HAVE_GENERIC_GUP</span>
 	def_bool y
 
 config ARCH_DMA_ADDR_T_64BIT
<span class="p_header">diff --git a/arch/powerpc/Kconfig b/arch/powerpc/Kconfig</span>
<span class="p_header">index f7c8f9972f61..7d898796c819 100644</span>
<span class="p_header">--- a/arch/powerpc/Kconfig</span>
<span class="p_header">+++ b/arch/powerpc/Kconfig</span>
<span class="p_chunk">@@ -184,7 +184,7 @@</span> <span class="p_context"> config PPC</span>
 	select HAVE_FUNCTION_GRAPH_TRACER
 	select HAVE_FUNCTION_TRACER
 	select HAVE_GCC_PLUGINS
<span class="p_del">-	select HAVE_GENERIC_RCU_GUP</span>
<span class="p_add">+	select HAVE_GENERIC_GUP</span>
 	select HAVE_HW_BREAKPOINT		if PERF_EVENTS &amp;&amp; (PPC_BOOK3S || PPC_8xx)
 	select HAVE_IDE
 	select HAVE_IOREMAP_PROT
<span class="p_header">diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig</span>
<span class="p_header">index 7a065d81dc43..de71b6aca0be 100644</span>
<span class="p_header">--- a/arch/x86/Kconfig</span>
<span class="p_header">+++ b/arch/x86/Kconfig</span>
<span class="p_chunk">@@ -2797,6 +2797,9 @@</span> <span class="p_context"> config X86_DMA_REMAP</span>
 	bool
 	depends on STA2X11
 
<span class="p_add">+config HAVE_GENERIC_GUP</span>
<span class="p_add">+	def_bool y</span>
<span class="p_add">+</span>
 source &quot;net/Kconfig&quot;
 
 source &quot;drivers/Kconfig&quot;
<span class="p_header">diff --git a/arch/x86/include/asm/mmu_context.h b/arch/x86/include/asm/mmu_context.h</span>
<span class="p_header">index f20d7ea47095..0db8bac8b42d 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/mmu_context.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/mmu_context.h</span>
<span class="p_chunk">@@ -218,18 +218,6 @@</span> <span class="p_context"> static inline int vma_pkey(struct vm_area_struct *vma)</span>
 }
 #endif
 
<span class="p_del">-static inline bool __pkru_allows_pkey(u16 pkey, bool write)</span>
<span class="p_del">-{</span>
<span class="p_del">-	u32 pkru = read_pkru();</span>
<span class="p_del">-</span>
<span class="p_del">-	if (!__pkru_allows_read(pkru, pkey))</span>
<span class="p_del">-		return false;</span>
<span class="p_del">-	if (write &amp;&amp; !__pkru_allows_write(pkru, pkey))</span>
<span class="p_del">-		return false;</span>
<span class="p_del">-</span>
<span class="p_del">-	return true;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 /*
  * We only want to enforce protection keys on the current process
  * because we effectively have no access to PKRU for other
<span class="p_header">diff --git a/arch/x86/include/asm/pgtable-3level.h b/arch/x86/include/asm/pgtable-3level.h</span>
<span class="p_header">index 50d35e3185f5..c8821bab938f 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/pgtable-3level.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/pgtable-3level.h</span>
<span class="p_chunk">@@ -212,4 +212,51 @@</span> <span class="p_context"> static inline pud_t native_pudp_get_and_clear(pud_t *pudp)</span>
 #define __pte_to_swp_entry(pte)		((swp_entry_t){ (pte).pte_high })
 #define __swp_entry_to_pte(x)		((pte_t){ { .pte_high = (x).val } })
 
<span class="p_add">+#define gup_get_pte gup_get_pte</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * WARNING: only to be used in the get_user_pages_fast() implementation.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * With get_user_pages_fast(), we walk down the pagetables without taking</span>
<span class="p_add">+ * any locks.  For this we would like to load the pointers atomically,</span>
<span class="p_add">+ * but that is not possible (without expensive cmpxchg8b) on PAE.  What</span>
<span class="p_add">+ * we do have is the guarantee that a PTE will only either go from not</span>
<span class="p_add">+ * present to present, or present to not present or both -- it will not</span>
<span class="p_add">+ * switch to a completely different present page without a TLB flush in</span>
<span class="p_add">+ * between; something that we are blocking by holding interrupts off.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Setting ptes from not present to present goes:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   ptep-&gt;pte_high = h;</span>
<span class="p_add">+ *   smp_wmb();</span>
<span class="p_add">+ *   ptep-&gt;pte_low = l;</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * And present to not present goes:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   ptep-&gt;pte_low = 0;</span>
<span class="p_add">+ *   smp_wmb();</span>
<span class="p_add">+ *   ptep-&gt;pte_high = 0;</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * We must ensure here that the load of pte_low sees &#39;l&#39; iff pte_high</span>
<span class="p_add">+ * sees &#39;h&#39;. We load pte_high *after* loading pte_low, which ensures we</span>
<span class="p_add">+ * don&#39;t see an older value of pte_high.  *Then* we recheck pte_low,</span>
<span class="p_add">+ * which ensures that we haven&#39;t picked up a changed pte high. We might</span>
<span class="p_add">+ * have gotten rubbish values from pte_low and pte_high, but we are</span>
<span class="p_add">+ * guaranteed that pte_low will not have the present bit set *unless*</span>
<span class="p_add">+ * it is &#39;l&#39;. Because get_user_pages_fast() only operates on present ptes</span>
<span class="p_add">+ * we&#39;re safe.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline pte_t gup_get_pte(pte_t *ptep)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pte_t pte;</span>
<span class="p_add">+</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		pte.pte_low = ptep-&gt;pte_low;</span>
<span class="p_add">+		smp_rmb();</span>
<span class="p_add">+		pte.pte_high = ptep-&gt;pte_high;</span>
<span class="p_add">+		smp_rmb();</span>
<span class="p_add">+	} while (unlikely(pte.pte_low != ptep-&gt;pte_low));</span>
<span class="p_add">+</span>
<span class="p_add">+	return pte;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 #endif /* _ASM_X86_PGTABLE_3LEVEL_H */
<span class="p_header">diff --git a/arch/x86/include/asm/pgtable.h b/arch/x86/include/asm/pgtable.h</span>
<span class="p_header">index f5af95a0c6b8..942482ac36a8 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/pgtable.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/pgtable.h</span>
<span class="p_chunk">@@ -244,6 +244,11 @@</span> <span class="p_context"> static inline int pud_devmap(pud_t pud)</span>
 	return 0;
 }
 #endif
<span class="p_add">+</span>
<span class="p_add">+static inline int pgd_devmap(pgd_t pgd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
 #endif
 #endif /* CONFIG_TRANSPARENT_HUGEPAGE */
 
<span class="p_chunk">@@ -1185,6 +1190,54 @@</span> <span class="p_context"> static inline u16 pte_flags_pkey(unsigned long pte_flags)</span>
 #endif
 }
 
<span class="p_add">+static inline bool __pkru_allows_pkey(u16 pkey, bool write)</span>
<span class="p_add">+{</span>
<span class="p_add">+	u32 pkru = read_pkru();</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!__pkru_allows_read(pkru, pkey))</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+	if (write &amp;&amp; !__pkru_allows_write(pkru, pkey))</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+</span>
<span class="p_add">+	return true;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * &#39;pteval&#39; can come from a PTE, PMD or PUD.  We only check</span>
<span class="p_add">+ * _PAGE_PRESENT, _PAGE_USER, and _PAGE_RW in here which are the</span>
<span class="p_add">+ * same value on all 3 types.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline bool __pte_access_permitted(unsigned long pteval, bool write)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long need_pte_bits = _PAGE_PRESENT|_PAGE_USER;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (write)</span>
<span class="p_add">+		need_pte_bits |= _PAGE_RW;</span>
<span class="p_add">+</span>
<span class="p_add">+	if ((pteval &amp; need_pte_bits) != need_pte_bits)</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	return __pkru_allows_pkey(pte_flags_pkey(pteval), write);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define pte_access_permitted pte_access_permitted</span>
<span class="p_add">+static inline bool pte_access_permitted(pte_t pte, bool write)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __pte_access_permitted(pte_val(pte), write);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define pmd_access_permitted pmd_access_permitted</span>
<span class="p_add">+static inline bool pmd_access_permitted(pmd_t pmd, bool write)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __pte_access_permitted(pmd_val(pmd), write);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define pud_access_permitted pud_access_permitted</span>
<span class="p_add">+static inline bool pud_access_permitted(pud_t pud, bool write)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __pte_access_permitted(pud_val(pud), write);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 #include &lt;asm-generic/pgtable.h&gt;
 #endif	/* __ASSEMBLY__ */
 
<span class="p_header">diff --git a/arch/x86/include/asm/pgtable_64.h b/arch/x86/include/asm/pgtable_64.h</span>
<span class="p_header">index 9991224f6238..12ea31274eb6 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/pgtable_64.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/pgtable_64.h</span>
<span class="p_chunk">@@ -227,6 +227,20 @@</span> <span class="p_context"> extern void cleanup_highmap(void);</span>
 extern void init_extra_mapping_uc(unsigned long phys, unsigned long size);
 extern void init_extra_mapping_wb(unsigned long phys, unsigned long size);
 
<span class="p_del">-#endif /* !__ASSEMBLY__ */</span>
<span class="p_add">+#define gup_fast_permitted gup_fast_permitted</span>
<span class="p_add">+static inline bool gup_fast_permitted(unsigned long start, int nr_pages,</span>
<span class="p_add">+		int write)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long len, end;</span>
<span class="p_add">+</span>
<span class="p_add">+	len = (unsigned long)nr_pages &lt;&lt; PAGE_SHIFT;</span>
<span class="p_add">+	end = start + len;</span>
<span class="p_add">+	if (end &lt; start)</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+	if (end &gt;&gt; __VIRTUAL_MASK_SHIFT)</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+	return true;</span>
<span class="p_add">+}</span>
 
<span class="p_add">+#endif /* !__ASSEMBLY__ */</span>
 #endif /* _ASM_X86_PGTABLE_64_H */
<span class="p_header">diff --git a/arch/x86/mm/Makefile b/arch/x86/mm/Makefile</span>
<span class="p_header">index 96d2b847e09e..0fbdcb64f9f8 100644</span>
<span class="p_header">--- a/arch/x86/mm/Makefile</span>
<span class="p_header">+++ b/arch/x86/mm/Makefile</span>
<span class="p_chunk">@@ -2,7 +2,7 @@</span> <span class="p_context"></span>
 KCOV_INSTRUMENT_tlb.o	:= n
 
 obj-y	:=  init.o init_$(BITS).o fault.o ioremap.o extable.o pageattr.o mmap.o \
<span class="p_del">-	    pat.o pgtable.o physaddr.o gup.o setup_nx.o tlb.o</span>
<span class="p_add">+	    pat.o pgtable.o physaddr.o setup_nx.o tlb.o</span>
 
 # Make sure __phys_addr has no stackprotector
 nostackp := $(call cc-option, -fno-stack-protector)
<span class="p_header">diff --git a/arch/x86/mm/gup.c b/arch/x86/mm/gup.c</span>
deleted file mode 100644
<span class="p_header">index 456dfdfd2249..000000000000</span>
<span class="p_header">--- a/arch/x86/mm/gup.c</span>
<span class="p_header">+++ /dev/null</span>
<span class="p_chunk">@@ -1,496 +0,0 @@</span> <span class="p_context"></span>
<span class="p_del">-/*</span>
<span class="p_del">- * Lockless get_user_pages_fast for x86</span>
<span class="p_del">- *</span>
<span class="p_del">- * Copyright (C) 2008 Nick Piggin</span>
<span class="p_del">- * Copyright (C) 2008 Novell Inc.</span>
<span class="p_del">- */</span>
<span class="p_del">-#include &lt;linux/sched.h&gt;</span>
<span class="p_del">-#include &lt;linux/mm.h&gt;</span>
<span class="p_del">-#include &lt;linux/vmstat.h&gt;</span>
<span class="p_del">-#include &lt;linux/highmem.h&gt;</span>
<span class="p_del">-#include &lt;linux/swap.h&gt;</span>
<span class="p_del">-#include &lt;linux/memremap.h&gt;</span>
<span class="p_del">-</span>
<span class="p_del">-#include &lt;asm/mmu_context.h&gt;</span>
<span class="p_del">-#include &lt;asm/pgtable.h&gt;</span>
<span class="p_del">-</span>
<span class="p_del">-static inline pte_t gup_get_pte(pte_t *ptep)</span>
<span class="p_del">-{</span>
<span class="p_del">-#ifndef CONFIG_X86_PAE</span>
<span class="p_del">-	return READ_ONCE(*ptep);</span>
<span class="p_del">-#else</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * With get_user_pages_fast, we walk down the pagetables without taking</span>
<span class="p_del">-	 * any locks.  For this we would like to load the pointers atomically,</span>
<span class="p_del">-	 * but that is not possible (without expensive cmpxchg8b) on PAE.  What</span>
<span class="p_del">-	 * we do have is the guarantee that a pte will only either go from not</span>
<span class="p_del">-	 * present to present, or present to not present or both -- it will not</span>
<span class="p_del">-	 * switch to a completely different present page without a TLB flush in</span>
<span class="p_del">-	 * between; something that we are blocking by holding interrupts off.</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 * Setting ptes from not present to present goes:</span>
<span class="p_del">-	 * ptep-&gt;pte_high = h;</span>
<span class="p_del">-	 * smp_wmb();</span>
<span class="p_del">-	 * ptep-&gt;pte_low = l;</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 * And present to not present goes:</span>
<span class="p_del">-	 * ptep-&gt;pte_low = 0;</span>
<span class="p_del">-	 * smp_wmb();</span>
<span class="p_del">-	 * ptep-&gt;pte_high = 0;</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 * We must ensure here that the load of pte_low sees l iff pte_high</span>
<span class="p_del">-	 * sees h. We load pte_high *after* loading pte_low, which ensures we</span>
<span class="p_del">-	 * don&#39;t see an older value of pte_high.  *Then* we recheck pte_low,</span>
<span class="p_del">-	 * which ensures that we haven&#39;t picked up a changed pte high. We might</span>
<span class="p_del">-	 * have got rubbish values from pte_low and pte_high, but we are</span>
<span class="p_del">-	 * guaranteed that pte_low will not have the present bit set *unless*</span>
<span class="p_del">-	 * it is &#39;l&#39;. And get_user_pages_fast only operates on present ptes, so</span>
<span class="p_del">-	 * we&#39;re safe.</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 * gup_get_pte should not be used or copied outside gup.c without being</span>
<span class="p_del">-	 * very careful -- it does not atomically load the pte or anything that</span>
<span class="p_del">-	 * is likely to be useful for you.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	pte_t pte;</span>
<span class="p_del">-</span>
<span class="p_del">-retry:</span>
<span class="p_del">-	pte.pte_low = ptep-&gt;pte_low;</span>
<span class="p_del">-	smp_rmb();</span>
<span class="p_del">-	pte.pte_high = ptep-&gt;pte_high;</span>
<span class="p_del">-	smp_rmb();</span>
<span class="p_del">-	if (unlikely(pte.pte_low != ptep-&gt;pte_low))</span>
<span class="p_del">-		goto retry;</span>
<span class="p_del">-</span>
<span class="p_del">-	return pte;</span>
<span class="p_del">-#endif</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static void undo_dev_pagemap(int *nr, int nr_start, struct page **pages)</span>
<span class="p_del">-{</span>
<span class="p_del">-	while ((*nr) - nr_start) {</span>
<span class="p_del">-		struct page *page = pages[--(*nr)];</span>
<span class="p_del">-</span>
<span class="p_del">-		ClearPageReferenced(page);</span>
<span class="p_del">-		put_page(page);</span>
<span class="p_del">-	}</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
<span class="p_del">- * &#39;pteval&#39; can come from a pte, pmd, pud or p4d.  We only check</span>
<span class="p_del">- * _PAGE_PRESENT, _PAGE_USER, and _PAGE_RW in here which are the</span>
<span class="p_del">- * same value on all 4 types.</span>
<span class="p_del">- */</span>
<span class="p_del">-static inline int pte_allows_gup(unsigned long pteval, int write)</span>
<span class="p_del">-{</span>
<span class="p_del">-	unsigned long need_pte_bits = _PAGE_PRESENT|_PAGE_USER;</span>
<span class="p_del">-</span>
<span class="p_del">-	if (write)</span>
<span class="p_del">-		need_pte_bits |= _PAGE_RW;</span>
<span class="p_del">-</span>
<span class="p_del">-	if ((pteval &amp; need_pte_bits) != need_pte_bits)</span>
<span class="p_del">-		return 0;</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Check memory protection keys permissions. */</span>
<span class="p_del">-	if (!__pkru_allows_pkey(pte_flags_pkey(pteval), write))</span>
<span class="p_del">-		return 0;</span>
<span class="p_del">-</span>
<span class="p_del">-	return 1;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
<span class="p_del">- * The performance critical leaf functions are made noinline otherwise gcc</span>
<span class="p_del">- * inlines everything into a single function which results in too much</span>
<span class="p_del">- * register pressure.</span>
<span class="p_del">- */</span>
<span class="p_del">-static noinline int gup_pte_range(pmd_t pmd, unsigned long addr,</span>
<span class="p_del">-		unsigned long end, int write, struct page **pages, int *nr)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct dev_pagemap *pgmap = NULL;</span>
<span class="p_del">-	int nr_start = *nr, ret = 0;</span>
<span class="p_del">-	pte_t *ptep, *ptem;</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Keep the original mapped PTE value (ptem) around since we</span>
<span class="p_del">-	 * might increment ptep off the end of the page when finishing</span>
<span class="p_del">-	 * our loop iteration.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	ptem = ptep = pte_offset_map(&amp;pmd, addr);</span>
<span class="p_del">-	do {</span>
<span class="p_del">-		pte_t pte = gup_get_pte(ptep);</span>
<span class="p_del">-		struct page *page;</span>
<span class="p_del">-</span>
<span class="p_del">-		/* Similar to the PMD case, NUMA hinting must take slow path */</span>
<span class="p_del">-		if (pte_protnone(pte))</span>
<span class="p_del">-			break;</span>
<span class="p_del">-</span>
<span class="p_del">-		if (!pte_allows_gup(pte_val(pte), write))</span>
<span class="p_del">-			break;</span>
<span class="p_del">-</span>
<span class="p_del">-		if (pte_devmap(pte)) {</span>
<span class="p_del">-			pgmap = get_dev_pagemap(pte_pfn(pte), pgmap);</span>
<span class="p_del">-			if (unlikely(!pgmap)) {</span>
<span class="p_del">-				undo_dev_pagemap(nr, nr_start, pages);</span>
<span class="p_del">-				break;</span>
<span class="p_del">-			}</span>
<span class="p_del">-		} else if (pte_special(pte))</span>
<span class="p_del">-			break;</span>
<span class="p_del">-</span>
<span class="p_del">-		VM_BUG_ON(!pfn_valid(pte_pfn(pte)));</span>
<span class="p_del">-		page = pte_page(pte);</span>
<span class="p_del">-		get_page(page);</span>
<span class="p_del">-		put_dev_pagemap(pgmap);</span>
<span class="p_del">-		SetPageReferenced(page);</span>
<span class="p_del">-		pages[*nr] = page;</span>
<span class="p_del">-		(*nr)++;</span>
<span class="p_del">-</span>
<span class="p_del">-	} while (ptep++, addr += PAGE_SIZE, addr != end);</span>
<span class="p_del">-	if (addr == end)</span>
<span class="p_del">-		ret = 1;</span>
<span class="p_del">-	pte_unmap(ptem);</span>
<span class="p_del">-</span>
<span class="p_del">-	return ret;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline void get_head_page_multiple(struct page *page, int nr)</span>
<span class="p_del">-{</span>
<span class="p_del">-	VM_BUG_ON_PAGE(page != compound_head(page), page);</span>
<span class="p_del">-	VM_BUG_ON_PAGE(page_count(page) == 0, page);</span>
<span class="p_del">-	page_ref_add(page, nr);</span>
<span class="p_del">-	SetPageReferenced(page);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static int __gup_device_huge(unsigned long pfn, unsigned long addr,</span>
<span class="p_del">-		unsigned long end, struct page **pages, int *nr)</span>
<span class="p_del">-{</span>
<span class="p_del">-	int nr_start = *nr;</span>
<span class="p_del">-	struct dev_pagemap *pgmap = NULL;</span>
<span class="p_del">-</span>
<span class="p_del">-	do {</span>
<span class="p_del">-		struct page *page = pfn_to_page(pfn);</span>
<span class="p_del">-</span>
<span class="p_del">-		pgmap = get_dev_pagemap(pfn, pgmap);</span>
<span class="p_del">-		if (unlikely(!pgmap)) {</span>
<span class="p_del">-			undo_dev_pagemap(nr, nr_start, pages);</span>
<span class="p_del">-			return 0;</span>
<span class="p_del">-		}</span>
<span class="p_del">-		SetPageReferenced(page);</span>
<span class="p_del">-		pages[*nr] = page;</span>
<span class="p_del">-		get_page(page);</span>
<span class="p_del">-		put_dev_pagemap(pgmap);</span>
<span class="p_del">-		(*nr)++;</span>
<span class="p_del">-		pfn++;</span>
<span class="p_del">-	} while (addr += PAGE_SIZE, addr != end);</span>
<span class="p_del">-	return 1;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static int __gup_device_huge_pmd(pmd_t pmd, unsigned long addr,</span>
<span class="p_del">-		unsigned long end, struct page **pages, int *nr)</span>
<span class="p_del">-{</span>
<span class="p_del">-	unsigned long fault_pfn;</span>
<span class="p_del">-</span>
<span class="p_del">-	fault_pfn = pmd_pfn(pmd) + ((addr &amp; ~PMD_MASK) &gt;&gt; PAGE_SHIFT);</span>
<span class="p_del">-	return __gup_device_huge(fault_pfn, addr, end, pages, nr);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static int __gup_device_huge_pud(pud_t pud, unsigned long addr,</span>
<span class="p_del">-		unsigned long end, struct page **pages, int *nr)</span>
<span class="p_del">-{</span>
<span class="p_del">-	unsigned long fault_pfn;</span>
<span class="p_del">-</span>
<span class="p_del">-	fault_pfn = pud_pfn(pud) + ((addr &amp; ~PUD_MASK) &gt;&gt; PAGE_SHIFT);</span>
<span class="p_del">-	return __gup_device_huge(fault_pfn, addr, end, pages, nr);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static noinline int gup_huge_pmd(pmd_t pmd, unsigned long addr,</span>
<span class="p_del">-		unsigned long end, int write, struct page **pages, int *nr)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct page *head, *page;</span>
<span class="p_del">-	int refs;</span>
<span class="p_del">-</span>
<span class="p_del">-	if (!pte_allows_gup(pmd_val(pmd), write))</span>
<span class="p_del">-		return 0;</span>
<span class="p_del">-</span>
<span class="p_del">-	VM_BUG_ON(!pfn_valid(pmd_pfn(pmd)));</span>
<span class="p_del">-	if (pmd_devmap(pmd))</span>
<span class="p_del">-		return __gup_device_huge_pmd(pmd, addr, end, pages, nr);</span>
<span class="p_del">-</span>
<span class="p_del">-	/* hugepages are never &quot;special&quot; */</span>
<span class="p_del">-	VM_BUG_ON(pmd_flags(pmd) &amp; _PAGE_SPECIAL);</span>
<span class="p_del">-</span>
<span class="p_del">-	refs = 0;</span>
<span class="p_del">-	head = pmd_page(pmd);</span>
<span class="p_del">-	page = head + ((addr &amp; ~PMD_MASK) &gt;&gt; PAGE_SHIFT);</span>
<span class="p_del">-	do {</span>
<span class="p_del">-		VM_BUG_ON_PAGE(compound_head(page) != head, page);</span>
<span class="p_del">-		pages[*nr] = page;</span>
<span class="p_del">-		(*nr)++;</span>
<span class="p_del">-		page++;</span>
<span class="p_del">-		refs++;</span>
<span class="p_del">-	} while (addr += PAGE_SIZE, addr != end);</span>
<span class="p_del">-	get_head_page_multiple(head, refs);</span>
<span class="p_del">-</span>
<span class="p_del">-	return 1;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static int gup_pmd_range(pud_t pud, unsigned long addr, unsigned long end,</span>
<span class="p_del">-		int write, struct page **pages, int *nr)</span>
<span class="p_del">-{</span>
<span class="p_del">-	unsigned long next;</span>
<span class="p_del">-	pmd_t *pmdp;</span>
<span class="p_del">-</span>
<span class="p_del">-	pmdp = pmd_offset(&amp;pud, addr);</span>
<span class="p_del">-	do {</span>
<span class="p_del">-		pmd_t pmd = *pmdp;</span>
<span class="p_del">-</span>
<span class="p_del">-		next = pmd_addr_end(addr, end);</span>
<span class="p_del">-		if (pmd_none(pmd))</span>
<span class="p_del">-			return 0;</span>
<span class="p_del">-		if (unlikely(pmd_large(pmd) || !pmd_present(pmd))) {</span>
<span class="p_del">-			/*</span>
<span class="p_del">-			 * NUMA hinting faults need to be handled in the GUP</span>
<span class="p_del">-			 * slowpath for accounting purposes and so that they</span>
<span class="p_del">-			 * can be serialised against THP migration.</span>
<span class="p_del">-			 */</span>
<span class="p_del">-			if (pmd_protnone(pmd))</span>
<span class="p_del">-				return 0;</span>
<span class="p_del">-			if (!gup_huge_pmd(pmd, addr, next, write, pages, nr))</span>
<span class="p_del">-				return 0;</span>
<span class="p_del">-		} else {</span>
<span class="p_del">-			if (!gup_pte_range(pmd, addr, next, write, pages, nr))</span>
<span class="p_del">-				return 0;</span>
<span class="p_del">-		}</span>
<span class="p_del">-	} while (pmdp++, addr = next, addr != end);</span>
<span class="p_del">-</span>
<span class="p_del">-	return 1;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static noinline int gup_huge_pud(pud_t pud, unsigned long addr,</span>
<span class="p_del">-		unsigned long end, int write, struct page **pages, int *nr)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct page *head, *page;</span>
<span class="p_del">-	int refs;</span>
<span class="p_del">-</span>
<span class="p_del">-	if (!pte_allows_gup(pud_val(pud), write))</span>
<span class="p_del">-		return 0;</span>
<span class="p_del">-</span>
<span class="p_del">-	VM_BUG_ON(!pfn_valid(pud_pfn(pud)));</span>
<span class="p_del">-	if (pud_devmap(pud))</span>
<span class="p_del">-		return __gup_device_huge_pud(pud, addr, end, pages, nr);</span>
<span class="p_del">-</span>
<span class="p_del">-	/* hugepages are never &quot;special&quot; */</span>
<span class="p_del">-	VM_BUG_ON(pud_flags(pud) &amp; _PAGE_SPECIAL);</span>
<span class="p_del">-</span>
<span class="p_del">-	refs = 0;</span>
<span class="p_del">-	head = pud_page(pud);</span>
<span class="p_del">-	page = head + ((addr &amp; ~PUD_MASK) &gt;&gt; PAGE_SHIFT);</span>
<span class="p_del">-	do {</span>
<span class="p_del">-		VM_BUG_ON_PAGE(compound_head(page) != head, page);</span>
<span class="p_del">-		pages[*nr] = page;</span>
<span class="p_del">-		(*nr)++;</span>
<span class="p_del">-		page++;</span>
<span class="p_del">-		refs++;</span>
<span class="p_del">-	} while (addr += PAGE_SIZE, addr != end);</span>
<span class="p_del">-	get_head_page_multiple(head, refs);</span>
<span class="p_del">-</span>
<span class="p_del">-	return 1;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static int gup_pud_range(p4d_t p4d, unsigned long addr, unsigned long end,</span>
<span class="p_del">-			int write, struct page **pages, int *nr)</span>
<span class="p_del">-{</span>
<span class="p_del">-	unsigned long next;</span>
<span class="p_del">-	pud_t *pudp;</span>
<span class="p_del">-</span>
<span class="p_del">-	pudp = pud_offset(&amp;p4d, addr);</span>
<span class="p_del">-	do {</span>
<span class="p_del">-		pud_t pud = *pudp;</span>
<span class="p_del">-</span>
<span class="p_del">-		next = pud_addr_end(addr, end);</span>
<span class="p_del">-		if (pud_none(pud))</span>
<span class="p_del">-			return 0;</span>
<span class="p_del">-		if (unlikely(pud_large(pud))) {</span>
<span class="p_del">-			if (!gup_huge_pud(pud, addr, next, write, pages, nr))</span>
<span class="p_del">-				return 0;</span>
<span class="p_del">-		} else {</span>
<span class="p_del">-			if (!gup_pmd_range(pud, addr, next, write, pages, nr))</span>
<span class="p_del">-				return 0;</span>
<span class="p_del">-		}</span>
<span class="p_del">-	} while (pudp++, addr = next, addr != end);</span>
<span class="p_del">-</span>
<span class="p_del">-	return 1;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static int gup_p4d_range(pgd_t pgd, unsigned long addr, unsigned long end,</span>
<span class="p_del">-			int write, struct page **pages, int *nr)</span>
<span class="p_del">-{</span>
<span class="p_del">-	unsigned long next;</span>
<span class="p_del">-	p4d_t *p4dp;</span>
<span class="p_del">-</span>
<span class="p_del">-	p4dp = p4d_offset(&amp;pgd, addr);</span>
<span class="p_del">-	do {</span>
<span class="p_del">-		p4d_t p4d = *p4dp;</span>
<span class="p_del">-</span>
<span class="p_del">-		next = p4d_addr_end(addr, end);</span>
<span class="p_del">-		if (p4d_none(p4d))</span>
<span class="p_del">-			return 0;</span>
<span class="p_del">-		BUILD_BUG_ON(p4d_large(p4d));</span>
<span class="p_del">-		if (!gup_pud_range(p4d, addr, next, write, pages, nr))</span>
<span class="p_del">-			return 0;</span>
<span class="p_del">-	} while (p4dp++, addr = next, addr != end);</span>
<span class="p_del">-</span>
<span class="p_del">-	return 1;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
<span class="p_del">- * Like get_user_pages_fast() except its IRQ-safe in that it won&#39;t fall</span>
<span class="p_del">- * back to the regular GUP.</span>
<span class="p_del">- */</span>
<span class="p_del">-int __get_user_pages_fast(unsigned long start, int nr_pages, int write,</span>
<span class="p_del">-			  struct page **pages)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct mm_struct *mm = current-&gt;mm;</span>
<span class="p_del">-	unsigned long addr, len, end;</span>
<span class="p_del">-	unsigned long next;</span>
<span class="p_del">-	unsigned long flags;</span>
<span class="p_del">-	pgd_t *pgdp;</span>
<span class="p_del">-	int nr = 0;</span>
<span class="p_del">-</span>
<span class="p_del">-	start &amp;= PAGE_MASK;</span>
<span class="p_del">-	addr = start;</span>
<span class="p_del">-	len = (unsigned long) nr_pages &lt;&lt; PAGE_SHIFT;</span>
<span class="p_del">-	end = start + len;</span>
<span class="p_del">-	if (unlikely(!access_ok(write ? VERIFY_WRITE : VERIFY_READ,</span>
<span class="p_del">-					(void __user *)start, len)))</span>
<span class="p_del">-		return 0;</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * XXX: batch / limit &#39;nr&#39;, to avoid large irq off latency</span>
<span class="p_del">-	 * needs some instrumenting to determine the common sizes used by</span>
<span class="p_del">-	 * important workloads (eg. DB2), and whether limiting the batch size</span>
<span class="p_del">-	 * will decrease performance.</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 * It seems like we&#39;re in the clear for the moment. Direct-IO is</span>
<span class="p_del">-	 * the main guy that batches up lots of get_user_pages, and even</span>
<span class="p_del">-	 * they are limited to 64-at-a-time which is not so many.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * This doesn&#39;t prevent pagetable teardown, but does prevent</span>
<span class="p_del">-	 * the pagetables and pages from being freed on x86.</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 * So long as we atomically load page table pointers versus teardown</span>
<span class="p_del">-	 * (which we do on x86, with the above PAE exception), we can follow the</span>
<span class="p_del">-	 * address down to the the page and take a ref on it.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	local_irq_save(flags);</span>
<span class="p_del">-	pgdp = pgd_offset(mm, addr);</span>
<span class="p_del">-	do {</span>
<span class="p_del">-		pgd_t pgd = *pgdp;</span>
<span class="p_del">-</span>
<span class="p_del">-		next = pgd_addr_end(addr, end);</span>
<span class="p_del">-		if (pgd_none(pgd))</span>
<span class="p_del">-			break;</span>
<span class="p_del">-		if (!gup_p4d_range(pgd, addr, next, write, pages, &amp;nr))</span>
<span class="p_del">-			break;</span>
<span class="p_del">-	} while (pgdp++, addr = next, addr != end);</span>
<span class="p_del">-	local_irq_restore(flags);</span>
<span class="p_del">-</span>
<span class="p_del">-	return nr;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-/**</span>
<span class="p_del">- * get_user_pages_fast() - pin user pages in memory</span>
<span class="p_del">- * @start:	starting user address</span>
<span class="p_del">- * @nr_pages:	number of pages from start to pin</span>
<span class="p_del">- * @write:	whether pages will be written to</span>
<span class="p_del">- * @pages:	array that receives pointers to the pages pinned.</span>
<span class="p_del">- * 		Should be at least nr_pages long.</span>
<span class="p_del">- *</span>
<span class="p_del">- * Attempt to pin user pages in memory without taking mm-&gt;mmap_sem.</span>
<span class="p_del">- * If not successful, it will fall back to taking the lock and</span>
<span class="p_del">- * calling get_user_pages().</span>
<span class="p_del">- *</span>
<span class="p_del">- * Returns number of pages pinned. This may be fewer than the number</span>
<span class="p_del">- * requested. If nr_pages is 0 or negative, returns 0. If no pages</span>
<span class="p_del">- * were pinned, returns -errno.</span>
<span class="p_del">- */</span>
<span class="p_del">-int get_user_pages_fast(unsigned long start, int nr_pages, int write,</span>
<span class="p_del">-			struct page **pages)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct mm_struct *mm = current-&gt;mm;</span>
<span class="p_del">-	unsigned long addr, len, end;</span>
<span class="p_del">-	unsigned long next;</span>
<span class="p_del">-	pgd_t *pgdp;</span>
<span class="p_del">-	int nr = 0;</span>
<span class="p_del">-</span>
<span class="p_del">-	start &amp;= PAGE_MASK;</span>
<span class="p_del">-	addr = start;</span>
<span class="p_del">-	len = (unsigned long) nr_pages &lt;&lt; PAGE_SHIFT;</span>
<span class="p_del">-</span>
<span class="p_del">-	end = start + len;</span>
<span class="p_del">-	if (end &lt; start)</span>
<span class="p_del">-		goto slow_irqon;</span>
<span class="p_del">-</span>
<span class="p_del">-#ifdef CONFIG_X86_64</span>
<span class="p_del">-	if (end &gt;&gt; __VIRTUAL_MASK_SHIFT)</span>
<span class="p_del">-		goto slow_irqon;</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * XXX: batch / limit &#39;nr&#39;, to avoid large irq off latency</span>
<span class="p_del">-	 * needs some instrumenting to determine the common sizes used by</span>
<span class="p_del">-	 * important workloads (eg. DB2), and whether limiting the batch size</span>
<span class="p_del">-	 * will decrease performance.</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 * It seems like we&#39;re in the clear for the moment. Direct-IO is</span>
<span class="p_del">-	 * the main guy that batches up lots of get_user_pages, and even</span>
<span class="p_del">-	 * they are limited to 64-at-a-time which is not so many.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * This doesn&#39;t prevent pagetable teardown, but does prevent</span>
<span class="p_del">-	 * the pagetables and pages from being freed on x86.</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 * So long as we atomically load page table pointers versus teardown</span>
<span class="p_del">-	 * (which we do on x86, with the above PAE exception), we can follow the</span>
<span class="p_del">-	 * address down to the the page and take a ref on it.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	local_irq_disable();</span>
<span class="p_del">-	pgdp = pgd_offset(mm, addr);</span>
<span class="p_del">-	do {</span>
<span class="p_del">-		pgd_t pgd = *pgdp;</span>
<span class="p_del">-</span>
<span class="p_del">-		next = pgd_addr_end(addr, end);</span>
<span class="p_del">-		if (pgd_none(pgd))</span>
<span class="p_del">-			goto slow;</span>
<span class="p_del">-		if (!gup_p4d_range(pgd, addr, next, write, pages, &amp;nr))</span>
<span class="p_del">-			goto slow;</span>
<span class="p_del">-	} while (pgdp++, addr = next, addr != end);</span>
<span class="p_del">-	local_irq_enable();</span>
<span class="p_del">-</span>
<span class="p_del">-	VM_BUG_ON(nr != (end - start) &gt;&gt; PAGE_SHIFT);</span>
<span class="p_del">-	return nr;</span>
<span class="p_del">-</span>
<span class="p_del">-	{</span>
<span class="p_del">-		int ret;</span>
<span class="p_del">-</span>
<span class="p_del">-slow:</span>
<span class="p_del">-		local_irq_enable();</span>
<span class="p_del">-slow_irqon:</span>
<span class="p_del">-		/* Try to get the remaining pages with get_user_pages */</span>
<span class="p_del">-		start += nr &lt;&lt; PAGE_SHIFT;</span>
<span class="p_del">-		pages += nr;</span>
<span class="p_del">-</span>
<span class="p_del">-		ret = get_user_pages_unlocked(start,</span>
<span class="p_del">-					      (end - start) &gt;&gt; PAGE_SHIFT,</span>
<span class="p_del">-					      pages, write ? FOLL_WRITE : 0);</span>
<span class="p_del">-</span>
<span class="p_del">-		/* Have to be a bit careful with return values */</span>
<span class="p_del">-		if (nr &gt; 0) {</span>
<span class="p_del">-			if (ret &lt; 0)</span>
<span class="p_del">-				ret = nr;</span>
<span class="p_del">-			else</span>
<span class="p_del">-				ret += nr;</span>
<span class="p_del">-		}</span>
<span class="p_del">-</span>
<span class="p_del">-		return ret;</span>
<span class="p_del">-	}</span>
<span class="p_del">-}</span>
<span class="p_header">diff --git a/mm/Kconfig b/mm/Kconfig</span>
<span class="p_header">index beb7a455915d..398b46064544 100644</span>
<span class="p_header">--- a/mm/Kconfig</span>
<span class="p_header">+++ b/mm/Kconfig</span>
<span class="p_chunk">@@ -137,7 +137,7 @@</span> <span class="p_context"> config HAVE_MEMBLOCK_NODE_MAP</span>
 config HAVE_MEMBLOCK_PHYS_MAP
 	bool
 
<span class="p_del">-config HAVE_GENERIC_RCU_GUP</span>
<span class="p_add">+config HAVE_GENERIC_GUP</span>
 	bool
 
 config ARCH_DISCARD_MEMBLOCK
<span class="p_header">diff --git a/mm/gup.c b/mm/gup.c</span>
<span class="p_header">index b3c7214d710d..2050f9fe121d 100644</span>
<span class="p_header">--- a/mm/gup.c</span>
<span class="p_header">+++ b/mm/gup.c</span>
<span class="p_chunk">@@ -1151,7 +1151,7 @@</span> <span class="p_context"> struct page *get_dump_page(unsigned long addr)</span>
 #endif /* CONFIG_ELF_CORE */
 
 /*
<span class="p_del">- * Generic RCU Fast GUP</span>
<span class="p_add">+ * Generic Fast GUP</span>
  *
  * get_user_pages_fast attempts to pin user pages by walking the page
  * tables directly and avoids taking locks. Thus the walker needs to be
<span class="p_chunk">@@ -1172,8 +1172,8 @@</span> <span class="p_context"> struct page *get_dump_page(unsigned long addr)</span>
  * Before activating this code, please be aware that the following assumptions
  * are currently made:
  *
<span class="p_del">- *  *) HAVE_RCU_TABLE_FREE is enabled, and tlb_remove_table is used to free</span>
<span class="p_del">- *      pages containing page tables.</span>
<span class="p_add">+ *  *) Either HAVE_RCU_TABLE_FREE is enabled, and tlb_remove_table() is used to</span>
<span class="p_add">+ *  free pages containing page tables or TLB flushing requires IPI broadcast.</span>
  *
  *  *) ptes can be read atomically by the architecture.
  *
<span class="p_chunk">@@ -1183,7 +1183,7 @@</span> <span class="p_context"> struct page *get_dump_page(unsigned long addr)</span>
  *
  * This code is based heavily on the PowerPC implementation by Nick Piggin.
  */
<span class="p_del">-#ifdef CONFIG_HAVE_GENERIC_RCU_GUP</span>
<span class="p_add">+#ifdef CONFIG_HAVE_GENERIC_GUP</span>
 
 #ifndef gup_get_pte
 /*
<span class="p_chunk">@@ -1673,4 +1673,4 @@</span> <span class="p_context"> int get_user_pages_fast(unsigned long start, int nr_pages, int write,</span>
 	return ret;
 }
 
<span class="p_del">-#endif /* CONFIG_HAVE_GENERIC_RCU_GUP */</span>
<span class="p_add">+#endif /* CONFIG_HAVE_GENERIC_GUP */</span>

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



