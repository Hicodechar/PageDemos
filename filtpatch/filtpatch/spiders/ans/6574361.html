
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[2/4] mm: Send one IPI per CPU to TLB flush all entries after unmapping pages - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [2/4] mm: Send one IPI per CPU to TLB flush all entries after unmapping pages</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=24451">Mel Gorman</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>June 9, 2015, 5:31 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1433871118-15207-3-git-send-email-mgorman@suse.de&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/6574361/mbox/"
   >mbox</a>
|
   <a href="/patch/6574361/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/6574361/">/patch/6574361/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork2.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork2.web.kernel.org (Postfix) with ESMTP id 92037C0020
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  9 Jun 2015 17:33:18 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 00CBB2034B
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  9 Jun 2015 17:33:17 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id 649B0204AD
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  9 Jun 2015 17:33:15 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S933236AbbFIRdH (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 9 Jun 2015 13:33:07 -0400
Received: from cantor2.suse.de ([195.135.220.15]:50267 &quot;EHLO mx2.suse.de&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1753887AbbFIRcJ (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 9 Jun 2015 13:32:09 -0400
X-Virus-Scanned: by amavisd-new at test-mx.suse.de
Received: from relay1.suse.de (charybdis-ext.suse.de [195.135.220.254])
	by mx2.suse.de (Postfix) with ESMTP id 92D59ADD0;
	Tue,  9 Jun 2015 17:32:07 +0000 (UTC)
From: Mel Gorman &lt;mgorman@suse.de&gt;
To: Andrew Morton &lt;akpm@linux-foundation.org&gt;
Cc: Rik van Riel &lt;riel@redhat.com&gt;, Hugh Dickins &lt;hughd@google.com&gt;,
	Minchan Kim &lt;minchan@kernel.org&gt;, Dave Hansen &lt;dave.hansen@intel.com&gt;,
	Andi Kleen &lt;andi@firstfloor.org&gt;, H Peter Anvin &lt;hpa@zytor.com&gt;,
	Ingo Molnar &lt;mingo@kernel.org&gt;,
	Linus Torvalds &lt;torvalds@linux-foundation.org&gt;,
	Thomas Gleixner &lt;tglx@linutronix.de&gt;,
	Peter Zijlstra &lt;a.p.zijlstra@chello.nl&gt;, Linux-MM &lt;linux-mm@kvack.org&gt;,
	LKML &lt;linux-kernel@vger.kernel.org&gt;, Mel Gorman &lt;mgorman@suse.de&gt;
Subject: [PATCH 2/4] mm: Send one IPI per CPU to TLB flush all entries after
	unmapping pages
Date: Tue,  9 Jun 2015 18:31:56 +0100
Message-Id: &lt;1433871118-15207-3-git-send-email-mgorman@suse.de&gt;
X-Mailer: git-send-email 2.3.5
In-Reply-To: &lt;1433871118-15207-1-git-send-email-mgorman@suse.de&gt;
References: &lt;1433871118-15207-1-git-send-email-mgorman@suse.de&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-6.9 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	T_RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=24451">Mel Gorman</a> - June 9, 2015, 5:31 p.m.</div>
<pre class="content">
An IPI is sent to flush remote TLBs when a page is unmapped that was
potentially accesssed by other CPUs. There are many circumstances where
this happens but the obvious one is kswapd reclaiming pages belonging to
a running process as kswapd and the task are likely running on separate CPUs.

On small machines, this is not a significant problem but as machine gets
larger with more cores and more memory, the cost of these IPIs can be
high. This patch uses a simple structure that tracks CPUs that potentially
have TLB entries for pages being unmapped. When the unmapping is complete,
the full TLB is flushed on the assumption that a refill cost is lower than
flushing individual entries.

Architectures wishing to do this must give the following guarantee.

	If a clean page is unmapped and not immediately flushed, the
	architecture must guarantee that a write to that linear address
	from a CPU with a cached TLB entry will trap a page fault.

This is essentially what the kernel already depends on but the window is much
larger with this patch applied and is worth highlighting. The architecture
should consider whether the cost of the full TLB flush is higher than
sending an IPI to flush each individual entry. An additional architecture
helper may be required to flush the local TLB but it is expected this will
be a trivial alias of an internal function in most cases.  In this case,
the existing x86 helper was used.

The impact of this patch depends on the workload as measuring any benefit
requires both mapped pages co-located on the LRU and memory pressure. The
case with the biggest impact is multiple processes reading mapped pages
taken from the vm-scalability test suite. The test case uses NR_CPU readers
of mapped files that consume 10*RAM.

Linear mapped reader on a 4-node machine with 64G RAM and 48 CPUs

                                        4.1.0-rc6          4.1.0-rc6
                                          vanilla       flushfull-v6
Ops lru-file-mmap-read-elapsed   162.88 (  0.00%)   120.81 ( 25.83%)

           4.1.0-rc6   4.1.0-rc6
             vanillaflushfull-v6r5
User          568.96      614.68
System       6085.61     4226.61
Elapsed       164.24      122.17

This is showing that the readers completed 25.83% faster with 30% less
system CPU time. From vmstats, it is known that the vanilla kernel was
interrupted roughly 900K times per second during the steady phase of the
test and the patched kernel was interrupts 180K times per second.

The impact is lower on a single socket machine.

                                        4.1.0-rc6          4.1.0-rc6
                                          vanilla       flushfull-v6
Ops lru-file-mmap-read-elapsed    25.43 (  0.00%)    20.59 ( 19.03%)

           4.1.0-rc6    4.1.0-rc6
             vanilla flushfull-v6
User           59.14        58.99
System        109.15        77.84
Elapsed        27.32        22.31

It&#39;s still a noticeable improvement with vmstat showing interrupts went
from roughly 500K per second to 45K per second.

The patch will have no impact on workloads with no memory pressure or
have relatively few mapped pages. It will have an unpredictable impact
on the workload running on the CPU being flushed as it&#39;ll depend on how
many TLB entries need to be refilled and how long that takes. Worst case,
the TLB will be completely cleared of active entries when the target PFNs
were not resident at all.
<span class="signed-off-by">
Signed-off-by: Mel Gorman &lt;mgorman@suse.de&gt;</span>
---
 arch/x86/Kconfig      |   1 +
 include/linux/rmap.h  |   3 ++
 include/linux/sched.h |  16 ++++++++
 init/Kconfig          |  10 +++++
 kernel/fork.c         |   5 +++
 kernel/sched/core.c   |   3 ++
 mm/internal.h         |  11 ++++++
 mm/rmap.c             | 103 +++++++++++++++++++++++++++++++++++++++++++++++++-
 mm/vmscan.c           |  26 ++++++++++++-
 9 files changed, 176 insertions(+), 2 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=181">Rik van Riel</a> - June 9, 2015, 8:01 p.m.</div>
<pre class="content">
On 06/09/2015 01:31 PM, Mel Gorman wrote:
<span class="quote">&gt; An IPI is sent to flush remote TLBs when a page is unmapped that was</span>
<span class="quote">&gt; potentially accesssed by other CPUs. There are many circumstances where</span>
<span class="quote">&gt; this happens but the obvious one is kswapd reclaiming pages belonging to</span>
<span class="quote">&gt; a running process as kswapd and the task are likely running on separate CPUs.</span>
<span class="quote">
&gt; It&#39;s still a noticeable improvement with vmstat showing interrupts went</span>
<span class="quote">&gt; from roughly 500K per second to 45K per second.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The patch will have no impact on workloads with no memory pressure or</span>
<span class="quote">&gt; have relatively few mapped pages. It will have an unpredictable impact</span>
<span class="quote">&gt; on the workload running on the CPU being flushed as it&#39;ll depend on how</span>
<span class="quote">&gt; many TLB entries need to be refilled and how long that takes. Worst case,</span>
<span class="quote">&gt; the TLB will be completely cleared of active entries when the target PFNs</span>
<span class="quote">&gt; were not resident at all.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Mel Gorman &lt;mgorman@suse.de&gt;</span>
<span class="reviewed-by">
Reviewed-by: Rik van Riel &lt;riel@redhat.com&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=35552">Ingo Molnar</a> - June 10, 2015, 7:47 a.m.</div>
<pre class="content">
* Mel Gorman &lt;mgorman@suse.de&gt; wrote:
<span class="quote">
&gt; --- a/include/linux/sched.h</span>
<span class="quote">&gt; +++ b/include/linux/sched.h</span>
<span class="quote">&gt; @@ -1289,6 +1289,18 @@ enum perf_event_task_context {</span>
<span class="quote">&gt;  	perf_nr_task_contexts,</span>
<span class="quote">&gt;  };</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +/* Track pages that require TLB flushes */</span>
<span class="quote">&gt; +struct tlbflush_unmap_batch {</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * Each bit set is a CPU that potentially has a TLB entry for one of</span>
<span class="quote">&gt; +	 * the PFNs being flushed. See set_tlb_ubc_flush_pending().</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	struct cpumask cpumask;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/* True if any bit in cpumask is set */</span>
<span class="quote">&gt; +	bool flush_required;</span>
<span class="quote">&gt; +};</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  struct task_struct {</span>
<span class="quote">&gt;  	volatile long state;	/* -1 unrunnable, 0 runnable, &gt;0 stopped */</span>
<span class="quote">&gt;  	void *stack;</span>
<span class="quote">&gt; @@ -1648,6 +1660,10 @@ struct task_struct {</span>
<span class="quote">&gt;  	unsigned long numa_pages_migrated;</span>
<span class="quote">&gt;  #endif /* CONFIG_NUMA_BALANCING */</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +#ifdef CONFIG_ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH</span>
<span class="quote">&gt; +	struct tlbflush_unmap_batch *tlb_ubc;</span>
<span class="quote">&gt; +#endif</span>

Please embedd this constant size structure in task_struct directly so that the 
whole per task allocation overhead goes away:
<span class="quote">
&gt; +#ifdef CONFIG_ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * Allocate the control structure for batch TLB flushing. An allocation</span>
<span class="quote">&gt; + * failure is harmless as the reclaimer will send IPIs where necessary.</span>
<span class="quote">&gt; + * A GFP_KERNEL allocation from this context is normally not advised but</span>
<span class="quote">&gt; + * we are depending on PF_MEMALLOC (set by direct reclaim or kswapd) to</span>
<span class="quote">&gt; + * limit the depth of the call.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +static void alloc_tlb_ubc(void)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	if (!current-&gt;tlb_ubc)</span>
<span class="quote">&gt; +		current-&gt;tlb_ubc = kzalloc(sizeof(struct tlbflush_unmap_batch),</span>
<span class="quote">&gt; +						GFP_KERNEL | __GFP_NOWARN);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +#else</span>
<span class="quote">&gt; +static inline void alloc_tlb_ubc(void)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +#endif /* CONFIG_ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt;   * This is a basic per-zone page freer.  Used by both kswapd and direct reclaim.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt; @@ -2152,6 +2174,8 @@ static void shrink_lruvec(struct lruvec *lruvec, int swappiness,</span>
<span class="quote">&gt;  	scan_adjusted = (global_reclaim(sc) &amp;&amp; !current_is_kswapd() &amp;&amp;</span>
<span class="quote">&gt;  			 sc-&gt;priority == DEF_PRIORITY);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +	alloc_tlb_ubc();</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	blk_start_plug(&amp;plug);</span>
<span class="quote">&gt;  	while (nr[LRU_INACTIVE_ANON] || nr[LRU_ACTIVE_FILE] ||</span>
<span class="quote">&gt;  					nr[LRU_INACTIVE_FILE]) {</span>

the whole patch series will become even simpler.

Thanks,

	Ingo
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=24451">Mel Gorman</a> - June 10, 2015, 8:14 a.m.</div>
<pre class="content">
On Wed, Jun 10, 2015 at 09:47:04AM +0200, Ingo Molnar wrote:
<span class="quote">&gt; </span>
<span class="quote">&gt; * Mel Gorman &lt;mgorman@suse.de&gt; wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; --- a/include/linux/sched.h</span>
<span class="quote">&gt; &gt; +++ b/include/linux/sched.h</span>
<span class="quote">&gt; &gt; @@ -1289,6 +1289,18 @@ enum perf_event_task_context {</span>
<span class="quote">&gt; &gt;  	perf_nr_task_contexts,</span>
<span class="quote">&gt; &gt;  };</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; +/* Track pages that require TLB flushes */</span>
<span class="quote">&gt; &gt; +struct tlbflush_unmap_batch {</span>
<span class="quote">&gt; &gt; +	/*</span>
<span class="quote">&gt; &gt; +	 * Each bit set is a CPU that potentially has a TLB entry for one of</span>
<span class="quote">&gt; &gt; +	 * the PFNs being flushed. See set_tlb_ubc_flush_pending().</span>
<span class="quote">&gt; &gt; +	 */</span>
<span class="quote">&gt; &gt; +	struct cpumask cpumask;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	/* True if any bit in cpumask is set */</span>
<span class="quote">&gt; &gt; +	bool flush_required;</span>
<span class="quote">&gt; &gt; +};</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt;  struct task_struct {</span>
<span class="quote">&gt; &gt;  	volatile long state;	/* -1 unrunnable, 0 runnable, &gt;0 stopped */</span>
<span class="quote">&gt; &gt;  	void *stack;</span>
<span class="quote">&gt; &gt; @@ -1648,6 +1660,10 @@ struct task_struct {</span>
<span class="quote">&gt; &gt;  	unsigned long numa_pages_migrated;</span>
<span class="quote">&gt; &gt;  #endif /* CONFIG_NUMA_BALANCING */</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; +#ifdef CONFIG_ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH</span>
<span class="quote">&gt; &gt; +	struct tlbflush_unmap_batch *tlb_ubc;</span>
<span class="quote">&gt; &gt; +#endif</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Please embedd this constant size structure in task_struct directly so that the </span>
<span class="quote">&gt; whole per task allocation overhead goes away:</span>
<span class="quote">&gt; </span>

That puts a structure (72 bytes in the config I used) within the task struct
even when it&#39;s not required. On a lightly loaded system direct reclaim
will not be active and for some processes, it&#39;ll never be active. It&#39;s
very wasteful.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=35552">Ingo Molnar</a> - June 10, 2015, 8:21 a.m.</div>
<pre class="content">
* Mel Gorman &lt;mgorman@suse.de&gt; wrote:
<span class="quote">
&gt; On Wed, Jun 10, 2015 at 09:47:04AM +0200, Ingo Molnar wrote:</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; * Mel Gorman &lt;mgorman@suse.de&gt; wrote:</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; --- a/include/linux/sched.h</span>
<span class="quote">&gt; &gt; &gt; +++ b/include/linux/sched.h</span>
<span class="quote">&gt; &gt; &gt; @@ -1289,6 +1289,18 @@ enum perf_event_task_context {</span>
<span class="quote">&gt; &gt; &gt;  	perf_nr_task_contexts,</span>
<span class="quote">&gt; &gt; &gt;  };</span>
<span class="quote">&gt; &gt; &gt;  </span>
<span class="quote">&gt; &gt; &gt; +/* Track pages that require TLB flushes */</span>
<span class="quote">&gt; &gt; &gt; +struct tlbflush_unmap_batch {</span>
<span class="quote">&gt; &gt; &gt; +	/*</span>
<span class="quote">&gt; &gt; &gt; +	 * Each bit set is a CPU that potentially has a TLB entry for one of</span>
<span class="quote">&gt; &gt; &gt; +	 * the PFNs being flushed. See set_tlb_ubc_flush_pending().</span>
<span class="quote">&gt; &gt; &gt; +	 */</span>
<span class="quote">&gt; &gt; &gt; +	struct cpumask cpumask;</span>
<span class="quote">&gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; &gt; +	/* True if any bit in cpumask is set */</span>
<span class="quote">&gt; &gt; &gt; +	bool flush_required;</span>
<span class="quote">&gt; &gt; &gt; +};</span>
<span class="quote">&gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; &gt;  struct task_struct {</span>
<span class="quote">&gt; &gt; &gt;  	volatile long state;	/* -1 unrunnable, 0 runnable, &gt;0 stopped */</span>
<span class="quote">&gt; &gt; &gt;  	void *stack;</span>
<span class="quote">&gt; &gt; &gt; @@ -1648,6 +1660,10 @@ struct task_struct {</span>
<span class="quote">&gt; &gt; &gt;  	unsigned long numa_pages_migrated;</span>
<span class="quote">&gt; &gt; &gt;  #endif /* CONFIG_NUMA_BALANCING */</span>
<span class="quote">&gt; &gt; &gt;  </span>
<span class="quote">&gt; &gt; &gt; +#ifdef CONFIG_ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH</span>
<span class="quote">&gt; &gt; &gt; +	struct tlbflush_unmap_batch *tlb_ubc;</span>
<span class="quote">&gt; &gt; &gt; +#endif</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Please embedd this constant size structure in task_struct directly so that the </span>
<span class="quote">&gt; &gt; whole per task allocation overhead goes away:</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; That puts a structure (72 bytes in the config I used) within the task struct </span>
<span class="quote">&gt; even when it&#39;s not required. On a lightly loaded system direct reclaim will not </span>
<span class="quote">&gt; be active and for some processes, it&#39;ll never be active. It&#39;s very wasteful.</span>

For certain values of &#39;very&#39;.

 - 72 bytes suggests that you have NR_CPUS set to 512 or so? On a kernel sized to 
   such large systems with 1000 active tasks we are talking about about +72K of 
   RAM...

 - Furthermore, by embedding it it gets packed better with neighboring task_struct 
   fields, while by allocating it dynamically it&#39;s a separate cache line wasted.

 - Plus by allocating it separately you spend two cachelines on it: each slab will 
   be at least cacheline aligned, and 72 bytes will allocate 128 bytes. So when 
   this gets triggered you&#39;ve just wasted some more RAM.

 - I mean, if it had dynamic size, or was arguably huge. But this is just a 
   cpumask and a boolean!

 - The cpumask will be dynamic if you increase the NR_CPUS count any more than 
   that - in which case embedding the structure is the right choice again.

Thanks,

	Ingo
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=35552">Ingo Molnar</a> - June 10, 2015, 8:26 a.m.</div>
<pre class="content">
* Mel Gorman &lt;mgorman@suse.de&gt; wrote:
<span class="quote">
&gt; On a 4-socket machine the results were</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;                                         4.1.0-rc6          4.1.0-rc6</span>
<span class="quote">&gt;                                     batchdirty-v6      batchunmap-v6</span>
<span class="quote">&gt; Ops lru-file-mmap-read-elapsed   121.27 (  0.00%)   118.79 (  2.05%)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;            4.1.0-rc6      4.1.0-rc6</span>
<span class="quote">&gt;         batchdirty-v6 batchunmap-v6</span>
<span class="quote">&gt; User          620.84         608.48</span>
<span class="quote">&gt; System       4245.35        4152.89</span>
<span class="quote">&gt; Elapsed       122.65         120.15</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; In this case the workload completed faster and there was less CPU overhead</span>
<span class="quote">&gt; but as it&#39;s a NUMA machine there are a lot of factors at play. It&#39;s easier</span>
<span class="quote">&gt; to quantify on a single socket machine;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;                                         4.1.0-rc6          4.1.0-rc6</span>
<span class="quote">&gt;                                     batchdirty-v6      batchunmap-v6</span>
<span class="quote">&gt; Ops lru-file-mmap-read-elapsed    20.35 (  0.00%)    21.52 ( -5.75%)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;            4.1.0-rc6   4.1.0-rc6</span>
<span class="quote">&gt;         batchdirty-v6r5batchunmap-v6r5</span>
<span class="quote">&gt; User           58.02       60.70</span>
<span class="quote">&gt; System         77.57       81.92</span>
<span class="quote">&gt; Elapsed        22.14       23.16</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; That shows the workload takes 5.75% longer to complete with a similar</span>
<span class="quote">&gt; increase in the system CPU usage.</span>

Btw., do you have any stddev noise numbers?

The batching speedup is brutal enough to not need any noise estimations, it&#39;s a 
clear winner.

But this PFN tracking patch is more difficult to judge as the numbers are pretty 
close to each other.
<span class="quote">
&gt; It is expected that there is overhead to tracking the PFNs and flushing </span>
<span class="quote">&gt; individual pages. This can be quantified but we cannot quantify the indirect </span>
<span class="quote">&gt; savings due to active unrelated TLB entries being preserved. Whether this </span>
<span class="quote">&gt; matters depends on whether the workload was using those entries and if they </span>
<span class="quote">&gt; would be used before a context switch but targeting the TLB flushes is the </span>
<span class="quote">&gt; conservative and safer choice.</span>

So this is how I picture a realistic TLB flushing &#39;worst case&#39;: a workload that 
uses about 80% of the TLB cache in a &#39;fast&#39; function and trashes memory in a 
&#39;slow&#39; function, and does alternate calls to the two functions from the same task.

Typical dTLB sizes on x86 are a couple of hundred entries (you can see the precise 
count in x86info -c), up to 1024 entries on the latest uarchs.

A cached TLB miss will take about 10-20 cycles (progressively more if the lookup 
chain misses in the cache) - but that cost is partially hidden if the L1 data 
cache was missed (which is likely for most TLB-flush intense workloads), and will 
be almost completely hidden if it goes out to the L3 cache or goes to RAM. (It 
takes up cache/memory bandwidth though, but unless the access patters are totally 
sparse, it should be a small fraction.)

A single INVLPG with its 200+ cycles cost is equivalent to about 10-20 TLB misses. 
That&#39;s a lot.

So this kind of workload should trigger the TLB flushing &#39;worst case&#39;: with say 
512 dTLB entries you could see up to 5k-10k cycles of hidden/indirect cost, but 
potentially parallelized with other misses going on with the same data accesses.

The current limit for INVLPG flushing is 33 entries: that&#39;s 10k-20k cycles max 
with an INVLPG cost of 250 cycles - this could explain the results you got.

But the problem is: AFAICS you can only decrease the INVLPG count by decreasing 
the batching size - the additional IPI costs will overwhelm any TLB preservation 
benefits. So depending on the cost relationship between INVLPG, TLB miss cost and 
IPI cost, it might not be possible to see a speedup even in the worst-case.

Thanks,

	Ingo
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=35552">Ingo Molnar</a> - June 10, 2015, 8:33 a.m.</div>
<pre class="content">
* Mel Gorman &lt;mgorman@suse.de&gt; wrote:
<span class="quote">
&gt; Linear mapped reader on a 4-node machine with 64G RAM and 48 CPUs</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;                                         4.1.0-rc6          4.1.0-rc6</span>
<span class="quote">&gt;                                           vanilla       flushfull-v6</span>
<span class="quote">&gt; Ops lru-file-mmap-read-elapsed   162.88 (  0.00%)   120.81 ( 25.83%)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;            4.1.0-rc6   4.1.0-rc6</span>
<span class="quote">&gt;              vanillaflushfull-v6r5</span>
<span class="quote">&gt; User          568.96      614.68</span>
<span class="quote">&gt; System       6085.61     4226.61</span>
<span class="quote">&gt; Elapsed       164.24      122.17</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This is showing that the readers completed 25.83% faster with 30% less</span>
<span class="quote">&gt; system CPU time. From vmstats, it is known that the vanilla kernel was</span>
<span class="quote">&gt; interrupted roughly 900K times per second during the steady phase of the</span>
<span class="quote">&gt; test and the patched kernel was interrupts 180K times per second.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The impact is lower on a single socket machine.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;                                         4.1.0-rc6          4.1.0-rc6</span>
<span class="quote">&gt;                                           vanilla       flushfull-v6</span>
<span class="quote">&gt; Ops lru-file-mmap-read-elapsed    25.43 (  0.00%)    20.59 ( 19.03%)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;            4.1.0-rc6    4.1.0-rc6</span>
<span class="quote">&gt;              vanilla flushfull-v6</span>
<span class="quote">&gt; User           59.14        58.99</span>
<span class="quote">&gt; System        109.15        77.84</span>
<span class="quote">&gt; Elapsed        27.32        22.31</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It&#39;s still a noticeable improvement with vmstat showing interrupts went</span>
<span class="quote">&gt; from roughly 500K per second to 45K per second.</span>

Btw., I tried to compare your previous (v5) pfn-tracking numbers with these 
full-flushing numbers, and found that the IRQ rate appears to be the same:
<span class="quote">
&gt; &gt; From vmstats, it is known that the vanilla kernel was interrupted roughly 900K </span>
<span class="quote">&gt; &gt; times per second during the steady phase of the test and the patched kernel </span>
<span class="quote">&gt; &gt; was interrupts 180K times per second.</span>
<span class="quote">
&gt; &gt; It&#39;s still a noticeable improvement with vmstat showing interrupts went from </span>
<span class="quote">&gt; &gt; roughly 500K per second to 45K per second.</span>

... is that because the batching limit in the pfn-tracking case was high enough to 
not be noticeable in the vmstat?

In the full-flushing case (v6 without patch 4) the batching limit is &#39;infinite&#39;, 
we&#39;ll batch as long as possible, right?

Or have I managed to get confused somewhere ...

Thanks,

	Ingo
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=24451">Mel Gorman</a> - June 10, 2015, 8:51 a.m.</div>
<pre class="content">
On Wed, Jun 10, 2015 at 10:21:07AM +0200, Ingo Molnar wrote:
<span class="quote">&gt; </span>
<span class="quote">&gt; * Mel Gorman &lt;mgorman@suse.de&gt; wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; On Wed, Jun 10, 2015 at 09:47:04AM +0200, Ingo Molnar wrote:</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; * Mel Gorman &lt;mgorman@suse.de&gt; wrote:</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; --- a/include/linux/sched.h</span>
<span class="quote">&gt; &gt; &gt; &gt; +++ b/include/linux/sched.h</span>
<span class="quote">&gt; &gt; &gt; &gt; @@ -1289,6 +1289,18 @@ enum perf_event_task_context {</span>
<span class="quote">&gt; &gt; &gt; &gt;  	perf_nr_task_contexts,</span>
<span class="quote">&gt; &gt; &gt; &gt;  };</span>
<span class="quote">&gt; &gt; &gt; &gt;  </span>
<span class="quote">&gt; &gt; &gt; &gt; +/* Track pages that require TLB flushes */</span>
<span class="quote">&gt; &gt; &gt; &gt; +struct tlbflush_unmap_batch {</span>
<span class="quote">&gt; &gt; &gt; &gt; +	/*</span>
<span class="quote">&gt; &gt; &gt; &gt; +	 * Each bit set is a CPU that potentially has a TLB entry for one of</span>
<span class="quote">&gt; &gt; &gt; &gt; +	 * the PFNs being flushed. See set_tlb_ubc_flush_pending().</span>
<span class="quote">&gt; &gt; &gt; &gt; +	 */</span>
<span class="quote">&gt; &gt; &gt; &gt; +	struct cpumask cpumask;</span>
<span class="quote">&gt; &gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; &gt; &gt; +	/* True if any bit in cpumask is set */</span>
<span class="quote">&gt; &gt; &gt; &gt; +	bool flush_required;</span>
<span class="quote">&gt; &gt; &gt; &gt; +};</span>
<span class="quote">&gt; &gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; &gt; &gt;  struct task_struct {</span>
<span class="quote">&gt; &gt; &gt; &gt;  	volatile long state;	/* -1 unrunnable, 0 runnable, &gt;0 stopped */</span>
<span class="quote">&gt; &gt; &gt; &gt;  	void *stack;</span>
<span class="quote">&gt; &gt; &gt; &gt; @@ -1648,6 +1660,10 @@ struct task_struct {</span>
<span class="quote">&gt; &gt; &gt; &gt;  	unsigned long numa_pages_migrated;</span>
<span class="quote">&gt; &gt; &gt; &gt;  #endif /* CONFIG_NUMA_BALANCING */</span>
<span class="quote">&gt; &gt; &gt; &gt;  </span>
<span class="quote">&gt; &gt; &gt; &gt; +#ifdef CONFIG_ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH</span>
<span class="quote">&gt; &gt; &gt; &gt; +	struct tlbflush_unmap_batch *tlb_ubc;</span>
<span class="quote">&gt; &gt; &gt; &gt; +#endif</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Please embedd this constant size structure in task_struct directly so that the </span>
<span class="quote">&gt; &gt; &gt; whole per task allocation overhead goes away:</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; That puts a structure (72 bytes in the config I used) within the task struct </span>
<span class="quote">&gt; &gt; even when it&#39;s not required. On a lightly loaded system direct reclaim will not </span>
<span class="quote">&gt; &gt; be active and for some processes, it&#39;ll never be active. It&#39;s very wasteful.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; For certain values of &#39;very&#39;.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  - 72 bytes suggests that you have NR_CPUS set to 512 or so? On a kernel sized to </span>
<span class="quote">&gt;    such large systems with 1000 active tasks we are talking about about +72K of </span>
<span class="quote">&gt;    RAM...</span>
<span class="quote">&gt; </span>

The NR_CPUS is based on the openSUSE 13.1 distro config so yes, it&#39;s large but I also
expect it to be a common configuration.
<span class="quote">
&gt;  - Furthermore, by embedding it it gets packed better with neighboring task_struct </span>
<span class="quote">&gt;    fields, while by allocating it dynamically it&#39;s a separate cache line wasted.</span>
<span class="quote">&gt; </span>

A separate cache line that is only used during direct reclaim when the
process is taking a large hit anyway
<span class="quote">
&gt;  - Plus by allocating it separately you spend two cachelines on it: each slab will </span>
<span class="quote">&gt;    be at least cacheline aligned, and 72 bytes will allocate 128 bytes. So when </span>
<span class="quote">&gt;    this gets triggered you&#39;ve just wasted some more RAM.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  - I mean, if it had dynamic size, or was arguably huge. But this is just a </span>
<span class="quote">&gt;    cpumask and a boolean!</span>
<span class="quote">&gt; </span>

It gets larger with enterprise configs.
<span class="quote">
&gt;  - The cpumask will be dynamic if you increase the NR_CPUS count any more than </span>
<span class="quote">&gt;    that - in which case embedding the structure is the right choice again.</span>
<span class="quote">&gt; </span>

Enterprise configurations are larger. The most recent one I checked defined
NR_CPUS as 8192. If it&#39;s embedded in the structure, it means that we need
to call cpumask_clear on every fork even if it&#39;s never used. That adds
constant overhead to a fast path to avoid an allocation and a few cache
misses in a direct reclaim path. Are you certain you want that trade-off?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=24451">Mel Gorman</a> - June 10, 2015, 8:59 a.m.</div>
<pre class="content">
On Wed, Jun 10, 2015 at 10:33:32AM +0200, Ingo Molnar wrote:
<span class="quote">&gt; </span>
<span class="quote">&gt; * Mel Gorman &lt;mgorman@suse.de&gt; wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; Linear mapped reader on a 4-node machine with 64G RAM and 48 CPUs</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt;                                         4.1.0-rc6          4.1.0-rc6</span>
<span class="quote">&gt; &gt;                                           vanilla       flushfull-v6</span>
<span class="quote">&gt; &gt; Ops lru-file-mmap-read-elapsed   162.88 (  0.00%)   120.81 ( 25.83%)</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt;            4.1.0-rc6   4.1.0-rc6</span>
<span class="quote">&gt; &gt;              vanillaflushfull-v6r5</span>
<span class="quote">&gt; &gt; User          568.96      614.68</span>
<span class="quote">&gt; &gt; System       6085.61     4226.61</span>
<span class="quote">&gt; &gt; Elapsed       164.24      122.17</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; This is showing that the readers completed 25.83% faster with 30% less</span>
<span class="quote">&gt; &gt; system CPU time. From vmstats, it is known that the vanilla kernel was</span>
<span class="quote">&gt; &gt; interrupted roughly 900K times per second during the steady phase of the</span>
<span class="quote">&gt; &gt; test and the patched kernel was interrupts 180K times per second.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; The impact is lower on a single socket machine.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt;                                         4.1.0-rc6          4.1.0-rc6</span>
<span class="quote">&gt; &gt;                                           vanilla       flushfull-v6</span>
<span class="quote">&gt; &gt; Ops lru-file-mmap-read-elapsed    25.43 (  0.00%)    20.59 ( 19.03%)</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt;            4.1.0-rc6    4.1.0-rc6</span>
<span class="quote">&gt; &gt;              vanilla flushfull-v6</span>
<span class="quote">&gt; &gt; User           59.14        58.99</span>
<span class="quote">&gt; &gt; System        109.15        77.84</span>
<span class="quote">&gt; &gt; Elapsed        27.32        22.31</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; It&#39;s still a noticeable improvement with vmstat showing interrupts went</span>
<span class="quote">&gt; &gt; from roughly 500K per second to 45K per second.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Btw., I tried to compare your previous (v5) pfn-tracking numbers with these </span>
<span class="quote">&gt; full-flushing numbers, and found that the IRQ rate appears to be the same:</span>
<span class="quote">&gt; </span>

That&#39;s expected because the number of IPIs sent is the same. What
changes is the tracking of the PFNs and then the work within the IPI
itself.
<span class="quote">
&gt; &gt; &gt; From vmstats, it is known that the vanilla kernel was interrupted roughly 900K </span>
<span class="quote">&gt; &gt; &gt; times per second during the steady phase of the test and the patched kernel </span>
<span class="quote">&gt; &gt; &gt; was interrupts 180K times per second.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; &gt; It&#39;s still a noticeable improvement with vmstat showing interrupts went from </span>
<span class="quote">&gt; &gt; &gt; roughly 500K per second to 45K per second.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; ... is that because the batching limit in the pfn-tracking case was high enough to </span>
<span class="quote">&gt; not be noticeable in the vmstat?</span>
<span class="quote">&gt; </span>

It&#39;s just the case that there are fewer cores and less activity in the
machine overall.
<span class="quote">
&gt; In the full-flushing case (v6 without patch 4) the batching limit is &#39;infinite&#39;, </span>
<span class="quote">&gt; we&#39;ll batch as long as possible, right?</span>
<span class="quote">&gt; </span>

No because we must flush before pages are freed so the maximum batching
is related to SWAP_CLUSTER_MAX. If we free a page before the flush then
in theory the page can be reallocated and a stale TLB entry can allow
access to unrelated data. It would be almost impossible to trigger
corruption this way but it&#39;s a concern.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=24451">Mel Gorman</a> - June 10, 2015, 9:58 a.m.</div>
<pre class="content">
On Wed, Jun 10, 2015 at 10:26:40AM +0200, Ingo Molnar wrote:
<span class="quote">&gt; </span>
<span class="quote">&gt; * Mel Gorman &lt;mgorman@suse.de&gt; wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; On a 4-socket machine the results were</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt;                                         4.1.0-rc6          4.1.0-rc6</span>
<span class="quote">&gt; &gt;                                     batchdirty-v6      batchunmap-v6</span>
<span class="quote">&gt; &gt; Ops lru-file-mmap-read-elapsed   121.27 (  0.00%)   118.79 (  2.05%)</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt;            4.1.0-rc6      4.1.0-rc6</span>
<span class="quote">&gt; &gt;         batchdirty-v6 batchunmap-v6</span>
<span class="quote">&gt; &gt; User          620.84         608.48</span>
<span class="quote">&gt; &gt; System       4245.35        4152.89</span>
<span class="quote">&gt; &gt; Elapsed       122.65         120.15</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; In this case the workload completed faster and there was less CPU overhead</span>
<span class="quote">&gt; &gt; but as it&#39;s a NUMA machine there are a lot of factors at play. It&#39;s easier</span>
<span class="quote">&gt; &gt; to quantify on a single socket machine;</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt;                                         4.1.0-rc6          4.1.0-rc6</span>
<span class="quote">&gt; &gt;                                     batchdirty-v6      batchunmap-v6</span>
<span class="quote">&gt; &gt; Ops lru-file-mmap-read-elapsed    20.35 (  0.00%)    21.52 ( -5.75%)</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt;            4.1.0-rc6   4.1.0-rc6</span>
<span class="quote">&gt; &gt;         batchdirty-v6r5batchunmap-v6r5</span>
<span class="quote">&gt; &gt; User           58.02       60.70</span>
<span class="quote">&gt; &gt; System         77.57       81.92</span>
<span class="quote">&gt; &gt; Elapsed        22.14       23.16</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; That shows the workload takes 5.75% longer to complete with a similar</span>
<span class="quote">&gt; &gt; increase in the system CPU usage.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Btw., do you have any stddev noise numbers?</span>
<span class="quote">&gt; </span>

                                           4.1.0-rc6          4.1.0-rc6          4.1.0-rc6          4.1.0-rc6
                                             vanilla     flushfull-v6r5    batchdirty-v6r5    batchunmap-v6r5
Ops lru-file-mmap-read-elapsed       25.43 (  0.00%)    20.59 ( 19.03%)    20.35 ( 19.98%)    21.52 ( 15.38%)
Ops lru-file-mmap-read-time_stddv     0.32 (  0.00%)     0.32 ( -1.30%)     0.39 (-23.00%)     0.45 (-40.91%)


flushfull  -- patch 2
batchdirty -- patch 3
batchunmap -- patch 4

So the impact of tracking the PFNs is outside the noise and there is
definite direct cost to it. This was expected for both the PFN tracking
and the individual flushes.
<span class="quote">
&gt; The batching speedup is brutal enough to not need any noise estimations, it&#39;s a </span>
<span class="quote">&gt; clear winner.</span>
<span class="quote">&gt; </span>

Agreed.
<span class="quote">
&gt; But this PFN tracking patch is more difficult to judge as the numbers are pretty </span>
<span class="quote">&gt; close to each other.</span>
<span class="quote">&gt; </span>

It&#39;s definitely measurable, no doubt about it and there never was. The
concerns were always the refill costs due to flushing potentially active
TLB entries unnecessarily. From https://lkml.org/lkml/2014/7/31/825, this
is potentially high where it says that a 512 DTLB refill takes 22,000
cycles which is higher than the individual flushes. However, this is an
estimate and it&#39;ll always be a case of &quot;it depends&quot;. It&#39;s been asserted
that the refill costs are really low so lets just go with that, drop
patch 4 and wait and see who complains.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=35552">Ingo Molnar</a> - June 11, 2015, 3:02 p.m.</div>
<pre class="content">
* Mel Gorman &lt;mgorman@suse.de&gt; wrote:
<span class="quote">
&gt; &gt; In the full-flushing case (v6 without patch 4) the batching limit is </span>
<span class="quote">&gt; &gt; &#39;infinite&#39;, we&#39;ll batch as long as possible, right?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; No because we must flush before pages are freed so the maximum batching is </span>
<span class="quote">&gt; related to SWAP_CLUSTER_MAX. If we free a page before the flush then in theory </span>
<span class="quote">&gt; the page can be reallocated and a stale TLB entry can allow access to unrelated </span>
<span class="quote">&gt; data. It would be almost impossible to trigger corruption this way but it&#39;s a </span>
<span class="quote">&gt; concern.</span>

Well, could we say double SWAP_CLUSTER_MAX to further reduce the IPI rate?

Thanks,

	Ingo
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=24451">Mel Gorman</a> - June 11, 2015, 3:25 p.m.</div>
<pre class="content">
On Thu, Jun 11, 2015 at 05:02:51PM +0200, Ingo Molnar wrote:
<span class="quote">&gt; </span>
<span class="quote">&gt; * Mel Gorman &lt;mgorman@suse.de&gt; wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; &gt; In the full-flushing case (v6 without patch 4) the batching limit is </span>
<span class="quote">&gt; &gt; &gt; &#39;infinite&#39;, we&#39;ll batch as long as possible, right?</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; No because we must flush before pages are freed so the maximum batching is </span>
<span class="quote">&gt; &gt; related to SWAP_CLUSTER_MAX. If we free a page before the flush then in theory </span>
<span class="quote">&gt; &gt; the page can be reallocated and a stale TLB entry can allow access to unrelated </span>
<span class="quote">&gt; &gt; data. It would be almost impossible to trigger corruption this way but it&#39;s a </span>
<span class="quote">&gt; &gt; concern.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Well, could we say double SWAP_CLUSTER_MAX to further reduce the IPI rate?</span>
<span class="quote">&gt; </span>

We could but it&#39;s a suprisingly subtle change. The impacts I can think
of are;

1. LRU lock hold times increase slightly because more pages are being
   isolated
2. There are slight timing changes due to more pages having to be
   processed before they are freed. There is a slight risk that more
   pages than are necessary get reclaimed but I doubt it&#39;ll be
   measurable
3. There is a risk that too_many_isolated checks will be easier to
   trigger resulting in a HZ/10 stall
4. The rotation rate of active-&gt;inactive is slightly faster but there
   should be fewer rotations before the lists get balanced so it
   shouldn&#39;t matter.
5. More pages are reclaimed in a single pass if zone_reclaim_mode is
   active but that thing sucks hard when it&#39;s enabled no matter what
6. More pages are isolated for compaction so page hold times there
   are longer while they are being copied

There might be others. To be honest, I&#39;m struggling to think of any serious
problems such a change would cause. The biggest risk is issue 3 but I expect
that hitting that requires that the system is already getting badly hammered.
The main downside is that it affects all page reclaim activity, not just
the mapped pages which are triggering the IPIs. I&#39;ll add a patch to the
series that alters SWAP_CLUSTER_MAX with the intent to further reduce
IPIs and see what falls out and see if any other VM person complains.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig</span>
<span class="p_header">index 226d5696e1d1..0810703bdc9a 100644</span>
<span class="p_header">--- a/arch/x86/Kconfig</span>
<span class="p_header">+++ b/arch/x86/Kconfig</span>
<span class="p_chunk">@@ -44,6 +44,7 @@</span> <span class="p_context"> config X86</span>
 	select ARCH_DISCARD_MEMBLOCK
 	select ARCH_WANT_OPTIONAL_GPIOLIB
 	select ARCH_WANT_FRAME_POINTERS
<span class="p_add">+	select ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH</span>
 	select HAVE_DMA_ATTRS
 	select HAVE_DMA_CONTIGUOUS
 	select HAVE_KRETPROBES
<span class="p_header">diff --git a/include/linux/rmap.h b/include/linux/rmap.h</span>
<span class="p_header">index c89c53a113a8..29446aeef36e 100644</span>
<span class="p_header">--- a/include/linux/rmap.h</span>
<span class="p_header">+++ b/include/linux/rmap.h</span>
<span class="p_chunk">@@ -89,6 +89,9 @@</span> <span class="p_context"> enum ttu_flags {</span>
 	TTU_IGNORE_MLOCK = (1 &lt;&lt; 8),	/* ignore mlock */
 	TTU_IGNORE_ACCESS = (1 &lt;&lt; 9),	/* don&#39;t age */
 	TTU_IGNORE_HWPOISON = (1 &lt;&lt; 10),/* corrupted page is recoverable */
<span class="p_add">+	TTU_BATCH_FLUSH = (1 &lt;&lt; 11),	/* Batch TLB flushes where possible</span>
<span class="p_add">+					 * and caller guarantees they will</span>
<span class="p_add">+					 * do a final flush if necessary */</span>
 };
 
 #ifdef CONFIG_MMU
<span class="p_header">diff --git a/include/linux/sched.h b/include/linux/sched.h</span>
<span class="p_header">index 26a2e6122734..d891e01f0445 100644</span>
<span class="p_header">--- a/include/linux/sched.h</span>
<span class="p_header">+++ b/include/linux/sched.h</span>
<span class="p_chunk">@@ -1289,6 +1289,18 @@</span> <span class="p_context"> enum perf_event_task_context {</span>
 	perf_nr_task_contexts,
 };
 
<span class="p_add">+/* Track pages that require TLB flushes */</span>
<span class="p_add">+struct tlbflush_unmap_batch {</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Each bit set is a CPU that potentially has a TLB entry for one of</span>
<span class="p_add">+	 * the PFNs being flushed. See set_tlb_ubc_flush_pending().</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	struct cpumask cpumask;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* True if any bit in cpumask is set */</span>
<span class="p_add">+	bool flush_required;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
 struct task_struct {
 	volatile long state;	/* -1 unrunnable, 0 runnable, &gt;0 stopped */
 	void *stack;
<span class="p_chunk">@@ -1648,6 +1660,10 @@</span> <span class="p_context"> struct task_struct {</span>
 	unsigned long numa_pages_migrated;
 #endif /* CONFIG_NUMA_BALANCING */
 
<span class="p_add">+#ifdef CONFIG_ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH</span>
<span class="p_add">+	struct tlbflush_unmap_batch *tlb_ubc;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 	struct rcu_head rcu;
 
 	/*
<span class="p_header">diff --git a/init/Kconfig b/init/Kconfig</span>
<span class="p_header">index dc24dec60232..6e6fa4842250 100644</span>
<span class="p_header">--- a/init/Kconfig</span>
<span class="p_header">+++ b/init/Kconfig</span>
<span class="p_chunk">@@ -904,6 +904,16 @@</span> <span class="p_context"> config ARCH_SUPPORTS_NUMA_BALANCING</span>
 	bool
 
 #
<span class="p_add">+# For architectures that prefer to flush all TLBs after a number of pages</span>
<span class="p_add">+# are unmapped instead of sending one IPI per page to flush. The architecture</span>
<span class="p_add">+# must provide guarantees on what happens if a clean TLB cache entry is</span>
<span class="p_add">+# written after the unmap. Details are in mm/rmap.c near the check for</span>
<span class="p_add">+# should_defer_flush. The architecture should also consider if the full flush</span>
<span class="p_add">+# and the refill costs are offset by the savings of sending fewer IPIs.</span>
<span class="p_add">+config ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH</span>
<span class="p_add">+	bool</span>
<span class="p_add">+</span>
<span class="p_add">+#</span>
 # For architectures that know their GCC __int128 support is sound
 #
 config ARCH_SUPPORTS_INT128
<span class="p_header">diff --git a/kernel/fork.c b/kernel/fork.c</span>
<span class="p_header">index 03c1eaaa6ef5..3fb3e776cfcf 100644</span>
<span class="p_header">--- a/kernel/fork.c</span>
<span class="p_header">+++ b/kernel/fork.c</span>
<span class="p_chunk">@@ -257,6 +257,11 @@</span> <span class="p_context"> void __put_task_struct(struct task_struct *tsk)</span>
 	delayacct_tsk_free(tsk);
 	put_signal_struct(tsk-&gt;signal);
 
<span class="p_add">+#ifdef CONFIG_ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH</span>
<span class="p_add">+	kfree(tsk-&gt;tlb_ubc);</span>
<span class="p_add">+	tsk-&gt;tlb_ubc = NULL;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 	if (!profile_handoff_task(tsk))
 		free_task(tsk);
 }
<span class="p_header">diff --git a/kernel/sched/core.c b/kernel/sched/core.c</span>
<span class="p_header">index 123673291ffb..d58ebdf4d759 100644</span>
<span class="p_header">--- a/kernel/sched/core.c</span>
<span class="p_header">+++ b/kernel/sched/core.c</span>
<span class="p_chunk">@@ -1843,6 +1843,9 @@</span> <span class="p_context"> static void __sched_fork(unsigned long clone_flags, struct task_struct *p)</span>
 
 	p-&gt;numa_group = NULL;
 #endif /* CONFIG_NUMA_BALANCING */
<span class="p_add">+#ifdef CONFIG_ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH</span>
<span class="p_add">+	p-&gt;tlb_ubc = NULL;</span>
<span class="p_add">+#endif /* CONFIG_ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH */</span>
 }
 
 #ifdef CONFIG_NUMA_BALANCING
<span class="p_header">diff --git a/mm/internal.h b/mm/internal.h</span>
<span class="p_header">index a25e359a4039..465e621b86b1 100644</span>
<span class="p_header">--- a/mm/internal.h</span>
<span class="p_header">+++ b/mm/internal.h</span>
<span class="p_chunk">@@ -433,4 +433,15 @@</span> <span class="p_context"> unsigned long reclaim_clean_pages_from_list(struct zone *zone,</span>
 #define ALLOC_CMA		0x80 /* allow allocations from CMA areas */
 #define ALLOC_FAIR		0x100 /* fair zone allocation */
 
<span class="p_add">+enum ttu_flags;</span>
<span class="p_add">+struct tlbflush_unmap_batch;</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH</span>
<span class="p_add">+void try_to_unmap_flush(void);</span>
<span class="p_add">+#else</span>
<span class="p_add">+static inline void try_to_unmap_flush(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* CONFIG_ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH */</span>
 #endif	/* __MM_INTERNAL_H */
<span class="p_header">diff --git a/mm/rmap.c b/mm/rmap.c</span>
<span class="p_header">index 24dd3f9fee27..4cadb60df74a 100644</span>
<span class="p_header">--- a/mm/rmap.c</span>
<span class="p_header">+++ b/mm/rmap.c</span>
<span class="p_chunk">@@ -60,6 +60,8 @@</span> <span class="p_context"></span>
 
 #include &lt;asm/tlbflush.h&gt;
 
<span class="p_add">+#include &lt;trace/events/tlb.h&gt;</span>
<span class="p_add">+</span>
 #include &quot;internal.h&quot;
 
 static struct kmem_cache *anon_vma_cachep;
<span class="p_chunk">@@ -581,6 +583,88 @@</span> <span class="p_context"> vma_address(struct page *page, struct vm_area_struct *vma)</span>
 	return address;
 }
 
<span class="p_add">+#ifdef CONFIG_ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH</span>
<span class="p_add">+static void percpu_flush_tlb_batch_pages(void *data)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * All TLB entries are flushed on the assumption that it is</span>
<span class="p_add">+	 * cheaper to flush all TLBs and let them be refilled than</span>
<span class="p_add">+	 * flushing individual PFNs. Note that we do not track mm&#39;s</span>
<span class="p_add">+	 * to flush as that might simply be multiple full TLB flushes</span>
<span class="p_add">+	 * for no gain.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	count_vm_tlb_event(NR_TLB_REMOTE_FLUSH_RECEIVED);</span>
<span class="p_add">+	local_flush_tlb();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Flush TLB entries for recently unmapped pages from remote CPUs. It is</span>
<span class="p_add">+ * important if a PTE was dirty when it was unmapped that it&#39;s flushed</span>
<span class="p_add">+ * before any IO is initiated on the page to prevent lost writes. Similarly,</span>
<span class="p_add">+ * it must be flushed before freeing to prevent data leakage.</span>
<span class="p_add">+ */</span>
<span class="p_add">+void try_to_unmap_flush(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct tlbflush_unmap_batch *tlb_ubc = current-&gt;tlb_ubc;</span>
<span class="p_add">+	int cpu;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!tlb_ubc || !tlb_ubc-&gt;flush_required)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	trace_tlb_flush(TLB_REMOTE_SHOOTDOWN, -1UL);</span>
<span class="p_add">+</span>
<span class="p_add">+	cpu = get_cpu();</span>
<span class="p_add">+	if (cpumask_test_cpu(cpu, &amp;tlb_ubc-&gt;cpumask))</span>
<span class="p_add">+		percpu_flush_tlb_batch_pages(&amp;tlb_ubc-&gt;cpumask);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (cpumask_any_but(&amp;tlb_ubc-&gt;cpumask, cpu) &lt; nr_cpu_ids) {</span>
<span class="p_add">+		smp_call_function_many(&amp;tlb_ubc-&gt;cpumask,</span>
<span class="p_add">+			percpu_flush_tlb_batch_pages, (void *)tlb_ubc, true);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	cpumask_clear(&amp;tlb_ubc-&gt;cpumask);</span>
<span class="p_add">+	tlb_ubc-&gt;flush_required = false;</span>
<span class="p_add">+	put_cpu();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void set_tlb_ubc_flush_pending(struct mm_struct *mm,</span>
<span class="p_add">+		struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct tlbflush_unmap_batch *tlb_ubc = current-&gt;tlb_ubc;</span>
<span class="p_add">+</span>
<span class="p_add">+	cpumask_or(&amp;tlb_ubc-&gt;cpumask, &amp;tlb_ubc-&gt;cpumask, mm_cpumask(mm));</span>
<span class="p_add">+	tlb_ubc-&gt;flush_required = true;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Returns true if the TLB flush should be deferred to the end of a batch of</span>
<span class="p_add">+ * unmap operations to reduce IPIs.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static bool should_defer_flush(struct mm_struct *mm, enum ttu_flags flags)</span>
<span class="p_add">+{</span>
<span class="p_add">+	bool should_defer = false;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!current-&gt;tlb_ubc || !(flags &amp; TTU_BATCH_FLUSH))</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* If remote CPUs need to be flushed then defer batch the flush */</span>
<span class="p_add">+	if (cpumask_any_but(mm_cpumask(mm), get_cpu()) &lt; nr_cpu_ids)</span>
<span class="p_add">+		should_defer = true;</span>
<span class="p_add">+	put_cpu();</span>
<span class="p_add">+</span>
<span class="p_add">+	return should_defer;</span>
<span class="p_add">+}</span>
<span class="p_add">+#else</span>
<span class="p_add">+static void set_tlb_ubc_flush_pending(struct mm_struct *mm,</span>
<span class="p_add">+		struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static bool should_defer_flush(struct mm_struct *mm, enum ttu_flags flags)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return false;</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif /* CONFIG_ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH */</span>
<span class="p_add">+</span>
 /*
  * At what user virtual address is page expected in vma?
  * Caller should check the page is actually part of the vma.
<span class="p_chunk">@@ -1213,7 +1297,24 @@</span> <span class="p_context"> static int try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
 
 	/* Nuke the page table entry. */
 	flush_cache_page(vma, address, page_to_pfn(page));
<span class="p_del">-	pteval = ptep_clear_flush(vma, address, pte);</span>
<span class="p_add">+	if (should_defer_flush(mm, flags)) {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * We clear the PTE but do not flush so potentially a remote</span>
<span class="p_add">+		 * CPU could still be writing to the page. If the entry was</span>
<span class="p_add">+		 * previously clean then the architecture must guarantee that</span>
<span class="p_add">+		 * a clear-&gt;dirty transition on a cached TLB entry is written</span>
<span class="p_add">+		 * through and traps if the PTE is unmapped.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		pteval = ptep_get_and_clear(mm, address, pte);</span>
<span class="p_add">+</span>
<span class="p_add">+		/* Potentially writable TLBs must be flushed before IO */</span>
<span class="p_add">+		if (pte_dirty(pteval))</span>
<span class="p_add">+			flush_tlb_page(vma, address);</span>
<span class="p_add">+		else</span>
<span class="p_add">+			set_tlb_ubc_flush_pending(mm, page);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		pteval = ptep_clear_flush(vma, address, pte);</span>
<span class="p_add">+	}</span>
 
 	/* Move the dirty bit to the physical page now the pte is gone. */
 	if (pte_dirty(pteval))
<span class="p_header">diff --git a/mm/vmscan.c b/mm/vmscan.c</span>
<span class="p_header">index 5e8eadd71bac..f16e07aaef59 100644</span>
<span class="p_header">--- a/mm/vmscan.c</span>
<span class="p_header">+++ b/mm/vmscan.c</span>
<span class="p_chunk">@@ -1024,7 +1024,8 @@</span> <span class="p_context"> static unsigned long shrink_page_list(struct list_head *page_list,</span>
 		 * processes. Try to unmap it here.
 		 */
 		if (page_mapped(page) &amp;&amp; mapping) {
<span class="p_del">-			switch (try_to_unmap(page, ttu_flags)) {</span>
<span class="p_add">+			switch (try_to_unmap(page,</span>
<span class="p_add">+					ttu_flags|TTU_BATCH_FLUSH)) {</span>
 			case SWAP_FAIL:
 				goto activate_locked;
 			case SWAP_AGAIN:
<span class="p_chunk">@@ -1175,6 +1176,7 @@</span> <span class="p_context"> keep:</span>
 	}
 
 	mem_cgroup_uncharge_list(&amp;free_pages);
<span class="p_add">+	try_to_unmap_flush();</span>
 	free_hot_cold_page_list(&amp;free_pages, true);
 
 	list_splice(&amp;ret_pages, page_list);
<span class="p_chunk">@@ -2118,6 +2120,26 @@</span> <span class="p_context"> out:</span>
 	}
 }
 
<span class="p_add">+#ifdef CONFIG_ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Allocate the control structure for batch TLB flushing. An allocation</span>
<span class="p_add">+ * failure is harmless as the reclaimer will send IPIs where necessary.</span>
<span class="p_add">+ * A GFP_KERNEL allocation from this context is normally not advised but</span>
<span class="p_add">+ * we are depending on PF_MEMALLOC (set by direct reclaim or kswapd) to</span>
<span class="p_add">+ * limit the depth of the call.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static void alloc_tlb_ubc(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!current-&gt;tlb_ubc)</span>
<span class="p_add">+		current-&gt;tlb_ubc = kzalloc(sizeof(struct tlbflush_unmap_batch),</span>
<span class="p_add">+						GFP_KERNEL | __GFP_NOWARN);</span>
<span class="p_add">+}</span>
<span class="p_add">+#else</span>
<span class="p_add">+static inline void alloc_tlb_ubc(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif /* CONFIG_ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH */</span>
<span class="p_add">+</span>
 /*
  * This is a basic per-zone page freer.  Used by both kswapd and direct reclaim.
  */
<span class="p_chunk">@@ -2152,6 +2174,8 @@</span> <span class="p_context"> static void shrink_lruvec(struct lruvec *lruvec, int swappiness,</span>
 	scan_adjusted = (global_reclaim(sc) &amp;&amp; !current_is_kswapd() &amp;&amp;
 			 sc-&gt;priority == DEF_PRIORITY);
 
<span class="p_add">+	alloc_tlb_ubc();</span>
<span class="p_add">+</span>
 	blk_start_plug(&amp;plug);
 	while (nr[LRU_INACTIVE_ANON] || nr[LRU_ACTIVE_FILE] ||
 					nr[LRU_INACTIVE_FILE]) {

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



