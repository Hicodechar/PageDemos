
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[tip:x86/urgent] x86/mm: Flush more aggressively in lazy TLB mode - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [tip:x86/urgent] x86/mm: Flush more aggressively in lazy TLB mode</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=60001">tip-bot for Jacob Shin</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Oct. 14, 2017, 10:49 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;tip-b956575bed91ecfb136a8300742ecbbf451471ab@git.kernel.org&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10006363/mbox/"
   >mbox</a>
|
   <a href="/patch/10006363/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10006363/">/patch/10006363/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	209C1601E9 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Sat, 14 Oct 2017 11:04:11 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id EC107291DA
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Sat, 14 Oct 2017 11:04:10 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id E041E291DC; Sat, 14 Oct 2017 11:04:10 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id F24BE291DA
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Sat, 14 Oct 2017 11:04:08 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752863AbdJNLBw (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Sat, 14 Oct 2017 07:01:52 -0400
Received: from terminus.zytor.com ([65.50.211.136]:59581 &quot;EHLO
	terminus.zytor.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1751728AbdJNLBt (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Sat, 14 Oct 2017 07:01:49 -0400
Received: from terminus.zytor.com (localhost [127.0.0.1])
	by terminus.zytor.com (8.15.2/8.15.2) with ESMTP id v9EAnHwF012573;
	Sat, 14 Oct 2017 03:49:17 -0700
Received: (from tipbot@localhost)
	by terminus.zytor.com (8.15.2/8.15.2/Submit) id v9EAnCuH012562;
	Sat, 14 Oct 2017 03:49:12 -0700
Date: Sat, 14 Oct 2017 03:49:12 -0700
X-Authentication-Warning: terminus.zytor.com: tipbot set sender to
	tipbot@zytor.com using -f
From: tip-bot for Andy Lutomirski &lt;tipbot@zytor.com&gt;
Message-ID: &lt;tip-b956575bed91ecfb136a8300742ecbbf451471ab@git.kernel.org&gt;
Cc: rkagan@virtuozzo.com, johannes.hirte@datenkhaos.de, bp@suse.de,
	bp@alien8.de, keescook@chromium.org, hpa@zytor.com,
	peterz@infradead.org, luto@kernel.org, nadav.amit@gmail.com,
	kilobyte@angband.pl, riel@redhat.com, tglx@linutronix.de,
	kirill.shutemov@linux.intel.com, linux-kernel@vger.kernel.org,
	markus@trippelsdorf.de, mingo@kernel.org, daniel@iogearbox.net,
	ebiggers@google.com, brgerst@gmail.com, torvalds@linux-foundation.org
Reply-To: tglx@linutronix.de, kirill.shutemov@linux.intel.com,
	linux-kernel@vger.kernel.org, mingo@kernel.org,
	markus@trippelsdorf.de, daniel@iogearbox.net, brgerst@gmail.com,
	torvalds@linux-foundation.org, ebiggers@google.com, bp@suse.de,
	rkagan@virtuozzo.com, johannes.hirte@datenkhaos.de,
	hpa@zytor.com, peterz@infradead.org, keescook@chromium.org,
	bp@alien8.de, luto@kernel.org, kilobyte@angband.pl,
	nadav.amit@gmail.com, riel@redhat.com
In-Reply-To: &lt;20171009170231.fkpraqokz6e4zeco@pd.tnic&gt;
References: &lt;20171009170231.fkpraqokz6e4zeco@pd.tnic&gt;
To: linux-tip-commits@vger.kernel.org
Subject: [tip:x86/urgent] x86/mm: Flush more aggressively in lazy TLB mode
Git-Commit-ID: b956575bed91ecfb136a8300742ecbbf451471ab
X-Mailer: tip-git-log-daemon
Robot-ID: &lt;tip-bot.git.kernel.org&gt;
Robot-Unsubscribe: Contact &lt;mailto:hpa@kernel.org&gt; to get blacklisted from
	these emails
MIME-Version: 1.0
Content-Transfer-Encoding: 8bit
Content-Type: text/plain; charset=UTF-8
Content-Disposition: inline
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=60001">tip-bot for Jacob Shin</a> - Oct. 14, 2017, 10:49 a.m.</div>
<pre class="content">
Commit-ID:  b956575bed91ecfb136a8300742ecbbf451471ab
Gitweb:     https://git.kernel.org/tip/b956575bed91ecfb136a8300742ecbbf451471ab
Author:     Andy Lutomirski &lt;luto@kernel.org&gt;
AuthorDate: Mon, 9 Oct 2017 09:50:49 -0700
Committer:  Ingo Molnar &lt;mingo@kernel.org&gt;
CommitDate: Sat, 14 Oct 2017 09:21:24 +0200

x86/mm: Flush more aggressively in lazy TLB mode

Since commit:

  94b1b03b519b (&quot;x86/mm: Rework lazy TLB mode and TLB freshness tracking&quot;)

x86&#39;s lazy TLB mode has been all the way lazy: when running a kernel thread
(including the idle thread), the kernel keeps using the last user mm&#39;s
page tables without attempting to maintain user TLB coherence at all.

From a pure semantic perspective, this is fine -- kernel threads won&#39;t
attempt to access user pages, so having stale TLB entries doesn&#39;t matter.

Unfortunately, I forgot about a subtlety.  By skipping TLB flushes,
we also allow any paging-structure caches that may exist on the CPU
to become incoherent.  This means that we can have a
paging-structure cache entry that references a freed page table, and
the CPU is within its rights to do a speculative page walk starting
at the freed page table.

I can imagine this causing two different problems:

 - A speculative page walk starting from a bogus page table could read
   IO addresses.  I haven&#39;t seen any reports of this causing problems.

 - A speculative page walk that involves a bogus page table can install
   garbage in the TLB.  Such garbage would always be at a user VA, but
   some AMD CPUs have logic that triggers a machine check when it notices
   these bogus entries.  I&#39;ve seen a couple reports of this.

Boris further explains the failure mode:
<span class="quote">
&gt; It is actually more of an optimization which assumes that paging-structure</span>
<span class="quote">&gt; entries are in WB DRAM:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; &quot;TlbCacheDis: cacheable memory disable. Read-write. 0=Enables</span>
<span class="quote">&gt; performance optimization that assumes PML4, PDP, PDE, and PTE entries</span>
<span class="quote">&gt; are in cacheable WB-DRAM; memory type checks may be bypassed, and</span>
<span class="quote">&gt; addresses outside of WB-DRAM may result in undefined behavior or NB</span>
<span class="quote">&gt; protocol errors. 1=Disables performance optimization and allows PML4,</span>
<span class="quote">&gt; PDP, PDE and PTE entries to be in any memory type. Operating systems</span>
<span class="quote">&gt; that maintain page tables in memory types other than WB- DRAM must set</span>
<span class="quote">&gt; TlbCacheDis to insure proper operation.&quot;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; The MCE generated is an NB protocol error to signal that</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; &quot;Link: A specific coherent-only packet from a CPU was issued to an</span>
<span class="quote">&gt; IO link. This may be caused by software which addresses page table</span>
<span class="quote">&gt; structures in a memory type other than cacheable WB-DRAM without</span>
<span class="quote">&gt; properly configuring MSRC001_0015[TlbCacheDis]. This may occur, for</span>
<span class="quote">&gt; example, when page table structure addresses are above top of memory. In</span>
<span class="quote">&gt; such cases, the NB will generate an MCE if it sees a mismatch between</span>
<span class="quote">&gt; the memory operation generated by the core and the link type.&quot;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I&#39;m assuming coherent-only packets don&#39;t go out on IO links, thus the</span>
<span class="quote">&gt; error.</span>

To fix this, reinstate TLB coherence in lazy mode.  With this patch
applied, we do it in one of two ways:

 - If we have PCID, we simply switch back to init_mm&#39;s page tables
   when we enter a kernel thread -- this seems to be quite cheap
   except for the cost of serializing the CPU.

 - If we don&#39;t have PCID, then we set a flag and switch to init_mm
   the first time we would otherwise need to flush the TLB.

The /sys/kernel/debug/x86/tlb_use_lazy_mode debug switch can be changed
to override the default mode for benchmarking.

In theory, we could optimize this better by only flushing the TLB in
lazy CPUs when a page table is freed.  Doing that would require
auditing the mm code to make sure that all page table freeing goes
through tlb_remove_page() as well as reworking some data structures
to implement the improved flush logic.

Reported-by: Markus Trippelsdorf &lt;markus@trippelsdorf.de&gt;
Reported-by: Adam Borowski &lt;kilobyte@angband.pl&gt;
<span class="signed-off-by">Signed-off-by: Andy Lutomirski &lt;luto@kernel.org&gt;</span>
<span class="signed-off-by">Signed-off-by: Borislav Petkov &lt;bp@suse.de&gt;</span>
Cc: Borislav Petkov &lt;bp@alien8.de&gt;
Cc: Brian Gerst &lt;brgerst@gmail.com&gt;
Cc: Daniel Borkmann &lt;daniel@iogearbox.net&gt;
Cc: Eric Biggers &lt;ebiggers@google.com&gt;
Cc: Johannes Hirte &lt;johannes.hirte@datenkhaos.de&gt;
Cc: Kees Cook &lt;keescook@chromium.org&gt;
Cc: Kirill A. Shutemov &lt;kirill.shutemov@linux.intel.com&gt;
Cc: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;
Cc: Nadav Amit &lt;nadav.amit@gmail.com&gt;
Cc: Peter Zijlstra &lt;peterz@infradead.org&gt;
Cc: Rik van Riel &lt;riel@redhat.com&gt;
Cc: Roman Kagan &lt;rkagan@virtuozzo.com&gt;
Cc: Thomas Gleixner &lt;tglx@linutronix.de&gt;
Fixes: 94b1b03b519b (&quot;x86/mm: Rework lazy TLB mode and TLB freshness tracking&quot;)
Link: http://lkml.kernel.org/r/20171009170231.fkpraqokz6e4zeco@pd.tnic
<span class="signed-off-by">Signed-off-by: Ingo Molnar &lt;mingo@kernel.org&gt;</span>
---
 arch/x86/include/asm/mmu_context.h |   8 +-
 arch/x86/include/asm/tlbflush.h    |  24 ++++++
 arch/x86/mm/tlb.c                  | 153 +++++++++++++++++++++++++++----------
 3 files changed, 136 insertions(+), 49 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=125831">Andrew Lutomirski</a> - Oct. 14, 2017, 4:34 p.m.</div>
<pre class="content">
On Sat, Oct 14, 2017 at 3:49 AM, tip-bot for Andy Lutomirski
&lt;tipbot@zytor.com&gt; wrote:
<span class="quote">&gt; Commit-ID:  b956575bed91ecfb136a8300742ecbbf451471ab</span>
<span class="quote">&gt; Gitweb:     https://git.kernel.org/tip/b956575bed91ecfb136a8300742ecbbf451471ab</span>
<span class="quote">&gt; Author:     Andy Lutomirski &lt;luto@kernel.org&gt;</span>
<span class="quote">&gt; AuthorDate: Mon, 9 Oct 2017 09:50:49 -0700</span>
<span class="quote">&gt; Committer:  Ingo Molnar &lt;mingo@kernel.org&gt;</span>
<span class="quote">&gt; CommitDate: Sat, 14 Oct 2017 09:21:24 +0200</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; x86/mm: Flush more aggressively in lazy TLB mode</span>
<span class="quote">&gt;</span>

Hey Ingo-

Are you willing to back this out and replace it with a version that
I&#39;m going to send in about two minutes?  This version is missing most
of Borislav&#39;s review comments annd has a crappy changelog message.  Or
have you already sent it to Linus?

Or I can rebase on top, but it&#39;s messy and it won&#39;t fix the changelog.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=125831">Andrew Lutomirski</a> - Oct. 14, 2017, 5 p.m.</div>
<pre class="content">
On Sat, Oct 14, 2017 at 9:34 AM, Andy Lutomirski &lt;luto@kernel.org&gt; wrote:
<span class="quote">&gt; On Sat, Oct 14, 2017 at 3:49 AM, tip-bot for Andy Lutomirski</span>
<span class="quote">&gt; &lt;tipbot@zytor.com&gt; wrote:</span>
<span class="quote">&gt;&gt; Commit-ID:  b956575bed91ecfb136a8300742ecbbf451471ab</span>
<span class="quote">&gt;&gt; Gitweb:     https://git.kernel.org/tip/b956575bed91ecfb136a8300742ecbbf451471ab</span>
<span class="quote">&gt;&gt; Author:     Andy Lutomirski &lt;luto@kernel.org&gt;</span>
<span class="quote">&gt;&gt; AuthorDate: Mon, 9 Oct 2017 09:50:49 -0700</span>
<span class="quote">&gt;&gt; Committer:  Ingo Molnar &lt;mingo@kernel.org&gt;</span>
<span class="quote">&gt;&gt; CommitDate: Sat, 14 Oct 2017 09:21:24 +0200</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; x86/mm: Flush more aggressively in lazy TLB mode</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Hey Ingo-</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Are you willing to back this out and replace it with a version that</span>
<span class="quote">&gt; I&#39;m going to send in about two minutes?  This version is missing most</span>
<span class="quote">&gt; of Borislav&#39;s review comments annd has a crappy changelog message.  Or</span>
<span class="quote">&gt; have you already sent it to Linus?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Or I can rebase on top, but it&#39;s messy and it won&#39;t fix the changelog.</span>

Meh, the rebased version is fine, too.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/include/asm/mmu_context.h b/arch/x86/include/asm/mmu_context.h</span>
<span class="p_header">index c120b5d..3c856a1 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/mmu_context.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/mmu_context.h</span>
<span class="p_chunk">@@ -126,13 +126,7 @@</span> <span class="p_context"> static inline void switch_ldt(struct mm_struct *prev, struct mm_struct *next)</span>
 	DEBUG_LOCKS_WARN_ON(preemptible());
 }
 
<span class="p_del">-static inline void enter_lazy_tlb(struct mm_struct *mm, struct task_struct *tsk)</span>
<span class="p_del">-{</span>
<span class="p_del">-	int cpu = smp_processor_id();</span>
<span class="p_del">-</span>
<span class="p_del">-	if (cpumask_test_cpu(cpu, mm_cpumask(mm)))</span>
<span class="p_del">-		cpumask_clear_cpu(cpu, mm_cpumask(mm));</span>
<span class="p_del">-}</span>
<span class="p_add">+void enter_lazy_tlb(struct mm_struct *mm, struct task_struct *tsk);</span>
 
 static inline int init_new_context(struct task_struct *tsk,
 				   struct mm_struct *mm)
<span class="p_header">diff --git a/arch/x86/include/asm/tlbflush.h b/arch/x86/include/asm/tlbflush.h</span>
<span class="p_header">index 4893abf..d362161 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/tlbflush.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/tlbflush.h</span>
<span class="p_chunk">@@ -83,6 +83,13 @@</span> <span class="p_context"> static inline u64 inc_mm_tlb_gen(struct mm_struct *mm)</span>
 #endif
 
 /*
<span class="p_add">+ * If tlb_use_lazy_mode is true, then we try to avoid switching CR3 to point</span>
<span class="p_add">+ * to init_mm when we switch to a kernel thread (e.g. the idle thread).  If</span>
<span class="p_add">+ * it&#39;s false, then we immediately switch CR3 when entering a kernel thread.</span>
<span class="p_add">+ */</span>
<span class="p_add">+DECLARE_STATIC_KEY_TRUE(tlb_use_lazy_mode);</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
  * 6 because 6 should be plenty and struct tlb_state will fit in
  * two cache lines.
  */
<span class="p_chunk">@@ -105,6 +112,23 @@</span> <span class="p_context"> struct tlb_state {</span>
 	u16 next_asid;
 
 	/*
<span class="p_add">+	 * We can be in one of several states:</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 *  - Actively using an mm.  Our CPU&#39;s bit will be set in</span>
<span class="p_add">+	 *    mm_cpumask(loaded_mm) and is_lazy == false;</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 *  - Not using a real mm.  loaded_mm == &amp;init_mm.  Our CPU&#39;s bit</span>
<span class="p_add">+	 *    will not be set in mm_cpumask(&amp;init_mm) and is_lazy == false.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 *  - Lazily using a real mm.  loaded_mm != &amp;init_mm, our bit</span>
<span class="p_add">+	 *    is set in mm_cpumask(loaded_mm), but is_lazy == true.</span>
<span class="p_add">+	 *    We&#39;re heuristically guessing that the CR3 load we</span>
<span class="p_add">+	 *    skipped more than makes up for the overhead added by</span>
<span class="p_add">+	 *    lazy mode.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	bool is_lazy;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
 	 * Access to this CR4 shadow and to H/W CR4 is protected by
 	 * disabling interrupts when modifying either one.
 	 */
<span class="p_header">diff --git a/arch/x86/mm/tlb.c b/arch/x86/mm/tlb.c</span>
<span class="p_header">index 49d9778..658bf00 100644</span>
<span class="p_header">--- a/arch/x86/mm/tlb.c</span>
<span class="p_header">+++ b/arch/x86/mm/tlb.c</span>
<span class="p_chunk">@@ -30,6 +30,8 @@</span> <span class="p_context"></span>
 
 atomic64_t last_mm_ctx_id = ATOMIC64_INIT(1);
 
<span class="p_add">+DEFINE_STATIC_KEY_TRUE(tlb_use_lazy_mode);</span>
<span class="p_add">+</span>
 static void choose_new_asid(struct mm_struct *next, u64 next_tlb_gen,
 			    u16 *new_asid, bool *need_flush)
 {
<span class="p_chunk">@@ -80,7 +82,7 @@</span> <span class="p_context"> void leave_mm(int cpu)</span>
 		return;
 
 	/* Warn if we&#39;re not lazy. */
<span class="p_del">-	WARN_ON(cpumask_test_cpu(smp_processor_id(), mm_cpumask(loaded_mm)));</span>
<span class="p_add">+	WARN_ON(!this_cpu_read(cpu_tlbstate.is_lazy));</span>
 
 	switch_mm(NULL, &amp;init_mm, NULL);
 }
<span class="p_chunk">@@ -142,45 +144,24 @@</span> <span class="p_context"> void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,</span>
 		__flush_tlb_all();
 	}
 #endif
<span class="p_add">+	this_cpu_write(cpu_tlbstate.is_lazy, false);</span>
 
 	if (real_prev == next) {
 		VM_BUG_ON(this_cpu_read(cpu_tlbstate.ctxs[prev_asid].ctx_id) !=
 			  next-&gt;context.ctx_id);
 
<span class="p_del">-		if (cpumask_test_cpu(cpu, mm_cpumask(next))) {</span>
<span class="p_del">-			/*</span>
<span class="p_del">-			 * There&#39;s nothing to do: we weren&#39;t lazy, and we</span>
<span class="p_del">-			 * aren&#39;t changing our mm.  We don&#39;t need to flush</span>
<span class="p_del">-			 * anything, nor do we need to update CR3, CR4, or</span>
<span class="p_del">-			 * LDTR.</span>
<span class="p_del">-			 */</span>
<span class="p_del">-			return;</span>
<span class="p_del">-		}</span>
<span class="p_del">-</span>
<span class="p_del">-		/* Resume remote flushes and then read tlb_gen. */</span>
<span class="p_del">-		cpumask_set_cpu(cpu, mm_cpumask(next));</span>
<span class="p_del">-		next_tlb_gen = atomic64_read(&amp;next-&gt;context.tlb_gen);</span>
<span class="p_del">-</span>
<span class="p_del">-		if (this_cpu_read(cpu_tlbstate.ctxs[prev_asid].tlb_gen) &lt;</span>
<span class="p_del">-		    next_tlb_gen) {</span>
<span class="p_del">-			/*</span>
<span class="p_del">-			 * Ideally, we&#39;d have a flush_tlb() variant that</span>
<span class="p_del">-			 * takes the known CR3 value as input.  This would</span>
<span class="p_del">-			 * be faster on Xen PV and on hypothetical CPUs</span>
<span class="p_del">-			 * on which INVPCID is fast.</span>
<span class="p_del">-			 */</span>
<span class="p_del">-			this_cpu_write(cpu_tlbstate.ctxs[prev_asid].tlb_gen,</span>
<span class="p_del">-				       next_tlb_gen);</span>
<span class="p_del">-			write_cr3(build_cr3(next, prev_asid));</span>
<span class="p_del">-			trace_tlb_flush(TLB_FLUSH_ON_TASK_SWITCH,</span>
<span class="p_del">-					TLB_FLUSH_ALL);</span>
<span class="p_del">-		}</span>
<span class="p_del">-</span>
 		/*
<span class="p_del">-		 * We just exited lazy mode, which means that CR4 and/or LDTR</span>
<span class="p_del">-		 * may be stale.  (Changes to the required CR4 and LDTR states</span>
<span class="p_del">-		 * are not reflected in tlb_gen.)</span>
<span class="p_add">+		 * We don&#39;t currently support having a real mm loaded without</span>
<span class="p_add">+		 * our cpu set in mm_cpumask().  We have all the bookkeeping</span>
<span class="p_add">+		 * in place to figure out whether we would need to flush</span>
<span class="p_add">+		 * if our cpu were cleared in mm_cpumask(), but we don&#39;t</span>
<span class="p_add">+		 * currently use it.</span>
 		 */
<span class="p_add">+		if (WARN_ON_ONCE(real_prev != &amp;init_mm &amp;&amp;</span>
<span class="p_add">+				 !cpumask_test_cpu(cpu, mm_cpumask(next))))</span>
<span class="p_add">+			cpumask_set_cpu(cpu, mm_cpumask(next));</span>
<span class="p_add">+</span>
<span class="p_add">+		return;</span>
 	} else {
 		u16 new_asid;
 		bool need_flush;
<span class="p_chunk">@@ -199,10 +180,9 @@</span> <span class="p_context"> void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,</span>
 		}
 
 		/* Stop remote flushes for the previous mm */
<span class="p_del">-		if (cpumask_test_cpu(cpu, mm_cpumask(real_prev)))</span>
<span class="p_del">-			cpumask_clear_cpu(cpu, mm_cpumask(real_prev));</span>
<span class="p_del">-</span>
<span class="p_del">-		VM_WARN_ON_ONCE(cpumask_test_cpu(cpu, mm_cpumask(next)));</span>
<span class="p_add">+		VM_WARN_ON_ONCE(!cpumask_test_cpu(cpu, mm_cpumask(real_prev)) &amp;&amp;</span>
<span class="p_add">+				real_prev != &amp;init_mm);</span>
<span class="p_add">+		cpumask_clear_cpu(cpu, mm_cpumask(real_prev));</span>
 
 		/*
 		 * Start remote flushes and then read tlb_gen.
<span class="p_chunk">@@ -233,6 +213,37 @@</span> <span class="p_context"> void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,</span>
 }
 
 /*
<span class="p_add">+ * enter_lazy_tlb() is a hint from the scheduler that we are entering a</span>
<span class="p_add">+ * kernel thread or other context without an mm.  Acceptable implementations</span>
<span class="p_add">+ * include doing nothing whatsoever, switching to init_mm, or various clever</span>
<span class="p_add">+ * lazy tricks to try to minimize TLB flushes.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * The scheduler reserves the right to call enter_lazy_tlb() several times</span>
<span class="p_add">+ * in a row.  It will notify us that we&#39;re going back to a real mm by</span>
<span class="p_add">+ * calling switch_mm_irqs_off().</span>
<span class="p_add">+ */</span>
<span class="p_add">+void enter_lazy_tlb(struct mm_struct *mm, struct task_struct *tsk)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (this_cpu_read(cpu_tlbstate.loaded_mm) == &amp;init_mm)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (static_branch_unlikely(&amp;tlb_use_lazy_mode)) {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * There&#39;s a significant optimization that may be possible</span>
<span class="p_add">+		 * here.  We have accurate enough TLB flush tracking that we</span>
<span class="p_add">+		 * don&#39;t need to maintain coherence of TLB per se when we&#39;re</span>
<span class="p_add">+		 * lazy.  We do, however, need to maintain coherence of</span>
<span class="p_add">+		 * paging-structure caches.  We could, in principle, leave our</span>
<span class="p_add">+		 * old mm loaded and only switch to init_mm when</span>
<span class="p_add">+		 * tlb_remove_page() happens.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		this_cpu_write(cpu_tlbstate.is_lazy, true);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		switch_mm(NULL, &amp;init_mm, NULL);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
  * Call this when reinitializing a CPU.  It fixes the following potential
  * problems:
  *
<span class="p_chunk">@@ -303,16 +314,20 @@</span> <span class="p_context"> static void flush_tlb_func_common(const struct flush_tlb_info *f,</span>
 	/* This code cannot presently handle being reentered. */
 	VM_WARN_ON(!irqs_disabled());
 
<span class="p_add">+	if (unlikely(loaded_mm == &amp;init_mm))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
 	VM_WARN_ON(this_cpu_read(cpu_tlbstate.ctxs[loaded_mm_asid].ctx_id) !=
 		   loaded_mm-&gt;context.ctx_id);
 
<span class="p_del">-	if (!cpumask_test_cpu(smp_processor_id(), mm_cpumask(loaded_mm))) {</span>
<span class="p_add">+	if (this_cpu_read(cpu_tlbstate.is_lazy)) {</span>
 		/*
<span class="p_del">-		 * We&#39;re in lazy mode -- don&#39;t flush.  We can get here on</span>
<span class="p_del">-		 * remote flushes due to races and on local flushes if a</span>
<span class="p_del">-		 * kernel thread coincidentally flushes the mm it&#39;s lazily</span>
<span class="p_del">-		 * still using.</span>
<span class="p_add">+		 * We&#39;re in lazy mode.  We need to at least flush our</span>
<span class="p_add">+		 * paging-structure cache to avoid speculatively reading</span>
<span class="p_add">+		 * garbage into our TLB.  Since switching to init_mm is barely</span>
<span class="p_add">+		 * slower than a minimal flush, just switch to init_mm.</span>
 		 */
<span class="p_add">+		switch_mm_irqs_off(NULL, &amp;init_mm, NULL);</span>
 		return;
 	}
 
<span class="p_chunk">@@ -611,3 +626,57 @@</span> <span class="p_context"> static int __init create_tlb_single_page_flush_ceiling(void)</span>
 	return 0;
 }
 late_initcall(create_tlb_single_page_flush_ceiling);
<span class="p_add">+</span>
<span class="p_add">+static ssize_t tlblazy_read_file(struct file *file, char __user *user_buf,</span>
<span class="p_add">+				 size_t count, loff_t *ppos)</span>
<span class="p_add">+{</span>
<span class="p_add">+	char buf[2];</span>
<span class="p_add">+</span>
<span class="p_add">+	buf[0] = static_branch_likely(&amp;tlb_use_lazy_mode) ? &#39;1&#39; : &#39;0&#39;;</span>
<span class="p_add">+	buf[1] = &#39;\n&#39;;</span>
<span class="p_add">+</span>
<span class="p_add">+	return simple_read_from_buffer(user_buf, count, ppos, buf, 2);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static ssize_t tlblazy_write_file(struct file *file,</span>
<span class="p_add">+		 const char __user *user_buf, size_t count, loff_t *ppos)</span>
<span class="p_add">+{</span>
<span class="p_add">+	bool val;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (kstrtobool_from_user(user_buf, count, &amp;val))</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (val)</span>
<span class="p_add">+		static_branch_enable(&amp;tlb_use_lazy_mode);</span>
<span class="p_add">+	else</span>
<span class="p_add">+		static_branch_disable(&amp;tlb_use_lazy_mode);</span>
<span class="p_add">+</span>
<span class="p_add">+	return count;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static const struct file_operations fops_tlblazy = {</span>
<span class="p_add">+	.read = tlblazy_read_file,</span>
<span class="p_add">+	.write = tlblazy_write_file,</span>
<span class="p_add">+	.llseek = default_llseek,</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+static int __init init_tlb_use_lazy_mode(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (boot_cpu_has(X86_FEATURE_PCID)) {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Heuristic: with PCID on, switching to and from</span>
<span class="p_add">+		 * init_mm is reasonably fast, but remote flush IPIs</span>
<span class="p_add">+		 * as expensive as ever, so turn off lazy TLB mode.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * We can&#39;t do this in setup_pcid() because static keys</span>
<span class="p_add">+		 * haven&#39;t been initialized yet, and it would blow up</span>
<span class="p_add">+		 * badly.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		static_branch_disable(&amp;tlb_use_lazy_mode);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	debugfs_create_file(&quot;tlb_use_lazy_mode&quot;, S_IRUSR | S_IWUSR,</span>
<span class="p_add">+			    arch_debugfs_dir, NULL, &amp;fops_tlblazy);</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+late_initcall(init_tlb_use_lazy_mode);</span>

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



