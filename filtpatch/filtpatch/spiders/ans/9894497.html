
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[tip:x86/platform] x86/hyper-v: Use hypercall for remote TLB flush - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [tip:x86/platform] x86/hyper-v: Use hypercall for remote TLB flush</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=60001">tip-bot for Jacob Shin</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Aug. 10, 2017, 6:21 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;tip-2ffd9e33ce4af4e8cfa3e17bf493defe8474e2eb@git.kernel.org&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9894497/mbox/"
   >mbox</a>
|
   <a href="/patch/9894497/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9894497/">/patch/9894497/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	5330960384 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 10 Aug 2017 18:26:44 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 49B8A28B72
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 10 Aug 2017 18:26:44 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 3E48628B80; Thu, 10 Aug 2017 18:26:44 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 085A828B72
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 10 Aug 2017 18:26:43 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1753049AbdHJS0k (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 10 Aug 2017 14:26:40 -0400
Received: from terminus.zytor.com ([65.50.211.136]:42595 &quot;EHLO
	terminus.zytor.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1752713AbdHJS0i (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 10 Aug 2017 14:26:38 -0400
Received: from terminus.zytor.com (localhost [127.0.0.1])
	by terminus.zytor.com (8.15.2/8.15.2) with ESMTP id v7AILotP001337;
	Thu, 10 Aug 2017 11:21:50 -0700
Received: (from tipbot@localhost)
	by terminus.zytor.com (8.15.2/8.15.2/Submit) id v7AILnRc001331;
	Thu, 10 Aug 2017 11:21:49 -0700
Date: Thu, 10 Aug 2017 11:21:49 -0700
X-Authentication-Warning: terminus.zytor.com: tipbot set sender to
	tipbot@zytor.com using -f
From: tip-bot for Vitaly Kuznetsov &lt;tipbot@zytor.com&gt;
Message-ID: &lt;tip-2ffd9e33ce4af4e8cfa3e17bf493defe8474e2eb@git.kernel.org&gt;
Cc: haiyangz@microsoft.com, sixiao@microsoft.com,
	sthemmin@microsoft.com, Jork.Loeser@microsoft.com, luto@kernel.org,
	torvalds@linux-foundation.org, kys@microsoft.com, mingo@kernel.org,
	linux-kernel@vger.kernel.org, andy.shevchenko@gmail.com,
	rostedt@goodmis.org, hpa@zytor.com, vkuznets@redhat.com,
	peterz@infradead.org, tglx@linutronix.de
Reply-To: sixiao@microsoft.com, haiyangz@microsoft.com,
	Jork.Loeser@microsoft.com, sthemmin@microsoft.com,
	torvalds@linux-foundation.org, luto@kernel.org, hpa@zytor.com,
	vkuznets@redhat.com, linux-kernel@vger.kernel.org,
	rostedt@goodmis.org, andy.shevchenko@gmail.com,
	tglx@linutronix.de, peterz@infradead.org, kys@microsoft.com,
	mingo@kernel.org
In-Reply-To: &lt;20170802160921.21791-8-vkuznets@redhat.com&gt;
References: &lt;20170802160921.21791-8-vkuznets@redhat.com&gt;
To: linux-tip-commits@vger.kernel.org
Subject: [tip:x86/platform] x86/hyper-v: Use hypercall for remote TLB flush
Git-Commit-ID: 2ffd9e33ce4af4e8cfa3e17bf493defe8474e2eb
X-Mailer: tip-git-log-daemon
Robot-ID: &lt;tip-bot.git.kernel.org&gt;
Robot-Unsubscribe: Contact &lt;mailto:hpa@kernel.org&gt; to get blacklisted from
	these emails
MIME-Version: 1.0
Content-Transfer-Encoding: 8bit
Content-Type: text/plain; charset=UTF-8
Content-Disposition: inline
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=60001">tip-bot for Jacob Shin</a> - Aug. 10, 2017, 6:21 p.m.</div>
<pre class="content">
Commit-ID:  2ffd9e33ce4af4e8cfa3e17bf493defe8474e2eb
Gitweb:     http://git.kernel.org/tip/2ffd9e33ce4af4e8cfa3e17bf493defe8474e2eb
Author:     Vitaly Kuznetsov &lt;vkuznets@redhat.com&gt;
AuthorDate: Wed, 2 Aug 2017 18:09:19 +0200
Committer:  Ingo Molnar &lt;mingo@kernel.org&gt;
CommitDate: Thu, 10 Aug 2017 20:16:44 +0200

x86/hyper-v: Use hypercall for remote TLB flush

Hyper-V host can suggest us to use hypercall for doing remote TLB flush,
this is supposed to work faster than IPIs.

Implementation details: to do HvFlushVirtualAddress{Space,List} hypercalls
we need to put the input somewhere in memory and we don&#39;t really want to
have memory allocation on each call so we pre-allocate per cpu memory areas
on boot.

pv_ops patching is happening very early so we need to separate
hyperv_setup_mmu_ops() and hyper_alloc_mmu().

It is possible and easy to implement local TLB flushing too and there is
even a hint for that. However, I don&#39;t see a room for optimization on the
host side as both hypercall and native tlb flush will result in vmexit. The
hint is also not set on modern Hyper-V versions.
<span class="signed-off-by">
Signed-off-by: Vitaly Kuznetsov &lt;vkuznets@redhat.com&gt;</span>
<span class="reviewed-by">Reviewed-by: Andy Shevchenko &lt;andy.shevchenko@gmail.com&gt;</span>
<span class="reviewed-by">Reviewed-by: Stephen Hemminger &lt;sthemmin@microsoft.com&gt;</span>
Cc: Andy Lutomirski &lt;luto@kernel.org&gt;
Cc: Haiyang Zhang &lt;haiyangz@microsoft.com&gt;
Cc: Jork Loeser &lt;Jork.Loeser@microsoft.com&gt;
Cc: K. Y. Srinivasan &lt;kys@microsoft.com&gt;
Cc: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;
Cc: Peter Zijlstra &lt;peterz@infradead.org&gt;
Cc: Simon Xiao &lt;sixiao@microsoft.com&gt;
Cc: Steven Rostedt &lt;rostedt@goodmis.org&gt;
Cc: Thomas Gleixner &lt;tglx@linutronix.de&gt;
Cc: devel@linuxdriverproject.org
Link: http://lkml.kernel.org/r/20170802160921.21791-8-vkuznets@redhat.com
<span class="signed-off-by">Signed-off-by: Ingo Molnar &lt;mingo@kernel.org&gt;</span>
---
 arch/x86/hyperv/Makefile           |   2 +-
 arch/x86/hyperv/hv_init.c          |   2 +
 arch/x86/hyperv/mmu.c              | 138 +++++++++++++++++++++++++++++++++++++
 arch/x86/include/asm/mshyperv.h    |   3 +
 arch/x86/include/uapi/asm/hyperv.h |   7 ++
 arch/x86/kernel/cpu/mshyperv.c     |   1 +
 drivers/hv/Kconfig                 |   1 +
 7 files changed, 153 insertions(+), 1 deletion(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - Aug. 10, 2017, 6:56 p.m.</div>
<pre class="content">
On Thu, Aug 10, 2017 at 11:21:49AM -0700, tip-bot for Vitaly Kuznetsov wrote:
<span class="quote">&gt; Commit-ID:  2ffd9e33ce4af4e8cfa3e17bf493defe8474e2eb</span>
<span class="quote">&gt; Gitweb:     http://git.kernel.org/tip/2ffd9e33ce4af4e8cfa3e17bf493defe8474e2eb</span>
<span class="quote">&gt; Author:     Vitaly Kuznetsov &lt;vkuznets@redhat.com&gt;</span>
<span class="quote">&gt; AuthorDate: Wed, 2 Aug 2017 18:09:19 +0200</span>
<span class="quote">&gt; Committer:  Ingo Molnar &lt;mingo@kernel.org&gt;</span>
<span class="quote">&gt; CommitDate: Thu, 10 Aug 2017 20:16:44 +0200</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; x86/hyper-v: Use hypercall for remote TLB flush</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Hyper-V host can suggest us to use hypercall for doing remote TLB flush,</span>
<span class="quote">&gt; this is supposed to work faster than IPIs.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Implementation details: to do HvFlushVirtualAddress{Space,List} hypercalls</span>
<span class="quote">&gt; we need to put the input somewhere in memory and we don&#39;t really want to</span>
<span class="quote">&gt; have memory allocation on each call so we pre-allocate per cpu memory areas</span>
<span class="quote">&gt; on boot.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; pv_ops patching is happening very early so we need to separate</span>
<span class="quote">&gt; hyperv_setup_mmu_ops() and hyper_alloc_mmu().</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It is possible and easy to implement local TLB flushing too and there is</span>
<span class="quote">&gt; even a hint for that. However, I don&#39;t see a room for optimization on the</span>
<span class="quote">&gt; host side as both hypercall and native tlb flush will result in vmexit. The</span>
<span class="quote">&gt; hint is also not set on modern Hyper-V versions.</span>

Hold on.. if we don&#39;t IPI for TLB invalidation. What serializes our
software page table walkers like fast_gup() ?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=22291">K. Y. Srinivasan</a> - Aug. 10, 2017, 6:59 p.m.</div>
<pre class="content">
<span class="quote">&gt; -----Original Message-----</span>
<span class="quote">&gt; From: Peter Zijlstra [mailto:peterz@infradead.org]</span>
<span class="quote">&gt; Sent: Thursday, August 10, 2017 11:57 AM</span>
<span class="quote">&gt; To: Simon Xiao &lt;sixiao@microsoft.com&gt;; Haiyang Zhang</span>
<span class="quote">&gt; &lt;haiyangz@microsoft.com&gt;; Jork Loeser &lt;Jork.Loeser@microsoft.com&gt;;</span>
<span class="quote">&gt; Stephen Hemminger &lt;sthemmin@microsoft.com&gt;; torvalds@linux-</span>
<span class="quote">&gt; foundation.org; luto@kernel.org; hpa@zytor.com; vkuznets@redhat.com;</span>
<span class="quote">&gt; linux-kernel@vger.kernel.org; rostedt@goodmis.org;</span>
<span class="quote">&gt; andy.shevchenko@gmail.com; tglx@linutronix.de; KY Srinivasan</span>
<span class="quote">&gt; &lt;kys@microsoft.com&gt;; mingo@kernel.org</span>
<span class="quote">&gt; Cc: linux-tip-commits@vger.kernel.org</span>
<span class="quote">&gt; Subject: Re: [tip:x86/platform] x86/hyper-v: Use hypercall for remote TLB</span>
<span class="quote">&gt; flush</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; On Thu, Aug 10, 2017 at 11:21:49AM -0700, tip-bot for Vitaly Kuznetsov</span>
<span class="quote">&gt; wrote:</span>
<span class="quote">&gt; &gt; Commit-ID:  2ffd9e33ce4af4e8cfa3e17bf493defe8474e2eb</span>
<span class="quote">&gt; &gt; Gitweb:</span>
<span class="quote">&gt; https://na01.safelinks.protection.outlook.com/?url=http%3A%2F%2Fgit.kern</span>
<span class="quote">&gt; el.org%2Ftip%2F2ffd9e33ce4af4e8cfa3e17bf493defe8474e2eb&amp;data=02%7C</span>
<span class="quote">&gt; 01%7Ckys%40microsoft.com%7C2537372f38d3414e999e08d4e0218ec8%7C72</span>
<span class="quote">&gt; f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C636379882129411812&amp;sdata</span>
<span class="quote">&gt; =odsJ2NnQdD8LCEtDPfVf5rL%2F2sQX4fKUhlqVSjKhjCI%3D&amp;reserved=0</span>
<span class="quote">&gt; &gt; Author:     Vitaly Kuznetsov &lt;vkuznets@redhat.com&gt;</span>
<span class="quote">&gt; &gt; AuthorDate: Wed, 2 Aug 2017 18:09:19 +0200</span>
<span class="quote">&gt; &gt; Committer:  Ingo Molnar &lt;mingo@kernel.org&gt;</span>
<span class="quote">&gt; &gt; CommitDate: Thu, 10 Aug 2017 20:16:44 +0200</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; x86/hyper-v: Use hypercall for remote TLB flush</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Hyper-V host can suggest us to use hypercall for doing remote TLB flush,</span>
<span class="quote">&gt; &gt; this is supposed to work faster than IPIs.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Implementation details: to do HvFlushVirtualAddress{Space,List} hypercalls</span>
<span class="quote">&gt; &gt; we need to put the input somewhere in memory and we don&#39;t really want</span>
<span class="quote">&gt; to</span>
<span class="quote">&gt; &gt; have memory allocation on each call so we pre-allocate per cpu memory</span>
<span class="quote">&gt; areas</span>
<span class="quote">&gt; &gt; on boot.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; pv_ops patching is happening very early so we need to separate</span>
<span class="quote">&gt; &gt; hyperv_setup_mmu_ops() and hyper_alloc_mmu().</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; It is possible and easy to implement local TLB flushing too and there is</span>
<span class="quote">&gt; &gt; even a hint for that. However, I don&#39;t see a room for optimization on the</span>
<span class="quote">&gt; &gt; host side as both hypercall and native tlb flush will result in vmexit. The</span>
<span class="quote">&gt; &gt; hint is also not set on modern Hyper-V versions.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Hold on.. if we don&#39;t IPI for TLB invalidation. What serializes our</span>
<span class="quote">&gt; software page table walkers like fast_gup() ?</span>

Hypervisor may implement this functionality via an IPI.

K. Y
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=173691">Jork Loeser</a> - Aug. 10, 2017, 7:08 p.m.</div>
<pre class="content">
<span class="quote">&gt; -----Original Message-----</span>
<span class="quote">&gt; From: KY Srinivasan</span>
<span class="quote">

&gt; &gt; -----Original Message-----</span>
<span class="quote">&gt; &gt; From: Peter Zijlstra [mailto:peterz@infradead.org]</span>
<span class="quote">&gt; &gt; Sent: Thursday, August 10, 2017 11:57 AM</span>
<span class="quote">&gt; &gt; To: Simon Xiao &lt;sixiao@microsoft.com&gt;; Haiyang Zhang</span>
<span class="quote">&gt; &gt; &lt;haiyangz@microsoft.com&gt;; Jork Loeser &lt;Jork.Loeser@microsoft.com&gt;;</span>
<span class="quote">&gt; &gt; Stephen Hemminger &lt;sthemmin@microsoft.com&gt;; torvalds@linux-</span>
<span class="quote">&gt; &gt; foundation.org; luto@kernel.org; hpa@zytor.com; vkuznets@redhat.com;</span>
<span class="quote">&gt; &gt; linux-kernel@vger.kernel.org; rostedt@goodmis.org;</span>
<span class="quote">&gt; &gt; andy.shevchenko@gmail.com; tglx@linutronix.de; KY Srinivasan</span>
<span class="quote">&gt; &gt; &lt;kys@microsoft.com&gt;; mingo@kernel.org</span>
<span class="quote">&gt; &gt; Cc: linux-tip-commits@vger.kernel.org</span>
<span class="quote">&gt; &gt; Subject: Re: [tip:x86/platform] x86/hyper-v: Use hypercall for remote</span>
<span class="quote">&gt; &gt; TLB flush</span>
<span class="quote">
&gt; &gt; Hold on.. if we don&#39;t IPI for TLB invalidation. What serializes our</span>
<span class="quote">&gt; &gt; software page table walkers like fast_gup() ?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Hypervisor may implement this functionality via an IPI.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; K. Y</span>

HvFlushVirtualAddressList() states:
This call guarantees that by the time control returns back to the caller, the observable effects of all flushes on the specified virtual processors have occurred.

HvFlushVirtualAddressListEx() refers to HvFlushVirtualAddressList() as adding sparse target VP lists.

Is this enough of a guarantee, or do you see other races?

Regards,
Jork
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - Aug. 10, 2017, 7:27 p.m.</div>
<pre class="content">
On Thu, Aug 10, 2017 at 07:08:22PM +0000, Jork Loeser wrote:
<span class="quote">
&gt; &gt; &gt; Subject: Re: [tip:x86/platform] x86/hyper-v: Use hypercall for remote TLB flush</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; &gt; Hold on.. if we don&#39;t IPI for TLB invalidation. What serializes our</span>
<span class="quote">&gt; &gt; &gt; software page table walkers like fast_gup() ?</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Hypervisor may implement this functionality via an IPI.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; K. Y</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; HvFlushVirtualAddressList() states:</span>
<span class="quote">&gt; This call guarantees that by the time control returns back to the</span>
<span class="quote">&gt; caller, the observable effects of all flushes on the specified virtual</span>
<span class="quote">&gt; processors have occurred.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; HvFlushVirtualAddressListEx() refers to HvFlushVirtualAddressList() as adding sparse target VP lists.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Is this enough of a guarantee, or do you see other races?</span>

That&#39;s nowhere near enough. We need the remote CPU to have completed any
guest IF section that was in progress at the time of the call.

So if a host IPI can interrupt a guest while the guest has IF cleared,
and we then process the host IPI -- clear the TLBs -- before resuming the
guest, which still has IF cleared, we&#39;ve got a problem.

Because at that point, our software page-table walker, that relies on IF
being clear to guarantee the page-tables exist, because it holds off the
TLB invalidate and thereby the freeing of the pages, gets its pages
ripped out from under it.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=173691">Jork Loeser</a> - Aug. 11, 2017, 1:15 a.m.</div>
<pre class="content">
<span class="quote">&gt; -----Original Message-----</span>
<span class="quote">&gt; From: Peter Zijlstra [mailto:peterz@infradead.org]</span>
<span class="quote">&gt; Sent: Thursday, August 10, 2017 12:28</span>
<span class="quote">&gt; To: Jork Loeser &lt;Jork.Loeser@microsoft.com&gt;</span>
<span class="quote">&gt; Cc: KY Srinivasan &lt;kys@microsoft.com&gt;; Simon Xiao &lt;sixiao@microsoft.com&gt;;</span>
<span class="quote">&gt; Haiyang Zhang &lt;haiyangz@microsoft.com&gt;; Stephen Hemminger</span>
<span class="quote">&gt; &lt;sthemmin@microsoft.com&gt;; torvalds@linux-foundation.org; luto@kernel.org;</span>
<span class="quote">&gt; hpa@zytor.com; vkuznets@redhat.com; linux-kernel@vger.kernel.org;</span>
<span class="quote">&gt; rostedt@goodmis.org; andy.shevchenko@gmail.com; tglx@linutronix.de;</span>
<span class="quote">&gt; mingo@kernel.org; linux-tip-commits@vger.kernel.org</span>
<span class="quote">&gt; Subject: Re: [tip:x86/platform] x86/hyper-v: Use hypercall for remote TLB flush</span>
<span class="quote">
&gt; &gt; &gt; &gt; Hold on.. if we don&#39;t IPI for TLB invalidation. What serializes</span>
<span class="quote">&gt; &gt; &gt; &gt; our software page table walkers like fast_gup() ?</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; Hypervisor may implement this functionality via an IPI.</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; K. Y</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; HvFlushVirtualAddressList() states:</span>
<span class="quote">&gt; &gt; This call guarantees that by the time control returns back to the</span>
<span class="quote">&gt; &gt; caller, the observable effects of all flushes on the specified virtual</span>
<span class="quote">&gt; &gt; processors have occurred.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; HvFlushVirtualAddressListEx() refers to HvFlushVirtualAddressList() as adding</span>
<span class="quote">&gt; sparse target VP lists.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Is this enough of a guarantee, or do you see other races?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; That&#39;s nowhere near enough. We need the remote CPU to have completed any</span>
<span class="quote">&gt; guest IF section that was in progress at the time of the call.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So if a host IPI can interrupt a guest while the guest has IF cleared, and we then</span>
<span class="quote">&gt; process the host IPI -- clear the TLBs -- before resuming the guest, which still has</span>
<span class="quote">&gt; IF cleared, we&#39;ve got a problem.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Because at that point, our software page-table walker, that relies on IF being</span>
<span class="quote">&gt; clear to guarantee the page-tables exist, because it holds off the TLB invalidate</span>
<span class="quote">&gt; and thereby the freeing of the pages, gets its pages ripped out from under it.</span>

I see, IF is used as a locking mechanism for the pages. Would CONFIG_HAVE_RCU_TABLE_FREE be an option for x86? There are caveats (statically enabled, RCU for page-free), yet if the resulting perf is still a gain it would be worthwhile for Hyper-V targeted kernels.

Regards,
Jork
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - Aug. 11, 2017, 9:03 a.m.</div>
<pre class="content">
On Fri, Aug 11, 2017 at 01:15:18AM +0000, Jork Loeser wrote:
<span class="quote">
&gt; &gt; &gt; HvFlushVirtualAddressList() states:</span>
<span class="quote">&gt; &gt; &gt; This call guarantees that by the time control returns back to the</span>
<span class="quote">&gt; &gt; &gt; caller, the observable effects of all flushes on the specified virtual</span>
<span class="quote">&gt; &gt; &gt; processors have occurred.</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; HvFlushVirtualAddressListEx() refers to HvFlushVirtualAddressList() as adding</span>
<span class="quote">&gt; &gt; &gt; sparse target VP lists.</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; Is this enough of a guarantee, or do you see other races?</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; That&#39;s nowhere near enough. We need the remote CPU to have completed any</span>
<span class="quote">&gt; &gt; guest IF section that was in progress at the time of the call.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; So if a host IPI can interrupt a guest while the guest has IF cleared, and we then</span>
<span class="quote">&gt; &gt; process the host IPI -- clear the TLBs -- before resuming the guest, which still has</span>
<span class="quote">&gt; &gt; IF cleared, we&#39;ve got a problem.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Because at that point, our software page-table walker, that relies on IF being</span>
<span class="quote">&gt; &gt; clear to guarantee the page-tables exist, because it holds off the TLB invalidate</span>
<span class="quote">&gt; &gt; and thereby the freeing of the pages, gets its pages ripped out from under it.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I see, IF is used as a locking mechanism for the pages. Would</span>
<span class="quote">&gt; CONFIG_HAVE_RCU_TABLE_FREE be an option for x86? There are caveats</span>
<span class="quote">&gt; (statically enabled, RCU for page-free), yet if the resulting perf is</span>
<span class="quote">&gt; still a gain it would be worthwhile for Hyper-V targeted kernels.</span>

I&#39;m sure we talked about using HAVE_RCU_TABLE_FREE for x86 (and yes that
would make it work again), but this was some years ago and I cannot
readily find those emails.

Kirill would you have any opinions?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=99981">Vitaly Kuznetsov</a> - Aug. 11, 2017, 9:23 a.m.</div>
<pre class="content">
Peter Zijlstra &lt;peterz@infradead.org&gt; writes:
<span class="quote">
&gt; On Thu, Aug 10, 2017 at 07:08:22PM +0000, Jork Loeser wrote:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; &gt; &gt; Subject: Re: [tip:x86/platform] x86/hyper-v: Use hypercall for remote TLB flush</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; &gt; &gt; Hold on.. if we don&#39;t IPI for TLB invalidation. What serializes our</span>
<span class="quote">&gt;&gt; &gt; &gt; software page table walkers like fast_gup() ?</span>
<span class="quote">&gt;&gt; &gt; </span>
<span class="quote">&gt;&gt; &gt; Hypervisor may implement this functionality via an IPI.</span>
<span class="quote">&gt;&gt; &gt; </span>
<span class="quote">&gt;&gt; &gt; K. Y</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; HvFlushVirtualAddressList() states:</span>
<span class="quote">&gt;&gt; This call guarantees that by the time control returns back to the</span>
<span class="quote">&gt;&gt; caller, the observable effects of all flushes on the specified virtual</span>
<span class="quote">&gt;&gt; processors have occurred.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; HvFlushVirtualAddressListEx() refers to HvFlushVirtualAddressList() as adding sparse target VP lists.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; Is this enough of a guarantee, or do you see other races?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; That&#39;s nowhere near enough. We need the remote CPU to have completed any</span>
<span class="quote">&gt; guest IF section that was in progress at the time of the call.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; So if a host IPI can interrupt a guest while the guest has IF cleared,</span>
<span class="quote">&gt; and we then process the host IPI -- clear the TLBs -- before resuming the</span>
<span class="quote">&gt; guest, which still has IF cleared, we&#39;ve got a problem.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Because at that point, our software page-table walker, that relies on IF</span>
<span class="quote">&gt; being clear to guarantee the page-tables exist, because it holds off the</span>
<span class="quote">&gt; TLB invalidate and thereby the freeing of the pages, gets its pages</span>
<span class="quote">&gt; ripped out from under it.</span>

Oh, I see your concern. Hyper-V, however, is not the first x86
hypervisor trying to avoid IPIs on remote TLB flush, Xen does this
too. Briefly looking at xen_flush_tlb_others() I don&#39;t see anything
special, do we know how serialization is achieved there?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - Aug. 11, 2017, 10:56 a.m.</div>
<pre class="content">
On Fri, Aug 11, 2017 at 11:23:10AM +0200, Vitaly Kuznetsov wrote:
<span class="quote">&gt; Peter Zijlstra &lt;peterz@infradead.org&gt; writes:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; On Thu, Aug 10, 2017 at 07:08:22PM +0000, Jork Loeser wrote:</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; &gt; &gt; Subject: Re: [tip:x86/platform] x86/hyper-v: Use hypercall for remote TLB flush</span>
<span class="quote">&gt; &gt;&gt; </span>
<span class="quote">&gt; &gt;&gt; &gt; &gt; Hold on.. if we don&#39;t IPI for TLB invalidation. What serializes our</span>
<span class="quote">&gt; &gt;&gt; &gt; &gt; software page table walkers like fast_gup() ?</span>
<span class="quote">&gt; &gt;&gt; &gt; </span>
<span class="quote">&gt; &gt;&gt; &gt; Hypervisor may implement this functionality via an IPI.</span>
<span class="quote">&gt; &gt;&gt; &gt; </span>
<span class="quote">&gt; &gt;&gt; &gt; K. Y</span>
<span class="quote">&gt; &gt;&gt; </span>
<span class="quote">&gt; &gt;&gt; HvFlushVirtualAddressList() states:</span>
<span class="quote">&gt; &gt;&gt; This call guarantees that by the time control returns back to the</span>
<span class="quote">&gt; &gt;&gt; caller, the observable effects of all flushes on the specified virtual</span>
<span class="quote">&gt; &gt;&gt; processors have occurred.</span>
<span class="quote">&gt; &gt;&gt; </span>
<span class="quote">&gt; &gt;&gt; HvFlushVirtualAddressListEx() refers to HvFlushVirtualAddressList() as adding sparse target VP lists.</span>
<span class="quote">&gt; &gt;&gt; </span>
<span class="quote">&gt; &gt;&gt; Is this enough of a guarantee, or do you see other races?</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; That&#39;s nowhere near enough. We need the remote CPU to have completed any</span>
<span class="quote">&gt; &gt; guest IF section that was in progress at the time of the call.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; So if a host IPI can interrupt a guest while the guest has IF cleared,</span>
<span class="quote">&gt; &gt; and we then process the host IPI -- clear the TLBs -- before resuming the</span>
<span class="quote">&gt; &gt; guest, which still has IF cleared, we&#39;ve got a problem.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Because at that point, our software page-table walker, that relies on IF</span>
<span class="quote">&gt; &gt; being clear to guarantee the page-tables exist, because it holds off the</span>
<span class="quote">&gt; &gt; TLB invalidate and thereby the freeing of the pages, gets its pages</span>
<span class="quote">&gt; &gt; ripped out from under it.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Oh, I see your concern. Hyper-V, however, is not the first x86</span>
<span class="quote">&gt; hypervisor trying to avoid IPIs on remote TLB flush, Xen does this</span>
<span class="quote">&gt; too. Briefly looking at xen_flush_tlb_others() I don&#39;t see anything</span>
<span class="quote">&gt; special, do we know how serialization is achieved there?</span>

No idea on how Xen works, I always just hope it goes away :-) But lets
ask some Xen folks.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=53881">Andrew Cooper</a> - Aug. 11, 2017, 11:05 a.m.</div>
<pre class="content">
On 11/08/17 11:56, Peter Zijlstra wrote:
<span class="quote">&gt; On Fri, Aug 11, 2017 at 11:23:10AM +0200, Vitaly Kuznetsov wrote:</span>
<span class="quote">&gt;&gt; Peter Zijlstra &lt;peterz@infradead.org&gt; writes:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; On Thu, Aug 10, 2017 at 07:08:22PM +0000, Jork Loeser wrote:</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; Subject: Re: [tip:x86/platform] x86/hyper-v: Use hypercall for remote TLB flush</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; Hold on.. if we don&#39;t IPI for TLB invalidation. What serializes our</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; software page table walkers like fast_gup() ?</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; Hypervisor may implement this functionality via an IPI.</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; K. Y</span>
<span class="quote">&gt;&gt;&gt;&gt; HvFlushVirtualAddressList() states:</span>
<span class="quote">&gt;&gt;&gt;&gt; This call guarantees that by the time control returns back to the</span>
<span class="quote">&gt;&gt;&gt;&gt; caller, the observable effects of all flushes on the specified virtual</span>
<span class="quote">&gt;&gt;&gt;&gt; processors have occurred.</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; HvFlushVirtualAddressListEx() refers to HvFlushVirtualAddressList() as adding sparse target VP lists.</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; Is this enough of a guarantee, or do you see other races?</span>
<span class="quote">&gt;&gt;&gt; That&#39;s nowhere near enough. We need the remote CPU to have completed any</span>
<span class="quote">&gt;&gt;&gt; guest IF section that was in progress at the time of the call.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; So if a host IPI can interrupt a guest while the guest has IF cleared,</span>
<span class="quote">&gt;&gt;&gt; and we then process the host IPI -- clear the TLBs -- before resuming the</span>
<span class="quote">&gt;&gt;&gt; guest, which still has IF cleared, we&#39;ve got a problem.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Because at that point, our software page-table walker, that relies on IF</span>
<span class="quote">&gt;&gt;&gt; being clear to guarantee the page-tables exist, because it holds off the</span>
<span class="quote">&gt;&gt;&gt; TLB invalidate and thereby the freeing of the pages, gets its pages</span>
<span class="quote">&gt;&gt;&gt; ripped out from under it.</span>
<span class="quote">&gt;&gt; Oh, I see your concern. Hyper-V, however, is not the first x86</span>
<span class="quote">&gt;&gt; hypervisor trying to avoid IPIs on remote TLB flush, Xen does this</span>
<span class="quote">&gt;&gt; too. Briefly looking at xen_flush_tlb_others() I don&#39;t see anything</span>
<span class="quote">&gt;&gt; special, do we know how serialization is achieved there?</span>
<span class="quote">&gt; No idea on how Xen works, I always just hope it goes away :-) But lets</span>
<span class="quote">&gt; ask some Xen folks.</span>

How is the software pagewalker relying on IF being clear safe at all (on
native, let alone under virtualisation)?  Hardware has no architectural
requirement to keep entries in the TLB.

In the virtualisation case, at any point the vcpu can be scheduled on a
different pcpu even during a critical region like that, so the TLB
really can empty itself under your feet.

~Andrew
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=874">Kirill A. Shutemov</a> - Aug. 11, 2017, 11:29 a.m.</div>
<pre class="content">
On Fri, Aug 11, 2017 at 11:03:36AM +0200, Peter Zijlstra wrote:
<span class="quote">&gt; On Fri, Aug 11, 2017 at 01:15:18AM +0000, Jork Loeser wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; HvFlushVirtualAddressList() states:</span>
<span class="quote">&gt; &gt; &gt; &gt; This call guarantees that by the time control returns back to the</span>
<span class="quote">&gt; &gt; &gt; &gt; caller, the observable effects of all flushes on the specified virtual</span>
<span class="quote">&gt; &gt; &gt; &gt; processors have occurred.</span>
<span class="quote">&gt; &gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; &gt; HvFlushVirtualAddressListEx() refers to HvFlushVirtualAddressList() as adding</span>
<span class="quote">&gt; &gt; &gt; &gt; sparse target VP lists.</span>
<span class="quote">&gt; &gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; &gt; Is this enough of a guarantee, or do you see other races?</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; That&#39;s nowhere near enough. We need the remote CPU to have completed any</span>
<span class="quote">&gt; &gt; &gt; guest IF section that was in progress at the time of the call.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; So if a host IPI can interrupt a guest while the guest has IF cleared, and we then</span>
<span class="quote">&gt; &gt; &gt; process the host IPI -- clear the TLBs -- before resuming the guest, which still has</span>
<span class="quote">&gt; &gt; &gt; IF cleared, we&#39;ve got a problem.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Because at that point, our software page-table walker, that relies on IF being</span>
<span class="quote">&gt; &gt; &gt; clear to guarantee the page-tables exist, because it holds off the TLB invalidate</span>
<span class="quote">&gt; &gt; &gt; and thereby the freeing of the pages, gets its pages ripped out from under it.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I see, IF is used as a locking mechanism for the pages. Would</span>
<span class="quote">&gt; &gt; CONFIG_HAVE_RCU_TABLE_FREE be an option for x86? There are caveats</span>
<span class="quote">&gt; &gt; (statically enabled, RCU for page-free), yet if the resulting perf is</span>
<span class="quote">&gt; &gt; still a gain it would be worthwhile for Hyper-V targeted kernels.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I&#39;m sure we talked about using HAVE_RCU_TABLE_FREE for x86 (and yes that</span>
<span class="quote">&gt; would make it work again), but this was some years ago and I cannot</span>
<span class="quote">&gt; readily find those emails.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Kirill would you have any opinions?</span>

I guess we can try this. The main question is what would be performance
implications of such move.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - Aug. 11, 2017, 12:07 p.m.</div>
<pre class="content">
On Fri, Aug 11, 2017 at 12:05:45PM +0100, Andrew Cooper wrote:
<span class="quote">&gt; &gt;&gt; Oh, I see your concern. Hyper-V, however, is not the first x86</span>
<span class="quote">&gt; &gt;&gt; hypervisor trying to avoid IPIs on remote TLB flush, Xen does this</span>
<span class="quote">&gt; &gt;&gt; too. Briefly looking at xen_flush_tlb_others() I don&#39;t see anything</span>
<span class="quote">&gt; &gt;&gt; special, do we know how serialization is achieved there?</span>
<span class="quote">&gt; &gt; No idea on how Xen works, I always just hope it goes away :-) But lets</span>
<span class="quote">&gt; &gt; ask some Xen folks.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; How is the software pagewalker relying on IF being clear safe at all (on</span>
<span class="quote">&gt; native, let alone under virtualisation)?  Hardware has no architectural</span>
<span class="quote">&gt; requirement to keep entries in the TLB.</span>

No, but it _can_, therefore when we unhook pages we _must_ invalidate.

It goes like:

	CPU0			CPU1

	unhook page
				cli
				traverse page tables
	TLB invalidate ---&gt;	&lt;IF clear, therefore CPU0 waits&gt;
				sti
				&lt;IPI&gt;
				 TLB invalidate
			&lt;------	 complete
				&lt;/IPI&gt;
	free page

So the CPU1 page-table walker gets an existence guarantee of the
page-tables by clearing IF.
<span class="quote">
&gt; In the virtualisation case, at any point the vcpu can be scheduled on a</span>
<span class="quote">&gt; different pcpu even during a critical region like that, so the TLB</span>
<span class="quote">&gt; really can empty itself under your feet.</span>

Not the point.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=100191">Juergen Gross</a> - Aug. 11, 2017, 12:22 p.m.</div>
<pre class="content">
On 11/08/17 12:56, Peter Zijlstra wrote:
<span class="quote">&gt; On Fri, Aug 11, 2017 at 11:23:10AM +0200, Vitaly Kuznetsov wrote:</span>
<span class="quote">&gt;&gt; Peter Zijlstra &lt;peterz@infradead.org&gt; writes:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; On Thu, Aug 10, 2017 at 07:08:22PM +0000, Jork Loeser wrote:</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; Subject: Re: [tip:x86/platform] x86/hyper-v: Use hypercall for remote TLB flush</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; Hold on.. if we don&#39;t IPI for TLB invalidation. What serializes our</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; software page table walkers like fast_gup() ?</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; Hypervisor may implement this functionality via an IPI.</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; K. Y</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; HvFlushVirtualAddressList() states:</span>
<span class="quote">&gt;&gt;&gt;&gt; This call guarantees that by the time control returns back to the</span>
<span class="quote">&gt;&gt;&gt;&gt; caller, the observable effects of all flushes on the specified virtual</span>
<span class="quote">&gt;&gt;&gt;&gt; processors have occurred.</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; HvFlushVirtualAddressListEx() refers to HvFlushVirtualAddressList() as adding sparse target VP lists.</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; Is this enough of a guarantee, or do you see other races?</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; That&#39;s nowhere near enough. We need the remote CPU to have completed any</span>
<span class="quote">&gt;&gt;&gt; guest IF section that was in progress at the time of the call.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; So if a host IPI can interrupt a guest while the guest has IF cleared,</span>
<span class="quote">&gt;&gt;&gt; and we then process the host IPI -- clear the TLBs -- before resuming the</span>
<span class="quote">&gt;&gt;&gt; guest, which still has IF cleared, we&#39;ve got a problem.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Because at that point, our software page-table walker, that relies on IF</span>
<span class="quote">&gt;&gt;&gt; being clear to guarantee the page-tables exist, because it holds off the</span>
<span class="quote">&gt;&gt;&gt; TLB invalidate and thereby the freeing of the pages, gets its pages</span>
<span class="quote">&gt;&gt;&gt; ripped out from under it.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Oh, I see your concern. Hyper-V, however, is not the first x86</span>
<span class="quote">&gt;&gt; hypervisor trying to avoid IPIs on remote TLB flush, Xen does this</span>
<span class="quote">&gt;&gt; too. Briefly looking at xen_flush_tlb_others() I don&#39;t see anything</span>
<span class="quote">&gt;&gt; special, do we know how serialization is achieved there?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; No idea on how Xen works, I always just hope it goes away :-) But lets</span>
<span class="quote">&gt; ask some Xen folks.</span>

Wait - the TLB can be cleared at any time, as Andrew was pointing out.
No cpu can rely on an address being accessible just because IF is being
cleared. All that matters is the existing and valid page table entry.

So clearing IF on a cpu isn&#39;t meant to secure the TLB from being
cleared, but just to avoid interrupts (as the name of the flag is
suggesting).

In the Xen case the hypervisor does the following:

- it checks whether any of the vcpus specified in the cpumask of the
  flush request is running on any physical cpu
- if any running vcpu is found an IPI will be sent to the physical cpu
  and the hypervisor will do the TLB flush there
- any vcpu addressed by the flush and not running will be flagged to
  flush its TLB when being scheduled the next time

This ensures no TLB entry to be flushed can be used after return of
xen_flush_tlb_others().


Juergen
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - Aug. 11, 2017, 12:35 p.m.</div>
<pre class="content">
On Fri, Aug 11, 2017 at 02:22:25PM +0200, Juergen Gross wrote:
<span class="quote">&gt; Wait - the TLB can be cleared at any time, as Andrew was pointing out.</span>
<span class="quote">&gt; No cpu can rely on an address being accessible just because IF is being</span>
<span class="quote">&gt; cleared. All that matters is the existing and valid page table entry.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So clearing IF on a cpu isn&#39;t meant to secure the TLB from being</span>
<span class="quote">&gt; cleared, but just to avoid interrupts (as the name of the flag is</span>
<span class="quote">&gt; suggesting).</span>

Yes, but by holding off the TLB invalidate IPI, we hold off the freeing
of the concurrently unhooked page-table.
<span class="quote">
&gt; In the Xen case the hypervisor does the following:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; - it checks whether any of the vcpus specified in the cpumask of the</span>
<span class="quote">&gt;   flush request is running on any physical cpu</span>
<span class="quote">&gt; - if any running vcpu is found an IPI will be sent to the physical cpu</span>
<span class="quote">&gt;   and the hypervisor will do the TLB flush there</span>

And this will preempt a vcpu which could have IF cleared, right?
<span class="quote">
&gt; - any vcpu addressed by the flush and not running will be flagged to</span>
<span class="quote">&gt;   flush its TLB when being scheduled the next time</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This ensures no TLB entry to be flushed can be used after return of</span>
<span class="quote">&gt; xen_flush_tlb_others().</span>

But that is not a sufficient guarantee. We need the IF to hold off the
TLB invalidate and thereby hold off the freeing of our page-table pages.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=100191">Juergen Gross</a> - Aug. 11, 2017, 12:46 p.m.</div>
<pre class="content">
On 11/08/17 14:35, Peter Zijlstra wrote:
<span class="quote">&gt; On Fri, Aug 11, 2017 at 02:22:25PM +0200, Juergen Gross wrote:</span>
<span class="quote">&gt;&gt; Wait - the TLB can be cleared at any time, as Andrew was pointing out.</span>
<span class="quote">&gt;&gt; No cpu can rely on an address being accessible just because IF is being</span>
<span class="quote">&gt;&gt; cleared. All that matters is the existing and valid page table entry.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; So clearing IF on a cpu isn&#39;t meant to secure the TLB from being</span>
<span class="quote">&gt;&gt; cleared, but just to avoid interrupts (as the name of the flag is</span>
<span class="quote">&gt;&gt; suggesting).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Yes, but by holding off the TLB invalidate IPI, we hold off the freeing</span>
<span class="quote">&gt; of the concurrently unhooked page-table.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; In the Xen case the hypervisor does the following:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; - it checks whether any of the vcpus specified in the cpumask of the</span>
<span class="quote">&gt;&gt;   flush request is running on any physical cpu</span>
<span class="quote">&gt;&gt; - if any running vcpu is found an IPI will be sent to the physical cpu</span>
<span class="quote">&gt;&gt;   and the hypervisor will do the TLB flush there</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; And this will preempt a vcpu which could have IF cleared, right?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; - any vcpu addressed by the flush and not running will be flagged to</span>
<span class="quote">&gt;&gt;   flush its TLB when being scheduled the next time</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; This ensures no TLB entry to be flushed can be used after return of</span>
<span class="quote">&gt;&gt; xen_flush_tlb_others().</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; But that is not a sufficient guarantee. We need the IF to hold off the</span>
<span class="quote">&gt; TLB invalidate and thereby hold off the freeing of our page-table pages.</span>

Aah, okay. Now I understand the problem. The TLB isn&#39;t the issue but the
IPI is serving two purposes here: TLB flushing (which is allowed to
happen at any time) and serialization regarding access to critical pages
(which seems to be broken in the Xen case as you suggest).

Juergen
<span class="quote">
&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - Aug. 11, 2017, 12:54 p.m.</div>
<pre class="content">
On Fri, Aug 11, 2017 at 02:46:41PM +0200, Juergen Gross wrote:
<span class="quote">&gt; Aah, okay. Now I understand the problem. The TLB isn&#39;t the issue but the</span>
<span class="quote">&gt; IPI is serving two purposes here: TLB flushing (which is allowed to</span>
<span class="quote">&gt; happen at any time) and serialization regarding access to critical pages</span>
<span class="quote">&gt; (which seems to be broken in the Xen case as you suggest).</span>

Indeed, and now hyper-v as well.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=100191">Juergen Gross</a> - Aug. 11, 2017, 1:07 p.m.</div>
<pre class="content">
On 11/08/17 14:54, Peter Zijlstra wrote:
<span class="quote">&gt; On Fri, Aug 11, 2017 at 02:46:41PM +0200, Juergen Gross wrote:</span>
<span class="quote">&gt;&gt; Aah, okay. Now I understand the problem. The TLB isn&#39;t the issue but the</span>
<span class="quote">&gt;&gt; IPI is serving two purposes here: TLB flushing (which is allowed to</span>
<span class="quote">&gt;&gt; happen at any time) and serialization regarding access to critical pages</span>
<span class="quote">&gt;&gt; (which seems to be broken in the Xen case as you suggest).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Indeed, and now hyper-v as well.</span>

Is it possible to distinguish between non-critical calls of
flush_tlb_others() (which should be the majority IMHO) and critical ones
regarding above problem? I guess the only problem is the case when a
page table can be freed because its last valid entry is gone, right?

We might want to add a serialization flag to indicate flushing _and_
serialization via IPI should be performed.


Juergen
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - Aug. 11, 2017, 1:39 p.m.</div>
<pre class="content">
On Fri, Aug 11, 2017 at 03:07:29PM +0200, Juergen Gross wrote:
<span class="quote">&gt; On 11/08/17 14:54, Peter Zijlstra wrote:</span>
<span class="quote">&gt; &gt; On Fri, Aug 11, 2017 at 02:46:41PM +0200, Juergen Gross wrote:</span>
<span class="quote">&gt; &gt;&gt; Aah, okay. Now I understand the problem. The TLB isn&#39;t the issue but the</span>
<span class="quote">&gt; &gt;&gt; IPI is serving two purposes here: TLB flushing (which is allowed to</span>
<span class="quote">&gt; &gt;&gt; happen at any time) and serialization regarding access to critical pages</span>
<span class="quote">&gt; &gt;&gt; (which seems to be broken in the Xen case as you suggest).</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Indeed, and now hyper-v as well.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Is it possible to distinguish between non-critical calls of</span>
<span class="quote">&gt; flush_tlb_others() (which should be the majority IMHO) and critical ones</span>
<span class="quote">&gt; regarding above problem? I guess the only problem is the case when a</span>
<span class="quote">&gt; page table can be freed because its last valid entry is gone, right?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; We might want to add a serialization flag to indicate flushing _and_</span>
<span class="quote">&gt; serialization via IPI should be performed.</span>

Possible, but not trivial. Esp things like transparent huge pages, which
swizzles PMDs around makes things tricky.

The by far easiest solution is to switch over to HAVE_RCU_TABLE_FREE
when either Xen or Hyper-V is doing this. Ideally it would not have a
significant performance hit (needs testing) and we can simply always do
this when PARAVIRT, or otherwise we need to get creative with
static_keys or something.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=77">Linus Torvalds</a> - Aug. 11, 2017, 4:16 p.m.</div>
<pre class="content">
On Fri, Aug 11, 2017 at 2:03 AM, Peter Zijlstra &lt;peterz@infradead.org&gt; wrote:
<span class="quote">&gt;</span>
<span class="quote">&gt; I&#39;m sure we talked about using HAVE_RCU_TABLE_FREE for x86 (and yes that</span>
<span class="quote">&gt; would make it work again), but this was some years ago and I cannot</span>
<span class="quote">&gt; readily find those emails.</span>

I think the only time we really talked about HAVE_RCU_TABLE_FREE for
x86 (at least that I was cc&#39;d on) was not because of RCU freeing, but
because we just wanted to use the generic page table lookup code on
x86 *despite* not using RCU freeing.

And we just ended up renaming HAVE_GENERIC_RCU_GUP as HAVE_GENERIC_GUP.

There was only passing mention of maybe making x86 use RCU, but the
discussion was really about why the IF flag meant that x86 didn&#39;t need
to, iirc.

I don&#39;t recall us ever discussing *really* making x86 use RCU.

                    Linus
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - Aug. 11, 2017, 4:26 p.m.</div>
<pre class="content">
On Fri, Aug 11, 2017 at 09:16:29AM -0700, Linus Torvalds wrote:
<span class="quote">&gt; On Fri, Aug 11, 2017 at 2:03 AM, Peter Zijlstra &lt;peterz@infradead.org&gt; wrote:</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; I&#39;m sure we talked about using HAVE_RCU_TABLE_FREE for x86 (and yes that</span>
<span class="quote">&gt; &gt; would make it work again), but this was some years ago and I cannot</span>
<span class="quote">&gt; &gt; readily find those emails.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I think the only time we really talked about HAVE_RCU_TABLE_FREE for</span>
<span class="quote">&gt; x86 (at least that I was cc&#39;d on) was not because of RCU freeing, but</span>
<span class="quote">&gt; because we just wanted to use the generic page table lookup code on</span>
<span class="quote">&gt; x86 *despite* not using RCU freeing.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; And we just ended up renaming HAVE_GENERIC_RCU_GUP as HAVE_GENERIC_GUP.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; There was only passing mention of maybe making x86 use RCU, but the</span>
<span class="quote">&gt; discussion was really about why the IF flag meant that x86 didn&#39;t need</span>
<span class="quote">&gt; to, iirc.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I don&#39;t recall us ever discussing *really* making x86 use RCU.</span>

Google finds me this:

  https://lwn.net/Articles/500188/

Which includes:

  http://www.mail-archive.com/kvm@vger.kernel.org/msg72918.html

which does as was suggested here, selects HAVE_RCU_TABLE_FREE for
PARAVIRT_TLB_FLUSH.

But yes, this is very much virt specific nonsense, native would never
need this.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=99981">Vitaly Kuznetsov</a> - Aug. 14, 2017, 1:20 p.m.</div>
<pre class="content">
Peter Zijlstra &lt;peterz@infradead.org&gt; writes:
<span class="quote">
&gt; On Fri, Aug 11, 2017 at 09:16:29AM -0700, Linus Torvalds wrote:</span>
<span class="quote">&gt;&gt; On Fri, Aug 11, 2017 at 2:03 AM, Peter Zijlstra &lt;peterz@infradead.org&gt; wrote:</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; I&#39;m sure we talked about using HAVE_RCU_TABLE_FREE for x86 (and yes that</span>
<span class="quote">&gt;&gt; &gt; would make it work again), but this was some years ago and I cannot</span>
<span class="quote">&gt;&gt; &gt; readily find those emails.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; I think the only time we really talked about HAVE_RCU_TABLE_FREE for</span>
<span class="quote">&gt;&gt; x86 (at least that I was cc&#39;d on) was not because of RCU freeing, but</span>
<span class="quote">&gt;&gt; because we just wanted to use the generic page table lookup code on</span>
<span class="quote">&gt;&gt; x86 *despite* not using RCU freeing.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; And we just ended up renaming HAVE_GENERIC_RCU_GUP as HAVE_GENERIC_GUP.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; There was only passing mention of maybe making x86 use RCU, but the</span>
<span class="quote">&gt;&gt; discussion was really about why the IF flag meant that x86 didn&#39;t need</span>
<span class="quote">&gt;&gt; to, iirc.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; I don&#39;t recall us ever discussing *really* making x86 use RCU.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Google finds me this:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;   https://lwn.net/Articles/500188/</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Which includes:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;   http://www.mail-archive.com/kvm@vger.kernel.org/msg72918.html</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; which does as was suggested here, selects HAVE_RCU_TABLE_FREE for</span>
<span class="quote">&gt; PARAVIRT_TLB_FLUSH.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; But yes, this is very much virt specific nonsense, native would never</span>
<span class="quote">&gt; need this.</span>

In case we decide to go HAVE_RCU_TABLE_FREE for all PARAVIRT-enabled
kernels (as it seems to be the easiest/fastest way to fix Xen PV) - what
do you think about the required testing? Any suggestion for a
specifically crafted micro benchmark in addition to standard
ebizzy/kernbench/...?

Additionally, I see another option for us: enable &#39;rcu table free&#39; on
boot (e.g. by taking tlb_remove_table to pv_ops and doing boot-time
patching for it) so bare metal and other hypervisors are not affected
by the change.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=310">Steven Rostedt</a> - Aug. 16, 2017, 12:02 a.m.</div>
<pre class="content">
On Fri, 11 Aug 2017 14:07:14 +0200
Peter Zijlstra &lt;peterz@infradead.org&gt; wrote:
<span class="quote">
&gt; It goes like:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	CPU0			CPU1</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	unhook page</span>
<span class="quote">&gt; 				cli</span>
<span class="quote">&gt; 				traverse page tables</span>
<span class="quote">&gt; 	TLB invalidate ---&gt;	&lt;IF clear, therefore CPU0 waits&gt;</span>
<span class="quote">&gt; 				sti</span>
<span class="quote">&gt; 				&lt;IPI&gt;</span>
<span class="quote">&gt; 				 TLB invalidate</span>
<span class="quote">&gt; 			&lt;------	 complete</span>

I guess the important part here is the above &quot;complete&quot;. CPU0 doesn&#39;t
proceed until its receives it. Thus it does act like
cli~rcu_read_lock(), sti~rcu_read_unlock(), and &quot;TLB invalidate&quot; is
equivalent to synchronize_rcu().

[ this response is for clarification for the casual observer of this
  thread ;-) ]

-- Steve
<span class="quote">
&gt; 				&lt;/IPI&gt;</span>
<span class="quote">&gt; 	free page</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So the CPU1 page-table walker gets an existence guarantee of the</span>
<span class="quote">&gt; page-tables by clearing IF.</span>
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/hyperv/Makefile b/arch/x86/hyperv/Makefile</span>
<span class="p_header">index 171ae09..367a820 100644</span>
<span class="p_header">--- a/arch/x86/hyperv/Makefile</span>
<span class="p_header">+++ b/arch/x86/hyperv/Makefile</span>
<span class="p_chunk">@@ -1 +1 @@</span> <span class="p_context"></span>
<span class="p_del">-obj-y		:= hv_init.o</span>
<span class="p_add">+obj-y		:= hv_init.o mmu.o</span>
<span class="p_header">diff --git a/arch/x86/hyperv/hv_init.c b/arch/x86/hyperv/hv_init.c</span>
<span class="p_header">index e93b9a0..1a8eb55 100644</span>
<span class="p_header">--- a/arch/x86/hyperv/hv_init.c</span>
<span class="p_header">+++ b/arch/x86/hyperv/hv_init.c</span>
<span class="p_chunk">@@ -140,6 +140,8 @@</span> <span class="p_context"> void hyperv_init(void)</span>
 	hypercall_msr.guest_physical_address = vmalloc_to_pfn(hv_hypercall_pg);
 	wrmsrl(HV_X64_MSR_HYPERCALL, hypercall_msr.as_uint64);
 
<span class="p_add">+	hyper_alloc_mmu();</span>
<span class="p_add">+</span>
 	/*
 	 * Register Hyper-V specific clocksource.
 	 */
<span class="p_header">diff --git a/arch/x86/hyperv/mmu.c b/arch/x86/hyperv/mmu.c</span>
new file mode 100644
<span class="p_header">index 0000000..9419a20</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/x86/hyperv/mmu.c</span>
<span class="p_chunk">@@ -0,0 +1,138 @@</span> <span class="p_context"></span>
<span class="p_add">+#define pr_fmt(fmt)  &quot;Hyper-V: &quot; fmt</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/hyperv.h&gt;</span>
<span class="p_add">+#include &lt;linux/log2.h&gt;</span>
<span class="p_add">+#include &lt;linux/slab.h&gt;</span>
<span class="p_add">+#include &lt;linux/types.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/fpu/api.h&gt;</span>
<span class="p_add">+#include &lt;asm/mshyperv.h&gt;</span>
<span class="p_add">+#include &lt;asm/msr.h&gt;</span>
<span class="p_add">+#include &lt;asm/tlbflush.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+/* HvFlushVirtualAddressSpace, HvFlushVirtualAddressList hypercalls */</span>
<span class="p_add">+struct hv_flush_pcpu {</span>
<span class="p_add">+	u64 address_space;</span>
<span class="p_add">+	u64 flags;</span>
<span class="p_add">+	u64 processor_mask;</span>
<span class="p_add">+	u64 gva_list[];</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+/* Each gva in gva_list encodes up to 4096 pages to flush */</span>
<span class="p_add">+#define HV_TLB_FLUSH_UNIT (4096 * PAGE_SIZE)</span>
<span class="p_add">+</span>
<span class="p_add">+static struct hv_flush_pcpu __percpu *pcpu_flush;</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Fills in gva_list starting from offset. Returns the number of items added.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline int fill_gva_list(u64 gva_list[], int offset,</span>
<span class="p_add">+				unsigned long start, unsigned long end)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int gva_n = offset;</span>
<span class="p_add">+	unsigned long cur = start, diff;</span>
<span class="p_add">+</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		diff = end &gt; cur ? end - cur : 0;</span>
<span class="p_add">+</span>
<span class="p_add">+		gva_list[gva_n] = cur &amp; PAGE_MASK;</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Lower 12 bits encode the number of additional</span>
<span class="p_add">+		 * pages to flush (in addition to the &#39;cur&#39; page).</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (diff &gt;= HV_TLB_FLUSH_UNIT)</span>
<span class="p_add">+			gva_list[gva_n] |= ~PAGE_MASK;</span>
<span class="p_add">+		else if (diff)</span>
<span class="p_add">+			gva_list[gva_n] |= (diff - 1) &gt;&gt; PAGE_SHIFT;</span>
<span class="p_add">+</span>
<span class="p_add">+		cur += HV_TLB_FLUSH_UNIT;</span>
<span class="p_add">+		gva_n++;</span>
<span class="p_add">+</span>
<span class="p_add">+	} while (cur &lt; end);</span>
<span class="p_add">+</span>
<span class="p_add">+	return gva_n - offset;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void hyperv_flush_tlb_others(const struct cpumask *cpus,</span>
<span class="p_add">+				    const struct flush_tlb_info *info)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int cpu, vcpu, gva_n, max_gvas;</span>
<span class="p_add">+	struct hv_flush_pcpu *flush;</span>
<span class="p_add">+	u64 status = U64_MAX;</span>
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!pcpu_flush || !hv_hypercall_pg)</span>
<span class="p_add">+		goto do_native;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (cpumask_empty(cpus))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	local_irq_save(flags);</span>
<span class="p_add">+</span>
<span class="p_add">+	flush = this_cpu_ptr(pcpu_flush);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (info-&gt;mm) {</span>
<span class="p_add">+		flush-&gt;address_space = virt_to_phys(info-&gt;mm-&gt;pgd);</span>
<span class="p_add">+		flush-&gt;flags = 0;</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		flush-&gt;address_space = 0;</span>
<span class="p_add">+		flush-&gt;flags = HV_FLUSH_ALL_VIRTUAL_ADDRESS_SPACES;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	flush-&gt;processor_mask = 0;</span>
<span class="p_add">+	if (cpumask_equal(cpus, cpu_present_mask)) {</span>
<span class="p_add">+		flush-&gt;flags |= HV_FLUSH_ALL_PROCESSORS;</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		for_each_cpu(cpu, cpus) {</span>
<span class="p_add">+			vcpu = hv_cpu_number_to_vp_number(cpu);</span>
<span class="p_add">+			if (vcpu &gt;= 64)</span>
<span class="p_add">+				goto do_native;</span>
<span class="p_add">+</span>
<span class="p_add">+			__set_bit(vcpu, (unsigned long *)</span>
<span class="p_add">+				  &amp;flush-&gt;processor_mask);</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We can flush not more than max_gvas with one hypercall. Flush the</span>
<span class="p_add">+	 * whole address space if we were asked to do more.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	max_gvas = (PAGE_SIZE - sizeof(*flush)) / sizeof(flush-&gt;gva_list[0]);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (info-&gt;end == TLB_FLUSH_ALL) {</span>
<span class="p_add">+		flush-&gt;flags |= HV_FLUSH_NON_GLOBAL_MAPPINGS_ONLY;</span>
<span class="p_add">+		status = hv_do_hypercall(HVCALL_FLUSH_VIRTUAL_ADDRESS_SPACE,</span>
<span class="p_add">+					 flush, NULL);</span>
<span class="p_add">+	} else if (info-&gt;end &amp;&amp;</span>
<span class="p_add">+		   ((info-&gt;end - info-&gt;start)/HV_TLB_FLUSH_UNIT) &gt; max_gvas) {</span>
<span class="p_add">+		status = hv_do_hypercall(HVCALL_FLUSH_VIRTUAL_ADDRESS_SPACE,</span>
<span class="p_add">+					 flush, NULL);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		gva_n = fill_gva_list(flush-&gt;gva_list, 0,</span>
<span class="p_add">+				      info-&gt;start, info-&gt;end);</span>
<span class="p_add">+		status = hv_do_rep_hypercall(HVCALL_FLUSH_VIRTUAL_ADDRESS_LIST,</span>
<span class="p_add">+					     gva_n, 0, flush, NULL);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	local_irq_restore(flags);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!(status &amp; HV_HYPERCALL_RESULT_MASK))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+do_native:</span>
<span class="p_add">+	native_flush_tlb_others(cpus, info);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void hyperv_setup_mmu_ops(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (ms_hyperv.hints &amp; HV_X64_REMOTE_TLB_FLUSH_RECOMMENDED) {</span>
<span class="p_add">+		pr_info(&quot;Using hypercall for remote TLB flush\n&quot;);</span>
<span class="p_add">+		pv_mmu_ops.flush_tlb_others = hyperv_flush_tlb_others;</span>
<span class="p_add">+		setup_clear_cpu_cap(X86_FEATURE_PCID);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void hyper_alloc_mmu(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (ms_hyperv.hints &amp; HV_X64_REMOTE_TLB_FLUSH_RECOMMENDED)</span>
<span class="p_add">+		pcpu_flush = __alloc_percpu(PAGE_SIZE, PAGE_SIZE);</span>
<span class="p_add">+}</span>
<span class="p_header">diff --git a/arch/x86/include/asm/mshyperv.h b/arch/x86/include/asm/mshyperv.h</span>
<span class="p_header">index efd2f80..0d4b01c 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/mshyperv.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/mshyperv.h</span>
<span class="p_chunk">@@ -307,6 +307,8 @@</span> <span class="p_context"> static inline int hv_cpu_number_to_vp_number(int cpu_number)</span>
 }
 
 void hyperv_init(void);
<span class="p_add">+void hyperv_setup_mmu_ops(void);</span>
<span class="p_add">+void hyper_alloc_mmu(void);</span>
 void hyperv_report_panic(struct pt_regs *regs);
 bool hv_is_hypercall_page_setup(void);
 void hyperv_cleanup(void);
<span class="p_chunk">@@ -314,6 +316,7 @@</span> <span class="p_context"> void hyperv_cleanup(void);</span>
 static inline void hyperv_init(void) {}
 static inline bool hv_is_hypercall_page_setup(void) { return false; }
 static inline void hyperv_cleanup(void) {}
<span class="p_add">+static inline void hyperv_setup_mmu_ops(void) {}</span>
 #endif /* CONFIG_HYPERV */
 
 #ifdef CONFIG_HYPERV_TSCPAGE
<span class="p_header">diff --git a/arch/x86/include/uapi/asm/hyperv.h b/arch/x86/include/uapi/asm/hyperv.h</span>
<span class="p_header">index 127ddad..a6fdd3b 100644</span>
<span class="p_header">--- a/arch/x86/include/uapi/asm/hyperv.h</span>
<span class="p_header">+++ b/arch/x86/include/uapi/asm/hyperv.h</span>
<span class="p_chunk">@@ -242,6 +242,8 @@</span> <span class="p_context"></span>
 		(~((1ull &lt;&lt; HV_X64_MSR_HYPERCALL_PAGE_ADDRESS_SHIFT) - 1))
 
 /* Declare the various hypercall operations. */
<span class="p_add">+#define HVCALL_FLUSH_VIRTUAL_ADDRESS_SPACE	0x0002</span>
<span class="p_add">+#define HVCALL_FLUSH_VIRTUAL_ADDRESS_LIST	0x0003</span>
 #define HVCALL_NOTIFY_LONG_SPIN_WAIT		0x0008
 #define HVCALL_POST_MESSAGE			0x005c
 #define HVCALL_SIGNAL_EVENT			0x005d
<span class="p_chunk">@@ -259,6 +261,11 @@</span> <span class="p_context"></span>
 #define HV_PROCESSOR_POWER_STATE_C2		2
 #define HV_PROCESSOR_POWER_STATE_C3		3
 
<span class="p_add">+#define HV_FLUSH_ALL_PROCESSORS			BIT(0)</span>
<span class="p_add">+#define HV_FLUSH_ALL_VIRTUAL_ADDRESS_SPACES	BIT(1)</span>
<span class="p_add">+#define HV_FLUSH_NON_GLOBAL_MAPPINGS_ONLY	BIT(2)</span>
<span class="p_add">+#define HV_FLUSH_USE_EXTENDED_RANGE_FORMAT	BIT(3)</span>
<span class="p_add">+</span>
 /* hypercall status code */
 #define HV_STATUS_SUCCESS			0
 #define HV_STATUS_INVALID_HYPERCALL_CODE	2
<span class="p_header">diff --git a/arch/x86/kernel/cpu/mshyperv.c b/arch/x86/kernel/cpu/mshyperv.c</span>
<span class="p_header">index 70e717f..daefd67 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/mshyperv.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/mshyperv.c</span>
<span class="p_chunk">@@ -249,6 +249,7 @@</span> <span class="p_context"> static void __init ms_hyperv_init_platform(void)</span>
 	 * Setup the hook to get control post apic initialization.
 	 */
 	x86_platform.apic_post_init = hyperv_init;
<span class="p_add">+	hyperv_setup_mmu_ops();</span>
 #endif
 }
 
<span class="p_header">diff --git a/drivers/hv/Kconfig b/drivers/hv/Kconfig</span>
<span class="p_header">index c29cd53..50b89ea 100644</span>
<span class="p_header">--- a/drivers/hv/Kconfig</span>
<span class="p_header">+++ b/drivers/hv/Kconfig</span>
<span class="p_chunk">@@ -3,6 +3,7 @@</span> <span class="p_context"> menu &quot;Microsoft Hyper-V guest support&quot;</span>
 config HYPERV
 	tristate &quot;Microsoft Hyper-V client drivers&quot;
 	depends on X86 &amp;&amp; ACPI &amp;&amp; PCI &amp;&amp; X86_LOCAL_APIC &amp;&amp; HYPERVISOR_GUEST
<span class="p_add">+	select PARAVIRT</span>
 	help
 	  Select this option to run Linux as a Hyper-V client operating
 	  system.

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



