
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[RFC,4/4] KVM: arm64: Add support for PUD hugepages at stage 2 - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [RFC,4/4] KVM: arm64: Add support for PUD hugepages at stage 2</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=58591">Punit Agrawal</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Jan. 10, 2018, 7:07 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20180110190729.18383-5-punit.agrawal@arm.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10155923/mbox/"
   >mbox</a>
|
   <a href="/patch/10155923/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10155923/">/patch/10155923/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	EC5F0601A1 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 10 Jan 2018 19:08:43 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id E140526B41
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 10 Jan 2018 19:08:43 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id D596628578; Wed, 10 Jan 2018 19:08:43 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 3409C26B41
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 10 Jan 2018 19:08:42 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1753271AbeAJTIk (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Wed, 10 Jan 2018 14:08:40 -0500
Received: from usa-sjc-mx-foss1.foss.arm.com ([217.140.101.70]:47608 &quot;EHLO
	foss.arm.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1751395AbeAJTIi (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Wed, 10 Jan 2018 14:08:38 -0500
Received: from usa-sjc-imap-foss1.foss.arm.com (unknown [10.72.51.249])
	by usa-sjc-mx-foss1.foss.arm.com (Postfix) with ESMTP id 678DA15A2;
	Wed, 10 Jan 2018 11:08:38 -0800 (PST)
Received: from localhost (e105922-lin.cambridge.arm.com [10.1.207.29])
	by usa-sjc-imap-foss1.foss.arm.com (Postfix) with ESMTPSA id
	0B6CD3F581; Wed, 10 Jan 2018 11:08:37 -0800 (PST)
From: Punit Agrawal &lt;punit.agrawal@arm.com&gt;
To: kvmarm@lists.cs.columbia.edu
Cc: Punit Agrawal &lt;punit.agrawal@arm.com&gt;,
	linux-arm-kernel@lists.infradead.org, linux-kernel@vger.kernel.org,
	suzuki.poulose@arm.com, Marc Zyngier &lt;marc.zyngier@arm.com&gt;,
	Christoffer Dall &lt;christoffer.dall@linaro.org&gt;,
	Catalin Marinas &lt;catalin.marinas@arm.com&gt;
Subject: [RFC 4/4] KVM: arm64: Add support for PUD hugepages at stage 2
Date: Wed, 10 Jan 2018 19:07:29 +0000
Message-Id: &lt;20180110190729.18383-5-punit.agrawal@arm.com&gt;
X-Mailer: git-send-email 2.15.1
In-Reply-To: &lt;20180110190729.18383-1-punit.agrawal@arm.com&gt;
References: &lt;20180110190729.18383-1-punit.agrawal@arm.com&gt;
X-ARM-No-Footer: FoSSMail
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=58591">Punit Agrawal</a> - Jan. 10, 2018, 7:07 p.m.</div>
<pre class="content">
KVM only supports PMD hugepages at stage 2. Extend the stage 2 fault
handling to add support for PUD hugepages.

Addition of PUD hugpage support enables additional hugepage sizes (1G
with 4K granule and 4TB with 64k granule) which can be useful on cores
that have support for mapping larger block sizes in the TLB entries.
<span class="signed-off-by">
Signed-off-by: Punit Agrawal &lt;punit.agrawal@arm.com&gt;</span>
Cc: Marc Zyngier &lt;marc.zyngier@arm.com&gt;
Cc: Christoffer Dall &lt;christoffer.dall@linaro.org&gt;
Cc: Catalin Marinas &lt;catalin.marinas@arm.com&gt;
---
 arch/arm/include/asm/kvm_mmu.h         | 10 +++++
 arch/arm/include/asm/pgtable-3level.h  |  2 +
 arch/arm64/include/asm/kvm_mmu.h       | 19 +++++++++
 arch/arm64/include/asm/pgtable-hwdef.h |  2 +
 arch/arm64/include/asm/pgtable.h       |  4 ++
 virt/kvm/arm/mmu.c                     | 72 +++++++++++++++++++++++++++++-----
 6 files changed, 99 insertions(+), 10 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=68151">Christoffer Dall</a> - Feb. 6, 2018, 2:55 p.m.</div>
<pre class="content">
On Wed, Jan 10, 2018 at 07:07:29PM +0000, Punit Agrawal wrote:
<span class="quote">&gt; KVM only supports PMD hugepages at stage 2. Extend the stage 2 fault</span>
<span class="quote">&gt; handling to add support for PUD hugepages.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Addition of PUD hugpage support enables additional hugepage sizes (1G</span>

                 *hugepage
<span class="quote">
&gt; with 4K granule and 4TB with 64k granule) which can be useful on cores</span>
<span class="quote">&gt; that have support for mapping larger block sizes in the TLB entries.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Punit Agrawal &lt;punit.agrawal@arm.com&gt;</span>
<span class="quote">&gt; Cc: Marc Zyngier &lt;marc.zyngier@arm.com&gt;</span>
<span class="quote">&gt; Cc: Christoffer Dall &lt;christoffer.dall@linaro.org&gt;</span>
<span class="quote">&gt; Cc: Catalin Marinas &lt;catalin.marinas@arm.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  arch/arm/include/asm/kvm_mmu.h         | 10 +++++</span>
<span class="quote">&gt;  arch/arm/include/asm/pgtable-3level.h  |  2 +</span>
<span class="quote">&gt;  arch/arm64/include/asm/kvm_mmu.h       | 19 +++++++++</span>
<span class="quote">&gt;  arch/arm64/include/asm/pgtable-hwdef.h |  2 +</span>
<span class="quote">&gt;  arch/arm64/include/asm/pgtable.h       |  4 ++</span>
<span class="quote">&gt;  virt/kvm/arm/mmu.c                     | 72 +++++++++++++++++++++++++++++-----</span>
<span class="quote">&gt;  6 files changed, 99 insertions(+), 10 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/arch/arm/include/asm/kvm_mmu.h b/arch/arm/include/asm/kvm_mmu.h</span>
<span class="quote">&gt; index 3fbe919b9181..6e2e34348cb3 100644</span>
<span class="quote">&gt; --- a/arch/arm/include/asm/kvm_mmu.h</span>
<span class="quote">&gt; +++ b/arch/arm/include/asm/kvm_mmu.h</span>
<span class="quote">&gt; @@ -59,6 +59,10 @@ phys_addr_t kvm_get_idmap_vector(void);</span>
<span class="quote">&gt;  int kvm_mmu_init(void);</span>
<span class="quote">&gt;  void kvm_clear_hyp_idmap(void);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +static inline void kvm_set_pud(pud_t *pud, pud_t new_pud)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  static inline void kvm_set_pmd(pmd_t *pmd, pmd_t new_pmd)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	*pmd = new_pmd;</span>
<span class="quote">&gt; @@ -230,6 +234,12 @@ static inline unsigned int kvm_get_vmid_bits(void)</span>
<span class="quote">&gt;  	return 8;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +static inline pud_t stage2_build_pud(kvm_pfn_t pfn, pgprot_t mem_type,</span>
<span class="quote">&gt; +				     bool writable)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return __pud(0);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  #endif	/* !__ASSEMBLY__ */</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #endif /* __ARM_KVM_MMU_H__ */</span>
<span class="quote">&gt; diff --git a/arch/arm/include/asm/pgtable-3level.h b/arch/arm/include/asm/pgtable-3level.h</span>
<span class="quote">&gt; index 1a7a17b2a1ba..97e04fdbfa85 100644</span>
<span class="quote">&gt; --- a/arch/arm/include/asm/pgtable-3level.h</span>
<span class="quote">&gt; +++ b/arch/arm/include/asm/pgtable-3level.h</span>
<span class="quote">&gt; @@ -249,6 +249,8 @@ PMD_BIT_FUNC(mkyoung,   |= PMD_SECT_AF);</span>
<span class="quote">&gt;  #define pfn_pmd(pfn,prot)	(__pmd(((phys_addr_t)(pfn) &lt;&lt; PAGE_SHIFT) | pgprot_val(prot)))</span>
<span class="quote">&gt;  #define mk_pmd(page,prot)	pfn_pmd(page_to_pfn(page),prot)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +#define pud_pfn(pud)		(((pud_val(pud) &amp; PUD_MASK) &amp; PHYS_MASK) &gt;&gt; PAGE_SHIFT)</span>
<span class="quote">&gt; +</span>

does this make sense on 32-bit arm?  Is this ever going to get called
and return something meaningful in that case?
<span class="quote">
&gt;  /* represent a notpresent pmd by faulting entry, this is used by pmdp_invalidate */</span>
<span class="quote">&gt;  static inline pmd_t pmd_mknotpresent(pmd_t pmd)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; diff --git a/arch/arm64/include/asm/kvm_mmu.h b/arch/arm64/include/asm/kvm_mmu.h</span>
<span class="quote">&gt; index dbfd18e08cfb..89eac3dbe123 100644</span>
<span class="quote">&gt; --- a/arch/arm64/include/asm/kvm_mmu.h</span>
<span class="quote">&gt; +++ b/arch/arm64/include/asm/kvm_mmu.h</span>
<span class="quote">&gt; @@ -160,6 +160,7 @@ void kvm_clear_hyp_idmap(void);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #define	kvm_set_pte(ptep, pte)		set_pte(ptep, pte)</span>
<span class="quote">&gt;  #define	kvm_set_pmd(pmdp, pmd)		set_pmd(pmdp, pmd)</span>
<span class="quote">&gt; +#define kvm_set_pud(pudp, pud)		set_pud(pudp, pud)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static inline pte_t kvm_s2pte_mkwrite(pte_t pte)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; @@ -173,6 +174,12 @@ static inline pmd_t kvm_s2pmd_mkwrite(pmd_t pmd)</span>
<span class="quote">&gt;  	return pmd;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +static inline pud_t kvm_s2pud_mkwrite(pud_t pud)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	pud_val(pud) |= PUD_S2_RDWR;</span>
<span class="quote">&gt; +	return pud;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  static inline void kvm_set_s2pte_readonly(pte_t *pte)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	pteval_t old_pteval, pteval;</span>
<span class="quote">&gt; @@ -319,5 +326,17 @@ static inline unsigned int kvm_get_vmid_bits(void)</span>
<span class="quote">&gt;  	return (cpuid_feature_extract_unsigned_field(reg, ID_AA64MMFR1_VMIDBITS_SHIFT) == 2) ? 16 : 8;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +static inline pud_t stage2_build_pud(kvm_pfn_t pfn, pgprot_t mem_type,</span>
<span class="quote">&gt; +				     bool writable)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	pud_t pud = pfn_pud(pfn, mem_type);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	pud = pud_mkhuge(pud);</span>
<span class="quote">&gt; +	if (writable)</span>
<span class="quote">&gt; +		pud = kvm_s2pud_mkwrite(pud);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return pud;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  #endif /* __ASSEMBLY__ */</span>
<span class="quote">&gt;  #endif /* __ARM64_KVM_MMU_H__ */</span>
<span class="quote">&gt; diff --git a/arch/arm64/include/asm/pgtable-hwdef.h b/arch/arm64/include/asm/pgtable-hwdef.h</span>
<span class="quote">&gt; index 40a998cdd399..a091a6192eee 100644</span>
<span class="quote">&gt; --- a/arch/arm64/include/asm/pgtable-hwdef.h</span>
<span class="quote">&gt; +++ b/arch/arm64/include/asm/pgtable-hwdef.h</span>
<span class="quote">&gt; @@ -181,6 +181,8 @@</span>
<span class="quote">&gt;  #define PMD_S2_RDONLY		(_AT(pmdval_t, 1) &lt;&lt; 6)   /* HAP[2:1] */</span>
<span class="quote">&gt;  #define PMD_S2_RDWR		(_AT(pmdval_t, 3) &lt;&lt; 6)   /* HAP[2:1] */</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +#define PUD_S2_RDWR		(_AT(pudval_t, 3) &lt;&lt; 6)   /* HAP[2:1] */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt;   * Memory Attribute override for Stage-2 (MemAttr[3:0])</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt; diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h</span>
<span class="quote">&gt; index bdcc7f1c9d06..d5ffff4369d2 100644</span>
<span class="quote">&gt; --- a/arch/arm64/include/asm/pgtable.h</span>
<span class="quote">&gt; +++ b/arch/arm64/include/asm/pgtable.h</span>
<span class="quote">&gt; @@ -362,7 +362,11 @@ static inline int pmd_protnone(pmd_t pmd)</span>
<span class="quote">&gt;  #define mk_pmd(page,prot)	pfn_pmd(page_to_pfn(page),prot)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #define pud_write(pud)		pte_write(pud_pte(pud))</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define pud_mkhuge(pud)		(__pud(pud_val(pud) &amp; ~PUD_TABLE_BIT))</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  #define pud_pfn(pud)		(((pud_val(pud) &amp; PUD_MASK) &amp; PHYS_MASK) &gt;&gt; PAGE_SHIFT)</span>
<span class="quote">&gt; +#define pfn_pud(pfn, prot)	(__pud(((phys_addr_t)(pfn) &lt;&lt; PAGE_SHIFT) | pgprot_val(prot)))</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #define set_pmd_at(mm, addr, pmdp, pmd)	set_pte_at(mm, addr, (pte_t *)pmdp, pmd_pte(pmd))</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/virt/kvm/arm/mmu.c b/virt/kvm/arm/mmu.c</span>
<span class="quote">&gt; index f02219a91b19..5362de098768 100644</span>
<span class="quote">&gt; --- a/virt/kvm/arm/mmu.c</span>
<span class="quote">&gt; +++ b/virt/kvm/arm/mmu.c</span>
<span class="quote">&gt; @@ -872,6 +872,32 @@ static pud_t *stage2_get_pud(struct kvm *kvm, struct kvm_mmu_memory_cache *cache</span>
<span class="quote">&gt;  	return stage2_pud_offset(pgd, addr);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +static int stage2_set_pud_huge(struct kvm *kvm, struct kvm_mmu_memory_cache</span>
<span class="quote">&gt; +			       *cache, phys_addr_t addr, const pud_t *new_pud)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	pud_t *pud, old_pud;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	pud = stage2_get_pud(kvm, cache, addr);</span>
<span class="quote">&gt; +	VM_BUG_ON(!pud);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * Mapping in huge pages should only happen through a fault.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	VM_BUG_ON(stage2_pud_present(*pud) &amp;&amp;</span>
<span class="quote">&gt; +		  pud_pfn(*pud) != pud_pfn(*new_pud));</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	old_pud = *pud;</span>
<span class="quote">&gt; +	if (stage2_pud_present(old_pud)) {</span>
<span class="quote">&gt; +		stage2_pud_clear(pud);</span>
<span class="quote">&gt; +		kvm_tlb_flush_vmid_ipa(kvm, addr);</span>
<span class="quote">&gt; +	} else {</span>
<span class="quote">&gt; +		get_page(virt_to_page(pud));</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	kvm_set_pud(pud, *new_pud);</span>
<span class="quote">&gt; +	return 0;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  static pmd_t *stage2_get_pmd(struct kvm *kvm, struct kvm_mmu_memory_cache *cache,</span>
<span class="quote">&gt;  			     phys_addr_t addr)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; @@ -1307,6 +1333,7 @@ static int user_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa,</span>
<span class="quote">&gt;  	kvm_pfn_t pfn;</span>
<span class="quote">&gt;  	pgprot_t mem_type = PAGE_S2;</span>
<span class="quote">&gt;  	bool logging_active = memslot_is_logging(memslot);</span>
<span class="quote">&gt; +	unsigned long vma_pagesize;</span>
<span class="quote">&gt;  	unsigned long flags = 0;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	write_fault = kvm_is_write_fault(vcpu);</span>
<span class="quote">&gt; @@ -1324,9 +1351,13 @@ static int user_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa,</span>
<span class="quote">&gt;  		return -EFAULT;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (vma_kernel_pagesize(vma) == PMD_SIZE &amp;&amp; !logging_active) {</span>
<span class="quote">&gt; +	vma_pagesize = vma_kernel_pagesize(vma);</span>
<span class="quote">&gt; +	if ((vma_pagesize == PMD_SIZE || vma_pagesize == PUD_SIZE) &amp;&amp;</span>
<span class="quote">&gt; +	    !logging_active) {</span>
<span class="quote">&gt; +		struct hstate *h = hstate_vma(vma);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  		hugetlb = true;</span>
<span class="quote">&gt; -		gfn = (fault_ipa &amp; PMD_MASK) &gt;&gt; PAGE_SHIFT;</span>
<span class="quote">&gt; +		gfn = (fault_ipa &amp; huge_page_mask(h)) &gt;&gt; PAGE_SHIFT;</span>
<span class="quote">&gt;  	} else {</span>
<span class="quote">&gt;  		/*</span>
<span class="quote">&gt;  		 * Pages belonging to memslots that don&#39;t have the same</span>
<span class="quote">&gt; @@ -1393,17 +1424,38 @@ static int user_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa,</span>
<span class="quote">&gt;  	if (mmu_notifier_retry(kvm, mmu_seq))</span>
<span class="quote">&gt;  		goto out_unlock;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (!hugetlb &amp;&amp; !force_pte)</span>
<span class="quote">&gt; +	if (!hugetlb &amp;&amp; !force_pte) {</span>
<span class="quote">&gt; +		/*</span>
<span class="quote">&gt; +		 * We only support PMD_SIZE transparent</span>
<span class="quote">&gt; +		 * hugepages. This code will need updates if we enable</span>
<span class="quote">&gt; +		 * other page sizes for THP.</span>
<span class="quote">&gt; +		 */</span>
<span class="quote">&gt;  		hugetlb = transparent_hugepage_adjust(&amp;pfn, &amp;fault_ipa);</span>
<span class="quote">&gt; +		vma_pagesize = PMD_SIZE;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (hugetlb) {</span>
<span class="quote">&gt; -		pmd_t new_pmd = stage2_build_pmd(pfn, mem_type, writable);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -		if (writable)</span>
<span class="quote">&gt; -			kvm_set_pfn_dirty(pfn);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -		coherent_cache_guest_page(vcpu, pfn, PMD_SIZE);</span>
<span class="quote">&gt; -		ret = stage2_set_pmd_huge(kvm, memcache, fault_ipa, &amp;new_pmd);</span>
<span class="quote">&gt; +		if (vma_pagesize == PUD_SIZE) {</span>
<span class="quote">&gt; +			pud_t new_pud;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			new_pud = stage2_build_pud(pfn, mem_type, writable);</span>
<span class="quote">&gt; +			if (writable)</span>
<span class="quote">&gt; +				kvm_set_pfn_dirty(pfn);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			coherent_cache_guest_page(vcpu, pfn, PUD_SIZE);</span>
<span class="quote">&gt; +			ret = stage2_set_pud_huge(kvm, memcache,</span>
<span class="quote">&gt; +						  fault_ipa, &amp;new_pud);</span>
<span class="quote">&gt; +		} else {</span>
<span class="quote">&gt; +			pmd_t new_pmd;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			new_pmd = stage2_build_pmd(pfn, mem_type, writable);</span>
<span class="quote">&gt; +			if (writable)</span>
<span class="quote">&gt; +				kvm_set_pfn_dirty(pfn);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			coherent_cache_guest_page(vcpu, pfn, PMD_SIZE);</span>
<span class="quote">&gt; +			ret = stage2_set_pmd_huge(kvm, memcache,</span>
<span class="quote">&gt; +						  fault_ipa, &amp;new_pmd);</span>
<span class="quote">&gt; +		}</span>

This stuff needs rebasing onto v4.16-rc1 when we get there, and it will
clash with Marc&#39;s icache optimizations.

But, you should be able to move kvm_set_pfn_dirty() out of the
size-conditional section and also call the cache maintenance functions
using vma_pagesize as parameter.
<span class="quote">
&gt;  	} else {</span>
<span class="quote">&gt;  		pte_t new_pte = pfn_pte(pfn, mem_type);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -- </span>
<span class="quote">&gt; 2.15.1</span>
<span class="quote">&gt; </span>

Thanks,
-Christoffer
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=58591">Punit Agrawal</a> - Feb. 6, 2018, 6:13 p.m.</div>
<pre class="content">
Christoffer Dall &lt;christoffer.dall@linaro.org&gt; writes:
<span class="quote">
&gt; On Wed, Jan 10, 2018 at 07:07:29PM +0000, Punit Agrawal wrote:</span>
<span class="quote">&gt;&gt; KVM only supports PMD hugepages at stage 2. Extend the stage 2 fault</span>
<span class="quote">&gt;&gt; handling to add support for PUD hugepages.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; Addition of PUD hugpage support enables additional hugepage sizes (1G</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;                  *hugepage</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; with 4K granule and 4TB with 64k granule) which can be useful on cores</span>
<span class="quote">&gt;&gt; that have support for mapping larger block sizes in the TLB entries.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; Signed-off-by: Punit Agrawal &lt;punit.agrawal@arm.com&gt;</span>
<span class="quote">&gt;&gt; Cc: Marc Zyngier &lt;marc.zyngier@arm.com&gt;</span>
<span class="quote">&gt;&gt; Cc: Christoffer Dall &lt;christoffer.dall@linaro.org&gt;</span>
<span class="quote">&gt;&gt; Cc: Catalin Marinas &lt;catalin.marinas@arm.com&gt;</span>
<span class="quote">&gt;&gt; ---</span>
<span class="quote">&gt;&gt;  arch/arm/include/asm/kvm_mmu.h         | 10 +++++</span>
<span class="quote">&gt;&gt;  arch/arm/include/asm/pgtable-3level.h  |  2 +</span>
<span class="quote">&gt;&gt;  arch/arm64/include/asm/kvm_mmu.h       | 19 +++++++++</span>
<span class="quote">&gt;&gt;  arch/arm64/include/asm/pgtable-hwdef.h |  2 +</span>
<span class="quote">&gt;&gt;  arch/arm64/include/asm/pgtable.h       |  4 ++</span>
<span class="quote">&gt;&gt;  virt/kvm/arm/mmu.c                     | 72 +++++++++++++++++++++++++++++-----</span>
<span class="quote">&gt;&gt;  6 files changed, 99 insertions(+), 10 deletions(-)</span>
<span class="quote">&gt;&gt; </span>

[...]
<span class="quote">
&gt;&gt; diff --git a/arch/arm/include/asm/pgtable-3level.h b/arch/arm/include/asm/pgtable-3level.h</span>
<span class="quote">&gt;&gt; index 1a7a17b2a1ba..97e04fdbfa85 100644</span>
<span class="quote">&gt;&gt; --- a/arch/arm/include/asm/pgtable-3level.h</span>
<span class="quote">&gt;&gt; +++ b/arch/arm/include/asm/pgtable-3level.h</span>
<span class="quote">&gt;&gt; @@ -249,6 +249,8 @@ PMD_BIT_FUNC(mkyoung,   |= PMD_SECT_AF);</span>
<span class="quote">&gt;&gt;  #define pfn_pmd(pfn,prot)	(__pmd(((phys_addr_t)(pfn) &lt;&lt; PAGE_SHIFT) | pgprot_val(prot)))</span>
<span class="quote">&gt;&gt;  #define mk_pmd(page,prot)	pfn_pmd(page_to_pfn(page),prot)</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; +#define pud_pfn(pud)		(((pud_val(pud) &amp; PUD_MASK) &amp; PHYS_MASK) &gt;&gt; PAGE_SHIFT)</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; does this make sense on 32-bit arm?  Is this ever going to get called</span>
<span class="quote">&gt; and return something meaningful in that case?</span>

This macro should never get called as there are no PUD_SIZE hugepages on
arm.

Ideally we want to fold the pud to fallback to pgd like in the rest of
the code. I&#39;ll have another go at this.
<span class="quote">
&gt;</span>
<span class="quote">&gt;&gt;  /* represent a notpresent pmd by faulting entry, this is used by pmdp_invalidate */</span>
<span class="quote">&gt;&gt;  static inline pmd_t pmd_mknotpresent(pmd_t pmd)</span>
<span class="quote">&gt;&gt;  {</span>

[...]
<span class="quote">

&gt;&gt; @@ -1393,17 +1424,38 @@ static int user_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa,</span>
<span class="quote">&gt;&gt;  	if (mmu_notifier_retry(kvm, mmu_seq))</span>
<span class="quote">&gt;&gt;  		goto out_unlock;</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; -	if (!hugetlb &amp;&amp; !force_pte)</span>
<span class="quote">&gt;&gt; +	if (!hugetlb &amp;&amp; !force_pte) {</span>
<span class="quote">&gt;&gt; +		/*</span>
<span class="quote">&gt;&gt; +		 * We only support PMD_SIZE transparent</span>
<span class="quote">&gt;&gt; +		 * hugepages. This code will need updates if we enable</span>
<span class="quote">&gt;&gt; +		 * other page sizes for THP.</span>
<span class="quote">&gt;&gt; +		 */</span>
<span class="quote">&gt;&gt;  		hugetlb = transparent_hugepage_adjust(&amp;pfn, &amp;fault_ipa);</span>
<span class="quote">&gt;&gt; +		vma_pagesize = PMD_SIZE;</span>
<span class="quote">&gt;&gt; +	}</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  	if (hugetlb) {</span>
<span class="quote">&gt;&gt; -		pmd_t new_pmd = stage2_build_pmd(pfn, mem_type, writable);</span>
<span class="quote">&gt;&gt; -</span>
<span class="quote">&gt;&gt; -		if (writable)</span>
<span class="quote">&gt;&gt; -			kvm_set_pfn_dirty(pfn);</span>
<span class="quote">&gt;&gt; -</span>
<span class="quote">&gt;&gt; -		coherent_cache_guest_page(vcpu, pfn, PMD_SIZE);</span>
<span class="quote">&gt;&gt; -		ret = stage2_set_pmd_huge(kvm, memcache, fault_ipa, &amp;new_pmd);</span>
<span class="quote">&gt;&gt; +		if (vma_pagesize == PUD_SIZE) {</span>
<span class="quote">&gt;&gt; +			pud_t new_pud;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +			new_pud = stage2_build_pud(pfn, mem_type, writable);</span>
<span class="quote">&gt;&gt; +			if (writable)</span>
<span class="quote">&gt;&gt; +				kvm_set_pfn_dirty(pfn);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +			coherent_cache_guest_page(vcpu, pfn, PUD_SIZE);</span>
<span class="quote">&gt;&gt; +			ret = stage2_set_pud_huge(kvm, memcache,</span>
<span class="quote">&gt;&gt; +						  fault_ipa, &amp;new_pud);</span>
<span class="quote">&gt;&gt; +		} else {</span>
<span class="quote">&gt;&gt; +			pmd_t new_pmd;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +			new_pmd = stage2_build_pmd(pfn, mem_type, writable);</span>
<span class="quote">&gt;&gt; +			if (writable)</span>
<span class="quote">&gt;&gt; +				kvm_set_pfn_dirty(pfn);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +			coherent_cache_guest_page(vcpu, pfn, PMD_SIZE);</span>
<span class="quote">&gt;&gt; +			ret = stage2_set_pmd_huge(kvm, memcache,</span>
<span class="quote">&gt;&gt; +						  fault_ipa, &amp;new_pmd);</span>
<span class="quote">&gt;&gt; +		}</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; This stuff needs rebasing onto v4.16-rc1 when we get there, and it will</span>
<span class="quote">&gt; clash with Marc&#39;s icache optimizations.</span>

Thanks for the heads up.
<span class="quote">
&gt;</span>
<span class="quote">&gt; But, you should be able to move kvm_set_pfn_dirty() out of the</span>
<span class="quote">&gt; size-conditional section and also call the cache maintenance functions</span>
<span class="quote">&gt; using vma_pagesize as parameter.</span>

Agreed - I&#39;ll roll these suggestions into the next version.

Thanks a lot for the review.

Punit
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/arm/include/asm/kvm_mmu.h b/arch/arm/include/asm/kvm_mmu.h</span>
<span class="p_header">index 3fbe919b9181..6e2e34348cb3 100644</span>
<span class="p_header">--- a/arch/arm/include/asm/kvm_mmu.h</span>
<span class="p_header">+++ b/arch/arm/include/asm/kvm_mmu.h</span>
<span class="p_chunk">@@ -59,6 +59,10 @@</span> <span class="p_context"> phys_addr_t kvm_get_idmap_vector(void);</span>
 int kvm_mmu_init(void);
 void kvm_clear_hyp_idmap(void);
 
<span class="p_add">+static inline void kvm_set_pud(pud_t *pud, pud_t new_pud)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static inline void kvm_set_pmd(pmd_t *pmd, pmd_t new_pmd)
 {
 	*pmd = new_pmd;
<span class="p_chunk">@@ -230,6 +234,12 @@</span> <span class="p_context"> static inline unsigned int kvm_get_vmid_bits(void)</span>
 	return 8;
 }
 
<span class="p_add">+static inline pud_t stage2_build_pud(kvm_pfn_t pfn, pgprot_t mem_type,</span>
<span class="p_add">+				     bool writable)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __pud(0);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 #endif	/* !__ASSEMBLY__ */
 
 #endif /* __ARM_KVM_MMU_H__ */
<span class="p_header">diff --git a/arch/arm/include/asm/pgtable-3level.h b/arch/arm/include/asm/pgtable-3level.h</span>
<span class="p_header">index 1a7a17b2a1ba..97e04fdbfa85 100644</span>
<span class="p_header">--- a/arch/arm/include/asm/pgtable-3level.h</span>
<span class="p_header">+++ b/arch/arm/include/asm/pgtable-3level.h</span>
<span class="p_chunk">@@ -249,6 +249,8 @@</span> <span class="p_context"> PMD_BIT_FUNC(mkyoung,   |= PMD_SECT_AF);</span>
 #define pfn_pmd(pfn,prot)	(__pmd(((phys_addr_t)(pfn) &lt;&lt; PAGE_SHIFT) | pgprot_val(prot)))
 #define mk_pmd(page,prot)	pfn_pmd(page_to_pfn(page),prot)
 
<span class="p_add">+#define pud_pfn(pud)		(((pud_val(pud) &amp; PUD_MASK) &amp; PHYS_MASK) &gt;&gt; PAGE_SHIFT)</span>
<span class="p_add">+</span>
 /* represent a notpresent pmd by faulting entry, this is used by pmdp_invalidate */
 static inline pmd_t pmd_mknotpresent(pmd_t pmd)
 {
<span class="p_header">diff --git a/arch/arm64/include/asm/kvm_mmu.h b/arch/arm64/include/asm/kvm_mmu.h</span>
<span class="p_header">index dbfd18e08cfb..89eac3dbe123 100644</span>
<span class="p_header">--- a/arch/arm64/include/asm/kvm_mmu.h</span>
<span class="p_header">+++ b/arch/arm64/include/asm/kvm_mmu.h</span>
<span class="p_chunk">@@ -160,6 +160,7 @@</span> <span class="p_context"> void kvm_clear_hyp_idmap(void);</span>
 
 #define	kvm_set_pte(ptep, pte)		set_pte(ptep, pte)
 #define	kvm_set_pmd(pmdp, pmd)		set_pmd(pmdp, pmd)
<span class="p_add">+#define kvm_set_pud(pudp, pud)		set_pud(pudp, pud)</span>
 
 static inline pte_t kvm_s2pte_mkwrite(pte_t pte)
 {
<span class="p_chunk">@@ -173,6 +174,12 @@</span> <span class="p_context"> static inline pmd_t kvm_s2pmd_mkwrite(pmd_t pmd)</span>
 	return pmd;
 }
 
<span class="p_add">+static inline pud_t kvm_s2pud_mkwrite(pud_t pud)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pud_val(pud) |= PUD_S2_RDWR;</span>
<span class="p_add">+	return pud;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static inline void kvm_set_s2pte_readonly(pte_t *pte)
 {
 	pteval_t old_pteval, pteval;
<span class="p_chunk">@@ -319,5 +326,17 @@</span> <span class="p_context"> static inline unsigned int kvm_get_vmid_bits(void)</span>
 	return (cpuid_feature_extract_unsigned_field(reg, ID_AA64MMFR1_VMIDBITS_SHIFT) == 2) ? 16 : 8;
 }
 
<span class="p_add">+static inline pud_t stage2_build_pud(kvm_pfn_t pfn, pgprot_t mem_type,</span>
<span class="p_add">+				     bool writable)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pud_t pud = pfn_pud(pfn, mem_type);</span>
<span class="p_add">+</span>
<span class="p_add">+	pud = pud_mkhuge(pud);</span>
<span class="p_add">+	if (writable)</span>
<span class="p_add">+		pud = kvm_s2pud_mkwrite(pud);</span>
<span class="p_add">+</span>
<span class="p_add">+	return pud;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 #endif /* __ASSEMBLY__ */
 #endif /* __ARM64_KVM_MMU_H__ */
<span class="p_header">diff --git a/arch/arm64/include/asm/pgtable-hwdef.h b/arch/arm64/include/asm/pgtable-hwdef.h</span>
<span class="p_header">index 40a998cdd399..a091a6192eee 100644</span>
<span class="p_header">--- a/arch/arm64/include/asm/pgtable-hwdef.h</span>
<span class="p_header">+++ b/arch/arm64/include/asm/pgtable-hwdef.h</span>
<span class="p_chunk">@@ -181,6 +181,8 @@</span> <span class="p_context"></span>
 #define PMD_S2_RDONLY		(_AT(pmdval_t, 1) &lt;&lt; 6)   /* HAP[2:1] */
 #define PMD_S2_RDWR		(_AT(pmdval_t, 3) &lt;&lt; 6)   /* HAP[2:1] */
 
<span class="p_add">+#define PUD_S2_RDWR		(_AT(pudval_t, 3) &lt;&lt; 6)   /* HAP[2:1] */</span>
<span class="p_add">+</span>
 /*
  * Memory Attribute override for Stage-2 (MemAttr[3:0])
  */
<span class="p_header">diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h</span>
<span class="p_header">index bdcc7f1c9d06..d5ffff4369d2 100644</span>
<span class="p_header">--- a/arch/arm64/include/asm/pgtable.h</span>
<span class="p_header">+++ b/arch/arm64/include/asm/pgtable.h</span>
<span class="p_chunk">@@ -362,7 +362,11 @@</span> <span class="p_context"> static inline int pmd_protnone(pmd_t pmd)</span>
 #define mk_pmd(page,prot)	pfn_pmd(page_to_pfn(page),prot)
 
 #define pud_write(pud)		pte_write(pud_pte(pud))
<span class="p_add">+</span>
<span class="p_add">+#define pud_mkhuge(pud)		(__pud(pud_val(pud) &amp; ~PUD_TABLE_BIT))</span>
<span class="p_add">+</span>
 #define pud_pfn(pud)		(((pud_val(pud) &amp; PUD_MASK) &amp; PHYS_MASK) &gt;&gt; PAGE_SHIFT)
<span class="p_add">+#define pfn_pud(pfn, prot)	(__pud(((phys_addr_t)(pfn) &lt;&lt; PAGE_SHIFT) | pgprot_val(prot)))</span>
 
 #define set_pmd_at(mm, addr, pmdp, pmd)	set_pte_at(mm, addr, (pte_t *)pmdp, pmd_pte(pmd))
 
<span class="p_header">diff --git a/virt/kvm/arm/mmu.c b/virt/kvm/arm/mmu.c</span>
<span class="p_header">index f02219a91b19..5362de098768 100644</span>
<span class="p_header">--- a/virt/kvm/arm/mmu.c</span>
<span class="p_header">+++ b/virt/kvm/arm/mmu.c</span>
<span class="p_chunk">@@ -872,6 +872,32 @@</span> <span class="p_context"> static pud_t *stage2_get_pud(struct kvm *kvm, struct kvm_mmu_memory_cache *cache</span>
 	return stage2_pud_offset(pgd, addr);
 }
 
<span class="p_add">+static int stage2_set_pud_huge(struct kvm *kvm, struct kvm_mmu_memory_cache</span>
<span class="p_add">+			       *cache, phys_addr_t addr, const pud_t *new_pud)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pud_t *pud, old_pud;</span>
<span class="p_add">+</span>
<span class="p_add">+	pud = stage2_get_pud(kvm, cache, addr);</span>
<span class="p_add">+	VM_BUG_ON(!pud);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Mapping in huge pages should only happen through a fault.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	VM_BUG_ON(stage2_pud_present(*pud) &amp;&amp;</span>
<span class="p_add">+		  pud_pfn(*pud) != pud_pfn(*new_pud));</span>
<span class="p_add">+</span>
<span class="p_add">+	old_pud = *pud;</span>
<span class="p_add">+	if (stage2_pud_present(old_pud)) {</span>
<span class="p_add">+		stage2_pud_clear(pud);</span>
<span class="p_add">+		kvm_tlb_flush_vmid_ipa(kvm, addr);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		get_page(virt_to_page(pud));</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	kvm_set_pud(pud, *new_pud);</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static pmd_t *stage2_get_pmd(struct kvm *kvm, struct kvm_mmu_memory_cache *cache,
 			     phys_addr_t addr)
 {
<span class="p_chunk">@@ -1307,6 +1333,7 @@</span> <span class="p_context"> static int user_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa,</span>
 	kvm_pfn_t pfn;
 	pgprot_t mem_type = PAGE_S2;
 	bool logging_active = memslot_is_logging(memslot);
<span class="p_add">+	unsigned long vma_pagesize;</span>
 	unsigned long flags = 0;
 
 	write_fault = kvm_is_write_fault(vcpu);
<span class="p_chunk">@@ -1324,9 +1351,13 @@</span> <span class="p_context"> static int user_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa,</span>
 		return -EFAULT;
 	}
 
<span class="p_del">-	if (vma_kernel_pagesize(vma) == PMD_SIZE &amp;&amp; !logging_active) {</span>
<span class="p_add">+	vma_pagesize = vma_kernel_pagesize(vma);</span>
<span class="p_add">+	if ((vma_pagesize == PMD_SIZE || vma_pagesize == PUD_SIZE) &amp;&amp;</span>
<span class="p_add">+	    !logging_active) {</span>
<span class="p_add">+		struct hstate *h = hstate_vma(vma);</span>
<span class="p_add">+</span>
 		hugetlb = true;
<span class="p_del">-		gfn = (fault_ipa &amp; PMD_MASK) &gt;&gt; PAGE_SHIFT;</span>
<span class="p_add">+		gfn = (fault_ipa &amp; huge_page_mask(h)) &gt;&gt; PAGE_SHIFT;</span>
 	} else {
 		/*
 		 * Pages belonging to memslots that don&#39;t have the same
<span class="p_chunk">@@ -1393,17 +1424,38 @@</span> <span class="p_context"> static int user_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa,</span>
 	if (mmu_notifier_retry(kvm, mmu_seq))
 		goto out_unlock;
 
<span class="p_del">-	if (!hugetlb &amp;&amp; !force_pte)</span>
<span class="p_add">+	if (!hugetlb &amp;&amp; !force_pte) {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * We only support PMD_SIZE transparent</span>
<span class="p_add">+		 * hugepages. This code will need updates if we enable</span>
<span class="p_add">+		 * other page sizes for THP.</span>
<span class="p_add">+		 */</span>
 		hugetlb = transparent_hugepage_adjust(&amp;pfn, &amp;fault_ipa);
<span class="p_add">+		vma_pagesize = PMD_SIZE;</span>
<span class="p_add">+	}</span>
 
 	if (hugetlb) {
<span class="p_del">-		pmd_t new_pmd = stage2_build_pmd(pfn, mem_type, writable);</span>
<span class="p_del">-</span>
<span class="p_del">-		if (writable)</span>
<span class="p_del">-			kvm_set_pfn_dirty(pfn);</span>
<span class="p_del">-</span>
<span class="p_del">-		coherent_cache_guest_page(vcpu, pfn, PMD_SIZE);</span>
<span class="p_del">-		ret = stage2_set_pmd_huge(kvm, memcache, fault_ipa, &amp;new_pmd);</span>
<span class="p_add">+		if (vma_pagesize == PUD_SIZE) {</span>
<span class="p_add">+			pud_t new_pud;</span>
<span class="p_add">+</span>
<span class="p_add">+			new_pud = stage2_build_pud(pfn, mem_type, writable);</span>
<span class="p_add">+			if (writable)</span>
<span class="p_add">+				kvm_set_pfn_dirty(pfn);</span>
<span class="p_add">+</span>
<span class="p_add">+			coherent_cache_guest_page(vcpu, pfn, PUD_SIZE);</span>
<span class="p_add">+			ret = stage2_set_pud_huge(kvm, memcache,</span>
<span class="p_add">+						  fault_ipa, &amp;new_pud);</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			pmd_t new_pmd;</span>
<span class="p_add">+</span>
<span class="p_add">+			new_pmd = stage2_build_pmd(pfn, mem_type, writable);</span>
<span class="p_add">+			if (writable)</span>
<span class="p_add">+				kvm_set_pfn_dirty(pfn);</span>
<span class="p_add">+</span>
<span class="p_add">+			coherent_cache_guest_page(vcpu, pfn, PMD_SIZE);</span>
<span class="p_add">+			ret = stage2_set_pmd_huge(kvm, memcache,</span>
<span class="p_add">+						  fault_ipa, &amp;new_pmd);</span>
<span class="p_add">+		}</span>
 	} else {
 		pte_t new_pte = pfn_pte(pfn, mem_type);
 

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



