
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>Linux 4.14.15 - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    Linux 4.14.15</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=37061">gregkh@linuxfoundation.org</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Jan. 23, 2018, 7:57 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20180123195719.GB6785@kroah.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10181013/mbox/"
   >mbox</a>
|
   <a href="/patch/10181013/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10181013/">/patch/10181013/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	603896037F for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 23 Jan 2018 19:57:38 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 2BD6D2879E
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 23 Jan 2018 19:57:38 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 1F062287A5; Tue, 23 Jan 2018 19:57:38 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 8E6192879E
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 23 Jan 2018 19:57:31 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752596AbeAWT5a (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 23 Jan 2018 14:57:30 -0500
Received: from mail.linuxfoundation.org ([140.211.169.12]:48572 &quot;EHLO
	mail.linuxfoundation.org&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1752372AbeAWT5U (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 23 Jan 2018 14:57:20 -0500
Received: from localhost (LFbn-1-12258-90.w90-92.abo.wanadoo.fr
	[90.92.71.90])
	by mail.linuxfoundation.org (Postfix) with ESMTPSA id 058C81062;
	Tue, 23 Jan 2018 19:57:18 +0000 (UTC)
Date: Tue, 23 Jan 2018 20:57:19 +0100
From: Greg KH &lt;gregkh@linuxfoundation.org&gt;
To: linux-kernel@vger.kernel.org, Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	torvalds@linux-foundation.org, stable@vger.kernel.org
Cc: lwn@lwn.net, Jiri Slaby &lt;jslaby@suse.cz&gt;
Subject: Re: Linux 4.14.15
Message-ID: &lt;20180123195719.GB6785@kroah.com&gt;
References: &lt;20180123195715.GA6785@kroah.com&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Disposition: inline
In-Reply-To: &lt;20180123195715.GA6785@kroah.com&gt;
User-Agent: Mutt/1.9.2 (2017-12-15)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=37061">gregkh@linuxfoundation.org</a> - Jan. 23, 2018, 7:57 p.m.</div>
<pre class="content">

</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/Documentation/x86/pti.txt b/Documentation/x86/pti.txt</span>
<span class="p_header">index d11eff61fc9a..5cd58439ad2d 100644</span>
<span class="p_header">--- a/Documentation/x86/pti.txt</span>
<span class="p_header">+++ b/Documentation/x86/pti.txt</span>
<span class="p_chunk">@@ -78,7 +78,7 @@</span> <span class="p_context"> this protection comes at a cost:</span>
      non-PTI SYSCALL entry code, so requires mapping fewer
      things into the userspace page tables.  The downside is
      that stacks must be switched at entry time.
<span class="p_del">-  d. Global pages are disabled for all kernel structures not</span>
<span class="p_add">+  c. Global pages are disabled for all kernel structures not</span>
      mapped into both kernel and userspace page tables.  This
      feature of the MMU allows different processes to share TLB
      entries mapping the kernel.  Losing the feature means more
<span class="p_header">diff --git a/Makefile b/Makefile</span>
<span class="p_header">index 4951305eb867..bf1a277a67a4 100644</span>
<span class="p_header">--- a/Makefile</span>
<span class="p_header">+++ b/Makefile</span>
<span class="p_chunk">@@ -1,7 +1,7 @@</span> <span class="p_context"></span>
 # SPDX-License-Identifier: GPL-2.0
 VERSION = 4
 PATCHLEVEL = 14
<span class="p_del">-SUBLEVEL = 14</span>
<span class="p_add">+SUBLEVEL = 15</span>
 EXTRAVERSION =
 NAME = Petit Gorille
 
<span class="p_header">diff --git a/arch/alpha/kernel/sys_sio.c b/arch/alpha/kernel/sys_sio.c</span>
<span class="p_header">index 37bd6d9b8eb9..a6bdc1da47ad 100644</span>
<span class="p_header">--- a/arch/alpha/kernel/sys_sio.c</span>
<span class="p_header">+++ b/arch/alpha/kernel/sys_sio.c</span>
<span class="p_chunk">@@ -102,6 +102,15 @@</span> <span class="p_context"> sio_pci_route(void)</span>
 				   alpha_mv.sys.sio.route_tab);
 }
 
<span class="p_add">+static bool sio_pci_dev_irq_needs_level(const struct pci_dev *dev)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if ((dev-&gt;class &gt;&gt; 16 == PCI_BASE_CLASS_BRIDGE) &amp;&amp;</span>
<span class="p_add">+	    (dev-&gt;class &gt;&gt; 8 != PCI_CLASS_BRIDGE_PCMCIA))</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+</span>
<span class="p_add">+	return true;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static unsigned int __init
 sio_collect_irq_levels(void)
 {
<span class="p_chunk">@@ -110,8 +119,7 @@</span> <span class="p_context"> sio_collect_irq_levels(void)</span>
 
 	/* Iterate through the devices, collecting IRQ levels.  */
 	for_each_pci_dev(dev) {
<span class="p_del">-		if ((dev-&gt;class &gt;&gt; 16 == PCI_BASE_CLASS_BRIDGE) &amp;&amp;</span>
<span class="p_del">-		    (dev-&gt;class &gt;&gt; 8 != PCI_CLASS_BRIDGE_PCMCIA))</span>
<span class="p_add">+		if (!sio_pci_dev_irq_needs_level(dev))</span>
 			continue;
 
 		if (dev-&gt;irq)
<span class="p_chunk">@@ -120,8 +128,7 @@</span> <span class="p_context"> sio_collect_irq_levels(void)</span>
 	return level_bits;
 }
 
<span class="p_del">-static void __init</span>
<span class="p_del">-sio_fixup_irq_levels(unsigned int level_bits)</span>
<span class="p_add">+static void __sio_fixup_irq_levels(unsigned int level_bits, bool reset)</span>
 {
 	unsigned int old_level_bits;
 
<span class="p_chunk">@@ -139,12 +146,21 @@</span> <span class="p_context"> sio_fixup_irq_levels(unsigned int level_bits)</span>
 	 */
 	old_level_bits = inb(0x4d0) | (inb(0x4d1) &lt;&lt; 8);
 
<span class="p_del">-	level_bits |= (old_level_bits &amp; 0x71ff);</span>
<span class="p_add">+	if (reset)</span>
<span class="p_add">+		old_level_bits &amp;= 0x71ff;</span>
<span class="p_add">+</span>
<span class="p_add">+	level_bits |= old_level_bits;</span>
 
 	outb((level_bits &gt;&gt; 0) &amp; 0xff, 0x4d0);
 	outb((level_bits &gt;&gt; 8) &amp; 0xff, 0x4d1);
 }
 
<span class="p_add">+static inline void</span>
<span class="p_add">+sio_fixup_irq_levels(unsigned int level_bits)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__sio_fixup_irq_levels(level_bits, true);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static inline int
 noname_map_irq(const struct pci_dev *dev, u8 slot, u8 pin)
 {
<span class="p_chunk">@@ -181,7 +197,14 @@</span> <span class="p_context"> noname_map_irq(const struct pci_dev *dev, u8 slot, u8 pin)</span>
 	const long min_idsel = 6, max_idsel = 14, irqs_per_slot = 5;
 	int irq = COMMON_TABLE_LOOKUP, tmp;
 	tmp = __kernel_extbl(alpha_mv.sys.sio.route_tab, irq);
<span class="p_del">-	return irq &gt;= 0 ? tmp : -1;</span>
<span class="p_add">+</span>
<span class="p_add">+	irq = irq &gt;= 0 ? tmp : -1;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Fixup IRQ level if an actual IRQ mapping is detected */</span>
<span class="p_add">+	if (sio_pci_dev_irq_needs_level(dev) &amp;&amp; irq &gt;= 0)</span>
<span class="p_add">+		__sio_fixup_irq_levels(1 &lt;&lt; irq, false);</span>
<span class="p_add">+</span>
<span class="p_add">+	return irq;</span>
 }
 
 static inline int
<span class="p_header">diff --git a/arch/arm/boot/dts/kirkwood-openblocks_a7.dts b/arch/arm/boot/dts/kirkwood-openblocks_a7.dts</span>
<span class="p_header">index cf2f5240e176..27cc913ca0f5 100644</span>
<span class="p_header">--- a/arch/arm/boot/dts/kirkwood-openblocks_a7.dts</span>
<span class="p_header">+++ b/arch/arm/boot/dts/kirkwood-openblocks_a7.dts</span>
<span class="p_chunk">@@ -53,7 +53,8 @@</span> <span class="p_context"></span>
 		};
 
 		pinctrl: pin-controller@10000 {
<span class="p_del">-			pinctrl-0 = &lt;&amp;pmx_dip_switches &amp;pmx_gpio_header&gt;;</span>
<span class="p_add">+			pinctrl-0 = &lt;&amp;pmx_dip_switches &amp;pmx_gpio_header</span>
<span class="p_add">+				     &amp;pmx_gpio_header_gpo&gt;;</span>
 			pinctrl-names = &quot;default&quot;;
 
 			pmx_uart0: pmx-uart0 {
<span class="p_chunk">@@ -85,11 +86,16 @@</span> <span class="p_context"></span>
 			 * ground.
 			 */
 			pmx_gpio_header: pmx-gpio-header {
<span class="p_del">-				marvell,pins = &quot;mpp17&quot;, &quot;mpp7&quot;, &quot;mpp29&quot;, &quot;mpp28&quot;,</span>
<span class="p_add">+				marvell,pins = &quot;mpp17&quot;, &quot;mpp29&quot;, &quot;mpp28&quot;,</span>
 					       &quot;mpp35&quot;, &quot;mpp34&quot;, &quot;mpp40&quot;;
 				marvell,function = &quot;gpio&quot;;
 			};
 
<span class="p_add">+			pmx_gpio_header_gpo: pxm-gpio-header-gpo {</span>
<span class="p_add">+				marvell,pins = &quot;mpp7&quot;;</span>
<span class="p_add">+				marvell,function = &quot;gpo&quot;;</span>
<span class="p_add">+			};</span>
<span class="p_add">+</span>
 			pmx_gpio_init: pmx-init {
 				marvell,pins = &quot;mpp38&quot;;
 				marvell,function = &quot;gpio&quot;;
<span class="p_header">diff --git a/arch/arm/configs/sunxi_defconfig b/arch/arm/configs/sunxi_defconfig</span>
<span class="p_header">index 5caaf971fb50..df433abfcb02 100644</span>
<span class="p_header">--- a/arch/arm/configs/sunxi_defconfig</span>
<span class="p_header">+++ b/arch/arm/configs/sunxi_defconfig</span>
<span class="p_chunk">@@ -10,6 +10,7 @@</span> <span class="p_context"> CONFIG_SMP=y</span>
 CONFIG_NR_CPUS=8
 CONFIG_AEABI=y
 CONFIG_HIGHMEM=y
<span class="p_add">+CONFIG_CMA=y</span>
 CONFIG_ARM_APPENDED_DTB=y
 CONFIG_ARM_ATAG_DTB_COMPAT=y
 CONFIG_CPU_FREQ=y
<span class="p_chunk">@@ -33,6 +34,7 @@</span> <span class="p_context"> CONFIG_CAN_SUN4I=y</span>
 # CONFIG_WIRELESS is not set
 CONFIG_DEVTMPFS=y
 CONFIG_DEVTMPFS_MOUNT=y
<span class="p_add">+CONFIG_DMA_CMA=y</span>
 CONFIG_BLK_DEV_SD=y
 CONFIG_ATA=y
 CONFIG_AHCI_SUNXI=y
<span class="p_header">diff --git a/arch/arm/mach-omap2/omap_hwmod_3xxx_data.c b/arch/arm/mach-omap2/omap_hwmod_3xxx_data.c</span>
<span class="p_header">index c3276436b0ae..c12e7b572a41 100644</span>
<span class="p_header">--- a/arch/arm/mach-omap2/omap_hwmod_3xxx_data.c</span>
<span class="p_header">+++ b/arch/arm/mach-omap2/omap_hwmod_3xxx_data.c</span>
<span class="p_chunk">@@ -1656,6 +1656,7 @@</span> <span class="p_context"> static struct omap_hwmod omap3xxx_mmc3_hwmod = {</span>
 	.main_clk	= &quot;mmchs3_fck&quot;,
 	.prcm		= {
 		.omap2 = {
<span class="p_add">+			.module_offs = CORE_MOD,</span>
 			.prcm_reg_id = 1,
 			.module_bit = OMAP3430_EN_MMC3_SHIFT,
 			.idlest_reg_id = 1,
<span class="p_header">diff --git a/arch/arm64/boot/dts/marvell/armada-cp110-master.dtsi b/arch/arm64/boot/dts/marvell/armada-cp110-master.dtsi</span>
<span class="p_header">index f2aa2a81de4d..32690107c1cc 100644</span>
<span class="p_header">--- a/arch/arm64/boot/dts/marvell/armada-cp110-master.dtsi</span>
<span class="p_header">+++ b/arch/arm64/boot/dts/marvell/armada-cp110-master.dtsi</span>
<span class="p_chunk">@@ -63,8 +63,10 @@</span> <span class="p_context"></span>
 			cpm_ethernet: ethernet@0 {
 				compatible = &quot;marvell,armada-7k-pp22&quot;;
 				reg = &lt;0x0 0x100000&gt;, &lt;0x129000 0xb000&gt;;
<span class="p_del">-				clocks = &lt;&amp;cpm_clk 1 3&gt;, &lt;&amp;cpm_clk 1 9&gt;, &lt;&amp;cpm_clk 1 5&gt;;</span>
<span class="p_del">-				clock-names = &quot;pp_clk&quot;, &quot;gop_clk&quot;, &quot;mg_clk&quot;;</span>
<span class="p_add">+				clocks = &lt;&amp;cpm_clk 1 3&gt;, &lt;&amp;cpm_clk 1 9&gt;,</span>
<span class="p_add">+					 &lt;&amp;cpm_clk 1 5&gt;, &lt;&amp;cpm_clk 1 18&gt;;</span>
<span class="p_add">+				clock-names = &quot;pp_clk&quot;, &quot;gop_clk&quot;,</span>
<span class="p_add">+					      &quot;mg_clk&quot;,&quot;axi_clk&quot;;</span>
 				marvell,system-controller = &lt;&amp;cpm_syscon0&gt;;
 				status = &quot;disabled&quot;;
 				dma-coherent;
<span class="p_chunk">@@ -114,7 +116,8 @@</span> <span class="p_context"></span>
 				#size-cells = &lt;0&gt;;
 				compatible = &quot;marvell,orion-mdio&quot;;
 				reg = &lt;0x12a200 0x10&gt;;
<span class="p_del">-				clocks = &lt;&amp;cpm_clk 1 9&gt;, &lt;&amp;cpm_clk 1 5&gt;;</span>
<span class="p_add">+				clocks = &lt;&amp;cpm_clk 1 9&gt;, &lt;&amp;cpm_clk 1 5&gt;,</span>
<span class="p_add">+					 &lt;&amp;cpm_clk 1 6&gt;, &lt;&amp;cpm_clk 1 18&gt;;</span>
 				status = &quot;disabled&quot;;
 			};
 
<span class="p_chunk">@@ -295,8 +298,8 @@</span> <span class="p_context"></span>
 				compatible = &quot;marvell,armada-cp110-sdhci&quot;;
 				reg = &lt;0x780000 0x300&gt;;
 				interrupts = &lt;ICU_GRP_NSR 27 IRQ_TYPE_LEVEL_HIGH&gt;;
<span class="p_del">-				clock-names = &quot;core&quot;;</span>
<span class="p_del">-				clocks = &lt;&amp;cpm_clk 1 4&gt;;</span>
<span class="p_add">+				clock-names = &quot;core&quot;,&quot;axi&quot;;</span>
<span class="p_add">+				clocks = &lt;&amp;cpm_clk 1 4&gt;, &lt;&amp;cpm_clk 1 18&gt;;</span>
 				dma-coherent;
 				status = &quot;disabled&quot;;
 			};
<span class="p_header">diff --git a/arch/arm64/boot/dts/marvell/armada-cp110-slave.dtsi b/arch/arm64/boot/dts/marvell/armada-cp110-slave.dtsi</span>
<span class="p_header">index 4fe70323abb3..14e47c5c3816 100644</span>
<span class="p_header">--- a/arch/arm64/boot/dts/marvell/armada-cp110-slave.dtsi</span>
<span class="p_header">+++ b/arch/arm64/boot/dts/marvell/armada-cp110-slave.dtsi</span>
<span class="p_chunk">@@ -63,8 +63,10 @@</span> <span class="p_context"></span>
 			cps_ethernet: ethernet@0 {
 				compatible = &quot;marvell,armada-7k-pp22&quot;;
 				reg = &lt;0x0 0x100000&gt;, &lt;0x129000 0xb000&gt;;
<span class="p_del">-				clocks = &lt;&amp;cps_clk 1 3&gt;, &lt;&amp;cps_clk 1 9&gt;, &lt;&amp;cps_clk 1 5&gt;;</span>
<span class="p_del">-				clock-names = &quot;pp_clk&quot;, &quot;gop_clk&quot;, &quot;mg_clk&quot;;</span>
<span class="p_add">+				clocks = &lt;&amp;cps_clk 1 3&gt;, &lt;&amp;cps_clk 1 9&gt;,</span>
<span class="p_add">+					 &lt;&amp;cps_clk 1 5&gt;, &lt;&amp;cps_clk 1 18&gt;;</span>
<span class="p_add">+				clock-names = &quot;pp_clk&quot;, &quot;gop_clk&quot;,</span>
<span class="p_add">+					      &quot;mg_clk&quot;, &quot;axi_clk&quot;;</span>
 				marvell,system-controller = &lt;&amp;cps_syscon0&gt;;
 				status = &quot;disabled&quot;;
 				dma-coherent;
<span class="p_chunk">@@ -114,7 +116,8 @@</span> <span class="p_context"></span>
 				#size-cells = &lt;0&gt;;
 				compatible = &quot;marvell,orion-mdio&quot;;
 				reg = &lt;0x12a200 0x10&gt;;
<span class="p_del">-				clocks = &lt;&amp;cps_clk 1 9&gt;, &lt;&amp;cps_clk 1 5&gt;;</span>
<span class="p_add">+				clocks = &lt;&amp;cps_clk 1 9&gt;, &lt;&amp;cps_clk 1 5&gt;,</span>
<span class="p_add">+					 &lt;&amp;cps_clk 1 6&gt;, &lt;&amp;cps_clk 1 18&gt;;</span>
 				status = &quot;disabled&quot;;
 			};
 
<span class="p_header">diff --git a/arch/arm64/kvm/handle_exit.c b/arch/arm64/kvm/handle_exit.c</span>
<span class="p_header">index 7debb74843a0..380261e258ef 100644</span>
<span class="p_header">--- a/arch/arm64/kvm/handle_exit.c</span>
<span class="p_header">+++ b/arch/arm64/kvm/handle_exit.c</span>
<span class="p_chunk">@@ -44,7 +44,7 @@</span> <span class="p_context"> static int handle_hvc(struct kvm_vcpu *vcpu, struct kvm_run *run)</span>
 
 	ret = kvm_psci_call(vcpu);
 	if (ret &lt; 0) {
<span class="p_del">-		kvm_inject_undefined(vcpu);</span>
<span class="p_add">+		vcpu_set_reg(vcpu, 0, ~0UL);</span>
 		return 1;
 	}
 
<span class="p_chunk">@@ -53,7 +53,7 @@</span> <span class="p_context"> static int handle_hvc(struct kvm_vcpu *vcpu, struct kvm_run *run)</span>
 
 static int handle_smc(struct kvm_vcpu *vcpu, struct kvm_run *run)
 {
<span class="p_del">-	kvm_inject_undefined(vcpu);</span>
<span class="p_add">+	vcpu_set_reg(vcpu, 0, ~0UL);</span>
 	return 1;
 }
 
<span class="p_header">diff --git a/arch/mips/ar7/platform.c b/arch/mips/ar7/platform.c</span>
<span class="p_header">index 4674f1efbe7a..e1675c25d5d4 100644</span>
<span class="p_header">--- a/arch/mips/ar7/platform.c</span>
<span class="p_header">+++ b/arch/mips/ar7/platform.c</span>
<span class="p_chunk">@@ -575,7 +575,7 @@</span> <span class="p_context"> static int __init ar7_register_uarts(void)</span>
 	uart_port.type		= PORT_AR7;
 	uart_port.uartclk	= clk_get_rate(bus_clk) / 2;
 	uart_port.iotype	= UPIO_MEM32;
<span class="p_del">-	uart_port.flags		= UPF_FIXED_TYPE;</span>
<span class="p_add">+	uart_port.flags		= UPF_FIXED_TYPE | UPF_BOOT_AUTOCONF;</span>
 	uart_port.regshift	= 2;
 
 	uart_port.line		= 0;
<span class="p_header">diff --git a/arch/mips/kernel/mips-cm.c b/arch/mips/kernel/mips-cm.c</span>
<span class="p_header">index dd5567b1e305..8f5bd04f320a 100644</span>
<span class="p_header">--- a/arch/mips/kernel/mips-cm.c</span>
<span class="p_header">+++ b/arch/mips/kernel/mips-cm.c</span>
<span class="p_chunk">@@ -292,7 +292,6 @@</span> <span class="p_context"> void mips_cm_lock_other(unsigned int cluster, unsigned int core,</span>
 				  *this_cpu_ptr(&amp;cm_core_lock_flags));
 	} else {
 		WARN_ON(cluster != 0);
<span class="p_del">-		WARN_ON(vp != 0);</span>
 		WARN_ON(block != CM_GCR_Cx_OTHER_BLOCK_LOCAL);
 
 		/*
<span class="p_header">diff --git a/arch/powerpc/include/asm/exception-64e.h b/arch/powerpc/include/asm/exception-64e.h</span>
<span class="p_header">index a703452d67b6..555e22d5e07f 100644</span>
<span class="p_header">--- a/arch/powerpc/include/asm/exception-64e.h</span>
<span class="p_header">+++ b/arch/powerpc/include/asm/exception-64e.h</span>
<span class="p_chunk">@@ -209,5 +209,11 @@</span> <span class="p_context"> exc_##label##_book3e:</span>
 	ori	r3,r3,vector_offset@l;		\
 	mtspr	SPRN_IVOR##vector_number,r3;
 
<span class="p_add">+#define RFI_TO_KERNEL							\</span>
<span class="p_add">+	rfi</span>
<span class="p_add">+</span>
<span class="p_add">+#define RFI_TO_USER							\</span>
<span class="p_add">+	rfi</span>
<span class="p_add">+</span>
 #endif /* _ASM_POWERPC_EXCEPTION_64E_H */
 
<span class="p_header">diff --git a/arch/powerpc/include/asm/exception-64s.h b/arch/powerpc/include/asm/exception-64s.h</span>
<span class="p_header">index 9a318973af05..ccf10c2f8899 100644</span>
<span class="p_header">--- a/arch/powerpc/include/asm/exception-64s.h</span>
<span class="p_header">+++ b/arch/powerpc/include/asm/exception-64s.h</span>
<span class="p_chunk">@@ -69,6 +69,59 @@</span> <span class="p_context"></span>
  */
 #define EX_R3		EX_DAR
 
<span class="p_add">+/*</span>
<span class="p_add">+ * Macros for annotating the expected destination of (h)rfid</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * The nop instructions allow us to insert one or more instructions to flush the</span>
<span class="p_add">+ * L1-D cache when returning to userspace or a guest.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define RFI_FLUSH_SLOT							\</span>
<span class="p_add">+	RFI_FLUSH_FIXUP_SECTION;					\</span>
<span class="p_add">+	nop;								\</span>
<span class="p_add">+	nop;								\</span>
<span class="p_add">+	nop</span>
<span class="p_add">+</span>
<span class="p_add">+#define RFI_TO_KERNEL							\</span>
<span class="p_add">+	rfid</span>
<span class="p_add">+</span>
<span class="p_add">+#define RFI_TO_USER							\</span>
<span class="p_add">+	RFI_FLUSH_SLOT;							\</span>
<span class="p_add">+	rfid;								\</span>
<span class="p_add">+	b	rfi_flush_fallback</span>
<span class="p_add">+</span>
<span class="p_add">+#define RFI_TO_USER_OR_KERNEL						\</span>
<span class="p_add">+	RFI_FLUSH_SLOT;							\</span>
<span class="p_add">+	rfid;								\</span>
<span class="p_add">+	b	rfi_flush_fallback</span>
<span class="p_add">+</span>
<span class="p_add">+#define RFI_TO_GUEST							\</span>
<span class="p_add">+	RFI_FLUSH_SLOT;							\</span>
<span class="p_add">+	rfid;								\</span>
<span class="p_add">+	b	rfi_flush_fallback</span>
<span class="p_add">+</span>
<span class="p_add">+#define HRFI_TO_KERNEL							\</span>
<span class="p_add">+	hrfid</span>
<span class="p_add">+</span>
<span class="p_add">+#define HRFI_TO_USER							\</span>
<span class="p_add">+	RFI_FLUSH_SLOT;							\</span>
<span class="p_add">+	hrfid;								\</span>
<span class="p_add">+	b	hrfi_flush_fallback</span>
<span class="p_add">+</span>
<span class="p_add">+#define HRFI_TO_USER_OR_KERNEL						\</span>
<span class="p_add">+	RFI_FLUSH_SLOT;							\</span>
<span class="p_add">+	hrfid;								\</span>
<span class="p_add">+	b	hrfi_flush_fallback</span>
<span class="p_add">+</span>
<span class="p_add">+#define HRFI_TO_GUEST							\</span>
<span class="p_add">+	RFI_FLUSH_SLOT;							\</span>
<span class="p_add">+	hrfid;								\</span>
<span class="p_add">+	b	hrfi_flush_fallback</span>
<span class="p_add">+</span>
<span class="p_add">+#define HRFI_TO_UNKNOWN							\</span>
<span class="p_add">+	RFI_FLUSH_SLOT;							\</span>
<span class="p_add">+	hrfid;								\</span>
<span class="p_add">+	b	hrfi_flush_fallback</span>
<span class="p_add">+</span>
 #ifdef CONFIG_RELOCATABLE
 #define __EXCEPTION_RELON_PROLOG_PSERIES_1(label, h)			\
 	mfspr	r11,SPRN_##h##SRR0;	/* save SRR0 */			\
<span class="p_chunk">@@ -213,7 +266,7 @@</span> <span class="p_context"> END_FTR_SECTION_NESTED(ftr,ftr,943)</span>
 	mtspr	SPRN_##h##SRR0,r12;					\
 	mfspr	r12,SPRN_##h##SRR1;	/* and SRR1 */			\
 	mtspr	SPRN_##h##SRR1,r10;					\
<span class="p_del">-	h##rfid;							\</span>
<span class="p_add">+	h##RFI_TO_KERNEL;						\</span>
 	b	.	/* prevent speculative execution */
 #define EXCEPTION_PROLOG_PSERIES_1(label, h)				\
 	__EXCEPTION_PROLOG_PSERIES_1(label, h)
<span class="p_chunk">@@ -227,7 +280,7 @@</span> <span class="p_context"> END_FTR_SECTION_NESTED(ftr,ftr,943)</span>
 	mtspr	SPRN_##h##SRR0,r12;					\
 	mfspr	r12,SPRN_##h##SRR1;	/* and SRR1 */			\
 	mtspr	SPRN_##h##SRR1,r10;					\
<span class="p_del">-	h##rfid;							\</span>
<span class="p_add">+	h##RFI_TO_KERNEL;						\</span>
 	b	.	/* prevent speculative execution */
 
 #define EXCEPTION_PROLOG_PSERIES_1_NORI(label, h)			\
<span class="p_header">diff --git a/arch/powerpc/include/asm/feature-fixups.h b/arch/powerpc/include/asm/feature-fixups.h</span>
<span class="p_header">index 8f88f771cc55..1e82eb3caabd 100644</span>
<span class="p_header">--- a/arch/powerpc/include/asm/feature-fixups.h</span>
<span class="p_header">+++ b/arch/powerpc/include/asm/feature-fixups.h</span>
<span class="p_chunk">@@ -187,7 +187,20 @@</span> <span class="p_context"> label##3:					       	\</span>
 	FTR_ENTRY_OFFSET label##1b-label##3b;		\
 	.popsection;
 
<span class="p_add">+#define RFI_FLUSH_FIXUP_SECTION				\</span>
<span class="p_add">+951:							\</span>
<span class="p_add">+	.pushsection __rfi_flush_fixup,&quot;a&quot;;		\</span>
<span class="p_add">+	.align 2;					\</span>
<span class="p_add">+952:							\</span>
<span class="p_add">+	FTR_ENTRY_OFFSET 951b-952b;			\</span>
<span class="p_add">+	.popsection;</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
 #ifndef __ASSEMBLY__
<span class="p_add">+#include &lt;linux/types.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+extern long __start___rfi_flush_fixup, __stop___rfi_flush_fixup;</span>
<span class="p_add">+</span>
 void apply_feature_fixups(void);
 void setup_feature_keys(void);
 #endif
<span class="p_header">diff --git a/arch/powerpc/include/asm/hvcall.h b/arch/powerpc/include/asm/hvcall.h</span>
<span class="p_header">index a409177be8bd..f0461618bf7b 100644</span>
<span class="p_header">--- a/arch/powerpc/include/asm/hvcall.h</span>
<span class="p_header">+++ b/arch/powerpc/include/asm/hvcall.h</span>
<span class="p_chunk">@@ -241,6 +241,7 @@</span> <span class="p_context"></span>
 #define H_GET_HCA_INFO          0x1B8
 #define H_GET_PERF_COUNT        0x1BC
 #define H_MANAGE_TRACE          0x1C0
<span class="p_add">+#define H_GET_CPU_CHARACTERISTICS 0x1C8</span>
 #define H_FREE_LOGICAL_LAN_BUFFER 0x1D4
 #define H_QUERY_INT_STATE       0x1E4
 #define H_POLL_PENDING		0x1D8
<span class="p_chunk">@@ -330,6 +331,17 @@</span> <span class="p_context"></span>
 #define H_SIGNAL_SYS_RESET_ALL_OTHERS		-2
 /* &gt;= 0 values are CPU number */
 
<span class="p_add">+/* H_GET_CPU_CHARACTERISTICS return values */</span>
<span class="p_add">+#define H_CPU_CHAR_SPEC_BAR_ORI31	(1ull &lt;&lt; 63) // IBM bit 0</span>
<span class="p_add">+#define H_CPU_CHAR_BCCTRL_SERIALISED	(1ull &lt;&lt; 62) // IBM bit 1</span>
<span class="p_add">+#define H_CPU_CHAR_L1D_FLUSH_ORI30	(1ull &lt;&lt; 61) // IBM bit 2</span>
<span class="p_add">+#define H_CPU_CHAR_L1D_FLUSH_TRIG2	(1ull &lt;&lt; 60) // IBM bit 3</span>
<span class="p_add">+#define H_CPU_CHAR_L1D_THREAD_PRIV	(1ull &lt;&lt; 59) // IBM bit 4</span>
<span class="p_add">+</span>
<span class="p_add">+#define H_CPU_BEHAV_FAVOUR_SECURITY	(1ull &lt;&lt; 63) // IBM bit 0</span>
<span class="p_add">+#define H_CPU_BEHAV_L1D_FLUSH_PR	(1ull &lt;&lt; 62) // IBM bit 1</span>
<span class="p_add">+#define H_CPU_BEHAV_BNDS_CHK_SPEC_BAR	(1ull &lt;&lt; 61) // IBM bit 2</span>
<span class="p_add">+</span>
 /* Flag values used in H_REGISTER_PROC_TBL hcall */
 #define PROC_TABLE_OP_MASK	0x18
 #define PROC_TABLE_DEREG	0x10
<span class="p_chunk">@@ -436,6 +448,11 @@</span> <span class="p_context"> static inline unsigned int get_longbusy_msecs(int longbusy_rc)</span>
 	}
 }
 
<span class="p_add">+struct h_cpu_char_result {</span>
<span class="p_add">+	u64 character;</span>
<span class="p_add">+	u64 behaviour;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
 #endif /* __ASSEMBLY__ */
 #endif /* __KERNEL__ */
 #endif /* _ASM_POWERPC_HVCALL_H */
<span class="p_header">diff --git a/arch/powerpc/include/asm/paca.h b/arch/powerpc/include/asm/paca.h</span>
<span class="p_header">index 04b60af027ae..b8366df50d19 100644</span>
<span class="p_header">--- a/arch/powerpc/include/asm/paca.h</span>
<span class="p_header">+++ b/arch/powerpc/include/asm/paca.h</span>
<span class="p_chunk">@@ -231,6 +231,16 @@</span> <span class="p_context"> struct paca_struct {</span>
 	struct sibling_subcore_state *sibling_subcore_state;
 #endif
 #endif
<span class="p_add">+#ifdef CONFIG_PPC_BOOK3S_64</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * rfi fallback flush must be in its own cacheline to prevent</span>
<span class="p_add">+	 * other paca data leaking into the L1d</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	u64 exrfi[EX_SIZE] __aligned(0x80);</span>
<span class="p_add">+	void *rfi_flush_fallback_area;</span>
<span class="p_add">+	u64 l1d_flush_congruence;</span>
<span class="p_add">+	u64 l1d_flush_sets;</span>
<span class="p_add">+#endif</span>
 };
 
 extern void copy_mm_to_paca(struct mm_struct *mm);
<span class="p_header">diff --git a/arch/powerpc/include/asm/plpar_wrappers.h b/arch/powerpc/include/asm/plpar_wrappers.h</span>
<span class="p_header">index 7f01b22fa6cb..55eddf50d149 100644</span>
<span class="p_header">--- a/arch/powerpc/include/asm/plpar_wrappers.h</span>
<span class="p_header">+++ b/arch/powerpc/include/asm/plpar_wrappers.h</span>
<span class="p_chunk">@@ -326,4 +326,18 @@</span> <span class="p_context"> static inline long plapr_signal_sys_reset(long cpu)</span>
 	return plpar_hcall_norets(H_SIGNAL_SYS_RESET, cpu);
 }
 
<span class="p_add">+static inline long plpar_get_cpu_characteristics(struct h_cpu_char_result *p)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long retbuf[PLPAR_HCALL_BUFSIZE];</span>
<span class="p_add">+	long rc;</span>
<span class="p_add">+</span>
<span class="p_add">+	rc = plpar_hcall(H_GET_CPU_CHARACTERISTICS, retbuf);</span>
<span class="p_add">+	if (rc == H_SUCCESS) {</span>
<span class="p_add">+		p-&gt;character = retbuf[0];</span>
<span class="p_add">+		p-&gt;behaviour = retbuf[1];</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return rc;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 #endif /* _ASM_POWERPC_PLPAR_WRAPPERS_H */
<span class="p_header">diff --git a/arch/powerpc/include/asm/setup.h b/arch/powerpc/include/asm/setup.h</span>
<span class="p_header">index cf00ec26303a..469b7fdc9be4 100644</span>
<span class="p_header">--- a/arch/powerpc/include/asm/setup.h</span>
<span class="p_header">+++ b/arch/powerpc/include/asm/setup.h</span>
<span class="p_chunk">@@ -39,6 +39,19 @@</span> <span class="p_context"> static inline void pseries_big_endian_exceptions(void) {}</span>
 static inline void pseries_little_endian_exceptions(void) {}
 #endif /* CONFIG_PPC_PSERIES */
 
<span class="p_add">+void rfi_flush_enable(bool enable);</span>
<span class="p_add">+</span>
<span class="p_add">+/* These are bit flags */</span>
<span class="p_add">+enum l1d_flush_type {</span>
<span class="p_add">+	L1D_FLUSH_NONE		= 0x1,</span>
<span class="p_add">+	L1D_FLUSH_FALLBACK	= 0x2,</span>
<span class="p_add">+	L1D_FLUSH_ORI		= 0x4,</span>
<span class="p_add">+	L1D_FLUSH_MTTRIG	= 0x8,</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+void __init setup_rfi_flush(enum l1d_flush_type, bool enable);</span>
<span class="p_add">+void do_rfi_flush_fixups(enum l1d_flush_type types);</span>
<span class="p_add">+</span>
 #endif /* !__ASSEMBLY__ */
 
 #endif	/* _ASM_POWERPC_SETUP_H */
<span class="p_header">diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c</span>
<span class="p_header">index 8cfb20e38cfe..748cdc4bb89a 100644</span>
<span class="p_header">--- a/arch/powerpc/kernel/asm-offsets.c</span>
<span class="p_header">+++ b/arch/powerpc/kernel/asm-offsets.c</span>
<span class="p_chunk">@@ -237,6 +237,11 @@</span> <span class="p_context"> int main(void)</span>
 	OFFSET(PACA_NMI_EMERG_SP, paca_struct, nmi_emergency_sp);
 	OFFSET(PACA_IN_MCE, paca_struct, in_mce);
 	OFFSET(PACA_IN_NMI, paca_struct, in_nmi);
<span class="p_add">+	OFFSET(PACA_RFI_FLUSH_FALLBACK_AREA, paca_struct, rfi_flush_fallback_area);</span>
<span class="p_add">+	OFFSET(PACA_EXRFI, paca_struct, exrfi);</span>
<span class="p_add">+	OFFSET(PACA_L1D_FLUSH_CONGRUENCE, paca_struct, l1d_flush_congruence);</span>
<span class="p_add">+	OFFSET(PACA_L1D_FLUSH_SETS, paca_struct, l1d_flush_sets);</span>
<span class="p_add">+</span>
 #endif
 	OFFSET(PACAHWCPUID, paca_struct, hw_cpu_id);
 	OFFSET(PACAKEXECSTATE, paca_struct, kexec_state);
<span class="p_header">diff --git a/arch/powerpc/kernel/entry_64.S b/arch/powerpc/kernel/entry_64.S</span>
<span class="p_header">index 4a0fd4f40245..8a8a6d7ddcc6 100644</span>
<span class="p_header">--- a/arch/powerpc/kernel/entry_64.S</span>
<span class="p_header">+++ b/arch/powerpc/kernel/entry_64.S</span>
<span class="p_chunk">@@ -37,6 +37,11 @@</span> <span class="p_context"></span>
 #include &lt;asm/tm.h&gt;
 #include &lt;asm/ppc-opcode.h&gt;
 #include &lt;asm/export.h&gt;
<span class="p_add">+#ifdef CONFIG_PPC_BOOK3S</span>
<span class="p_add">+#include &lt;asm/exception-64s.h&gt;</span>
<span class="p_add">+#else</span>
<span class="p_add">+#include &lt;asm/exception-64e.h&gt;</span>
<span class="p_add">+#endif</span>
 
 /*
  * System calls.
<span class="p_chunk">@@ -262,13 +267,23 @@</span> <span class="p_context"> BEGIN_FTR_SECTION</span>
 END_FTR_SECTION_IFSET(CPU_FTR_HAS_PPR)
 
 	ld	r13,GPR13(r1)	/* only restore r13 if returning to usermode */
<span class="p_add">+	ld	r2,GPR2(r1)</span>
<span class="p_add">+	ld	r1,GPR1(r1)</span>
<span class="p_add">+	mtlr	r4</span>
<span class="p_add">+	mtcr	r5</span>
<span class="p_add">+	mtspr	SPRN_SRR0,r7</span>
<span class="p_add">+	mtspr	SPRN_SRR1,r8</span>
<span class="p_add">+	RFI_TO_USER</span>
<span class="p_add">+	b	.	/* prevent speculative execution */</span>
<span class="p_add">+</span>
<span class="p_add">+	/* exit to kernel */</span>
 1:	ld	r2,GPR2(r1)
 	ld	r1,GPR1(r1)
 	mtlr	r4
 	mtcr	r5
 	mtspr	SPRN_SRR0,r7
 	mtspr	SPRN_SRR1,r8
<span class="p_del">-	RFI</span>
<span class="p_add">+	RFI_TO_KERNEL</span>
 	b	.	/* prevent speculative execution */
 
 .Lsyscall_error:
<span class="p_chunk">@@ -397,8 +412,7 @@</span> <span class="p_context"> END_FTR_SECTION_IFSET(CPU_FTR_HAS_PPR)</span>
 	mtmsrd	r10, 1
 	mtspr	SPRN_SRR0, r11
 	mtspr	SPRN_SRR1, r12
<span class="p_del">-</span>
<span class="p_del">-	rfid</span>
<span class="p_add">+	RFI_TO_USER</span>
 	b	.	/* prevent speculative execution */
 #endif
 _ASM_NOKPROBE_SYMBOL(system_call_common);
<span class="p_chunk">@@ -878,7 +892,7 @@</span> <span class="p_context"> BEGIN_FTR_SECTION</span>
 END_FTR_SECTION_IFSET(CPU_FTR_HAS_PPR)
 	ACCOUNT_CPU_USER_EXIT(r13, r2, r4)
 	REST_GPR(13, r1)
<span class="p_del">-1:</span>
<span class="p_add">+</span>
 	mtspr	SPRN_SRR1,r3
 
 	ld	r2,_CCR(r1)
<span class="p_chunk">@@ -891,8 +905,22 @@</span> <span class="p_context"> END_FTR_SECTION_IFSET(CPU_FTR_HAS_PPR)</span>
 	ld	r3,GPR3(r1)
 	ld	r4,GPR4(r1)
 	ld	r1,GPR1(r1)
<span class="p_add">+	RFI_TO_USER</span>
<span class="p_add">+	b	.	/* prevent speculative execution */</span>
 
<span class="p_del">-	rfid</span>
<span class="p_add">+1:	mtspr	SPRN_SRR1,r3</span>
<span class="p_add">+</span>
<span class="p_add">+	ld	r2,_CCR(r1)</span>
<span class="p_add">+	mtcrf	0xFF,r2</span>
<span class="p_add">+	ld	r2,_NIP(r1)</span>
<span class="p_add">+	mtspr	SPRN_SRR0,r2</span>
<span class="p_add">+</span>
<span class="p_add">+	ld	r0,GPR0(r1)</span>
<span class="p_add">+	ld	r2,GPR2(r1)</span>
<span class="p_add">+	ld	r3,GPR3(r1)</span>
<span class="p_add">+	ld	r4,GPR4(r1)</span>
<span class="p_add">+	ld	r1,GPR1(r1)</span>
<span class="p_add">+	RFI_TO_KERNEL</span>
 	b	.	/* prevent speculative execution */
 
 #endif /* CONFIG_PPC_BOOK3E */
<span class="p_chunk">@@ -1073,7 +1101,7 @@</span> <span class="p_context"> __enter_rtas:</span>
 	
 	mtspr	SPRN_SRR0,r5
 	mtspr	SPRN_SRR1,r6
<span class="p_del">-	rfid</span>
<span class="p_add">+	RFI_TO_KERNEL</span>
 	b	.	/* prevent speculative execution */
 
 rtas_return_loc:
<span class="p_chunk">@@ -1098,7 +1126,7 @@</span> <span class="p_context"> rtas_return_loc:</span>
 
 	mtspr	SPRN_SRR0,r3
 	mtspr	SPRN_SRR1,r4
<span class="p_del">-	rfid</span>
<span class="p_add">+	RFI_TO_KERNEL</span>
 	b	.	/* prevent speculative execution */
 _ASM_NOKPROBE_SYMBOL(__enter_rtas)
 _ASM_NOKPROBE_SYMBOL(rtas_return_loc)
<span class="p_chunk">@@ -1171,7 +1199,7 @@</span> <span class="p_context"> _GLOBAL(enter_prom)</span>
 	LOAD_REG_IMMEDIATE(r12, MSR_SF | MSR_ISF | MSR_LE)
 	andc	r11,r11,r12
 	mtsrr1	r11
<span class="p_del">-	rfid</span>
<span class="p_add">+	RFI_TO_KERNEL</span>
 #endif /* CONFIG_PPC_BOOK3E */
 
 1:	/* Return from OF */
<span class="p_header">diff --git a/arch/powerpc/kernel/exceptions-64s.S b/arch/powerpc/kernel/exceptions-64s.S</span>
<span class="p_header">index 06598142d755..e9f72abc52b7 100644</span>
<span class="p_header">--- a/arch/powerpc/kernel/exceptions-64s.S</span>
<span class="p_header">+++ b/arch/powerpc/kernel/exceptions-64s.S</span>
<span class="p_chunk">@@ -254,7 +254,7 @@</span> <span class="p_context"> BEGIN_FTR_SECTION</span>
 	LOAD_HANDLER(r12, machine_check_handle_early)
 1:	mtspr	SPRN_SRR0,r12
 	mtspr	SPRN_SRR1,r11
<span class="p_del">-	rfid</span>
<span class="p_add">+	RFI_TO_KERNEL</span>
 	b	.	/* prevent speculative execution */
 2:
 	/* Stack overflow. Stay on emergency stack and panic.
<span class="p_chunk">@@ -443,7 +443,7 @@</span> <span class="p_context"> EXC_COMMON_BEGIN(machine_check_handle_early)</span>
 	li	r3,MSR_ME
 	andc	r10,r10,r3		/* Turn off MSR_ME */
 	mtspr	SPRN_SRR1,r10
<span class="p_del">-	rfid</span>
<span class="p_add">+	RFI_TO_KERNEL</span>
 	b	.
 2:
 	/*
<span class="p_chunk">@@ -461,7 +461,7 @@</span> <span class="p_context"> EXC_COMMON_BEGIN(machine_check_handle_early)</span>
 	 */
 	bl	machine_check_queue_event
 	MACHINE_CHECK_HANDLER_WINDUP
<span class="p_del">-	rfid</span>
<span class="p_add">+	RFI_TO_USER_OR_KERNEL</span>
 9:
 	/* Deliver the machine check to host kernel in V mode. */
 	MACHINE_CHECK_HANDLER_WINDUP
<span class="p_chunk">@@ -596,6 +596,9 @@</span> <span class="p_context"> EXC_COMMON_BEGIN(slb_miss_common)</span>
 	stw	r9,PACA_EXSLB+EX_CCR(r13)	/* save CR in exc. frame */
 	std	r10,PACA_EXSLB+EX_LR(r13)	/* save LR */
 
<span class="p_add">+	andi.	r9,r11,MSR_PR	// Check for exception from userspace</span>
<span class="p_add">+	cmpdi	cr4,r9,MSR_PR	// And save the result in CR4 for later</span>
<span class="p_add">+</span>
 	/*
 	 * Test MSR_RI before calling slb_allocate_realmode, because the
 	 * MSR in r11 gets clobbered. However we still want to allocate
<span class="p_chunk">@@ -622,9 +625,12 @@</span> <span class="p_context"> END_MMU_FTR_SECTION_IFCLR(MMU_FTR_TYPE_RADIX)</span>
 
 	/* All done -- return from exception. */
 
<span class="p_add">+	bne	cr4,1f		/* returning to kernel */</span>
<span class="p_add">+</span>
 .machine	push
 .machine	&quot;power4&quot;
 	mtcrf	0x80,r9
<span class="p_add">+	mtcrf	0x08,r9		/* MSR[PR] indication is in cr4 */</span>
 	mtcrf	0x04,r9		/* MSR[RI] indication is in cr5 */
 	mtcrf	0x02,r9		/* I/D indication is in cr6 */
 	mtcrf	0x01,r9		/* slb_allocate uses cr0 and cr7 */
<span class="p_chunk">@@ -638,9 +644,30 @@</span> <span class="p_context"> END_MMU_FTR_SECTION_IFCLR(MMU_FTR_TYPE_RADIX)</span>
 	ld	r11,PACA_EXSLB+EX_R11(r13)
 	ld	r12,PACA_EXSLB+EX_R12(r13)
 	ld	r13,PACA_EXSLB+EX_R13(r13)
<span class="p_del">-	rfid</span>
<span class="p_add">+	RFI_TO_USER</span>
<span class="p_add">+	b	.	/* prevent speculative execution */</span>
<span class="p_add">+1:</span>
<span class="p_add">+.machine	push</span>
<span class="p_add">+.machine	&quot;power4&quot;</span>
<span class="p_add">+	mtcrf	0x80,r9</span>
<span class="p_add">+	mtcrf	0x08,r9		/* MSR[PR] indication is in cr4 */</span>
<span class="p_add">+	mtcrf	0x04,r9		/* MSR[RI] indication is in cr5 */</span>
<span class="p_add">+	mtcrf	0x02,r9		/* I/D indication is in cr6 */</span>
<span class="p_add">+	mtcrf	0x01,r9		/* slb_allocate uses cr0 and cr7 */</span>
<span class="p_add">+.machine	pop</span>
<span class="p_add">+</span>
<span class="p_add">+	RESTORE_CTR(r9, PACA_EXSLB)</span>
<span class="p_add">+	RESTORE_PPR_PACA(PACA_EXSLB, r9)</span>
<span class="p_add">+	mr	r3,r12</span>
<span class="p_add">+	ld	r9,PACA_EXSLB+EX_R9(r13)</span>
<span class="p_add">+	ld	r10,PACA_EXSLB+EX_R10(r13)</span>
<span class="p_add">+	ld	r11,PACA_EXSLB+EX_R11(r13)</span>
<span class="p_add">+	ld	r12,PACA_EXSLB+EX_R12(r13)</span>
<span class="p_add">+	ld	r13,PACA_EXSLB+EX_R13(r13)</span>
<span class="p_add">+	RFI_TO_KERNEL</span>
 	b	.	/* prevent speculative execution */
 
<span class="p_add">+</span>
 2:	std     r3,PACA_EXSLB+EX_DAR(r13)
 	mr	r3,r12
 	mfspr	r11,SPRN_SRR0
<span class="p_chunk">@@ -649,7 +676,7 @@</span> <span class="p_context"> END_MMU_FTR_SECTION_IFCLR(MMU_FTR_TYPE_RADIX)</span>
 	mtspr	SPRN_SRR0,r10
 	ld	r10,PACAKMSR(r13)
 	mtspr	SPRN_SRR1,r10
<span class="p_del">-	rfid</span>
<span class="p_add">+	RFI_TO_KERNEL</span>
 	b	.
 
 8:	std     r3,PACA_EXSLB+EX_DAR(r13)
<span class="p_chunk">@@ -660,7 +687,7 @@</span> <span class="p_context"> END_MMU_FTR_SECTION_IFCLR(MMU_FTR_TYPE_RADIX)</span>
 	mtspr	SPRN_SRR0,r10
 	ld	r10,PACAKMSR(r13)
 	mtspr	SPRN_SRR1,r10
<span class="p_del">-	rfid</span>
<span class="p_add">+	RFI_TO_KERNEL</span>
 	b	.
 
 EXC_COMMON_BEGIN(unrecov_slb)
<span class="p_chunk">@@ -905,7 +932,7 @@</span> <span class="p_context"> END_FTR_SECTION_IFSET(CPU_FTR_REAL_LE)				\</span>
 	mtspr	SPRN_SRR0,r10 ; 				\
 	ld	r10,PACAKMSR(r13) ;				\
 	mtspr	SPRN_SRR1,r10 ; 				\
<span class="p_del">-	rfid ; 							\</span>
<span class="p_add">+	RFI_TO_KERNEL ;						\</span>
 	b	. ;	/* prevent speculative execution */
 
 #define SYSCALL_FASTENDIAN					\
<span class="p_chunk">@@ -914,7 +941,7 @@</span> <span class="p_context"> END_FTR_SECTION_IFSET(CPU_FTR_REAL_LE)				\</span>
 	xori	r12,r12,MSR_LE ;				\
 	mtspr	SPRN_SRR1,r12 ;					\
 	mr	r13,r9 ;					\
<span class="p_del">-	rfid ;		/* return to userspace */		\</span>
<span class="p_add">+	RFI_TO_USER ;	/* return to userspace */		\</span>
 	b	. ;	/* prevent speculative execution */
 
 #if defined(CONFIG_RELOCATABLE)
<span class="p_chunk">@@ -1299,7 +1326,7 @@</span> <span class="p_context"> END_FTR_SECTION_IFSET(CPU_FTR_CFAR)</span>
 	ld	r11,PACA_EXGEN+EX_R11(r13)
 	ld	r12,PACA_EXGEN+EX_R12(r13)
 	ld	r13,PACA_EXGEN+EX_R13(r13)
<span class="p_del">-	HRFID</span>
<span class="p_add">+	HRFI_TO_UNKNOWN</span>
 	b	.
 #endif
 
<span class="p_chunk">@@ -1403,10 +1430,94 @@</span> <span class="p_context"> masked_##_H##interrupt:					\</span>
 	ld	r10,PACA_EXGEN+EX_R10(r13);		\
 	ld	r11,PACA_EXGEN+EX_R11(r13);		\
 	/* returns to kernel where r13 must be set up, so don&#39;t restore it */ \
<span class="p_del">-	##_H##rfid;					\</span>
<span class="p_add">+	##_H##RFI_TO_KERNEL;				\</span>
 	b	.;					\
 	MASKED_DEC_HANDLER(_H)
 
<span class="p_add">+TRAMP_REAL_BEGIN(rfi_flush_fallback)</span>
<span class="p_add">+	SET_SCRATCH0(r13);</span>
<span class="p_add">+	GET_PACA(r13);</span>
<span class="p_add">+	std	r9,PACA_EXRFI+EX_R9(r13)</span>
<span class="p_add">+	std	r10,PACA_EXRFI+EX_R10(r13)</span>
<span class="p_add">+	std	r11,PACA_EXRFI+EX_R11(r13)</span>
<span class="p_add">+	std	r12,PACA_EXRFI+EX_R12(r13)</span>
<span class="p_add">+	std	r8,PACA_EXRFI+EX_R13(r13)</span>
<span class="p_add">+	mfctr	r9</span>
<span class="p_add">+	ld	r10,PACA_RFI_FLUSH_FALLBACK_AREA(r13)</span>
<span class="p_add">+	ld	r11,PACA_L1D_FLUSH_SETS(r13)</span>
<span class="p_add">+	ld	r12,PACA_L1D_FLUSH_CONGRUENCE(r13)</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The load adresses are at staggered offsets within cachelines,</span>
<span class="p_add">+	 * which suits some pipelines better (on others it should not</span>
<span class="p_add">+	 * hurt).</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	addi	r12,r12,8</span>
<span class="p_add">+	mtctr	r11</span>
<span class="p_add">+	DCBT_STOP_ALL_STREAM_IDS(r11) /* Stop prefetch streams */</span>
<span class="p_add">+</span>
<span class="p_add">+	/* order ld/st prior to dcbt stop all streams with flushing */</span>
<span class="p_add">+	sync</span>
<span class="p_add">+1:	li	r8,0</span>
<span class="p_add">+	.rept	8 /* 8-way set associative */</span>
<span class="p_add">+	ldx	r11,r10,r8</span>
<span class="p_add">+	add	r8,r8,r12</span>
<span class="p_add">+	xor	r11,r11,r11	// Ensure r11 is 0 even if fallback area is not</span>
<span class="p_add">+	add	r8,r8,r11	// Add 0, this creates a dependency on the ldx</span>
<span class="p_add">+	.endr</span>
<span class="p_add">+	addi	r10,r10,128 /* 128 byte cache line */</span>
<span class="p_add">+	bdnz	1b</span>
<span class="p_add">+</span>
<span class="p_add">+	mtctr	r9</span>
<span class="p_add">+	ld	r9,PACA_EXRFI+EX_R9(r13)</span>
<span class="p_add">+	ld	r10,PACA_EXRFI+EX_R10(r13)</span>
<span class="p_add">+	ld	r11,PACA_EXRFI+EX_R11(r13)</span>
<span class="p_add">+	ld	r12,PACA_EXRFI+EX_R12(r13)</span>
<span class="p_add">+	ld	r8,PACA_EXRFI+EX_R13(r13)</span>
<span class="p_add">+	GET_SCRATCH0(r13);</span>
<span class="p_add">+	rfid</span>
<span class="p_add">+</span>
<span class="p_add">+TRAMP_REAL_BEGIN(hrfi_flush_fallback)</span>
<span class="p_add">+	SET_SCRATCH0(r13);</span>
<span class="p_add">+	GET_PACA(r13);</span>
<span class="p_add">+	std	r9,PACA_EXRFI+EX_R9(r13)</span>
<span class="p_add">+	std	r10,PACA_EXRFI+EX_R10(r13)</span>
<span class="p_add">+	std	r11,PACA_EXRFI+EX_R11(r13)</span>
<span class="p_add">+	std	r12,PACA_EXRFI+EX_R12(r13)</span>
<span class="p_add">+	std	r8,PACA_EXRFI+EX_R13(r13)</span>
<span class="p_add">+	mfctr	r9</span>
<span class="p_add">+	ld	r10,PACA_RFI_FLUSH_FALLBACK_AREA(r13)</span>
<span class="p_add">+	ld	r11,PACA_L1D_FLUSH_SETS(r13)</span>
<span class="p_add">+	ld	r12,PACA_L1D_FLUSH_CONGRUENCE(r13)</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The load adresses are at staggered offsets within cachelines,</span>
<span class="p_add">+	 * which suits some pipelines better (on others it should not</span>
<span class="p_add">+	 * hurt).</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	addi	r12,r12,8</span>
<span class="p_add">+	mtctr	r11</span>
<span class="p_add">+	DCBT_STOP_ALL_STREAM_IDS(r11) /* Stop prefetch streams */</span>
<span class="p_add">+</span>
<span class="p_add">+	/* order ld/st prior to dcbt stop all streams with flushing */</span>
<span class="p_add">+	sync</span>
<span class="p_add">+1:	li	r8,0</span>
<span class="p_add">+	.rept	8 /* 8-way set associative */</span>
<span class="p_add">+	ldx	r11,r10,r8</span>
<span class="p_add">+	add	r8,r8,r12</span>
<span class="p_add">+	xor	r11,r11,r11	// Ensure r11 is 0 even if fallback area is not</span>
<span class="p_add">+	add	r8,r8,r11	// Add 0, this creates a dependency on the ldx</span>
<span class="p_add">+	.endr</span>
<span class="p_add">+	addi	r10,r10,128 /* 128 byte cache line */</span>
<span class="p_add">+	bdnz	1b</span>
<span class="p_add">+</span>
<span class="p_add">+	mtctr	r9</span>
<span class="p_add">+	ld	r9,PACA_EXRFI+EX_R9(r13)</span>
<span class="p_add">+	ld	r10,PACA_EXRFI+EX_R10(r13)</span>
<span class="p_add">+	ld	r11,PACA_EXRFI+EX_R11(r13)</span>
<span class="p_add">+	ld	r12,PACA_EXRFI+EX_R12(r13)</span>
<span class="p_add">+	ld	r8,PACA_EXRFI+EX_R13(r13)</span>
<span class="p_add">+	GET_SCRATCH0(r13);</span>
<span class="p_add">+	hrfid</span>
<span class="p_add">+</span>
 /*
  * Real mode exceptions actually use this too, but alternate
  * instruction code patches (which end up in the common .text area)
<span class="p_chunk">@@ -1426,7 +1537,7 @@</span> <span class="p_context"> TRAMP_REAL_BEGIN(kvmppc_skip_interrupt)</span>
 	addi	r13, r13, 4
 	mtspr	SPRN_SRR0, r13
 	GET_SCRATCH0(r13)
<span class="p_del">-	rfid</span>
<span class="p_add">+	RFI_TO_KERNEL</span>
 	b	.
 
 TRAMP_REAL_BEGIN(kvmppc_skip_Hinterrupt)
<span class="p_chunk">@@ -1438,7 +1549,7 @@</span> <span class="p_context"> TRAMP_REAL_BEGIN(kvmppc_skip_Hinterrupt)</span>
 	addi	r13, r13, 4
 	mtspr	SPRN_HSRR0, r13
 	GET_SCRATCH0(r13)
<span class="p_del">-	hrfid</span>
<span class="p_add">+	HRFI_TO_KERNEL</span>
 	b	.
 #endif
 
<span class="p_header">diff --git a/arch/powerpc/kernel/setup_64.c b/arch/powerpc/kernel/setup_64.c</span>
<span class="p_header">index b89c6aac48c9..935059cb9e40 100644</span>
<span class="p_header">--- a/arch/powerpc/kernel/setup_64.c</span>
<span class="p_header">+++ b/arch/powerpc/kernel/setup_64.c</span>
<span class="p_chunk">@@ -784,3 +784,104 @@</span> <span class="p_context"> static int __init disable_hardlockup_detector(void)</span>
 	return 0;
 }
 early_initcall(disable_hardlockup_detector);
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_PPC_BOOK3S_64</span>
<span class="p_add">+static enum l1d_flush_type enabled_flush_types;</span>
<span class="p_add">+static void *l1d_flush_fallback_area;</span>
<span class="p_add">+static bool no_rfi_flush;</span>
<span class="p_add">+bool rfi_flush;</span>
<span class="p_add">+</span>
<span class="p_add">+static int __init handle_no_rfi_flush(char *p)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pr_info(&quot;rfi-flush: disabled on command line.&quot;);</span>
<span class="p_add">+	no_rfi_flush = true;</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+early_param(&quot;no_rfi_flush&quot;, handle_no_rfi_flush);</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * The RFI flush is not KPTI, but because users will see doco that says to use</span>
<span class="p_add">+ * nopti we hijack that option here to also disable the RFI flush.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static int __init handle_no_pti(char *p)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pr_info(&quot;rfi-flush: disabling due to &#39;nopti&#39; on command line.\n&quot;);</span>
<span class="p_add">+	handle_no_rfi_flush(NULL);</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+early_param(&quot;nopti&quot;, handle_no_pti);</span>
<span class="p_add">+</span>
<span class="p_add">+static void do_nothing(void *unused)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We don&#39;t need to do the flush explicitly, just enter+exit kernel is</span>
<span class="p_add">+	 * sufficient, the RFI exit handlers will do the right thing.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void rfi_flush_enable(bool enable)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (rfi_flush == enable)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (enable) {</span>
<span class="p_add">+		do_rfi_flush_fixups(enabled_flush_types);</span>
<span class="p_add">+		on_each_cpu(do_nothing, NULL, 1);</span>
<span class="p_add">+	} else</span>
<span class="p_add">+		do_rfi_flush_fixups(L1D_FLUSH_NONE);</span>
<span class="p_add">+</span>
<span class="p_add">+	rfi_flush = enable;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void init_fallback_flush(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	u64 l1d_size, limit;</span>
<span class="p_add">+	int cpu;</span>
<span class="p_add">+</span>
<span class="p_add">+	l1d_size = ppc64_caches.l1d.size;</span>
<span class="p_add">+	limit = min(safe_stack_limit(), ppc64_rma_size);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Align to L1d size, and size it at 2x L1d size, to catch possible</span>
<span class="p_add">+	 * hardware prefetch runoff. We don&#39;t have a recipe for load patterns to</span>
<span class="p_add">+	 * reliably avoid the prefetcher.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	l1d_flush_fallback_area = __va(memblock_alloc_base(l1d_size * 2, l1d_size, limit));</span>
<span class="p_add">+	memset(l1d_flush_fallback_area, 0, l1d_size * 2);</span>
<span class="p_add">+</span>
<span class="p_add">+	for_each_possible_cpu(cpu) {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * The fallback flush is currently coded for 8-way</span>
<span class="p_add">+		 * associativity. Different associativity is possible, but it</span>
<span class="p_add">+		 * will be treated as 8-way and may not evict the lines as</span>
<span class="p_add">+		 * effectively.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * 128 byte lines are mandatory.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		u64 c = l1d_size / 8;</span>
<span class="p_add">+</span>
<span class="p_add">+		paca[cpu].rfi_flush_fallback_area = l1d_flush_fallback_area;</span>
<span class="p_add">+		paca[cpu].l1d_flush_congruence = c;</span>
<span class="p_add">+		paca[cpu].l1d_flush_sets = c / 128;</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void __init setup_rfi_flush(enum l1d_flush_type types, bool enable)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (types &amp; L1D_FLUSH_FALLBACK) {</span>
<span class="p_add">+		pr_info(&quot;rfi-flush: Using fallback displacement flush\n&quot;);</span>
<span class="p_add">+		init_fallback_flush();</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (types &amp; L1D_FLUSH_ORI)</span>
<span class="p_add">+		pr_info(&quot;rfi-flush: Using ori type flush\n&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (types &amp; L1D_FLUSH_MTTRIG)</span>
<span class="p_add">+		pr_info(&quot;rfi-flush: Using mttrig type flush\n&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+	enabled_flush_types = types;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!no_rfi_flush)</span>
<span class="p_add">+		rfi_flush_enable(enable);</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif /* CONFIG_PPC_BOOK3S_64 */</span>
<span class="p_header">diff --git a/arch/powerpc/kernel/vmlinux.lds.S b/arch/powerpc/kernel/vmlinux.lds.S</span>
<span class="p_header">index 0494e1566ee2..307843d23682 100644</span>
<span class="p_header">--- a/arch/powerpc/kernel/vmlinux.lds.S</span>
<span class="p_header">+++ b/arch/powerpc/kernel/vmlinux.lds.S</span>
<span class="p_chunk">@@ -132,6 +132,15 @@</span> <span class="p_context"> SECTIONS</span>
 	/* Read-only data */
 	RO_DATA(PAGE_SIZE)
 
<span class="p_add">+#ifdef CONFIG_PPC64</span>
<span class="p_add">+	. = ALIGN(8);</span>
<span class="p_add">+	__rfi_flush_fixup : AT(ADDR(__rfi_flush_fixup) - LOAD_OFFSET) {</span>
<span class="p_add">+		__start___rfi_flush_fixup = .;</span>
<span class="p_add">+		*(__rfi_flush_fixup)</span>
<span class="p_add">+		__stop___rfi_flush_fixup = .;</span>
<span class="p_add">+	}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 	EXCEPTION_TABLE(0)
 
 	NOTES :kernel :notes
<span class="p_header">diff --git a/arch/powerpc/kvm/book3s_hv_rmhandlers.S b/arch/powerpc/kvm/book3s_hv_rmhandlers.S</span>
<span class="p_header">index 42639fba89e8..c85ac5c83bd4 100644</span>
<span class="p_header">--- a/arch/powerpc/kvm/book3s_hv_rmhandlers.S</span>
<span class="p_header">+++ b/arch/powerpc/kvm/book3s_hv_rmhandlers.S</span>
<span class="p_chunk">@@ -78,7 +78,7 @@</span> <span class="p_context"> _GLOBAL_TOC(kvmppc_hv_entry_trampoline)</span>
 	mtmsrd	r0,1		/* clear RI in MSR */
 	mtsrr0	r5
 	mtsrr1	r6
<span class="p_del">-	RFI</span>
<span class="p_add">+	RFI_TO_KERNEL</span>
 
 kvmppc_call_hv_entry:
 	ld	r4, HSTATE_KVM_VCPU(r13)
<span class="p_chunk">@@ -187,7 +187,7 @@</span> <span class="p_context"> END_FTR_SECTION_IFCLR(CPU_FTR_ARCH_300)</span>
 	mtmsrd	r6, 1			/* Clear RI in MSR */
 	mtsrr0	r8
 	mtsrr1	r7
<span class="p_del">-	RFI</span>
<span class="p_add">+	RFI_TO_KERNEL</span>
 
 	/* Virtual-mode return */
 .Lvirt_return:
<span class="p_chunk">@@ -1131,8 +1131,7 @@</span> <span class="p_context"> END_FTR_SECTION_IFSET(CPU_FTR_ARCH_300)</span>
 
 	ld	r0, VCPU_GPR(R0)(r4)
 	ld	r4, VCPU_GPR(R4)(r4)
<span class="p_del">-</span>
<span class="p_del">-	hrfid</span>
<span class="p_add">+	HRFI_TO_GUEST</span>
 	b	.
 
 secondary_too_late:
<span class="p_header">diff --git a/arch/powerpc/kvm/book3s_rmhandlers.S b/arch/powerpc/kvm/book3s_rmhandlers.S</span>
<span class="p_header">index 42a4b237df5f..34a5adeff084 100644</span>
<span class="p_header">--- a/arch/powerpc/kvm/book3s_rmhandlers.S</span>
<span class="p_header">+++ b/arch/powerpc/kvm/book3s_rmhandlers.S</span>
<span class="p_chunk">@@ -46,6 +46,9 @@</span> <span class="p_context"></span>
 
 #define FUNC(name)		name
 
<span class="p_add">+#define RFI_TO_KERNEL	RFI</span>
<span class="p_add">+#define RFI_TO_GUEST	RFI</span>
<span class="p_add">+</span>
 .macro INTERRUPT_TRAMPOLINE intno
 
 .global kvmppc_trampoline_\intno
<span class="p_chunk">@@ -141,7 +144,7 @@</span> <span class="p_context"> kvmppc_handler_skip_ins:</span>
 	GET_SCRATCH0(r13)
 
 	/* And get back into the code */
<span class="p_del">-	RFI</span>
<span class="p_add">+	RFI_TO_KERNEL</span>
 #endif
 
 /*
<span class="p_chunk">@@ -164,6 +167,6 @@</span> <span class="p_context"> _GLOBAL_TOC(kvmppc_entry_trampoline)</span>
 	ori	r5, r5, MSR_EE
 	mtsrr0	r7
 	mtsrr1	r6
<span class="p_del">-	RFI</span>
<span class="p_add">+	RFI_TO_KERNEL</span>
 
 #include &quot;book3s_segment.S&quot;
<span class="p_header">diff --git a/arch/powerpc/kvm/book3s_segment.S b/arch/powerpc/kvm/book3s_segment.S</span>
<span class="p_header">index 2a2b96d53999..93a180ceefad 100644</span>
<span class="p_header">--- a/arch/powerpc/kvm/book3s_segment.S</span>
<span class="p_header">+++ b/arch/powerpc/kvm/book3s_segment.S</span>
<span class="p_chunk">@@ -156,7 +156,7 @@</span> <span class="p_context"> no_dcbz32_on:</span>
 	PPC_LL	r9, SVCPU_R9(r3)
 	PPC_LL	r3, (SVCPU_R3)(r3)
 
<span class="p_del">-	RFI</span>
<span class="p_add">+	RFI_TO_GUEST</span>
 kvmppc_handler_trampoline_enter_end:
 
 
<span class="p_chunk">@@ -407,5 +407,5 @@</span> <span class="p_context"> END_FTR_SECTION_IFSET(CPU_FTR_HVMODE)</span>
 	cmpwi	r12, BOOK3S_INTERRUPT_DOORBELL
 	beqa	BOOK3S_INTERRUPT_DOORBELL
 
<span class="p_del">-	RFI</span>
<span class="p_add">+	RFI_TO_KERNEL</span>
 kvmppc_handler_trampoline_exit_end:
<span class="p_header">diff --git a/arch/powerpc/lib/feature-fixups.c b/arch/powerpc/lib/feature-fixups.c</span>
<span class="p_header">index 41cf5ae273cf..a95ea007d654 100644</span>
<span class="p_header">--- a/arch/powerpc/lib/feature-fixups.c</span>
<span class="p_header">+++ b/arch/powerpc/lib/feature-fixups.c</span>
<span class="p_chunk">@@ -116,6 +116,47 @@</span> <span class="p_context"> void do_feature_fixups(unsigned long value, void *fixup_start, void *fixup_end)</span>
 	}
 }
 
<span class="p_add">+#ifdef CONFIG_PPC_BOOK3S_64</span>
<span class="p_add">+void do_rfi_flush_fixups(enum l1d_flush_type types)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned int instrs[3], *dest;</span>
<span class="p_add">+	long *start, *end;</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+</span>
<span class="p_add">+	start = PTRRELOC(&amp;__start___rfi_flush_fixup),</span>
<span class="p_add">+	end = PTRRELOC(&amp;__stop___rfi_flush_fixup);</span>
<span class="p_add">+</span>
<span class="p_add">+	instrs[0] = 0x60000000; /* nop */</span>
<span class="p_add">+	instrs[1] = 0x60000000; /* nop */</span>
<span class="p_add">+	instrs[2] = 0x60000000; /* nop */</span>
<span class="p_add">+</span>
<span class="p_add">+	if (types &amp; L1D_FLUSH_FALLBACK)</span>
<span class="p_add">+		/* b .+16 to fallback flush */</span>
<span class="p_add">+		instrs[0] = 0x48000010;</span>
<span class="p_add">+</span>
<span class="p_add">+	i = 0;</span>
<span class="p_add">+	if (types &amp; L1D_FLUSH_ORI) {</span>
<span class="p_add">+		instrs[i++] = 0x63ff0000; /* ori 31,31,0 speculation barrier */</span>
<span class="p_add">+		instrs[i++] = 0x63de0000; /* ori 30,30,0 L1d flush*/</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (types &amp; L1D_FLUSH_MTTRIG)</span>
<span class="p_add">+		instrs[i++] = 0x7c12dba6; /* mtspr TRIG2,r0 (SPR #882) */</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; start &lt; end; start++, i++) {</span>
<span class="p_add">+		dest = (void *)start + *start;</span>
<span class="p_add">+</span>
<span class="p_add">+		pr_devel(&quot;patching dest %lx\n&quot;, (unsigned long)dest);</span>
<span class="p_add">+</span>
<span class="p_add">+		patch_instruction(dest, instrs[0]);</span>
<span class="p_add">+		patch_instruction(dest + 1, instrs[1]);</span>
<span class="p_add">+		patch_instruction(dest + 2, instrs[2]);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	printk(KERN_DEBUG &quot;rfi-flush: patched %d locations\n&quot;, i);</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif /* CONFIG_PPC_BOOK3S_64 */</span>
<span class="p_add">+</span>
 void do_lwsync_fixups(unsigned long value, void *fixup_start, void *fixup_end)
 {
 	long *start, *end;
<span class="p_header">diff --git a/arch/powerpc/platforms/powernv/setup.c b/arch/powerpc/platforms/powernv/setup.c</span>
<span class="p_header">index bfe2aa702973..7966a314d93a 100644</span>
<span class="p_header">--- a/arch/powerpc/platforms/powernv/setup.c</span>
<span class="p_header">+++ b/arch/powerpc/platforms/powernv/setup.c</span>
<span class="p_chunk">@@ -36,13 +36,62 @@</span> <span class="p_context"></span>
 #include &lt;asm/opal.h&gt;
 #include &lt;asm/kexec.h&gt;
 #include &lt;asm/smp.h&gt;
<span class="p_add">+#include &lt;asm/setup.h&gt;</span>
 
 #include &quot;powernv.h&quot;
 
<span class="p_add">+static void pnv_setup_rfi_flush(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct device_node *np, *fw_features;</span>
<span class="p_add">+	enum l1d_flush_type type;</span>
<span class="p_add">+	int enable;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Default to fallback in case fw-features are not available */</span>
<span class="p_add">+	type = L1D_FLUSH_FALLBACK;</span>
<span class="p_add">+	enable = 1;</span>
<span class="p_add">+</span>
<span class="p_add">+	np = of_find_node_by_name(NULL, &quot;ibm,opal&quot;);</span>
<span class="p_add">+	fw_features = of_get_child_by_name(np, &quot;fw-features&quot;);</span>
<span class="p_add">+	of_node_put(np);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (fw_features) {</span>
<span class="p_add">+		np = of_get_child_by_name(fw_features, &quot;inst-l1d-flush-trig2&quot;);</span>
<span class="p_add">+		if (np &amp;&amp; of_property_read_bool(np, &quot;enabled&quot;))</span>
<span class="p_add">+			type = L1D_FLUSH_MTTRIG;</span>
<span class="p_add">+</span>
<span class="p_add">+		of_node_put(np);</span>
<span class="p_add">+</span>
<span class="p_add">+		np = of_get_child_by_name(fw_features, &quot;inst-l1d-flush-ori30,30,0&quot;);</span>
<span class="p_add">+		if (np &amp;&amp; of_property_read_bool(np, &quot;enabled&quot;))</span>
<span class="p_add">+			type = L1D_FLUSH_ORI;</span>
<span class="p_add">+</span>
<span class="p_add">+		of_node_put(np);</span>
<span class="p_add">+</span>
<span class="p_add">+		/* Enable unless firmware says NOT to */</span>
<span class="p_add">+		enable = 2;</span>
<span class="p_add">+		np = of_get_child_by_name(fw_features, &quot;needs-l1d-flush-msr-hv-1-to-0&quot;);</span>
<span class="p_add">+		if (np &amp;&amp; of_property_read_bool(np, &quot;disabled&quot;))</span>
<span class="p_add">+			enable--;</span>
<span class="p_add">+</span>
<span class="p_add">+		of_node_put(np);</span>
<span class="p_add">+</span>
<span class="p_add">+		np = of_get_child_by_name(fw_features, &quot;needs-l1d-flush-msr-pr-0-to-1&quot;);</span>
<span class="p_add">+		if (np &amp;&amp; of_property_read_bool(np, &quot;disabled&quot;))</span>
<span class="p_add">+			enable--;</span>
<span class="p_add">+</span>
<span class="p_add">+		of_node_put(np);</span>
<span class="p_add">+		of_node_put(fw_features);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	setup_rfi_flush(type, enable &gt; 0);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static void __init pnv_setup_arch(void)
 {
 	set_arch_panic_timeout(10, ARCH_PANIC_TIMEOUT);
 
<span class="p_add">+	pnv_setup_rfi_flush();</span>
<span class="p_add">+</span>
 	/* Initialize SMP */
 	pnv_smp_init();
 
<span class="p_header">diff --git a/arch/powerpc/platforms/pseries/setup.c b/arch/powerpc/platforms/pseries/setup.c</span>
<span class="p_header">index a8531e012658..ae4f596273b5 100644</span>
<span class="p_header">--- a/arch/powerpc/platforms/pseries/setup.c</span>
<span class="p_header">+++ b/arch/powerpc/platforms/pseries/setup.c</span>
<span class="p_chunk">@@ -459,6 +459,39 @@</span> <span class="p_context"> static void __init find_and_init_phbs(void)</span>
 	of_pci_check_probe_only();
 }
 
<span class="p_add">+static void pseries_setup_rfi_flush(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct h_cpu_char_result result;</span>
<span class="p_add">+	enum l1d_flush_type types;</span>
<span class="p_add">+	bool enable;</span>
<span class="p_add">+	long rc;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Enable by default */</span>
<span class="p_add">+	enable = true;</span>
<span class="p_add">+</span>
<span class="p_add">+	rc = plpar_get_cpu_characteristics(&amp;result);</span>
<span class="p_add">+	if (rc == H_SUCCESS) {</span>
<span class="p_add">+		types = L1D_FLUSH_NONE;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (result.character &amp; H_CPU_CHAR_L1D_FLUSH_TRIG2)</span>
<span class="p_add">+			types |= L1D_FLUSH_MTTRIG;</span>
<span class="p_add">+		if (result.character &amp; H_CPU_CHAR_L1D_FLUSH_ORI30)</span>
<span class="p_add">+			types |= L1D_FLUSH_ORI;</span>
<span class="p_add">+</span>
<span class="p_add">+		/* Use fallback if nothing set in hcall */</span>
<span class="p_add">+		if (types == L1D_FLUSH_NONE)</span>
<span class="p_add">+			types = L1D_FLUSH_FALLBACK;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!(result.behaviour &amp; H_CPU_BEHAV_L1D_FLUSH_PR))</span>
<span class="p_add">+			enable = false;</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		/* Default to fallback if case hcall is not available */</span>
<span class="p_add">+		types = L1D_FLUSH_FALLBACK;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	setup_rfi_flush(types, enable);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static void __init pSeries_setup_arch(void)
 {
 	set_arch_panic_timeout(10, ARCH_PANIC_TIMEOUT);
<span class="p_chunk">@@ -476,6 +509,8 @@</span> <span class="p_context"> static void __init pSeries_setup_arch(void)</span>
 
 	fwnmi_init();
 
<span class="p_add">+	pseries_setup_rfi_flush();</span>
<span class="p_add">+</span>
 	/* By default, only probe PCI (can be overridden by rtas_pci) */
 	pci_add_flags(PCI_PROBE_ONLY);
 
<span class="p_header">diff --git a/arch/x86/entry/entry_32.S b/arch/x86/entry/entry_32.S</span>
<span class="p_header">index a1f28a54f23a..60c4c342316c 100644</span>
<span class="p_header">--- a/arch/x86/entry/entry_32.S</span>
<span class="p_header">+++ b/arch/x86/entry/entry_32.S</span>
<span class="p_chunk">@@ -244,6 +244,17 @@</span> <span class="p_context"> ENTRY(__switch_to_asm)</span>
 	movl	%ebx, PER_CPU_VAR(stack_canary)+stack_canary_offset
 #endif
 
<span class="p_add">+#ifdef CONFIG_RETPOLINE</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * When switching from a shallower to a deeper call stack</span>
<span class="p_add">+	 * the RSB may either underflow or use entries populated</span>
<span class="p_add">+	 * with userspace addresses. On CPUs where those concerns</span>
<span class="p_add">+	 * exist, overwrite the RSB with entries which capture</span>
<span class="p_add">+	 * speculative execution to prevent attack.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	FILL_RETURN_BUFFER %ebx, RSB_CLEAR_LOOPS, X86_FEATURE_RSB_CTXSW</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 	/* restore callee-saved registers */
 	popl	%esi
 	popl	%edi
<span class="p_header">diff --git a/arch/x86/entry/entry_64.S b/arch/x86/entry/entry_64.S</span>
<span class="p_header">index f5fda5f26e34..be6b66464f6a 100644</span>
<span class="p_header">--- a/arch/x86/entry/entry_64.S</span>
<span class="p_header">+++ b/arch/x86/entry/entry_64.S</span>
<span class="p_chunk">@@ -487,6 +487,17 @@</span> <span class="p_context"> ENTRY(__switch_to_asm)</span>
 	movq	%rbx, PER_CPU_VAR(irq_stack_union)+stack_canary_offset
 #endif
 
<span class="p_add">+#ifdef CONFIG_RETPOLINE</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * When switching from a shallower to a deeper call stack</span>
<span class="p_add">+	 * the RSB may either underflow or use entries populated</span>
<span class="p_add">+	 * with userspace addresses. On CPUs where those concerns</span>
<span class="p_add">+	 * exist, overwrite the RSB with entries which capture</span>
<span class="p_add">+	 * speculative execution to prevent attack.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	FILL_RETURN_BUFFER %r12, RSB_CLEAR_LOOPS, X86_FEATURE_RSB_CTXSW</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 	/* restore callee-saved registers */
 	popq	%r15
 	popq	%r14
<span class="p_chunk">@@ -1247,7 +1258,7 @@</span> <span class="p_context"> idtentry async_page_fault	do_async_page_fault	has_error_code=1</span>
 #endif
 
 #ifdef CONFIG_X86_MCE
<span class="p_del">-idtentry machine_check					has_error_code=0	paranoid=1 do_sym=*machine_check_vector(%rip)</span>
<span class="p_add">+idtentry machine_check		do_mce			has_error_code=0	paranoid=1</span>
 #endif
 
 /*
<span class="p_header">diff --git a/arch/x86/include/asm/cpufeatures.h b/arch/x86/include/asm/cpufeatures.h</span>
<span class="p_header">index f275447862f4..25b9375c1484 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/cpufeatures.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/cpufeatures.h</span>
<span class="p_chunk">@@ -206,11 +206,11 @@</span> <span class="p_context"></span>
 #define X86_FEATURE_RETPOLINE		( 7*32+12) /* Generic Retpoline mitigation for Spectre variant 2 */
 #define X86_FEATURE_RETPOLINE_AMD	( 7*32+13) /* AMD Retpoline mitigation for Spectre variant 2 */
 #define X86_FEATURE_INTEL_PPIN		( 7*32+14) /* Intel Processor Inventory Number */
<span class="p_del">-#define X86_FEATURE_INTEL_PT		( 7*32+15) /* Intel Processor Trace */</span>
 #define X86_FEATURE_AVX512_4VNNIW	( 7*32+16) /* AVX-512 Neural Network Instructions */
 #define X86_FEATURE_AVX512_4FMAPS	( 7*32+17) /* AVX-512 Multiply Accumulation Single precision */
 
 #define X86_FEATURE_MBA			( 7*32+18) /* Memory Bandwidth Allocation */
<span class="p_add">+#define X86_FEATURE_RSB_CTXSW		( 7*32+19) /* Fill RSB on context switches */</span>
 
 /* Virtualization flags: Linux defined, word 8 */
 #define X86_FEATURE_TPR_SHADOW		( 8*32+ 0) /* Intel TPR Shadow */
<span class="p_chunk">@@ -245,6 +245,7 @@</span> <span class="p_context"></span>
 #define X86_FEATURE_AVX512IFMA		( 9*32+21) /* AVX-512 Integer Fused Multiply-Add instructions */
 #define X86_FEATURE_CLFLUSHOPT		( 9*32+23) /* CLFLUSHOPT instruction */
 #define X86_FEATURE_CLWB		( 9*32+24) /* CLWB instruction */
<span class="p_add">+#define X86_FEATURE_INTEL_PT		( 9*32+25) /* Intel Processor Trace */</span>
 #define X86_FEATURE_AVX512PF		( 9*32+26) /* AVX-512 Prefetch */
 #define X86_FEATURE_AVX512ER		( 9*32+27) /* AVX-512 Exponential and Reciprocal */
 #define X86_FEATURE_AVX512CD		( 9*32+28) /* AVX-512 Conflict Detection */
<span class="p_header">diff --git a/arch/x86/include/asm/mem_encrypt.h b/arch/x86/include/asm/mem_encrypt.h</span>
<span class="p_header">index 6a77c63540f7..e7d96c0766fe 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/mem_encrypt.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/mem_encrypt.h</span>
<span class="p_chunk">@@ -39,7 +39,7 @@</span> <span class="p_context"> void __init sme_unmap_bootdata(char *real_mode_data);</span>
 
 void __init sme_early_init(void);
 
<span class="p_del">-void __init sme_encrypt_kernel(void);</span>
<span class="p_add">+void __init sme_encrypt_kernel(struct boot_params *bp);</span>
 void __init sme_enable(struct boot_params *bp);
 
 /* Architecture __weak replacement functions */
<span class="p_chunk">@@ -61,7 +61,7 @@</span> <span class="p_context"> static inline void __init sme_unmap_bootdata(char *real_mode_data) { }</span>
 
 static inline void __init sme_early_init(void) { }
 
<span class="p_del">-static inline void __init sme_encrypt_kernel(void) { }</span>
<span class="p_add">+static inline void __init sme_encrypt_kernel(struct boot_params *bp) { }</span>
 static inline void __init sme_enable(struct boot_params *bp) { }
 
 #endif	/* CONFIG_AMD_MEM_ENCRYPT */
<span class="p_header">diff --git a/arch/x86/include/asm/nospec-branch.h b/arch/x86/include/asm/nospec-branch.h</span>
<span class="p_header">index 402a11c803c3..4ad41087ce0e 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/nospec-branch.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/nospec-branch.h</span>
<span class="p_chunk">@@ -11,7 +11,7 @@</span> <span class="p_context"></span>
  * Fill the CPU return stack buffer.
  *
  * Each entry in the RSB, if used for a speculative &#39;ret&#39;, contains an
<span class="p_del">- * infinite &#39;pause; jmp&#39; loop to capture speculative execution.</span>
<span class="p_add">+ * infinite &#39;pause; lfence; jmp&#39; loop to capture speculative execution.</span>
  *
  * This is required in various cases for retpoline and IBRS-based
  * mitigations for the Spectre variant 2 vulnerability. Sometimes to
<span class="p_chunk">@@ -38,11 +38,13 @@</span> <span class="p_context"></span>
 	call	772f;				\
 773:	/* speculation trap */			\
 	pause;					\
<span class="p_add">+	lfence;					\</span>
 	jmp	773b;				\
 772:						\
 	call	774f;				\
 775:	/* speculation trap */			\
 	pause;					\
<span class="p_add">+	lfence;					\</span>
 	jmp	775b;				\
 774:						\
 	dec	reg;				\
<span class="p_chunk">@@ -73,6 +75,7 @@</span> <span class="p_context"></span>
 	call	.Ldo_rop_\@
 .Lspec_trap_\@:
 	pause
<span class="p_add">+	lfence</span>
 	jmp	.Lspec_trap_\@
 .Ldo_rop_\@:
 	mov	\reg, (%_ASM_SP)
<span class="p_chunk">@@ -165,6 +168,7 @@</span> <span class="p_context"></span>
 	&quot;       .align 16\n&quot;					\
 	&quot;901:	call   903f;\n&quot;					\
 	&quot;902:	pause;\n&quot;					\
<span class="p_add">+	&quot;    	lfence;\n&quot;					\</span>
 	&quot;       jmp    902b;\n&quot;					\
 	&quot;       .align 16\n&quot;					\
 	&quot;903:	addl   $4, %%esp;\n&quot;				\
<span class="p_chunk">@@ -190,6 +194,9 @@</span> <span class="p_context"> enum spectre_v2_mitigation {</span>
 	SPECTRE_V2_IBRS,
 };
 
<span class="p_add">+extern char __indirect_thunk_start[];</span>
<span class="p_add">+extern char __indirect_thunk_end[];</span>
<span class="p_add">+</span>
 /*
  * On VMEXIT we must ensure that no RSB predictions learned in the guest
  * can be followed in the host, by overwriting the RSB completely. Both
<span class="p_chunk">@@ -199,16 +206,17 @@</span> <span class="p_context"> enum spectre_v2_mitigation {</span>
 static inline void vmexit_fill_RSB(void)
 {
 #ifdef CONFIG_RETPOLINE
<span class="p_del">-	unsigned long loops = RSB_CLEAR_LOOPS / 2;</span>
<span class="p_add">+	unsigned long loops;</span>
 
 	asm volatile (ANNOTATE_NOSPEC_ALTERNATIVE
 		      ALTERNATIVE(&quot;jmp 910f&quot;,
 				  __stringify(__FILL_RETURN_BUFFER(%0, RSB_CLEAR_LOOPS, %1)),
 				  X86_FEATURE_RETPOLINE)
 		      &quot;910:&quot;
<span class="p_del">-		      : &quot;=&amp;r&quot; (loops), ASM_CALL_CONSTRAINT</span>
<span class="p_del">-		      : &quot;r&quot; (loops) : &quot;memory&quot; );</span>
<span class="p_add">+		      : &quot;=r&quot; (loops), ASM_CALL_CONSTRAINT</span>
<span class="p_add">+		      : : &quot;memory&quot; );</span>
 #endif
 }
<span class="p_add">+</span>
 #endif /* __ASSEMBLY__ */
 #endif /* __NOSPEC_BRANCH_H__ */
<span class="p_header">diff --git a/arch/x86/include/asm/traps.h b/arch/x86/include/asm/traps.h</span>
<span class="p_header">index 31051f35cbb7..3de69330e6c5 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/traps.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/traps.h</span>
<span class="p_chunk">@@ -88,6 +88,7 @@</span> <span class="p_context"> dotraplinkage void do_simd_coprocessor_error(struct pt_regs *, long);</span>
 #ifdef CONFIG_X86_32
 dotraplinkage void do_iret_error(struct pt_regs *, long);
 #endif
<span class="p_add">+dotraplinkage void do_mce(struct pt_regs *, long);</span>
 
 static inline int get_si_code(unsigned long condition)
 {
<span class="p_header">diff --git a/arch/x86/kernel/apic/vector.c b/arch/x86/kernel/apic/vector.c</span>
<span class="p_header">index 88c214e75a6b..2ce1c708b8ee 100644</span>
<span class="p_header">--- a/arch/x86/kernel/apic/vector.c</span>
<span class="p_header">+++ b/arch/x86/kernel/apic/vector.c</span>
<span class="p_chunk">@@ -369,8 +369,11 @@</span> <span class="p_context"> static int x86_vector_alloc_irqs(struct irq_domain *domain, unsigned int virq,</span>
 		irq_data-&gt;hwirq = virq + i;
 		err = assign_irq_vector_policy(virq + i, node, data, info,
 					       irq_data);
<span class="p_del">-		if (err)</span>
<span class="p_add">+		if (err) {</span>
<span class="p_add">+			irq_data-&gt;chip_data = NULL;</span>
<span class="p_add">+			free_apic_chip_data(data);</span>
 			goto error;
<span class="p_add">+		}</span>
 		/*
 		 * If the apic destination mode is physical, then the
 		 * effective affinity is restricted to a single target
<span class="p_chunk">@@ -383,7 +386,7 @@</span> <span class="p_context"> static int x86_vector_alloc_irqs(struct irq_domain *domain, unsigned int virq,</span>
 	return 0;
 
 error:
<span class="p_del">-	x86_vector_free_irqs(domain, virq, i + 1);</span>
<span class="p_add">+	x86_vector_free_irqs(domain, virq, i);</span>
 	return err;
 }
 
<span class="p_header">diff --git a/arch/x86/kernel/cpu/bugs.c b/arch/x86/kernel/cpu/bugs.c</span>
<span class="p_header">index e4dc26185aa7..390b3dc3d438 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/bugs.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/bugs.c</span>
<span class="p_chunk">@@ -23,6 +23,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/alternative.h&gt;
 #include &lt;asm/pgtable.h&gt;
 #include &lt;asm/set_memory.h&gt;
<span class="p_add">+#include &lt;asm/intel-family.h&gt;</span>
 
 static void __init spectre_v2_select_mitigation(void);
 
<span class="p_chunk">@@ -155,6 +156,23 @@</span> <span class="p_context"> static enum spectre_v2_mitigation_cmd __init spectre_v2_parse_cmdline(void)</span>
 	return SPECTRE_V2_CMD_NONE;
 }
 
<span class="p_add">+/* Check for Skylake-like CPUs (for RSB handling) */</span>
<span class="p_add">+static bool __init is_skylake_era(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (boot_cpu_data.x86_vendor == X86_VENDOR_INTEL &amp;&amp;</span>
<span class="p_add">+	    boot_cpu_data.x86 == 6) {</span>
<span class="p_add">+		switch (boot_cpu_data.x86_model) {</span>
<span class="p_add">+		case INTEL_FAM6_SKYLAKE_MOBILE:</span>
<span class="p_add">+		case INTEL_FAM6_SKYLAKE_DESKTOP:</span>
<span class="p_add">+		case INTEL_FAM6_SKYLAKE_X:</span>
<span class="p_add">+		case INTEL_FAM6_KABYLAKE_MOBILE:</span>
<span class="p_add">+		case INTEL_FAM6_KABYLAKE_DESKTOP:</span>
<span class="p_add">+			return true;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return false;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static void __init spectre_v2_select_mitigation(void)
 {
 	enum spectre_v2_mitigation_cmd cmd = spectre_v2_parse_cmdline();
<span class="p_chunk">@@ -213,6 +231,24 @@</span> <span class="p_context"> static void __init spectre_v2_select_mitigation(void)</span>
 
 	spectre_v2_enabled = mode;
 	pr_info(&quot;%s\n&quot;, spectre_v2_strings[mode]);
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If neither SMEP or KPTI are available, there is a risk of</span>
<span class="p_add">+	 * hitting userspace addresses in the RSB after a context switch</span>
<span class="p_add">+	 * from a shallow call stack to a deeper one. To prevent this fill</span>
<span class="p_add">+	 * the entire RSB, even when using IBRS.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * Skylake era CPUs have a separate issue with *underflow* of the</span>
<span class="p_add">+	 * RSB, when they will predict &#39;ret&#39; targets from the generic BTB.</span>
<span class="p_add">+	 * The proper mitigation for this is IBRS. If IBRS is not supported</span>
<span class="p_add">+	 * or deactivated in favour of retpolines the RSB fill on context</span>
<span class="p_add">+	 * switch is required.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if ((!boot_cpu_has(X86_FEATURE_PTI) &amp;&amp;</span>
<span class="p_add">+	     !boot_cpu_has(X86_FEATURE_SMEP)) || is_skylake_era()) {</span>
<span class="p_add">+		setup_force_cpu_cap(X86_FEATURE_RSB_CTXSW);</span>
<span class="p_add">+		pr_info(&quot;Filling RSB on context switch\n&quot;);</span>
<span class="p_add">+	}</span>
 }
 
 #undef pr_fmt
<span class="p_header">diff --git a/arch/x86/kernel/cpu/intel_rdt.c b/arch/x86/kernel/cpu/intel_rdt.c</span>
<span class="p_header">index 88dcf8479013..99442370de40 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/intel_rdt.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/intel_rdt.c</span>
<span class="p_chunk">@@ -525,10 +525,6 @@</span> <span class="p_context"> static void domain_remove_cpu(int cpu, struct rdt_resource *r)</span>
 		 */
 		if (static_branch_unlikely(&amp;rdt_mon_enable_key))
 			rmdir_mondata_subdir_allrdtgrp(r, d-&gt;id);
<span class="p_del">-		kfree(d-&gt;ctrl_val);</span>
<span class="p_del">-		kfree(d-&gt;rmid_busy_llc);</span>
<span class="p_del">-		kfree(d-&gt;mbm_total);</span>
<span class="p_del">-		kfree(d-&gt;mbm_local);</span>
 		list_del(&amp;d-&gt;list);
 		if (is_mbm_enabled())
 			cancel_delayed_work(&amp;d-&gt;mbm_over);
<span class="p_chunk">@@ -545,6 +541,10 @@</span> <span class="p_context"> static void domain_remove_cpu(int cpu, struct rdt_resource *r)</span>
 			cancel_delayed_work(&amp;d-&gt;cqm_limbo);
 		}
 
<span class="p_add">+		kfree(d-&gt;ctrl_val);</span>
<span class="p_add">+		kfree(d-&gt;rmid_busy_llc);</span>
<span class="p_add">+		kfree(d-&gt;mbm_total);</span>
<span class="p_add">+		kfree(d-&gt;mbm_local);</span>
 		kfree(d);
 		return;
 	}
<span class="p_header">diff --git a/arch/x86/kernel/cpu/mcheck/mce.c b/arch/x86/kernel/cpu/mcheck/mce.c</span>
<span class="p_header">index 3b413065c613..a9e898b71208 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/mcheck/mce.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/mcheck/mce.c</span>
<span class="p_chunk">@@ -1788,6 +1788,11 @@</span> <span class="p_context"> static void unexpected_machine_check(struct pt_regs *regs, long error_code)</span>
 void (*machine_check_vector)(struct pt_regs *, long error_code) =
 						unexpected_machine_check;
 
<span class="p_add">+dotraplinkage void do_mce(struct pt_regs *regs, long error_code)</span>
<span class="p_add">+{</span>
<span class="p_add">+	machine_check_vector(regs, error_code);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /*
  * Called for each booted CPU to set up machine checks.
  * Must be called with preempt off:
<span class="p_header">diff --git a/arch/x86/kernel/cpu/scattered.c b/arch/x86/kernel/cpu/scattered.c</span>
<span class="p_header">index 05459ad3db46..d0e69769abfd 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/scattered.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/scattered.c</span>
<span class="p_chunk">@@ -21,7 +21,6 @@</span> <span class="p_context"> struct cpuid_bit {</span>
 static const struct cpuid_bit cpuid_bits[] = {
 	{ X86_FEATURE_APERFMPERF,       CPUID_ECX,  0, 0x00000006, 0 },
 	{ X86_FEATURE_EPB,		CPUID_ECX,  3, 0x00000006, 0 },
<span class="p_del">-	{ X86_FEATURE_INTEL_PT,		CPUID_EBX, 25, 0x00000007, 0 },</span>
 	{ X86_FEATURE_AVX512_4VNNIW,    CPUID_EDX,  2, 0x00000007, 0 },
 	{ X86_FEATURE_AVX512_4FMAPS,    CPUID_EDX,  3, 0x00000007, 0 },
 	{ X86_FEATURE_CAT_L3,		CPUID_EBX,  1, 0x00000010, 0 },
<span class="p_header">diff --git a/arch/x86/kernel/head64.c b/arch/x86/kernel/head64.c</span>
<span class="p_header">index 6a5d757b9cfd..7ba5d819ebe3 100644</span>
<span class="p_header">--- a/arch/x86/kernel/head64.c</span>
<span class="p_header">+++ b/arch/x86/kernel/head64.c</span>
<span class="p_chunk">@@ -157,8 +157,8 @@</span> <span class="p_context"> unsigned long __head __startup_64(unsigned long physaddr,</span>
 	p = fixup_pointer(&amp;phys_base, physaddr);
 	*p += load_delta - sme_get_me_mask();
 
<span class="p_del">-	/* Encrypt the kernel (if SME is active) */</span>
<span class="p_del">-	sme_encrypt_kernel();</span>
<span class="p_add">+	/* Encrypt the kernel and related (if SME is active) */</span>
<span class="p_add">+	sme_encrypt_kernel(bp);</span>
 
 	/*
 	 * Return the SME encryption mask (if SME is active) to be used as a
<span class="p_header">diff --git a/arch/x86/kernel/idt.c b/arch/x86/kernel/idt.c</span>
<span class="p_header">index 014cb2fc47ff..236917bac5f2 100644</span>
<span class="p_header">--- a/arch/x86/kernel/idt.c</span>
<span class="p_header">+++ b/arch/x86/kernel/idt.c</span>
<span class="p_chunk">@@ -56,7 +56,7 @@</span> <span class="p_context"> struct idt_data {</span>
  * Early traps running on the DEFAULT_STACK because the other interrupt
  * stacks work only after cpu_init().
  */
<span class="p_del">-static const __initdata struct idt_data early_idts[] = {</span>
<span class="p_add">+static const __initconst struct idt_data early_idts[] = {</span>
 	INTG(X86_TRAP_DB,		debug),
 	SYSG(X86_TRAP_BP,		int3),
 #ifdef CONFIG_X86_32
<span class="p_chunk">@@ -70,7 +70,7 @@</span> <span class="p_context"> static const __initdata struct idt_data early_idts[] = {</span>
  * the traps which use them are reinitialized with IST after cpu_init() has
  * set up TSS.
  */
<span class="p_del">-static const __initdata struct idt_data def_idts[] = {</span>
<span class="p_add">+static const __initconst struct idt_data def_idts[] = {</span>
 	INTG(X86_TRAP_DE,		divide_error),
 	INTG(X86_TRAP_NMI,		nmi),
 	INTG(X86_TRAP_BR,		bounds),
<span class="p_chunk">@@ -108,7 +108,7 @@</span> <span class="p_context"> static const __initdata struct idt_data def_idts[] = {</span>
 /*
  * The APIC and SMP idt entries
  */
<span class="p_del">-static const __initdata struct idt_data apic_idts[] = {</span>
<span class="p_add">+static const __initconst struct idt_data apic_idts[] = {</span>
 #ifdef CONFIG_SMP
 	INTG(RESCHEDULE_VECTOR,		reschedule_interrupt),
 	INTG(CALL_FUNCTION_VECTOR,	call_function_interrupt),
<span class="p_chunk">@@ -150,7 +150,7 @@</span> <span class="p_context"> static const __initdata struct idt_data apic_idts[] = {</span>
  * Early traps running on the DEFAULT_STACK because the other interrupt
  * stacks work only after cpu_init().
  */
<span class="p_del">-static const __initdata struct idt_data early_pf_idts[] = {</span>
<span class="p_add">+static const __initconst struct idt_data early_pf_idts[] = {</span>
 	INTG(X86_TRAP_PF,		page_fault),
 };
 
<span class="p_chunk">@@ -158,7 +158,7 @@</span> <span class="p_context"> static const __initdata struct idt_data early_pf_idts[] = {</span>
  * Override for the debug_idt. Same as the default, but with interrupt
  * stack set to DEFAULT_STACK (0). Required for NMI trap handling.
  */
<span class="p_del">-static const __initdata struct idt_data dbg_idts[] = {</span>
<span class="p_add">+static const __initconst struct idt_data dbg_idts[] = {</span>
 	INTG(X86_TRAP_DB,	debug),
 	INTG(X86_TRAP_BP,	int3),
 };
<span class="p_chunk">@@ -180,7 +180,7 @@</span> <span class="p_context"> gate_desc debug_idt_table[IDT_ENTRIES] __page_aligned_bss;</span>
  * The exceptions which use Interrupt stacks. They are setup after
  * cpu_init() when the TSS has been initialized.
  */
<span class="p_del">-static const __initdata struct idt_data ist_idts[] = {</span>
<span class="p_add">+static const __initconst struct idt_data ist_idts[] = {</span>
 	ISTG(X86_TRAP_DB,	debug,		DEBUG_STACK),
 	ISTG(X86_TRAP_NMI,	nmi,		NMI_STACK),
 	SISTG(X86_TRAP_BP,	int3,		DEBUG_STACK),
<span class="p_header">diff --git a/arch/x86/kernel/kprobes/opt.c b/arch/x86/kernel/kprobes/opt.c</span>
<span class="p_header">index 4f98aad38237..3668f28cf5fc 100644</span>
<span class="p_header">--- a/arch/x86/kernel/kprobes/opt.c</span>
<span class="p_header">+++ b/arch/x86/kernel/kprobes/opt.c</span>
<span class="p_chunk">@@ -40,6 +40,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/debugreg.h&gt;
 #include &lt;asm/set_memory.h&gt;
 #include &lt;asm/sections.h&gt;
<span class="p_add">+#include &lt;asm/nospec-branch.h&gt;</span>
 
 #include &quot;common.h&quot;
 
<span class="p_chunk">@@ -205,7 +206,7 @@</span> <span class="p_context"> static int copy_optimized_instructions(u8 *dest, u8 *src)</span>
 }
 
 /* Check whether insn is indirect jump */
<span class="p_del">-static int insn_is_indirect_jump(struct insn *insn)</span>
<span class="p_add">+static int __insn_is_indirect_jump(struct insn *insn)</span>
 {
 	return ((insn-&gt;opcode.bytes[0] == 0xff &amp;&amp;
 		(X86_MODRM_REG(insn-&gt;modrm.value) &amp; 6) == 4) || /* Jump */
<span class="p_chunk">@@ -239,6 +240,26 @@</span> <span class="p_context"> static int insn_jump_into_range(struct insn *insn, unsigned long start, int len)</span>
 	return (start &lt;= target &amp;&amp; target &lt;= start + len);
 }
 
<span class="p_add">+static int insn_is_indirect_jump(struct insn *insn)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int ret = __insn_is_indirect_jump(insn);</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_RETPOLINE</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Jump to x86_indirect_thunk_* is treated as an indirect jump.</span>
<span class="p_add">+	 * Note that even with CONFIG_RETPOLINE=y, the kernel compiled with</span>
<span class="p_add">+	 * older gcc may use indirect jump. So we add this check instead of</span>
<span class="p_add">+	 * replace indirect-jump check.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (!ret)</span>
<span class="p_add">+		ret = insn_jump_into_range(insn,</span>
<span class="p_add">+				(unsigned long)__indirect_thunk_start,</span>
<span class="p_add">+				(unsigned long)__indirect_thunk_end -</span>
<span class="p_add">+				(unsigned long)__indirect_thunk_start);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /* Decode whole function to ensure any instructions don&#39;t jump into target */
 static int can_optimize(unsigned long paddr)
 {
<span class="p_header">diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c</span>
<span class="p_header">index 3cb2486c47e4..8bd1d8292cf7 100644</span>
<span class="p_header">--- a/arch/x86/kernel/process.c</span>
<span class="p_header">+++ b/arch/x86/kernel/process.c</span>
<span class="p_chunk">@@ -380,19 +380,24 @@</span> <span class="p_context"> void stop_this_cpu(void *dummy)</span>
 	disable_local_APIC();
 	mcheck_cpu_clear(this_cpu_ptr(&amp;cpu_info));
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Use wbinvd on processors that support SME. This provides support</span>
<span class="p_add">+	 * for performing a successful kexec when going from SME inactive</span>
<span class="p_add">+	 * to SME active (or vice-versa). The cache must be cleared so that</span>
<span class="p_add">+	 * if there are entries with the same physical address, both with and</span>
<span class="p_add">+	 * without the encryption bit, they don&#39;t race each other when flushed</span>
<span class="p_add">+	 * and potentially end up with the wrong entry being committed to</span>
<span class="p_add">+	 * memory.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (boot_cpu_has(X86_FEATURE_SME))</span>
<span class="p_add">+		native_wbinvd();</span>
 	for (;;) {
 		/*
<span class="p_del">-		 * Use wbinvd followed by hlt to stop the processor. This</span>
<span class="p_del">-		 * provides support for kexec on a processor that supports</span>
<span class="p_del">-		 * SME. With kexec, going from SME inactive to SME active</span>
<span class="p_del">-		 * requires clearing cache entries so that addresses without</span>
<span class="p_del">-		 * the encryption bit set don&#39;t corrupt the same physical</span>
<span class="p_del">-		 * address that has the encryption bit set when caches are</span>
<span class="p_del">-		 * flushed. To achieve this a wbinvd is performed followed by</span>
<span class="p_del">-		 * a hlt. Even if the processor is not in the kexec/SME</span>
<span class="p_del">-		 * scenario this only adds a wbinvd to a halting processor.</span>
<span class="p_add">+		 * Use native_halt() so that memory contents don&#39;t change</span>
<span class="p_add">+		 * (stack usage and variables) after possibly issuing the</span>
<span class="p_add">+		 * native_wbinvd() above.</span>
 		 */
<span class="p_del">-		asm volatile(&quot;wbinvd; hlt&quot; : : : &quot;memory&quot;);</span>
<span class="p_add">+		native_halt();</span>
 	}
 }
 
<span class="p_header">diff --git a/arch/x86/kernel/setup.c b/arch/x86/kernel/setup.c</span>
<span class="p_header">index 0957dd73d127..e84cb4c75cd0 100644</span>
<span class="p_header">--- a/arch/x86/kernel/setup.c</span>
<span class="p_header">+++ b/arch/x86/kernel/setup.c</span>
<span class="p_chunk">@@ -376,14 +376,6 @@</span> <span class="p_context"> static void __init reserve_initrd(void)</span>
 	    !ramdisk_image || !ramdisk_size)
 		return;		/* No initrd provided by bootloader */
 
<span class="p_del">-	/*</span>
<span class="p_del">-	 * If SME is active, this memory will be marked encrypted by the</span>
<span class="p_del">-	 * kernel when it is accessed (including relocation). However, the</span>
<span class="p_del">-	 * ramdisk image was loaded decrypted by the bootloader, so make</span>
<span class="p_del">-	 * sure that it is encrypted before accessing it.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	sme_early_encrypt(ramdisk_image, ramdisk_end - ramdisk_image);</span>
<span class="p_del">-</span>
 	initrd_start = 0;
 
 	mapped_size = memblock_mem_size(max_pfn_mapped);
<span class="p_header">diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c</span>
<span class="p_header">index ad2b925a808e..47506567435e 100644</span>
<span class="p_header">--- a/arch/x86/kernel/tsc.c</span>
<span class="p_header">+++ b/arch/x86/kernel/tsc.c</span>
<span class="p_chunk">@@ -602,7 +602,6 @@</span> <span class="p_context"> unsigned long native_calibrate_tsc(void)</span>
 		case INTEL_FAM6_KABYLAKE_DESKTOP:
 			crystal_khz = 24000;	/* 24.0 MHz */
 			break;
<span class="p_del">-		case INTEL_FAM6_SKYLAKE_X:</span>
 		case INTEL_FAM6_ATOM_DENVERTON:
 			crystal_khz = 25000;	/* 25.0 MHz */
 			break;
<span class="p_chunk">@@ -612,6 +611,8 @@</span> <span class="p_context"> unsigned long native_calibrate_tsc(void)</span>
 		}
 	}
 
<span class="p_add">+	if (crystal_khz == 0)</span>
<span class="p_add">+		return 0;</span>
 	/*
 	 * TSC frequency determined by CPUID is a &quot;hardware reported&quot;
 	 * frequency and is the most accurate one so far we have. This
<span class="p_header">diff --git a/arch/x86/kernel/vmlinux.lds.S b/arch/x86/kernel/vmlinux.lds.S</span>
<span class="p_header">index 1e413a9326aa..9b138a06c1a4 100644</span>
<span class="p_header">--- a/arch/x86/kernel/vmlinux.lds.S</span>
<span class="p_header">+++ b/arch/x86/kernel/vmlinux.lds.S</span>
<span class="p_chunk">@@ -124,6 +124,12 @@</span> <span class="p_context"> SECTIONS</span>
 		ASSERT(. - _entry_trampoline == PAGE_SIZE, &quot;entry trampoline is too big&quot;);
 #endif
 
<span class="p_add">+#ifdef CONFIG_RETPOLINE</span>
<span class="p_add">+		__indirect_thunk_start = .;</span>
<span class="p_add">+		*(.text.__x86.indirect_thunk)</span>
<span class="p_add">+		__indirect_thunk_end = .;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 		/* End of text section */
 		_etext = .;
 	} :text = 0x9090
<span class="p_header">diff --git a/arch/x86/lib/retpoline.S b/arch/x86/lib/retpoline.S</span>
<span class="p_header">index cb45c6cb465f..dfb2ba91b670 100644</span>
<span class="p_header">--- a/arch/x86/lib/retpoline.S</span>
<span class="p_header">+++ b/arch/x86/lib/retpoline.S</span>
<span class="p_chunk">@@ -9,7 +9,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/nospec-branch.h&gt;
 
 .macro THUNK reg
<span class="p_del">-	.section .text.__x86.indirect_thunk.\reg</span>
<span class="p_add">+	.section .text.__x86.indirect_thunk</span>
 
 ENTRY(__x86_indirect_thunk_\reg)
 	CFI_STARTPROC
<span class="p_chunk">@@ -25,7 +25,8 @@</span> <span class="p_context"> ENDPROC(__x86_indirect_thunk_\reg)</span>
  * than one per register with the correct names. So we do it
  * the simple and nasty way...
  */
<span class="p_del">-#define EXPORT_THUNK(reg) EXPORT_SYMBOL(__x86_indirect_thunk_ ## reg)</span>
<span class="p_add">+#define __EXPORT_THUNK(sym) _ASM_NOKPROBE(sym); EXPORT_SYMBOL(sym)</span>
<span class="p_add">+#define EXPORT_THUNK(reg) __EXPORT_THUNK(__x86_indirect_thunk_ ## reg)</span>
 #define GENERATE_THUNK(reg) THUNK reg ; EXPORT_THUNK(reg)
 
 GENERATE_THUNK(_ASM_AX)
<span class="p_header">diff --git a/arch/x86/mm/fault.c b/arch/x86/mm/fault.c</span>
<span class="p_header">index 3109ba6c6ede..b264b590eeec 100644</span>
<span class="p_header">--- a/arch/x86/mm/fault.c</span>
<span class="p_header">+++ b/arch/x86/mm/fault.c</span>
<span class="p_chunk">@@ -173,14 +173,15 @@</span> <span class="p_context"> is_prefetch(struct pt_regs *regs, unsigned long error_code, unsigned long addr)</span>
  * 6. T1   : reaches here, sees vma_pkey(vma)=5, when we really
  *	     faulted on a pte with its pkey=4.
  */
<span class="p_del">-static void fill_sig_info_pkey(int si_code, siginfo_t *info, u32 *pkey)</span>
<span class="p_add">+static void fill_sig_info_pkey(int si_signo, int si_code, siginfo_t *info,</span>
<span class="p_add">+		u32 *pkey)</span>
 {
 	/* This is effectively an #ifdef */
 	if (!boot_cpu_has(X86_FEATURE_OSPKE))
 		return;
 
 	/* Fault not from Protection Keys: nothing to do */
<span class="p_del">-	if (si_code != SEGV_PKUERR)</span>
<span class="p_add">+	if ((si_code != SEGV_PKUERR) || (si_signo != SIGSEGV))</span>
 		return;
 	/*
 	 * force_sig_info_fault() is called from a number of
<span class="p_chunk">@@ -219,7 +220,7 @@</span> <span class="p_context"> force_sig_info_fault(int si_signo, int si_code, unsigned long address,</span>
 		lsb = PAGE_SHIFT;
 	info.si_addr_lsb = lsb;
 
<span class="p_del">-	fill_sig_info_pkey(si_code, &amp;info, pkey);</span>
<span class="p_add">+	fill_sig_info_pkey(si_signo, si_code, &amp;info, pkey);</span>
 
 	force_sig_info(si_signo, &amp;info, tsk);
 }
<span class="p_header">diff --git a/arch/x86/mm/kasan_init_64.c b/arch/x86/mm/kasan_init_64.c</span>
<span class="p_header">index 47388f0c0e59..af6f2f9c6a26 100644</span>
<span class="p_header">--- a/arch/x86/mm/kasan_init_64.c</span>
<span class="p_header">+++ b/arch/x86/mm/kasan_init_64.c</span>
<span class="p_chunk">@@ -21,10 +21,14 @@</span> <span class="p_context"> extern struct range pfn_mapped[E820_MAX_ENTRIES];</span>
 
 static p4d_t tmp_p4d_table[PTRS_PER_P4D] __initdata __aligned(PAGE_SIZE);
 
<span class="p_del">-static __init void *early_alloc(size_t size, int nid)</span>
<span class="p_add">+static __init void *early_alloc(size_t size, int nid, bool panic)</span>
 {
<span class="p_del">-	return memblock_virt_alloc_try_nid_nopanic(size, size,</span>
<span class="p_del">-		__pa(MAX_DMA_ADDRESS), BOOTMEM_ALLOC_ACCESSIBLE, nid);</span>
<span class="p_add">+	if (panic)</span>
<span class="p_add">+		return memblock_virt_alloc_try_nid(size, size,</span>
<span class="p_add">+			__pa(MAX_DMA_ADDRESS), BOOTMEM_ALLOC_ACCESSIBLE, nid);</span>
<span class="p_add">+	else</span>
<span class="p_add">+		return memblock_virt_alloc_try_nid_nopanic(size, size,</span>
<span class="p_add">+			__pa(MAX_DMA_ADDRESS), BOOTMEM_ALLOC_ACCESSIBLE, nid);</span>
 }
 
 static void __init kasan_populate_pmd(pmd_t *pmd, unsigned long addr,
<span class="p_chunk">@@ -38,14 +42,14 @@</span> <span class="p_context"> static void __init kasan_populate_pmd(pmd_t *pmd, unsigned long addr,</span>
 		if (boot_cpu_has(X86_FEATURE_PSE) &amp;&amp;
 		    ((end - addr) == PMD_SIZE) &amp;&amp;
 		    IS_ALIGNED(addr, PMD_SIZE)) {
<span class="p_del">-			p = early_alloc(PMD_SIZE, nid);</span>
<span class="p_add">+			p = early_alloc(PMD_SIZE, nid, false);</span>
 			if (p &amp;&amp; pmd_set_huge(pmd, __pa(p), PAGE_KERNEL))
 				return;
 			else if (p)
 				memblock_free(__pa(p), PMD_SIZE);
 		}
 
<span class="p_del">-		p = early_alloc(PAGE_SIZE, nid);</span>
<span class="p_add">+		p = early_alloc(PAGE_SIZE, nid, true);</span>
 		pmd_populate_kernel(&amp;init_mm, pmd, p);
 	}
 
<span class="p_chunk">@@ -57,7 +61,7 @@</span> <span class="p_context"> static void __init kasan_populate_pmd(pmd_t *pmd, unsigned long addr,</span>
 		if (!pte_none(*pte))
 			continue;
 
<span class="p_del">-		p = early_alloc(PAGE_SIZE, nid);</span>
<span class="p_add">+		p = early_alloc(PAGE_SIZE, nid, true);</span>
 		entry = pfn_pte(PFN_DOWN(__pa(p)), PAGE_KERNEL);
 		set_pte_at(&amp;init_mm, addr, pte, entry);
 	} while (pte++, addr += PAGE_SIZE, addr != end);
<span class="p_chunk">@@ -75,14 +79,14 @@</span> <span class="p_context"> static void __init kasan_populate_pud(pud_t *pud, unsigned long addr,</span>
 		if (boot_cpu_has(X86_FEATURE_GBPAGES) &amp;&amp;
 		    ((end - addr) == PUD_SIZE) &amp;&amp;
 		    IS_ALIGNED(addr, PUD_SIZE)) {
<span class="p_del">-			p = early_alloc(PUD_SIZE, nid);</span>
<span class="p_add">+			p = early_alloc(PUD_SIZE, nid, false);</span>
 			if (p &amp;&amp; pud_set_huge(pud, __pa(p), PAGE_KERNEL))
 				return;
 			else if (p)
 				memblock_free(__pa(p), PUD_SIZE);
 		}
 
<span class="p_del">-		p = early_alloc(PAGE_SIZE, nid);</span>
<span class="p_add">+		p = early_alloc(PAGE_SIZE, nid, true);</span>
 		pud_populate(&amp;init_mm, pud, p);
 	}
 
<span class="p_chunk">@@ -101,7 +105,7 @@</span> <span class="p_context"> static void __init kasan_populate_p4d(p4d_t *p4d, unsigned long addr,</span>
 	unsigned long next;
 
 	if (p4d_none(*p4d)) {
<span class="p_del">-		void *p = early_alloc(PAGE_SIZE, nid);</span>
<span class="p_add">+		void *p = early_alloc(PAGE_SIZE, nid, true);</span>
 
 		p4d_populate(&amp;init_mm, p4d, p);
 	}
<span class="p_chunk">@@ -122,7 +126,7 @@</span> <span class="p_context"> static void __init kasan_populate_pgd(pgd_t *pgd, unsigned long addr,</span>
 	unsigned long next;
 
 	if (pgd_none(*pgd)) {
<span class="p_del">-		p = early_alloc(PAGE_SIZE, nid);</span>
<span class="p_add">+		p = early_alloc(PAGE_SIZE, nid, true);</span>
 		pgd_populate(&amp;init_mm, pgd, p);
 	}
 
<span class="p_header">diff --git a/arch/x86/mm/mem_encrypt.c b/arch/x86/mm/mem_encrypt.c</span>
<span class="p_header">index 0286327e65fa..48c03c74c7f4 100644</span>
<span class="p_header">--- a/arch/x86/mm/mem_encrypt.c</span>
<span class="p_header">+++ b/arch/x86/mm/mem_encrypt.c</span>
<span class="p_chunk">@@ -213,37 +213,62 @@</span> <span class="p_context"> void swiotlb_set_mem_attributes(void *vaddr, unsigned long size)</span>
 	set_memory_decrypted((unsigned long)vaddr, size &gt;&gt; PAGE_SHIFT);
 }
 
<span class="p_del">-static void __init sme_clear_pgd(pgd_t *pgd_base, unsigned long start,</span>
<span class="p_del">-				 unsigned long end)</span>
<span class="p_add">+struct sme_populate_pgd_data {</span>
<span class="p_add">+	void	*pgtable_area;</span>
<span class="p_add">+	pgd_t	*pgd;</span>
<span class="p_add">+</span>
<span class="p_add">+	pmdval_t pmd_flags;</span>
<span class="p_add">+	pteval_t pte_flags;</span>
<span class="p_add">+	unsigned long paddr;</span>
<span class="p_add">+</span>
<span class="p_add">+	unsigned long vaddr;</span>
<span class="p_add">+	unsigned long vaddr_end;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+static void __init sme_clear_pgd(struct sme_populate_pgd_data *ppd)</span>
 {
 	unsigned long pgd_start, pgd_end, pgd_size;
 	pgd_t *pgd_p;
 
<span class="p_del">-	pgd_start = start &amp; PGDIR_MASK;</span>
<span class="p_del">-	pgd_end = end &amp; PGDIR_MASK;</span>
<span class="p_add">+	pgd_start = ppd-&gt;vaddr &amp; PGDIR_MASK;</span>
<span class="p_add">+	pgd_end = ppd-&gt;vaddr_end &amp; PGDIR_MASK;</span>
 
<span class="p_del">-	pgd_size = (((pgd_end - pgd_start) / PGDIR_SIZE) + 1);</span>
<span class="p_del">-	pgd_size *= sizeof(pgd_t);</span>
<span class="p_add">+	pgd_size = (((pgd_end - pgd_start) / PGDIR_SIZE) + 1) * sizeof(pgd_t);</span>
 
<span class="p_del">-	pgd_p = pgd_base + pgd_index(start);</span>
<span class="p_add">+	pgd_p = ppd-&gt;pgd + pgd_index(ppd-&gt;vaddr);</span>
 
 	memset(pgd_p, 0, pgd_size);
 }
 
<span class="p_del">-#define PGD_FLAGS	_KERNPG_TABLE_NOENC</span>
<span class="p_del">-#define P4D_FLAGS	_KERNPG_TABLE_NOENC</span>
<span class="p_del">-#define PUD_FLAGS	_KERNPG_TABLE_NOENC</span>
<span class="p_del">-#define PMD_FLAGS	(__PAGE_KERNEL_LARGE_EXEC &amp; ~_PAGE_GLOBAL)</span>
<span class="p_add">+#define PGD_FLAGS		_KERNPG_TABLE_NOENC</span>
<span class="p_add">+#define P4D_FLAGS		_KERNPG_TABLE_NOENC</span>
<span class="p_add">+#define PUD_FLAGS		_KERNPG_TABLE_NOENC</span>
<span class="p_add">+#define PMD_FLAGS		_KERNPG_TABLE_NOENC</span>
<span class="p_add">+</span>
<span class="p_add">+#define PMD_FLAGS_LARGE		(__PAGE_KERNEL_LARGE_EXEC &amp; ~_PAGE_GLOBAL)</span>
<span class="p_add">+</span>
<span class="p_add">+#define PMD_FLAGS_DEC		PMD_FLAGS_LARGE</span>
<span class="p_add">+#define PMD_FLAGS_DEC_WP	((PMD_FLAGS_DEC &amp; ~_PAGE_CACHE_MASK) | \</span>
<span class="p_add">+				 (_PAGE_PAT | _PAGE_PWT))</span>
<span class="p_add">+</span>
<span class="p_add">+#define PMD_FLAGS_ENC		(PMD_FLAGS_LARGE | _PAGE_ENC)</span>
<span class="p_add">+</span>
<span class="p_add">+#define PTE_FLAGS		(__PAGE_KERNEL_EXEC &amp; ~_PAGE_GLOBAL)</span>
<span class="p_add">+</span>
<span class="p_add">+#define PTE_FLAGS_DEC		PTE_FLAGS</span>
<span class="p_add">+#define PTE_FLAGS_DEC_WP	((PTE_FLAGS_DEC &amp; ~_PAGE_CACHE_MASK) | \</span>
<span class="p_add">+				 (_PAGE_PAT | _PAGE_PWT))</span>
<span class="p_add">+</span>
<span class="p_add">+#define PTE_FLAGS_ENC		(PTE_FLAGS | _PAGE_ENC)</span>
 
<span class="p_del">-static void __init *sme_populate_pgd(pgd_t *pgd_base, void *pgtable_area,</span>
<span class="p_del">-				     unsigned long vaddr, pmdval_t pmd_val)</span>
<span class="p_add">+static pmd_t __init *sme_prepare_pgd(struct sme_populate_pgd_data *ppd)</span>
 {
 	pgd_t *pgd_p;
 	p4d_t *p4d_p;
 	pud_t *pud_p;
 	pmd_t *pmd_p;
 
<span class="p_del">-	pgd_p = pgd_base + pgd_index(vaddr);</span>
<span class="p_add">+	pgd_p = ppd-&gt;pgd + pgd_index(ppd-&gt;vaddr);</span>
 	if (native_pgd_val(*pgd_p)) {
 		if (IS_ENABLED(CONFIG_X86_5LEVEL))
 			p4d_p = (p4d_t *)(native_pgd_val(*pgd_p) &amp; ~PTE_FLAGS_MASK);
<span class="p_chunk">@@ -253,15 +278,15 @@</span> <span class="p_context"> static void __init *sme_populate_pgd(pgd_t *pgd_base, void *pgtable_area,</span>
 		pgd_t pgd;
 
 		if (IS_ENABLED(CONFIG_X86_5LEVEL)) {
<span class="p_del">-			p4d_p = pgtable_area;</span>
<span class="p_add">+			p4d_p = ppd-&gt;pgtable_area;</span>
 			memset(p4d_p, 0, sizeof(*p4d_p) * PTRS_PER_P4D);
<span class="p_del">-			pgtable_area += sizeof(*p4d_p) * PTRS_PER_P4D;</span>
<span class="p_add">+			ppd-&gt;pgtable_area += sizeof(*p4d_p) * PTRS_PER_P4D;</span>
 
 			pgd = native_make_pgd((pgdval_t)p4d_p + PGD_FLAGS);
 		} else {
<span class="p_del">-			pud_p = pgtable_area;</span>
<span class="p_add">+			pud_p = ppd-&gt;pgtable_area;</span>
 			memset(pud_p, 0, sizeof(*pud_p) * PTRS_PER_PUD);
<span class="p_del">-			pgtable_area += sizeof(*pud_p) * PTRS_PER_PUD;</span>
<span class="p_add">+			ppd-&gt;pgtable_area += sizeof(*pud_p) * PTRS_PER_PUD;</span>
 
 			pgd = native_make_pgd((pgdval_t)pud_p + PGD_FLAGS);
 		}
<span class="p_chunk">@@ -269,58 +294,160 @@</span> <span class="p_context"> static void __init *sme_populate_pgd(pgd_t *pgd_base, void *pgtable_area,</span>
 	}
 
 	if (IS_ENABLED(CONFIG_X86_5LEVEL)) {
<span class="p_del">-		p4d_p += p4d_index(vaddr);</span>
<span class="p_add">+		p4d_p += p4d_index(ppd-&gt;vaddr);</span>
 		if (native_p4d_val(*p4d_p)) {
 			pud_p = (pud_t *)(native_p4d_val(*p4d_p) &amp; ~PTE_FLAGS_MASK);
 		} else {
 			p4d_t p4d;
 
<span class="p_del">-			pud_p = pgtable_area;</span>
<span class="p_add">+			pud_p = ppd-&gt;pgtable_area;</span>
 			memset(pud_p, 0, sizeof(*pud_p) * PTRS_PER_PUD);
<span class="p_del">-			pgtable_area += sizeof(*pud_p) * PTRS_PER_PUD;</span>
<span class="p_add">+			ppd-&gt;pgtable_area += sizeof(*pud_p) * PTRS_PER_PUD;</span>
 
 			p4d = native_make_p4d((pudval_t)pud_p + P4D_FLAGS);
 			native_set_p4d(p4d_p, p4d);
 		}
 	}
 
<span class="p_del">-	pud_p += pud_index(vaddr);</span>
<span class="p_add">+	pud_p += pud_index(ppd-&gt;vaddr);</span>
 	if (native_pud_val(*pud_p)) {
 		if (native_pud_val(*pud_p) &amp; _PAGE_PSE)
<span class="p_del">-			goto out;</span>
<span class="p_add">+			return NULL;</span>
 
 		pmd_p = (pmd_t *)(native_pud_val(*pud_p) &amp; ~PTE_FLAGS_MASK);
 	} else {
 		pud_t pud;
 
<span class="p_del">-		pmd_p = pgtable_area;</span>
<span class="p_add">+		pmd_p = ppd-&gt;pgtable_area;</span>
 		memset(pmd_p, 0, sizeof(*pmd_p) * PTRS_PER_PMD);
<span class="p_del">-		pgtable_area += sizeof(*pmd_p) * PTRS_PER_PMD;</span>
<span class="p_add">+		ppd-&gt;pgtable_area += sizeof(*pmd_p) * PTRS_PER_PMD;</span>
 
 		pud = native_make_pud((pmdval_t)pmd_p + PUD_FLAGS);
 		native_set_pud(pud_p, pud);
 	}
 
<span class="p_del">-	pmd_p += pmd_index(vaddr);</span>
<span class="p_add">+	return pmd_p;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void __init sme_populate_pgd_large(struct sme_populate_pgd_data *ppd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pmd_t *pmd_p;</span>
<span class="p_add">+</span>
<span class="p_add">+	pmd_p = sme_prepare_pgd(ppd);</span>
<span class="p_add">+	if (!pmd_p)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	pmd_p += pmd_index(ppd-&gt;vaddr);</span>
 	if (!native_pmd_val(*pmd_p) || !(native_pmd_val(*pmd_p) &amp; _PAGE_PSE))
<span class="p_del">-		native_set_pmd(pmd_p, native_make_pmd(pmd_val));</span>
<span class="p_add">+		native_set_pmd(pmd_p, native_make_pmd(ppd-&gt;paddr | ppd-&gt;pmd_flags));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void __init sme_populate_pgd(struct sme_populate_pgd_data *ppd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pmd_t *pmd_p;</span>
<span class="p_add">+	pte_t *pte_p;</span>
<span class="p_add">+</span>
<span class="p_add">+	pmd_p = sme_prepare_pgd(ppd);</span>
<span class="p_add">+	if (!pmd_p)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	pmd_p += pmd_index(ppd-&gt;vaddr);</span>
<span class="p_add">+	if (native_pmd_val(*pmd_p)) {</span>
<span class="p_add">+		if (native_pmd_val(*pmd_p) &amp; _PAGE_PSE)</span>
<span class="p_add">+			return;</span>
<span class="p_add">+</span>
<span class="p_add">+		pte_p = (pte_t *)(native_pmd_val(*pmd_p) &amp; ~PTE_FLAGS_MASK);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		pmd_t pmd;</span>
<span class="p_add">+</span>
<span class="p_add">+		pte_p = ppd-&gt;pgtable_area;</span>
<span class="p_add">+		memset(pte_p, 0, sizeof(*pte_p) * PTRS_PER_PTE);</span>
<span class="p_add">+		ppd-&gt;pgtable_area += sizeof(*pte_p) * PTRS_PER_PTE;</span>
<span class="p_add">+</span>
<span class="p_add">+		pmd = native_make_pmd((pteval_t)pte_p + PMD_FLAGS);</span>
<span class="p_add">+		native_set_pmd(pmd_p, pmd);</span>
<span class="p_add">+	}</span>
 
<span class="p_del">-out:</span>
<span class="p_del">-	return pgtable_area;</span>
<span class="p_add">+	pte_p += pte_index(ppd-&gt;vaddr);</span>
<span class="p_add">+	if (!native_pte_val(*pte_p))</span>
<span class="p_add">+		native_set_pte(pte_p, native_make_pte(ppd-&gt;paddr | ppd-&gt;pte_flags));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void __init __sme_map_range_pmd(struct sme_populate_pgd_data *ppd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	while (ppd-&gt;vaddr &lt; ppd-&gt;vaddr_end) {</span>
<span class="p_add">+		sme_populate_pgd_large(ppd);</span>
<span class="p_add">+</span>
<span class="p_add">+		ppd-&gt;vaddr += PMD_PAGE_SIZE;</span>
<span class="p_add">+		ppd-&gt;paddr += PMD_PAGE_SIZE;</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void __init __sme_map_range_pte(struct sme_populate_pgd_data *ppd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	while (ppd-&gt;vaddr &lt; ppd-&gt;vaddr_end) {</span>
<span class="p_add">+		sme_populate_pgd(ppd);</span>
<span class="p_add">+</span>
<span class="p_add">+		ppd-&gt;vaddr += PAGE_SIZE;</span>
<span class="p_add">+		ppd-&gt;paddr += PAGE_SIZE;</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void __init __sme_map_range(struct sme_populate_pgd_data *ppd,</span>
<span class="p_add">+				   pmdval_t pmd_flags, pteval_t pte_flags)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long vaddr_end;</span>
<span class="p_add">+</span>
<span class="p_add">+	ppd-&gt;pmd_flags = pmd_flags;</span>
<span class="p_add">+	ppd-&gt;pte_flags = pte_flags;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Save original end value since we modify the struct value */</span>
<span class="p_add">+	vaddr_end = ppd-&gt;vaddr_end;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* If start is not 2MB aligned, create PTE entries */</span>
<span class="p_add">+	ppd-&gt;vaddr_end = ALIGN(ppd-&gt;vaddr, PMD_PAGE_SIZE);</span>
<span class="p_add">+	__sme_map_range_pte(ppd);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Create PMD entries */</span>
<span class="p_add">+	ppd-&gt;vaddr_end = vaddr_end &amp; PMD_PAGE_MASK;</span>
<span class="p_add">+	__sme_map_range_pmd(ppd);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* If end is not 2MB aligned, create PTE entries */</span>
<span class="p_add">+	ppd-&gt;vaddr_end = vaddr_end;</span>
<span class="p_add">+	__sme_map_range_pte(ppd);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void __init sme_map_range_encrypted(struct sme_populate_pgd_data *ppd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__sme_map_range(ppd, PMD_FLAGS_ENC, PTE_FLAGS_ENC);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void __init sme_map_range_decrypted(struct sme_populate_pgd_data *ppd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__sme_map_range(ppd, PMD_FLAGS_DEC, PTE_FLAGS_DEC);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void __init sme_map_range_decrypted_wp(struct sme_populate_pgd_data *ppd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__sme_map_range(ppd, PMD_FLAGS_DEC_WP, PTE_FLAGS_DEC_WP);</span>
 }
 
 static unsigned long __init sme_pgtable_calc(unsigned long len)
 {
<span class="p_del">-	unsigned long p4d_size, pud_size, pmd_size;</span>
<span class="p_add">+	unsigned long p4d_size, pud_size, pmd_size, pte_size;</span>
 	unsigned long total;
 
 	/*
 	 * Perform a relatively simplistic calculation of the pagetable
<span class="p_del">-	 * entries that are needed. That mappings will be covered by 2MB</span>
<span class="p_del">-	 * PMD entries so we can conservatively calculate the required</span>
<span class="p_add">+	 * entries that are needed. Those mappings will be covered mostly</span>
<span class="p_add">+	 * by 2MB PMD entries so we can conservatively calculate the required</span>
 	 * number of P4D, PUD and PMD structures needed to perform the
<span class="p_del">-	 * mappings. Incrementing the count for each covers the case where</span>
<span class="p_del">-	 * the addresses cross entries.</span>
<span class="p_add">+	 * mappings.  For mappings that are not 2MB aligned, PTE mappings</span>
<span class="p_add">+	 * would be needed for the start and end portion of the address range</span>
<span class="p_add">+	 * that fall outside of the 2MB alignment.  This results in, at most,</span>
<span class="p_add">+	 * two extra pages to hold PTE entries for each range that is mapped.</span>
<span class="p_add">+	 * Incrementing the count for each covers the case where the addresses</span>
<span class="p_add">+	 * cross entries.</span>
 	 */
 	if (IS_ENABLED(CONFIG_X86_5LEVEL)) {
 		p4d_size = (ALIGN(len, PGDIR_SIZE) / PGDIR_SIZE) + 1;
<span class="p_chunk">@@ -334,8 +461,9 @@</span> <span class="p_context"> static unsigned long __init sme_pgtable_calc(unsigned long len)</span>
 	}
 	pmd_size = (ALIGN(len, PUD_SIZE) / PUD_SIZE) + 1;
 	pmd_size *= sizeof(pmd_t) * PTRS_PER_PMD;
<span class="p_add">+	pte_size = 2 * sizeof(pte_t) * PTRS_PER_PTE;</span>
 
<span class="p_del">-	total = p4d_size + pud_size + pmd_size;</span>
<span class="p_add">+	total = p4d_size + pud_size + pmd_size + pte_size;</span>
 
 	/*
 	 * Now calculate the added pagetable structures needed to populate
<span class="p_chunk">@@ -359,29 +487,29 @@</span> <span class="p_context"> static unsigned long __init sme_pgtable_calc(unsigned long len)</span>
 	return total;
 }
 
<span class="p_del">-void __init sme_encrypt_kernel(void)</span>
<span class="p_add">+void __init __nostackprotector sme_encrypt_kernel(struct boot_params *bp)</span>
 {
 	unsigned long workarea_start, workarea_end, workarea_len;
 	unsigned long execute_start, execute_end, execute_len;
 	unsigned long kernel_start, kernel_end, kernel_len;
<span class="p_add">+	unsigned long initrd_start, initrd_end, initrd_len;</span>
<span class="p_add">+	struct sme_populate_pgd_data ppd;</span>
 	unsigned long pgtable_area_len;
<span class="p_del">-	unsigned long paddr, pmd_flags;</span>
 	unsigned long decrypted_base;
<span class="p_del">-	void *pgtable_area;</span>
<span class="p_del">-	pgd_t *pgd;</span>
 
 	if (!sme_active())
 		return;
 
 	/*
<span class="p_del">-	 * Prepare for encrypting the kernel by building new pagetables with</span>
<span class="p_del">-	 * the necessary attributes needed to encrypt the kernel in place.</span>
<span class="p_add">+	 * Prepare for encrypting the kernel and initrd by building new</span>
<span class="p_add">+	 * pagetables with the necessary attributes needed to encrypt the</span>
<span class="p_add">+	 * kernel in place.</span>
 	 *
 	 *   One range of virtual addresses will map the memory occupied
<span class="p_del">-	 *   by the kernel as encrypted.</span>
<span class="p_add">+	 *   by the kernel and initrd as encrypted.</span>
 	 *
 	 *   Another range of virtual addresses will map the memory occupied
<span class="p_del">-	 *   by the kernel as decrypted and write-protected.</span>
<span class="p_add">+	 *   by the kernel and initrd as decrypted and write-protected.</span>
 	 *
 	 *     The use of write-protect attribute will prevent any of the
 	 *     memory from being cached.
<span class="p_chunk">@@ -392,6 +520,20 @@</span> <span class="p_context"> void __init sme_encrypt_kernel(void)</span>
 	kernel_end = ALIGN(__pa_symbol(_end), PMD_PAGE_SIZE);
 	kernel_len = kernel_end - kernel_start;
 
<span class="p_add">+	initrd_start = 0;</span>
<span class="p_add">+	initrd_end = 0;</span>
<span class="p_add">+	initrd_len = 0;</span>
<span class="p_add">+#ifdef CONFIG_BLK_DEV_INITRD</span>
<span class="p_add">+	initrd_len = (unsigned long)bp-&gt;hdr.ramdisk_size |</span>
<span class="p_add">+		     ((unsigned long)bp-&gt;ext_ramdisk_size &lt;&lt; 32);</span>
<span class="p_add">+	if (initrd_len) {</span>
<span class="p_add">+		initrd_start = (unsigned long)bp-&gt;hdr.ramdisk_image |</span>
<span class="p_add">+			       ((unsigned long)bp-&gt;ext_ramdisk_image &lt;&lt; 32);</span>
<span class="p_add">+		initrd_end = PAGE_ALIGN(initrd_start + initrd_len);</span>
<span class="p_add">+		initrd_len = initrd_end - initrd_start;</span>
<span class="p_add">+	}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 	/* Set the encryption workarea to be immediately after the kernel */
 	workarea_start = kernel_end;
 
<span class="p_chunk">@@ -414,16 +556,21 @@</span> <span class="p_context"> void __init sme_encrypt_kernel(void)</span>
 	 */
 	pgtable_area_len = sizeof(pgd_t) * PTRS_PER_PGD;
 	pgtable_area_len += sme_pgtable_calc(execute_end - kernel_start) * 2;
<span class="p_add">+	if (initrd_len)</span>
<span class="p_add">+		pgtable_area_len += sme_pgtable_calc(initrd_len) * 2;</span>
 
 	/* PUDs and PMDs needed in the current pagetables for the workarea */
 	pgtable_area_len += sme_pgtable_calc(execute_len + pgtable_area_len);
 
 	/*
 	 * The total workarea includes the executable encryption area and
<span class="p_del">-	 * the pagetable area.</span>
<span class="p_add">+	 * the pagetable area. The start of the workarea is already 2MB</span>
<span class="p_add">+	 * aligned, align the end of the workarea on a 2MB boundary so that</span>
<span class="p_add">+	 * we don&#39;t try to create/allocate PTE entries from the workarea</span>
<span class="p_add">+	 * before it is mapped.</span>
 	 */
 	workarea_len = execute_len + pgtable_area_len;
<span class="p_del">-	workarea_end = workarea_start + workarea_len;</span>
<span class="p_add">+	workarea_end = ALIGN(workarea_start + workarea_len, PMD_PAGE_SIZE);</span>
 
 	/*
 	 * Set the address to the start of where newly created pagetable
<span class="p_chunk">@@ -432,45 +579,30 @@</span> <span class="p_context"> void __init sme_encrypt_kernel(void)</span>
 	 * pagetables and when the new encrypted and decrypted kernel
 	 * mappings are populated.
 	 */
<span class="p_del">-	pgtable_area = (void *)execute_end;</span>
<span class="p_add">+	ppd.pgtable_area = (void *)execute_end;</span>
 
 	/*
 	 * Make sure the current pagetable structure has entries for
 	 * addressing the workarea.
 	 */
<span class="p_del">-	pgd = (pgd_t *)native_read_cr3_pa();</span>
<span class="p_del">-	paddr = workarea_start;</span>
<span class="p_del">-	while (paddr &lt; workarea_end) {</span>
<span class="p_del">-		pgtable_area = sme_populate_pgd(pgd, pgtable_area,</span>
<span class="p_del">-						paddr,</span>
<span class="p_del">-						paddr + PMD_FLAGS);</span>
<span class="p_del">-</span>
<span class="p_del">-		paddr += PMD_PAGE_SIZE;</span>
<span class="p_del">-	}</span>
<span class="p_add">+	ppd.pgd = (pgd_t *)native_read_cr3_pa();</span>
<span class="p_add">+	ppd.paddr = workarea_start;</span>
<span class="p_add">+	ppd.vaddr = workarea_start;</span>
<span class="p_add">+	ppd.vaddr_end = workarea_end;</span>
<span class="p_add">+	sme_map_range_decrypted(&amp;ppd);</span>
 
 	/* Flush the TLB - no globals so cr3 is enough */
 	native_write_cr3(__native_read_cr3());
 
 	/*
 	 * A new pagetable structure is being built to allow for the kernel
<span class="p_del">-	 * to be encrypted. It starts with an empty PGD that will then be</span>
<span class="p_del">-	 * populated with new PUDs and PMDs as the encrypted and decrypted</span>
<span class="p_del">-	 * kernel mappings are created.</span>
<span class="p_add">+	 * and initrd to be encrypted. It starts with an empty PGD that will</span>
<span class="p_add">+	 * then be populated with new PUDs and PMDs as the encrypted and</span>
<span class="p_add">+	 * decrypted kernel mappings are created.</span>
 	 */
<span class="p_del">-	pgd = pgtable_area;</span>
<span class="p_del">-	memset(pgd, 0, sizeof(*pgd) * PTRS_PER_PGD);</span>
<span class="p_del">-	pgtable_area += sizeof(*pgd) * PTRS_PER_PGD;</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Add encrypted kernel (identity) mappings */</span>
<span class="p_del">-	pmd_flags = PMD_FLAGS | _PAGE_ENC;</span>
<span class="p_del">-	paddr = kernel_start;</span>
<span class="p_del">-	while (paddr &lt; kernel_end) {</span>
<span class="p_del">-		pgtable_area = sme_populate_pgd(pgd, pgtable_area,</span>
<span class="p_del">-						paddr,</span>
<span class="p_del">-						paddr + pmd_flags);</span>
<span class="p_del">-</span>
<span class="p_del">-		paddr += PMD_PAGE_SIZE;</span>
<span class="p_del">-	}</span>
<span class="p_add">+	ppd.pgd = ppd.pgtable_area;</span>
<span class="p_add">+	memset(ppd.pgd, 0, sizeof(pgd_t) * PTRS_PER_PGD);</span>
<span class="p_add">+	ppd.pgtable_area += sizeof(pgd_t) * PTRS_PER_PGD;</span>
 
 	/*
 	 * A different PGD index/entry must be used to get different
<span class="p_chunk">@@ -479,47 +611,79 @@</span> <span class="p_context"> void __init sme_encrypt_kernel(void)</span>
 	 * the base of the mapping.
 	 */
 	decrypted_base = (pgd_index(workarea_end) + 1) &amp; (PTRS_PER_PGD - 1);
<span class="p_add">+	if (initrd_len) {</span>
<span class="p_add">+		unsigned long check_base;</span>
<span class="p_add">+</span>
<span class="p_add">+		check_base = (pgd_index(initrd_end) + 1) &amp; (PTRS_PER_PGD - 1);</span>
<span class="p_add">+		decrypted_base = max(decrypted_base, check_base);</span>
<span class="p_add">+	}</span>
 	decrypted_base &lt;&lt;= PGDIR_SHIFT;
 
<span class="p_add">+	/* Add encrypted kernel (identity) mappings */</span>
<span class="p_add">+	ppd.paddr = kernel_start;</span>
<span class="p_add">+	ppd.vaddr = kernel_start;</span>
<span class="p_add">+	ppd.vaddr_end = kernel_end;</span>
<span class="p_add">+	sme_map_range_encrypted(&amp;ppd);</span>
<span class="p_add">+</span>
 	/* Add decrypted, write-protected kernel (non-identity) mappings */
<span class="p_del">-	pmd_flags = (PMD_FLAGS &amp; ~_PAGE_CACHE_MASK) | (_PAGE_PAT | _PAGE_PWT);</span>
<span class="p_del">-	paddr = kernel_start;</span>
<span class="p_del">-	while (paddr &lt; kernel_end) {</span>
<span class="p_del">-		pgtable_area = sme_populate_pgd(pgd, pgtable_area,</span>
<span class="p_del">-						paddr + decrypted_base,</span>
<span class="p_del">-						paddr + pmd_flags);</span>
<span class="p_del">-</span>
<span class="p_del">-		paddr += PMD_PAGE_SIZE;</span>
<span class="p_add">+	ppd.paddr = kernel_start;</span>
<span class="p_add">+	ppd.vaddr = kernel_start + decrypted_base;</span>
<span class="p_add">+	ppd.vaddr_end = kernel_end + decrypted_base;</span>
<span class="p_add">+	sme_map_range_decrypted_wp(&amp;ppd);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (initrd_len) {</span>
<span class="p_add">+		/* Add encrypted initrd (identity) mappings */</span>
<span class="p_add">+		ppd.paddr = initrd_start;</span>
<span class="p_add">+		ppd.vaddr = initrd_start;</span>
<span class="p_add">+		ppd.vaddr_end = initrd_end;</span>
<span class="p_add">+		sme_map_range_encrypted(&amp;ppd);</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Add decrypted, write-protected initrd (non-identity) mappings</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		ppd.paddr = initrd_start;</span>
<span class="p_add">+		ppd.vaddr = initrd_start + decrypted_base;</span>
<span class="p_add">+		ppd.vaddr_end = initrd_end + decrypted_base;</span>
<span class="p_add">+		sme_map_range_decrypted_wp(&amp;ppd);</span>
 	}
 
 	/* Add decrypted workarea mappings to both kernel mappings */
<span class="p_del">-	paddr = workarea_start;</span>
<span class="p_del">-	while (paddr &lt; workarea_end) {</span>
<span class="p_del">-		pgtable_area = sme_populate_pgd(pgd, pgtable_area,</span>
<span class="p_del">-						paddr,</span>
<span class="p_del">-						paddr + PMD_FLAGS);</span>
<span class="p_add">+	ppd.paddr = workarea_start;</span>
<span class="p_add">+	ppd.vaddr = workarea_start;</span>
<span class="p_add">+	ppd.vaddr_end = workarea_end;</span>
<span class="p_add">+	sme_map_range_decrypted(&amp;ppd);</span>
 
<span class="p_del">-		pgtable_area = sme_populate_pgd(pgd, pgtable_area,</span>
<span class="p_del">-						paddr + decrypted_base,</span>
<span class="p_del">-						paddr + PMD_FLAGS);</span>
<span class="p_del">-</span>
<span class="p_del">-		paddr += PMD_PAGE_SIZE;</span>
<span class="p_del">-	}</span>
<span class="p_add">+	ppd.paddr = workarea_start;</span>
<span class="p_add">+	ppd.vaddr = workarea_start + decrypted_base;</span>
<span class="p_add">+	ppd.vaddr_end = workarea_end + decrypted_base;</span>
<span class="p_add">+	sme_map_range_decrypted(&amp;ppd);</span>
 
 	/* Perform the encryption */
 	sme_encrypt_execute(kernel_start, kernel_start + decrypted_base,
<span class="p_del">-			    kernel_len, workarea_start, (unsigned long)pgd);</span>
<span class="p_add">+			    kernel_len, workarea_start, (unsigned long)ppd.pgd);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (initrd_len)</span>
<span class="p_add">+		sme_encrypt_execute(initrd_start, initrd_start + decrypted_base,</span>
<span class="p_add">+				    initrd_len, workarea_start,</span>
<span class="p_add">+				    (unsigned long)ppd.pgd);</span>
 
 	/*
 	 * At this point we are running encrypted.  Remove the mappings for
 	 * the decrypted areas - all that is needed for this is to remove
 	 * the PGD entry/entries.
 	 */
<span class="p_del">-	sme_clear_pgd(pgd, kernel_start + decrypted_base,</span>
<span class="p_del">-		      kernel_end + decrypted_base);</span>
<span class="p_add">+	ppd.vaddr = kernel_start + decrypted_base;</span>
<span class="p_add">+	ppd.vaddr_end = kernel_end + decrypted_base;</span>
<span class="p_add">+	sme_clear_pgd(&amp;ppd);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (initrd_len) {</span>
<span class="p_add">+		ppd.vaddr = initrd_start + decrypted_base;</span>
<span class="p_add">+		ppd.vaddr_end = initrd_end + decrypted_base;</span>
<span class="p_add">+		sme_clear_pgd(&amp;ppd);</span>
<span class="p_add">+	}</span>
 
<span class="p_del">-	sme_clear_pgd(pgd, workarea_start + decrypted_base,</span>
<span class="p_del">-		      workarea_end + decrypted_base);</span>
<span class="p_add">+	ppd.vaddr = workarea_start + decrypted_base;</span>
<span class="p_add">+	ppd.vaddr_end = workarea_end + decrypted_base;</span>
<span class="p_add">+	sme_clear_pgd(&amp;ppd);</span>
 
 	/* Flush the TLB - no globals so cr3 is enough */
 	native_write_cr3(__native_read_cr3());
<span class="p_header">diff --git a/arch/x86/mm/mem_encrypt_boot.S b/arch/x86/mm/mem_encrypt_boot.S</span>
<span class="p_header">index 730e6d541df1..01f682cf77a8 100644</span>
<span class="p_header">--- a/arch/x86/mm/mem_encrypt_boot.S</span>
<span class="p_header">+++ b/arch/x86/mm/mem_encrypt_boot.S</span>
<span class="p_chunk">@@ -22,9 +22,9 @@</span> <span class="p_context"> ENTRY(sme_encrypt_execute)</span>
 
 	/*
 	 * Entry parameters:
<span class="p_del">-	 *   RDI - virtual address for the encrypted kernel mapping</span>
<span class="p_del">-	 *   RSI - virtual address for the decrypted kernel mapping</span>
<span class="p_del">-	 *   RDX - length of kernel</span>
<span class="p_add">+	 *   RDI - virtual address for the encrypted mapping</span>
<span class="p_add">+	 *   RSI - virtual address for the decrypted mapping</span>
<span class="p_add">+	 *   RDX - length to encrypt</span>
 	 *   RCX - virtual address of the encryption workarea, including:
 	 *     - stack page (PAGE_SIZE)
 	 *     - encryption routine page (PAGE_SIZE)
<span class="p_chunk">@@ -41,9 +41,9 @@</span> <span class="p_context"> ENTRY(sme_encrypt_execute)</span>
 	addq	$PAGE_SIZE, %rax	/* Workarea encryption routine */
 
 	push	%r12
<span class="p_del">-	movq	%rdi, %r10		/* Encrypted kernel */</span>
<span class="p_del">-	movq	%rsi, %r11		/* Decrypted kernel */</span>
<span class="p_del">-	movq	%rdx, %r12		/* Kernel length */</span>
<span class="p_add">+	movq	%rdi, %r10		/* Encrypted area */</span>
<span class="p_add">+	movq	%rsi, %r11		/* Decrypted area */</span>
<span class="p_add">+	movq	%rdx, %r12		/* Area length */</span>
 
 	/* Copy encryption routine into the workarea */
 	movq	%rax, %rdi				/* Workarea encryption routine */
<span class="p_chunk">@@ -52,10 +52,10 @@</span> <span class="p_context"> ENTRY(sme_encrypt_execute)</span>
 	rep	movsb
 
 	/* Setup registers for call */
<span class="p_del">-	movq	%r10, %rdi		/* Encrypted kernel */</span>
<span class="p_del">-	movq	%r11, %rsi		/* Decrypted kernel */</span>
<span class="p_add">+	movq	%r10, %rdi		/* Encrypted area */</span>
<span class="p_add">+	movq	%r11, %rsi		/* Decrypted area */</span>
 	movq	%r8, %rdx		/* Pagetables used for encryption */
<span class="p_del">-	movq	%r12, %rcx		/* Kernel length */</span>
<span class="p_add">+	movq	%r12, %rcx		/* Area length */</span>
 	movq	%rax, %r8		/* Workarea encryption routine */
 	addq	$PAGE_SIZE, %r8		/* Workarea intermediate copy buffer */
 
<span class="p_chunk">@@ -71,7 +71,7 @@</span> <span class="p_context"> ENDPROC(sme_encrypt_execute)</span>
 
 ENTRY(__enc_copy)
 /*
<span class="p_del">- * Routine used to encrypt kernel.</span>
<span class="p_add">+ * Routine used to encrypt memory in place.</span>
  *   This routine must be run outside of the kernel proper since
  *   the kernel will be encrypted during the process. So this
  *   routine is defined here and then copied to an area outside
<span class="p_chunk">@@ -79,19 +79,19 @@</span> <span class="p_context"> ENTRY(__enc_copy)</span>
  *   during execution.
  *
  *   On entry the registers must be:
<span class="p_del">- *     RDI - virtual address for the encrypted kernel mapping</span>
<span class="p_del">- *     RSI - virtual address for the decrypted kernel mapping</span>
<span class="p_add">+ *     RDI - virtual address for the encrypted mapping</span>
<span class="p_add">+ *     RSI - virtual address for the decrypted mapping</span>
  *     RDX - address of the pagetables to use for encryption
<span class="p_del">- *     RCX - length of kernel</span>
<span class="p_add">+ *     RCX - length of area</span>
  *      R8 - intermediate copy buffer
  *
  *     RAX - points to this routine
  *
<span class="p_del">- * The kernel will be encrypted by copying from the non-encrypted</span>
<span class="p_del">- * kernel space to an intermediate buffer and then copying from the</span>
<span class="p_del">- * intermediate buffer back to the encrypted kernel space. The physical</span>
<span class="p_del">- * addresses of the two kernel space mappings are the same which</span>
<span class="p_del">- * results in the kernel being encrypted &quot;in place&quot;.</span>
<span class="p_add">+ * The area will be encrypted by copying from the non-encrypted</span>
<span class="p_add">+ * memory space to an intermediate buffer and then copying from the</span>
<span class="p_add">+ * intermediate buffer back to the encrypted memory space. The physical</span>
<span class="p_add">+ * addresses of the two mappings are the same which results in the area</span>
<span class="p_add">+ * being encrypted &quot;in place&quot;.</span>
  */
 	/* Enable the new page tables */
 	mov	%rdx, %cr3
<span class="p_chunk">@@ -103,47 +103,55 @@</span> <span class="p_context"> ENTRY(__enc_copy)</span>
 	orq	$X86_CR4_PGE, %rdx
 	mov	%rdx, %cr4
 
<span class="p_add">+	push	%r15</span>
<span class="p_add">+	push	%r12</span>
<span class="p_add">+</span>
<span class="p_add">+	movq	%rcx, %r9		/* Save area length */</span>
<span class="p_add">+	movq	%rdi, %r10		/* Save encrypted area address */</span>
<span class="p_add">+	movq	%rsi, %r11		/* Save decrypted area address */</span>
<span class="p_add">+</span>
 	/* Set the PAT register PA5 entry to write-protect */
<span class="p_del">-	push	%rcx</span>
 	movl	$MSR_IA32_CR_PAT, %ecx
 	rdmsr
<span class="p_del">-	push	%rdx			/* Save original PAT value */</span>
<span class="p_add">+	mov	%rdx, %r15		/* Save original PAT value */</span>
 	andl	$0xffff00ff, %edx	/* Clear PA5 */
 	orl	$0x00000500, %edx	/* Set PA5 to WP */
 	wrmsr
<span class="p_del">-	pop	%rdx			/* RDX contains original PAT value */</span>
<span class="p_del">-	pop	%rcx</span>
<span class="p_del">-</span>
<span class="p_del">-	movq	%rcx, %r9		/* Save kernel length */</span>
<span class="p_del">-	movq	%rdi, %r10		/* Save encrypted kernel address */</span>
<span class="p_del">-	movq	%rsi, %r11		/* Save decrypted kernel address */</span>
 
 	wbinvd				/* Invalidate any cache entries */
 
<span class="p_del">-	/* Copy/encrypt 2MB at a time */</span>
<span class="p_add">+	/* Copy/encrypt up to 2MB at a time */</span>
<span class="p_add">+	movq	$PMD_PAGE_SIZE, %r12</span>
 1:
<span class="p_del">-	movq	%r11, %rsi		/* Source - decrypted kernel */</span>
<span class="p_add">+	cmpq	%r12, %r9</span>
<span class="p_add">+	jnb	2f</span>
<span class="p_add">+	movq	%r9, %r12</span>
<span class="p_add">+</span>
<span class="p_add">+2:</span>
<span class="p_add">+	movq	%r11, %rsi		/* Source - decrypted area */</span>
 	movq	%r8, %rdi		/* Dest   - intermediate copy buffer */
<span class="p_del">-	movq	$PMD_PAGE_SIZE, %rcx	/* 2MB length */</span>
<span class="p_add">+	movq	%r12, %rcx</span>
 	rep	movsb
 
 	movq	%r8, %rsi		/* Source - intermediate copy buffer */
<span class="p_del">-	movq	%r10, %rdi		/* Dest   - encrypted kernel */</span>
<span class="p_del">-	movq	$PMD_PAGE_SIZE, %rcx	/* 2MB length */</span>
<span class="p_add">+	movq	%r10, %rdi		/* Dest   - encrypted area */</span>
<span class="p_add">+	movq	%r12, %rcx</span>
 	rep	movsb
 
<span class="p_del">-	addq	$PMD_PAGE_SIZE, %r11</span>
<span class="p_del">-	addq	$PMD_PAGE_SIZE, %r10</span>
<span class="p_del">-	subq	$PMD_PAGE_SIZE, %r9	/* Kernel length decrement */</span>
<span class="p_add">+	addq	%r12, %r11</span>
<span class="p_add">+	addq	%r12, %r10</span>
<span class="p_add">+	subq	%r12, %r9		/* Kernel length decrement */</span>
 	jnz	1b			/* Kernel length not zero? */
 
 	/* Restore PAT register */
<span class="p_del">-	push	%rdx			/* Save original PAT value */</span>
 	movl	$MSR_IA32_CR_PAT, %ecx
 	rdmsr
<span class="p_del">-	pop	%rdx			/* Restore original PAT value */</span>
<span class="p_add">+	mov	%r15, %rdx		/* Restore original PAT value */</span>
 	wrmsr
 
<span class="p_add">+	pop	%r12</span>
<span class="p_add">+	pop	%r15</span>
<span class="p_add">+</span>
 	ret
 .L__enc_copy_end:
 ENDPROC(__enc_copy)
<span class="p_header">diff --git a/drivers/ata/libata-core.c b/drivers/ata/libata-core.c</span>
<span class="p_header">index ee4c1ec9dca0..e7ded346d94b 100644</span>
<span class="p_header">--- a/drivers/ata/libata-core.c</span>
<span class="p_header">+++ b/drivers/ata/libata-core.c</span>
<span class="p_chunk">@@ -4439,6 +4439,7 @@</span> <span class="p_context"> static const struct ata_blacklist_entry ata_device_blacklist [] = {</span>
 	 * https://bugzilla.kernel.org/show_bug.cgi?id=121671
 	 */
 	{ &quot;LITEON CX1-JB*-HP&quot;,	NULL,		ATA_HORKAGE_MAX_SEC_1024 },
<span class="p_add">+	{ &quot;LITEON EP1-*&quot;,	NULL,		ATA_HORKAGE_MAX_SEC_1024 },</span>
 
 	/* Devices we expect to fail diagnostics */
 
<span class="p_header">diff --git a/drivers/gpu/drm/nouveau/nvkm/engine/disp/sorgf119.c b/drivers/gpu/drm/nouveau/nvkm/engine/disp/sorgf119.c</span>
<span class="p_header">index a2978a37b4f3..700fc754f28a 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/nouveau/nvkm/engine/disp/sorgf119.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/nouveau/nvkm/engine/disp/sorgf119.c</span>
<span class="p_chunk">@@ -174,6 +174,7 @@</span> <span class="p_context"> gf119_sor = {</span>
 		.links = gf119_sor_dp_links,
 		.power = g94_sor_dp_power,
 		.pattern = gf119_sor_dp_pattern,
<span class="p_add">+		.drive = gf119_sor_dp_drive,</span>
 		.vcpi = gf119_sor_dp_vcpi,
 		.audio = gf119_sor_dp_audio,
 		.audio_sym = gf119_sor_dp_audio_sym,
<span class="p_header">diff --git a/drivers/gpu/drm/vmwgfx/vmwgfx_ldu.c b/drivers/gpu/drm/vmwgfx/vmwgfx_ldu.c</span>
<span class="p_header">index b8a09807c5de..3824595fece1 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/vmwgfx/vmwgfx_ldu.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/vmwgfx/vmwgfx_ldu.c</span>
<span class="p_chunk">@@ -266,8 +266,8 @@</span> <span class="p_context"> static const struct drm_connector_funcs vmw_legacy_connector_funcs = {</span>
 	.set_property = vmw_du_connector_set_property,
 	.destroy = vmw_ldu_connector_destroy,
 	.reset = vmw_du_connector_reset,
<span class="p_del">-	.atomic_duplicate_state = drm_atomic_helper_connector_duplicate_state,</span>
<span class="p_del">-	.atomic_destroy_state = drm_atomic_helper_connector_destroy_state,</span>
<span class="p_add">+	.atomic_duplicate_state = vmw_du_connector_duplicate_state,</span>
<span class="p_add">+	.atomic_destroy_state = vmw_du_connector_destroy_state,</span>
 	.atomic_set_property = vmw_du_connector_atomic_set_property,
 	.atomic_get_property = vmw_du_connector_atomic_get_property,
 };
<span class="p_header">diff --git a/drivers/gpu/drm/vmwgfx/vmwgfx_scrn.c b/drivers/gpu/drm/vmwgfx/vmwgfx_scrn.c</span>
<span class="p_header">index d1552d3e0652..7ae38a67388c 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/vmwgfx/vmwgfx_scrn.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/vmwgfx/vmwgfx_scrn.c</span>
<span class="p_chunk">@@ -420,8 +420,8 @@</span> <span class="p_context"> static const struct drm_connector_funcs vmw_sou_connector_funcs = {</span>
 	.set_property = vmw_du_connector_set_property,
 	.destroy = vmw_sou_connector_destroy,
 	.reset = vmw_du_connector_reset,
<span class="p_del">-	.atomic_duplicate_state = drm_atomic_helper_connector_duplicate_state,</span>
<span class="p_del">-	.atomic_destroy_state = drm_atomic_helper_connector_destroy_state,</span>
<span class="p_add">+	.atomic_duplicate_state = vmw_du_connector_duplicate_state,</span>
<span class="p_add">+	.atomic_destroy_state = vmw_du_connector_destroy_state,</span>
 	.atomic_set_property = vmw_du_connector_atomic_set_property,
 	.atomic_get_property = vmw_du_connector_atomic_get_property,
 };
<span class="p_header">diff --git a/drivers/i2c/i2c-core-smbus.c b/drivers/i2c/i2c-core-smbus.c</span>
<span class="p_header">index 10f00a82ec9d..e54a9b835b62 100644</span>
<span class="p_header">--- a/drivers/i2c/i2c-core-smbus.c</span>
<span class="p_header">+++ b/drivers/i2c/i2c-core-smbus.c</span>
<span class="p_chunk">@@ -396,16 +396,17 @@</span> <span class="p_context"> static s32 i2c_smbus_xfer_emulated(struct i2c_adapter *adapter, u16 addr,</span>
 				   the underlying bus driver */
 		break;
 	case I2C_SMBUS_I2C_BLOCK_DATA:
<span class="p_add">+		if (data-&gt;block[0] &gt; I2C_SMBUS_BLOCK_MAX) {</span>
<span class="p_add">+			dev_err(&amp;adapter-&gt;dev, &quot;Invalid block %s size %d\n&quot;,</span>
<span class="p_add">+				read_write == I2C_SMBUS_READ ? &quot;read&quot; : &quot;write&quot;,</span>
<span class="p_add">+				data-&gt;block[0]);</span>
<span class="p_add">+			return -EINVAL;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
 		if (read_write == I2C_SMBUS_READ) {
 			msg[1].len = data-&gt;block[0];
 		} else {
 			msg[0].len = data-&gt;block[0] + 1;
<span class="p_del">-			if (msg[0].len &gt; I2C_SMBUS_BLOCK_MAX + 1) {</span>
<span class="p_del">-				dev_err(&amp;adapter-&gt;dev,</span>
<span class="p_del">-					&quot;Invalid block write size %d\n&quot;,</span>
<span class="p_del">-					data-&gt;block[0]);</span>
<span class="p_del">-				return -EINVAL;</span>
<span class="p_del">-			}</span>
 			for (i = 1; i &lt;= data-&gt;block[0]; i++)
 				msgbuf0[i] = data-&gt;block[i];
 		}
<span class="p_header">diff --git a/drivers/infiniband/hw/hfi1/file_ops.c b/drivers/infiniband/hw/hfi1/file_ops.c</span>
<span class="p_header">index d9a1e9893136..fd28f09b4445 100644</span>
<span class="p_header">--- a/drivers/infiniband/hw/hfi1/file_ops.c</span>
<span class="p_header">+++ b/drivers/infiniband/hw/hfi1/file_ops.c</span>
<span class="p_chunk">@@ -881,11 +881,11 @@</span> <span class="p_context"> static int complete_subctxt(struct hfi1_filedata *fd)</span>
 	}
 
 	if (ret) {
<span class="p_del">-		hfi1_rcd_put(fd-&gt;uctxt);</span>
<span class="p_del">-		fd-&gt;uctxt = NULL;</span>
 		spin_lock_irqsave(&amp;fd-&gt;dd-&gt;uctxt_lock, flags);
 		__clear_bit(fd-&gt;subctxt, fd-&gt;uctxt-&gt;in_use_ctxts);
 		spin_unlock_irqrestore(&amp;fd-&gt;dd-&gt;uctxt_lock, flags);
<span class="p_add">+		hfi1_rcd_put(fd-&gt;uctxt);</span>
<span class="p_add">+		fd-&gt;uctxt = NULL;</span>
 	}
 
 	return ret;
<span class="p_header">diff --git a/drivers/infiniband/hw/mlx5/qp.c b/drivers/infiniband/hw/mlx5/qp.c</span>
<span class="p_header">index acb79d3a4f1d..756ece6118c0 100644</span>
<span class="p_header">--- a/drivers/infiniband/hw/mlx5/qp.c</span>
<span class="p_header">+++ b/drivers/infiniband/hw/mlx5/qp.c</span>
<span class="p_chunk">@@ -4303,12 +4303,11 @@</span> <span class="p_context"> static void to_rdma_ah_attr(struct mlx5_ib_dev *ibdev,</span>
 
 	memset(ah_attr, 0, sizeof(*ah_attr));
 
<span class="p_del">-	ah_attr-&gt;type = rdma_ah_find_type(&amp;ibdev-&gt;ib_dev, path-&gt;port);</span>
<span class="p_del">-	rdma_ah_set_port_num(ah_attr, path-&gt;port);</span>
<span class="p_del">-	if (rdma_ah_get_port_num(ah_attr) == 0 ||</span>
<span class="p_del">-	    rdma_ah_get_port_num(ah_attr) &gt; MLX5_CAP_GEN(dev, num_ports))</span>
<span class="p_add">+	if (!path-&gt;port || path-&gt;port &gt; MLX5_CAP_GEN(dev, num_ports))</span>
 		return;
 
<span class="p_add">+	ah_attr-&gt;type = rdma_ah_find_type(&amp;ibdev-&gt;ib_dev, path-&gt;port);</span>
<span class="p_add">+</span>
 	rdma_ah_set_port_num(ah_attr, path-&gt;port);
 	rdma_ah_set_sl(ah_attr, path-&gt;dci_cfi_prio_sl &amp; 0xf);
 
<span class="p_header">diff --git a/drivers/infiniband/ulp/isert/ib_isert.c b/drivers/infiniband/ulp/isert/ib_isert.c</span>
<span class="p_header">index ceabdb85df8b..9d4785ba24cb 100644</span>
<span class="p_header">--- a/drivers/infiniband/ulp/isert/ib_isert.c</span>
<span class="p_header">+++ b/drivers/infiniband/ulp/isert/ib_isert.c</span>
<span class="p_chunk">@@ -741,6 +741,7 @@</span> <span class="p_context"> isert_connect_error(struct rdma_cm_id *cma_id)</span>
 {
 	struct isert_conn *isert_conn = cma_id-&gt;qp-&gt;qp_context;
 
<span class="p_add">+	ib_drain_qp(isert_conn-&gt;qp);</span>
 	list_del_init(&amp;isert_conn-&gt;node);
 	isert_conn-&gt;cm_id = NULL;
 	isert_put_conn(isert_conn);
<span class="p_header">diff --git a/drivers/input/misc/twl4030-vibra.c b/drivers/input/misc/twl4030-vibra.c</span>
<span class="p_header">index 6c51d404874b..c37aea9ac272 100644</span>
<span class="p_header">--- a/drivers/input/misc/twl4030-vibra.c</span>
<span class="p_header">+++ b/drivers/input/misc/twl4030-vibra.c</span>
<span class="p_chunk">@@ -178,12 +178,14 @@</span> <span class="p_context"> static SIMPLE_DEV_PM_OPS(twl4030_vibra_pm_ops,</span>
 			 twl4030_vibra_suspend, twl4030_vibra_resume);
 
 static bool twl4030_vibra_check_coexist(struct twl4030_vibra_data *pdata,
<span class="p_del">-			      struct device_node *node)</span>
<span class="p_add">+			      struct device_node *parent)</span>
 {
<span class="p_add">+	struct device_node *node;</span>
<span class="p_add">+</span>
 	if (pdata &amp;&amp; pdata-&gt;coexist)
 		return true;
 
<span class="p_del">-	node = of_find_node_by_name(node, &quot;codec&quot;);</span>
<span class="p_add">+	node = of_get_child_by_name(parent, &quot;codec&quot;);</span>
 	if (node) {
 		of_node_put(node);
 		return true;
<span class="p_header">diff --git a/drivers/input/misc/twl6040-vibra.c b/drivers/input/misc/twl6040-vibra.c</span>
<span class="p_header">index 5690eb7ff954..15e0d352c4cc 100644</span>
<span class="p_header">--- a/drivers/input/misc/twl6040-vibra.c</span>
<span class="p_header">+++ b/drivers/input/misc/twl6040-vibra.c</span>
<span class="p_chunk">@@ -248,8 +248,7 @@</span> <span class="p_context"> static int twl6040_vibra_probe(struct platform_device *pdev)</span>
 	int vddvibr_uV = 0;
 	int error;
 
<span class="p_del">-	of_node_get(twl6040_core_dev-&gt;of_node);</span>
<span class="p_del">-	twl6040_core_node = of_find_node_by_name(twl6040_core_dev-&gt;of_node,</span>
<span class="p_add">+	twl6040_core_node = of_get_child_by_name(twl6040_core_dev-&gt;of_node,</span>
 						 &quot;vibra&quot;);
 	if (!twl6040_core_node) {
 		dev_err(&amp;pdev-&gt;dev, &quot;parent of node is missing?\n&quot;);
<span class="p_header">diff --git a/drivers/input/mouse/alps.c b/drivers/input/mouse/alps.c</span>
<span class="p_header">index 850b00e3ad8e..3d9c294e84db 100644</span>
<span class="p_header">--- a/drivers/input/mouse/alps.c</span>
<span class="p_header">+++ b/drivers/input/mouse/alps.c</span>
<span class="p_chunk">@@ -1250,29 +1250,32 @@</span> <span class="p_context"> static int alps_decode_ss4_v2(struct alps_fields *f,</span>
 	case SS4_PACKET_ID_MULTI:
 		if (priv-&gt;flags &amp; ALPS_BUTTONPAD) {
 			if (IS_SS4PLUS_DEV(priv-&gt;dev_id)) {
<span class="p_del">-				f-&gt;mt[0].x = SS4_PLUS_BTL_MF_X_V2(p, 0);</span>
<span class="p_del">-				f-&gt;mt[1].x = SS4_PLUS_BTL_MF_X_V2(p, 1);</span>
<span class="p_add">+				f-&gt;mt[2].x = SS4_PLUS_BTL_MF_X_V2(p, 0);</span>
<span class="p_add">+				f-&gt;mt[3].x = SS4_PLUS_BTL_MF_X_V2(p, 1);</span>
<span class="p_add">+				no_data_x = SS4_PLUS_MFPACKET_NO_AX_BL;</span>
 			} else {
 				f-&gt;mt[2].x = SS4_BTL_MF_X_V2(p, 0);
 				f-&gt;mt[3].x = SS4_BTL_MF_X_V2(p, 1);
<span class="p_add">+				no_data_x = SS4_MFPACKET_NO_AX_BL;</span>
 			}
<span class="p_add">+			no_data_y = SS4_MFPACKET_NO_AY_BL;</span>
 
 			f-&gt;mt[2].y = SS4_BTL_MF_Y_V2(p, 0);
 			f-&gt;mt[3].y = SS4_BTL_MF_Y_V2(p, 1);
<span class="p_del">-			no_data_x = SS4_MFPACKET_NO_AX_BL;</span>
<span class="p_del">-			no_data_y = SS4_MFPACKET_NO_AY_BL;</span>
 		} else {
 			if (IS_SS4PLUS_DEV(priv-&gt;dev_id)) {
<span class="p_del">-				f-&gt;mt[0].x = SS4_PLUS_STD_MF_X_V2(p, 0);</span>
<span class="p_del">-				f-&gt;mt[1].x = SS4_PLUS_STD_MF_X_V2(p, 1);</span>
<span class="p_add">+				f-&gt;mt[2].x = SS4_PLUS_STD_MF_X_V2(p, 0);</span>
<span class="p_add">+				f-&gt;mt[3].x = SS4_PLUS_STD_MF_X_V2(p, 1);</span>
<span class="p_add">+				no_data_x = SS4_PLUS_MFPACKET_NO_AX;</span>
 			} else {
<span class="p_del">-				f-&gt;mt[0].x = SS4_STD_MF_X_V2(p, 0);</span>
<span class="p_del">-				f-&gt;mt[1].x = SS4_STD_MF_X_V2(p, 1);</span>
<span class="p_add">+				f-&gt;mt[2].x = SS4_STD_MF_X_V2(p, 0);</span>
<span class="p_add">+				f-&gt;mt[3].x = SS4_STD_MF_X_V2(p, 1);</span>
<span class="p_add">+				no_data_x = SS4_MFPACKET_NO_AX;</span>
 			}
<span class="p_add">+			no_data_y = SS4_MFPACKET_NO_AY;</span>
<span class="p_add">+</span>
 			f-&gt;mt[2].y = SS4_STD_MF_Y_V2(p, 0);
 			f-&gt;mt[3].y = SS4_STD_MF_Y_V2(p, 1);
<span class="p_del">-			no_data_x = SS4_MFPACKET_NO_AX;</span>
<span class="p_del">-			no_data_y = SS4_MFPACKET_NO_AY;</span>
 		}
 
 		f-&gt;first_mp = 0;
<span class="p_header">diff --git a/drivers/input/mouse/alps.h b/drivers/input/mouse/alps.h</span>
<span class="p_header">index c80a7c76cb76..79b6d69d1486 100644</span>
<span class="p_header">--- a/drivers/input/mouse/alps.h</span>
<span class="p_header">+++ b/drivers/input/mouse/alps.h</span>
<span class="p_chunk">@@ -141,10 +141,12 @@</span> <span class="p_context"> enum SS4_PACKET_ID {</span>
 #define SS4_TS_Z_V2(_b)		(s8)(_b[4] &amp; 0x7F)
 
 
<span class="p_del">-#define SS4_MFPACKET_NO_AX	8160	/* X-Coordinate value */</span>
<span class="p_del">-#define SS4_MFPACKET_NO_AY	4080	/* Y-Coordinate value */</span>
<span class="p_del">-#define SS4_MFPACKET_NO_AX_BL	8176	/* Buttonless X-Coordinate value */</span>
<span class="p_del">-#define SS4_MFPACKET_NO_AY_BL	4088	/* Buttonless Y-Coordinate value */</span>
<span class="p_add">+#define SS4_MFPACKET_NO_AX		8160	/* X-Coordinate value */</span>
<span class="p_add">+#define SS4_MFPACKET_NO_AY		4080	/* Y-Coordinate value */</span>
<span class="p_add">+#define SS4_MFPACKET_NO_AX_BL		8176	/* Buttonless X-Coord value */</span>
<span class="p_add">+#define SS4_MFPACKET_NO_AY_BL		4088	/* Buttonless Y-Coord value */</span>
<span class="p_add">+#define SS4_PLUS_MFPACKET_NO_AX		4080	/* SS4 PLUS, X */</span>
<span class="p_add">+#define SS4_PLUS_MFPACKET_NO_AX_BL	4088	/* Buttonless SS4 PLUS, X */</span>
 
 /*
  * enum V7_PACKET_ID - defines the packet type for V7
<span class="p_header">diff --git a/drivers/input/rmi4/rmi_driver.c b/drivers/input/rmi4/rmi_driver.c</span>
<span class="p_header">index 4f2bb5947a4e..141ea228aac6 100644</span>
<span class="p_header">--- a/drivers/input/rmi4/rmi_driver.c</span>
<span class="p_header">+++ b/drivers/input/rmi4/rmi_driver.c</span>
<span class="p_chunk">@@ -230,8 +230,10 @@</span> <span class="p_context"> static irqreturn_t rmi_irq_fn(int irq, void *dev_id)</span>
 		rmi_dbg(RMI_DEBUG_CORE, &amp;rmi_dev-&gt;dev,
 			&quot;Failed to process interrupt request: %d\n&quot;, ret);
 
<span class="p_del">-	if (count)</span>
<span class="p_add">+	if (count) {</span>
 		kfree(attn_data.data);
<span class="p_add">+		attn_data.data = NULL;</span>
<span class="p_add">+	}</span>
 
 	if (!kfifo_is_empty(&amp;drvdata-&gt;attn_fifo))
 		return rmi_irq_fn(irq, dev_id);
<span class="p_header">diff --git a/drivers/input/touchscreen/88pm860x-ts.c b/drivers/input/touchscreen/88pm860x-ts.c</span>
<span class="p_header">index 7ed828a51f4c..3486d9403805 100644</span>
<span class="p_header">--- a/drivers/input/touchscreen/88pm860x-ts.c</span>
<span class="p_header">+++ b/drivers/input/touchscreen/88pm860x-ts.c</span>
<span class="p_chunk">@@ -126,7 +126,7 @@</span> <span class="p_context"> static int pm860x_touch_dt_init(struct platform_device *pdev,</span>
 	int data, n, ret;
 	if (!np)
 		return -ENODEV;
<span class="p_del">-	np = of_find_node_by_name(np, &quot;touch&quot;);</span>
<span class="p_add">+	np = of_get_child_by_name(np, &quot;touch&quot;);</span>
 	if (!np) {
 		dev_err(&amp;pdev-&gt;dev, &quot;Can&#39;t find touch node\n&quot;);
 		return -EINVAL;
<span class="p_chunk">@@ -144,13 +144,13 @@</span> <span class="p_context"> static int pm860x_touch_dt_init(struct platform_device *pdev,</span>
 	if (data) {
 		ret = pm860x_reg_write(i2c, PM8607_GPADC_MISC1, data);
 		if (ret &lt; 0)
<span class="p_del">-			return -EINVAL;</span>
<span class="p_add">+			goto err_put_node;</span>
 	}
 	/* set tsi prebias time */
 	if (!of_property_read_u32(np, &quot;marvell,88pm860x-tsi-prebias&quot;, &amp;data)) {
 		ret = pm860x_reg_write(i2c, PM8607_TSI_PREBIAS, data);
 		if (ret &lt; 0)
<span class="p_del">-			return -EINVAL;</span>
<span class="p_add">+			goto err_put_node;</span>
 	}
 	/* set prebias &amp; prechg time of pen detect */
 	data = 0;
<span class="p_chunk">@@ -161,10 +161,18 @@</span> <span class="p_context"> static int pm860x_touch_dt_init(struct platform_device *pdev,</span>
 	if (data) {
 		ret = pm860x_reg_write(i2c, PM8607_PD_PREBIAS, data);
 		if (ret &lt; 0)
<span class="p_del">-			return -EINVAL;</span>
<span class="p_add">+			goto err_put_node;</span>
 	}
 	of_property_read_u32(np, &quot;marvell,88pm860x-resistor-X&quot;, res_x);
<span class="p_add">+</span>
<span class="p_add">+	of_node_put(np);</span>
<span class="p_add">+</span>
 	return 0;
<span class="p_add">+</span>
<span class="p_add">+err_put_node:</span>
<span class="p_add">+	of_node_put(np);</span>
<span class="p_add">+</span>
<span class="p_add">+	return -EINVAL;</span>
 }
 #else
 #define pm860x_touch_dt_init(x, y, z)	(-1)
<span class="p_header">diff --git a/drivers/md/dm-crypt.c b/drivers/md/dm-crypt.c</span>
<span class="p_header">index 9fc12f556534..554d60394c06 100644</span>
<span class="p_header">--- a/drivers/md/dm-crypt.c</span>
<span class="p_header">+++ b/drivers/md/dm-crypt.c</span>
<span class="p_chunk">@@ -1954,10 +1954,15 @@</span> <span class="p_context"> static int crypt_setkey(struct crypt_config *cc)</span>
 	/* Ignore extra keys (which are used for IV etc) */
 	subkey_size = crypt_subkey_size(cc);
 
<span class="p_del">-	if (crypt_integrity_hmac(cc))</span>
<span class="p_add">+	if (crypt_integrity_hmac(cc)) {</span>
<span class="p_add">+		if (subkey_size &lt; cc-&gt;key_mac_size)</span>
<span class="p_add">+			return -EINVAL;</span>
<span class="p_add">+</span>
 		crypt_copy_authenckey(cc-&gt;authenc_key, cc-&gt;key,
 				      subkey_size - cc-&gt;key_mac_size,
 				      cc-&gt;key_mac_size);
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	for (i = 0; i &lt; cc-&gt;tfms_count; i++) {
 		if (crypt_integrity_hmac(cc))
 			r = crypto_aead_setkey(cc-&gt;cipher_tfm.tfms_aead[i],
<span class="p_chunk">@@ -2053,9 +2058,6 @@</span> <span class="p_context"> static int crypt_set_keyring_key(struct crypt_config *cc, const char *key_string</span>
 
 	ret = crypt_setkey(cc);
 
<span class="p_del">-	/* wipe the kernel key payload copy in each case */</span>
<span class="p_del">-	memset(cc-&gt;key, 0, cc-&gt;key_size * sizeof(u8));</span>
<span class="p_del">-</span>
 	if (!ret) {
 		set_bit(DM_CRYPT_KEY_VALID, &amp;cc-&gt;flags);
 		kzfree(cc-&gt;key_string);
<span class="p_chunk">@@ -2523,6 +2525,10 @@</span> <span class="p_context"> static int crypt_ctr_cipher(struct dm_target *ti, char *cipher_in, char *key)</span>
 		}
 	}
 
<span class="p_add">+	/* wipe the kernel key payload copy */</span>
<span class="p_add">+	if (cc-&gt;key_string)</span>
<span class="p_add">+		memset(cc-&gt;key, 0, cc-&gt;key_size * sizeof(u8));</span>
<span class="p_add">+</span>
 	return ret;
 }
 
<span class="p_chunk">@@ -2740,6 +2746,7 @@</span> <span class="p_context"> static int crypt_ctr(struct dm_target *ti, unsigned int argc, char **argv)</span>
 			cc-&gt;tag_pool_max_sectors * cc-&gt;on_disk_tag_size);
 		if (!cc-&gt;tag_pool) {
 			ti-&gt;error = &quot;Cannot allocate integrity tags mempool&quot;;
<span class="p_add">+			ret = -ENOMEM;</span>
 			goto bad;
 		}
 
<span class="p_chunk">@@ -2961,6 +2968,9 @@</span> <span class="p_context"> static int crypt_message(struct dm_target *ti, unsigned argc, char **argv)</span>
 				return ret;
 			if (cc-&gt;iv_gen_ops &amp;&amp; cc-&gt;iv_gen_ops-&gt;init)
 				ret = cc-&gt;iv_gen_ops-&gt;init(cc);
<span class="p_add">+			/* wipe the kernel key payload copy */</span>
<span class="p_add">+			if (cc-&gt;key_string)</span>
<span class="p_add">+				memset(cc-&gt;key, 0, cc-&gt;key_size * sizeof(u8));</span>
 			return ret;
 		}
 		if (argc == 2 &amp;&amp; !strcasecmp(argv[1], &quot;wipe&quot;)) {
<span class="p_chunk">@@ -3007,7 +3017,7 @@</span> <span class="p_context"> static void crypt_io_hints(struct dm_target *ti, struct queue_limits *limits)</span>
 
 static struct target_type crypt_target = {
 	.name   = &quot;crypt&quot;,
<span class="p_del">-	.version = {1, 18, 0},</span>
<span class="p_add">+	.version = {1, 18, 1},</span>
 	.module = THIS_MODULE,
 	.ctr    = crypt_ctr,
 	.dtr    = crypt_dtr,
<span class="p_header">diff --git a/drivers/md/dm-integrity.c b/drivers/md/dm-integrity.c</span>
<span class="p_header">index 5e6737a44468..3cc2052f972c 100644</span>
<span class="p_header">--- a/drivers/md/dm-integrity.c</span>
<span class="p_header">+++ b/drivers/md/dm-integrity.c</span>
<span class="p_chunk">@@ -2558,7 +2558,8 @@</span> <span class="p_context"> static int create_journal(struct dm_integrity_c *ic, char **error)</span>
 	int r = 0;
 	unsigned i;
 	__u64 journal_pages, journal_desc_size, journal_tree_size;
<span class="p_del">-	unsigned char *crypt_data = NULL;</span>
<span class="p_add">+	unsigned char *crypt_data = NULL, *crypt_iv = NULL;</span>
<span class="p_add">+	struct skcipher_request *req = NULL;</span>
 
 	ic-&gt;commit_ids[0] = cpu_to_le64(0x1111111111111111ULL);
 	ic-&gt;commit_ids[1] = cpu_to_le64(0x2222222222222222ULL);
<span class="p_chunk">@@ -2616,9 +2617,20 @@</span> <span class="p_context"> static int create_journal(struct dm_integrity_c *ic, char **error)</span>
 
 		if (blocksize == 1) {
 			struct scatterlist *sg;
<span class="p_del">-			SKCIPHER_REQUEST_ON_STACK(req, ic-&gt;journal_crypt);</span>
<span class="p_del">-			unsigned char iv[ivsize];</span>
<span class="p_del">-			skcipher_request_set_tfm(req, ic-&gt;journal_crypt);</span>
<span class="p_add">+</span>
<span class="p_add">+			req = skcipher_request_alloc(ic-&gt;journal_crypt, GFP_KERNEL);</span>
<span class="p_add">+			if (!req) {</span>
<span class="p_add">+				*error = &quot;Could not allocate crypt request&quot;;</span>
<span class="p_add">+				r = -ENOMEM;</span>
<span class="p_add">+				goto bad;</span>
<span class="p_add">+			}</span>
<span class="p_add">+</span>
<span class="p_add">+			crypt_iv = kmalloc(ivsize, GFP_KERNEL);</span>
<span class="p_add">+			if (!crypt_iv) {</span>
<span class="p_add">+				*error = &quot;Could not allocate iv&quot;;</span>
<span class="p_add">+				r = -ENOMEM;</span>
<span class="p_add">+				goto bad;</span>
<span class="p_add">+			}</span>
 
 			ic-&gt;journal_xor = dm_integrity_alloc_page_list(ic);
 			if (!ic-&gt;journal_xor) {
<span class="p_chunk">@@ -2640,9 +2652,9 @@</span> <span class="p_context"> static int create_journal(struct dm_integrity_c *ic, char **error)</span>
 				sg_set_buf(&amp;sg[i], va, PAGE_SIZE);
 			}
 			sg_set_buf(&amp;sg[i], &amp;ic-&gt;commit_ids, sizeof ic-&gt;commit_ids);
<span class="p_del">-			memset(iv, 0x00, ivsize);</span>
<span class="p_add">+			memset(crypt_iv, 0x00, ivsize);</span>
 
<span class="p_del">-			skcipher_request_set_crypt(req, sg, sg, PAGE_SIZE * ic-&gt;journal_pages + sizeof ic-&gt;commit_ids, iv);</span>
<span class="p_add">+			skcipher_request_set_crypt(req, sg, sg, PAGE_SIZE * ic-&gt;journal_pages + sizeof ic-&gt;commit_ids, crypt_iv);</span>
 			init_completion(&amp;comp.comp);
 			comp.in_flight = (atomic_t)ATOMIC_INIT(1);
 			if (do_crypt(true, req, &amp;comp))
<span class="p_chunk">@@ -2658,10 +2670,22 @@</span> <span class="p_context"> static int create_journal(struct dm_integrity_c *ic, char **error)</span>
 			crypto_free_skcipher(ic-&gt;journal_crypt);
 			ic-&gt;journal_crypt = NULL;
 		} else {
<span class="p_del">-			SKCIPHER_REQUEST_ON_STACK(req, ic-&gt;journal_crypt);</span>
<span class="p_del">-			unsigned char iv[ivsize];</span>
 			unsigned crypt_len = roundup(ivsize, blocksize);
 
<span class="p_add">+			req = skcipher_request_alloc(ic-&gt;journal_crypt, GFP_KERNEL);</span>
<span class="p_add">+			if (!req) {</span>
<span class="p_add">+				*error = &quot;Could not allocate crypt request&quot;;</span>
<span class="p_add">+				r = -ENOMEM;</span>
<span class="p_add">+				goto bad;</span>
<span class="p_add">+			}</span>
<span class="p_add">+</span>
<span class="p_add">+			crypt_iv = kmalloc(ivsize, GFP_KERNEL);</span>
<span class="p_add">+			if (!crypt_iv) {</span>
<span class="p_add">+				*error = &quot;Could not allocate iv&quot;;</span>
<span class="p_add">+				r = -ENOMEM;</span>
<span class="p_add">+				goto bad;</span>
<span class="p_add">+			}</span>
<span class="p_add">+</span>
 			crypt_data = kmalloc(crypt_len, GFP_KERNEL);
 			if (!crypt_data) {
 				*error = &quot;Unable to allocate crypt data&quot;;
<span class="p_chunk">@@ -2669,8 +2693,6 @@</span> <span class="p_context"> static int create_journal(struct dm_integrity_c *ic, char **error)</span>
 				goto bad;
 			}
 
<span class="p_del">-			skcipher_request_set_tfm(req, ic-&gt;journal_crypt);</span>
<span class="p_del">-</span>
 			ic-&gt;journal_scatterlist = dm_integrity_alloc_journal_scatterlist(ic, ic-&gt;journal);
 			if (!ic-&gt;journal_scatterlist) {
 				*error = &quot;Unable to allocate sg list&quot;;
<span class="p_chunk">@@ -2694,12 +2716,12 @@</span> <span class="p_context"> static int create_journal(struct dm_integrity_c *ic, char **error)</span>
 				struct skcipher_request *section_req;
 				__u32 section_le = cpu_to_le32(i);
 
<span class="p_del">-				memset(iv, 0x00, ivsize);</span>
<span class="p_add">+				memset(crypt_iv, 0x00, ivsize);</span>
 				memset(crypt_data, 0x00, crypt_len);
 				memcpy(crypt_data, &amp;section_le, min((size_t)crypt_len, sizeof(section_le)));
 
 				sg_init_one(&amp;sg, crypt_data, crypt_len);
<span class="p_del">-				skcipher_request_set_crypt(req, &amp;sg, &amp;sg, crypt_len, iv);</span>
<span class="p_add">+				skcipher_request_set_crypt(req, &amp;sg, &amp;sg, crypt_len, crypt_iv);</span>
 				init_completion(&amp;comp.comp);
 				comp.in_flight = (atomic_t)ATOMIC_INIT(1);
 				if (do_crypt(true, req, &amp;comp))
<span class="p_chunk">@@ -2757,6 +2779,9 @@</span> <span class="p_context"> static int create_journal(struct dm_integrity_c *ic, char **error)</span>
 	}
 bad:
 	kfree(crypt_data);
<span class="p_add">+	kfree(crypt_iv);</span>
<span class="p_add">+	skcipher_request_free(req);</span>
<span class="p_add">+</span>
 	return r;
 }
 
<span class="p_header">diff --git a/drivers/md/dm-thin-metadata.c b/drivers/md/dm-thin-metadata.c</span>
<span class="p_header">index d31d18d9727c..36ef284ad086 100644</span>
<span class="p_header">--- a/drivers/md/dm-thin-metadata.c</span>
<span class="p_header">+++ b/drivers/md/dm-thin-metadata.c</span>
<span class="p_chunk">@@ -80,10 +80,14 @@</span> <span class="p_context"></span>
 #define SECTOR_TO_BLOCK_SHIFT 3
 
 /*
<span class="p_add">+ * For btree insert:</span>
  *  3 for btree insert +
  *  2 for btree lookup used within space map
<span class="p_add">+ * For btree remove:</span>
<span class="p_add">+ *  2 for shadow spine +</span>
<span class="p_add">+ *  4 for rebalance 3 child node</span>
  */
<span class="p_del">-#define THIN_MAX_CONCURRENT_LOCKS 5</span>
<span class="p_add">+#define THIN_MAX_CONCURRENT_LOCKS 6</span>
 
 /* This should be plenty */
 #define SPACE_MAP_ROOT_SIZE 128
<span class="p_header">diff --git a/drivers/md/persistent-data/dm-btree.c b/drivers/md/persistent-data/dm-btree.c</span>
<span class="p_header">index f21ce6a3d4cf..58b319757b1e 100644</span>
<span class="p_header">--- a/drivers/md/persistent-data/dm-btree.c</span>
<span class="p_header">+++ b/drivers/md/persistent-data/dm-btree.c</span>
<span class="p_chunk">@@ -683,23 +683,8 @@</span> <span class="p_context"> static int btree_split_beneath(struct shadow_spine *s, uint64_t key)</span>
 	pn-&gt;keys[1] = rn-&gt;keys[0];
 	memcpy_disk(value_ptr(pn, 1), &amp;val, sizeof(__le64));
 
<span class="p_del">-	/*</span>
<span class="p_del">-	 * rejig the spine.  This is ugly, since it knows too</span>
<span class="p_del">-	 * much about the spine</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	if (s-&gt;nodes[0] != new_parent) {</span>
<span class="p_del">-		unlock_block(s-&gt;info, s-&gt;nodes[0]);</span>
<span class="p_del">-		s-&gt;nodes[0] = new_parent;</span>
<span class="p_del">-	}</span>
<span class="p_del">-	if (key &lt; le64_to_cpu(rn-&gt;keys[0])) {</span>
<span class="p_del">-		unlock_block(s-&gt;info, right);</span>
<span class="p_del">-		s-&gt;nodes[1] = left;</span>
<span class="p_del">-	} else {</span>
<span class="p_del">-		unlock_block(s-&gt;info, left);</span>
<span class="p_del">-		s-&gt;nodes[1] = right;</span>
<span class="p_del">-	}</span>
<span class="p_del">-	s-&gt;count = 2;</span>
<span class="p_del">-</span>
<span class="p_add">+	unlock_block(s-&gt;info, left);</span>
<span class="p_add">+	unlock_block(s-&gt;info, right);</span>
 	return 0;
 }
 
<span class="p_header">diff --git a/drivers/mmc/host/sdhci-esdhc-imx.c b/drivers/mmc/host/sdhci-esdhc-imx.c</span>
<span class="p_header">index 85140c9af581..8b941f814472 100644</span>
<span class="p_header">--- a/drivers/mmc/host/sdhci-esdhc-imx.c</span>
<span class="p_header">+++ b/drivers/mmc/host/sdhci-esdhc-imx.c</span>
<span class="p_chunk">@@ -687,6 +687,20 @@</span> <span class="p_context"> static inline void esdhc_pltfm_set_clock(struct sdhci_host *host,</span>
 		return;
 	}
 
<span class="p_add">+	/* For i.MX53 eSDHCv3, SYSCTL.SDCLKFS may not be set to 0. */</span>
<span class="p_add">+	if (is_imx53_esdhc(imx_data)) {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * According to the i.MX53 reference manual, if DLLCTRL[10] can</span>
<span class="p_add">+		 * be set, then the controller is eSDHCv3, else it is eSDHCv2.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		val = readl(host-&gt;ioaddr + ESDHC_DLL_CTRL);</span>
<span class="p_add">+		writel(val | BIT(10), host-&gt;ioaddr + ESDHC_DLL_CTRL);</span>
<span class="p_add">+		temp = readl(host-&gt;ioaddr + ESDHC_DLL_CTRL);</span>
<span class="p_add">+		writel(val, host-&gt;ioaddr + ESDHC_DLL_CTRL);</span>
<span class="p_add">+		if (temp &amp; BIT(10))</span>
<span class="p_add">+			pre_div = 2;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	temp = sdhci_readl(host, ESDHC_SYSTEM_CONTROL);
 	temp &amp;= ~(ESDHC_CLOCK_IPGEN | ESDHC_CLOCK_HCKEN | ESDHC_CLOCK_PEREN
 		| ESDHC_CLOCK_MASK);
<span class="p_header">diff --git a/drivers/net/can/usb/peak_usb/pcan_usb_fd.c b/drivers/net/can/usb/peak_usb/pcan_usb_fd.c</span>
<span class="p_header">index 7ccdc3e30c98..53d6bb045e9e 100644</span>
<span class="p_header">--- a/drivers/net/can/usb/peak_usb/pcan_usb_fd.c</span>
<span class="p_header">+++ b/drivers/net/can/usb/peak_usb/pcan_usb_fd.c</span>
<span class="p_chunk">@@ -184,7 +184,7 @@</span> <span class="p_context"> static int pcan_usb_fd_send_cmd(struct peak_usb_device *dev, void *cmd_tail)</span>
 	void *cmd_head = pcan_usb_fd_cmd_buffer(dev);
 	int err = 0;
 	u8 *packet_ptr;
<span class="p_del">-	int i, n = 1, packet_len;</span>
<span class="p_add">+	int packet_len;</span>
 	ptrdiff_t cmd_len;
 
 	/* usb device unregistered? */
<span class="p_chunk">@@ -201,17 +201,13 @@</span> <span class="p_context"> static int pcan_usb_fd_send_cmd(struct peak_usb_device *dev, void *cmd_tail)</span>
 	}
 
 	packet_ptr = cmd_head;
<span class="p_add">+	packet_len = cmd_len;</span>
 
 	/* firmware is not able to re-assemble 512 bytes buffer in full-speed */
<span class="p_del">-	if ((dev-&gt;udev-&gt;speed != USB_SPEED_HIGH) &amp;&amp;</span>
<span class="p_del">-	    (cmd_len &gt; PCAN_UFD_LOSPD_PKT_SIZE)) {</span>
<span class="p_del">-		packet_len = PCAN_UFD_LOSPD_PKT_SIZE;</span>
<span class="p_del">-		n += cmd_len / packet_len;</span>
<span class="p_del">-	} else {</span>
<span class="p_del">-		packet_len = cmd_len;</span>
<span class="p_del">-	}</span>
<span class="p_add">+	if (unlikely(dev-&gt;udev-&gt;speed != USB_SPEED_HIGH))</span>
<span class="p_add">+		packet_len = min(packet_len, PCAN_UFD_LOSPD_PKT_SIZE);</span>
 
<span class="p_del">-	for (i = 0; i &lt; n; i++) {</span>
<span class="p_add">+	do {</span>
 		err = usb_bulk_msg(dev-&gt;udev,
 				   usb_sndbulkpipe(dev-&gt;udev,
 						   PCAN_USBPRO_EP_CMDOUT),
<span class="p_chunk">@@ -224,7 +220,12 @@</span> <span class="p_context"> static int pcan_usb_fd_send_cmd(struct peak_usb_device *dev, void *cmd_tail)</span>
 		}
 
 		packet_ptr += packet_len;
<span class="p_del">-	}</span>
<span class="p_add">+		cmd_len -= packet_len;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (cmd_len &lt; PCAN_UFD_LOSPD_PKT_SIZE)</span>
<span class="p_add">+			packet_len = cmd_len;</span>
<span class="p_add">+</span>
<span class="p_add">+	} while (packet_len &gt; 0);</span>
 
 	return err;
 }
<span class="p_header">diff --git a/drivers/net/ethernet/marvell/mvpp2.c b/drivers/net/ethernet/marvell/mvpp2.c</span>
<span class="p_header">index fcf9ba5eb8d1..d147dc7d0f77 100644</span>
<span class="p_header">--- a/drivers/net/ethernet/marvell/mvpp2.c</span>
<span class="p_header">+++ b/drivers/net/ethernet/marvell/mvpp2.c</span>
<span class="p_chunk">@@ -4552,11 +4552,6 @@</span> <span class="p_context"> static void mvpp2_port_mii_gmac_configure_mode(struct mvpp2_port *port)</span>
 		       MVPP22_CTRL4_QSGMII_BYPASS_ACTIVE;
 		val &amp;= ~MVPP22_CTRL4_EXT_PIN_GMII_SEL;
 		writel(val, port-&gt;base + MVPP22_GMAC_CTRL_4_REG);
<span class="p_del">-</span>
<span class="p_del">-		val = readl(port-&gt;base + MVPP2_GMAC_CTRL_2_REG);</span>
<span class="p_del">-		val |= MVPP2_GMAC_DISABLE_PADDING;</span>
<span class="p_del">-		val &amp;= ~MVPP2_GMAC_FLOW_CTRL_MASK;</span>
<span class="p_del">-		writel(val, port-&gt;base + MVPP2_GMAC_CTRL_2_REG);</span>
 	} else if (phy_interface_mode_is_rgmii(port-&gt;phy_interface)) {
 		val = readl(port-&gt;base + MVPP22_GMAC_CTRL_4_REG);
 		val |= MVPP22_CTRL4_EXT_PIN_GMII_SEL |
<span class="p_chunk">@@ -4564,10 +4559,6 @@</span> <span class="p_context"> static void mvpp2_port_mii_gmac_configure_mode(struct mvpp2_port *port)</span>
 		       MVPP22_CTRL4_QSGMII_BYPASS_ACTIVE;
 		val &amp;= ~MVPP22_CTRL4_DP_CLK_SEL;
 		writel(val, port-&gt;base + MVPP22_GMAC_CTRL_4_REG);
<span class="p_del">-</span>
<span class="p_del">-		val = readl(port-&gt;base + MVPP2_GMAC_CTRL_2_REG);</span>
<span class="p_del">-		val &amp;= ~MVPP2_GMAC_DISABLE_PADDING;</span>
<span class="p_del">-		writel(val, port-&gt;base + MVPP2_GMAC_CTRL_2_REG);</span>
 	}
 
 	/* The port is connected to a copper PHY */
<span class="p_header">diff --git a/drivers/phy/phy-core.c b/drivers/phy/phy-core.c</span>
<span class="p_header">index a268f4d6f3e9..48a365e303e5 100644</span>
<span class="p_header">--- a/drivers/phy/phy-core.c</span>
<span class="p_header">+++ b/drivers/phy/phy-core.c</span>
<span class="p_chunk">@@ -395,6 +395,10 @@</span> <span class="p_context"> static struct phy *_of_phy_get(struct device_node *np, int index)</span>
 	if (ret)
 		return ERR_PTR(-ENODEV);
 
<span class="p_add">+	/* This phy type handled by the usb-phy subsystem for now */</span>
<span class="p_add">+	if (of_device_is_compatible(args.np, &quot;usb-nop-xceiv&quot;))</span>
<span class="p_add">+		return ERR_PTR(-ENODEV);</span>
<span class="p_add">+</span>
 	mutex_lock(&amp;phy_provider_mutex);
 	phy_provider = of_phy_provider_lookup(args.np);
 	if (IS_ERR(phy_provider) || !try_module_get(phy_provider-&gt;owner)) {
<span class="p_header">diff --git a/drivers/scsi/libsas/sas_scsi_host.c b/drivers/scsi/libsas/sas_scsi_host.c</span>
<span class="p_header">index ea8ad06ff582..10b17da20176 100644</span>
<span class="p_header">--- a/drivers/scsi/libsas/sas_scsi_host.c</span>
<span class="p_header">+++ b/drivers/scsi/libsas/sas_scsi_host.c</span>
<span class="p_chunk">@@ -486,15 +486,28 @@</span> <span class="p_context"> static int sas_queue_reset(struct domain_device *dev, int reset_type,</span>
 
 int sas_eh_abort_handler(struct scsi_cmnd *cmd)
 {
<span class="p_del">-	int res;</span>
<span class="p_add">+	int res = TMF_RESP_FUNC_FAILED;</span>
 	struct sas_task *task = TO_SAS_TASK(cmd);
 	struct Scsi_Host *host = cmd-&gt;device-&gt;host;
<span class="p_add">+	struct domain_device *dev = cmd_to_domain_dev(cmd);</span>
 	struct sas_internal *i = to_sas_internal(host-&gt;transportt);
<span class="p_add">+	unsigned long flags;</span>
 
 	if (!i-&gt;dft-&gt;lldd_abort_task)
 		return FAILED;
 
<span class="p_del">-	res = i-&gt;dft-&gt;lldd_abort_task(task);</span>
<span class="p_add">+	spin_lock_irqsave(host-&gt;host_lock, flags);</span>
<span class="p_add">+	/* We cannot do async aborts for SATA devices */</span>
<span class="p_add">+	if (dev_is_sata(dev) &amp;&amp; !host-&gt;host_eh_scheduled) {</span>
<span class="p_add">+		spin_unlock_irqrestore(host-&gt;host_lock, flags);</span>
<span class="p_add">+		return FAILED;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	spin_unlock_irqrestore(host-&gt;host_lock, flags);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (task)</span>
<span class="p_add">+		res = i-&gt;dft-&gt;lldd_abort_task(task);</span>
<span class="p_add">+	else</span>
<span class="p_add">+		SAS_DPRINTK(&quot;no task to abort\n&quot;);</span>
 	if (res == TMF_RESP_FUNC_SUCC || res == TMF_RESP_FUNC_COMPLETE)
 		return SUCCESS;
 
<span class="p_header">diff --git a/fs/pipe.c b/fs/pipe.c</span>
<span class="p_header">index 3909c55ed389..f0f4ab36c444 100644</span>
<span class="p_header">--- a/fs/pipe.c</span>
<span class="p_header">+++ b/fs/pipe.c</span>
<span class="p_chunk">@@ -1018,13 +1018,19 @@</span> <span class="p_context"> const struct file_operations pipefifo_fops = {</span>
 
 /*
  * Currently we rely on the pipe array holding a power-of-2 number
<span class="p_del">- * of pages.</span>
<span class="p_add">+ * of pages. Returns 0 on error.</span>
  */
 static inline unsigned int round_pipe_size(unsigned int size)
 {
 	unsigned long nr_pages;
 
<span class="p_add">+	if (size &lt; pipe_min_size)</span>
<span class="p_add">+		size = pipe_min_size;</span>
<span class="p_add">+</span>
 	nr_pages = (size + PAGE_SIZE - 1) &gt;&gt; PAGE_SHIFT;
<span class="p_add">+	if (nr_pages == 0)</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
 	return roundup_pow_of_two(nr_pages) &lt;&lt; PAGE_SHIFT;
 }
 
<span class="p_chunk">@@ -1040,6 +1046,8 @@</span> <span class="p_context"> static long pipe_set_size(struct pipe_inode_info *pipe, unsigned long arg)</span>
 	long ret = 0;
 
 	size = round_pipe_size(arg);
<span class="p_add">+	if (size == 0)</span>
<span class="p_add">+		return -EINVAL;</span>
 	nr_pages = size &gt;&gt; PAGE_SHIFT;
 
 	if (!nr_pages)
<span class="p_chunk">@@ -1123,13 +1131,18 @@</span> <span class="p_context"> static long pipe_set_size(struct pipe_inode_info *pipe, unsigned long arg)</span>
 int pipe_proc_fn(struct ctl_table *table, int write, void __user *buf,
 		 size_t *lenp, loff_t *ppos)
 {
<span class="p_add">+	unsigned int rounded_pipe_max_size;</span>
 	int ret;
 
 	ret = proc_douintvec_minmax(table, write, buf, lenp, ppos);
 	if (ret &lt; 0 || !write)
 		return ret;
 
<span class="p_del">-	pipe_max_size = round_pipe_size(pipe_max_size);</span>
<span class="p_add">+	rounded_pipe_max_size = round_pipe_size(pipe_max_size);</span>
<span class="p_add">+	if (rounded_pipe_max_size == 0)</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+</span>
<span class="p_add">+	pipe_max_size = rounded_pipe_max_size;</span>
 	return ret;
 }
 
<span class="p_header">diff --git a/fs/proc/array.c b/fs/proc/array.c</span>
<span class="p_header">index 9390032a11e1..e6094a15ef30 100644</span>
<span class="p_header">--- a/fs/proc/array.c</span>
<span class="p_header">+++ b/fs/proc/array.c</span>
<span class="p_chunk">@@ -424,8 +424,11 @@</span> <span class="p_context"> static int do_task_stat(struct seq_file *m, struct pid_namespace *ns,</span>
 		 * safe because the task has stopped executing permanently.
 		 */
 		if (permitted &amp;&amp; (task-&gt;flags &amp; PF_DUMPCORE)) {
<span class="p_del">-			eip = KSTK_EIP(task);</span>
<span class="p_del">-			esp = KSTK_ESP(task);</span>
<span class="p_add">+			if (try_get_task_stack(task)) {</span>
<span class="p_add">+				eip = KSTK_EIP(task);</span>
<span class="p_add">+				esp = KSTK_ESP(task);</span>
<span class="p_add">+				put_task_stack(task);</span>
<span class="p_add">+			}</span>
 		}
 	}
 
<span class="p_header">diff --git a/include/linux/delayacct.h b/include/linux/delayacct.h</span>
<span class="p_header">index 4178d2493547..5e335b6203f4 100644</span>
<span class="p_header">--- a/include/linux/delayacct.h</span>
<span class="p_header">+++ b/include/linux/delayacct.h</span>
<span class="p_chunk">@@ -71,7 +71,7 @@</span> <span class="p_context"> extern void delayacct_init(void);</span>
 extern void __delayacct_tsk_init(struct task_struct *);
 extern void __delayacct_tsk_exit(struct task_struct *);
 extern void __delayacct_blkio_start(void);
<span class="p_del">-extern void __delayacct_blkio_end(void);</span>
<span class="p_add">+extern void __delayacct_blkio_end(struct task_struct *);</span>
 extern int __delayacct_add_tsk(struct taskstats *, struct task_struct *);
 extern __u64 __delayacct_blkio_ticks(struct task_struct *);
 extern void __delayacct_freepages_start(void);
<span class="p_chunk">@@ -122,10 +122,10 @@</span> <span class="p_context"> static inline void delayacct_blkio_start(void)</span>
 		__delayacct_blkio_start();
 }
 
<span class="p_del">-static inline void delayacct_blkio_end(void)</span>
<span class="p_add">+static inline void delayacct_blkio_end(struct task_struct *p)</span>
 {
 	if (current-&gt;delays)
<span class="p_del">-		__delayacct_blkio_end();</span>
<span class="p_add">+		__delayacct_blkio_end(p);</span>
 	delayacct_clear_flag(DELAYACCT_PF_BLKIO);
 }
 
<span class="p_chunk">@@ -169,7 +169,7 @@</span> <span class="p_context"> static inline void delayacct_tsk_free(struct task_struct *tsk)</span>
 {}
 static inline void delayacct_blkio_start(void)
 {}
<span class="p_del">-static inline void delayacct_blkio_end(void)</span>
<span class="p_add">+static inline void delayacct_blkio_end(struct task_struct *p)</span>
 {}
 static inline int delayacct_add_tsk(struct taskstats *d,
 					struct task_struct *tsk)
<span class="p_header">diff --git a/include/linux/swapops.h b/include/linux/swapops.h</span>
<span class="p_header">index 9c5a2628d6ce..1d3877c39a00 100644</span>
<span class="p_header">--- a/include/linux/swapops.h</span>
<span class="p_header">+++ b/include/linux/swapops.h</span>
<span class="p_chunk">@@ -124,6 +124,11 @@</span> <span class="p_context"> static inline bool is_write_device_private_entry(swp_entry_t entry)</span>
 	return unlikely(swp_type(entry) == SWP_DEVICE_WRITE);
 }
 
<span class="p_add">+static inline unsigned long device_private_entry_to_pfn(swp_entry_t entry)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return swp_offset(entry);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static inline struct page *device_private_entry_to_page(swp_entry_t entry)
 {
 	return pfn_to_page(swp_offset(entry));
<span class="p_chunk">@@ -154,6 +159,11 @@</span> <span class="p_context"> static inline bool is_write_device_private_entry(swp_entry_t entry)</span>
 	return false;
 }
 
<span class="p_add">+static inline unsigned long device_private_entry_to_pfn(swp_entry_t entry)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static inline struct page *device_private_entry_to_page(swp_entry_t entry)
 {
 	return NULL;
<span class="p_chunk">@@ -189,6 +199,11 @@</span> <span class="p_context"> static inline int is_write_migration_entry(swp_entry_t entry)</span>
 	return unlikely(swp_type(entry) == SWP_MIGRATION_WRITE);
 }
 
<span class="p_add">+static inline unsigned long migration_entry_to_pfn(swp_entry_t entry)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return swp_offset(entry);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static inline struct page *migration_entry_to_page(swp_entry_t entry)
 {
 	struct page *p = pfn_to_page(swp_offset(entry));
<span class="p_chunk">@@ -218,6 +233,12 @@</span> <span class="p_context"> static inline int is_migration_entry(swp_entry_t swp)</span>
 {
 	return 0;
 }
<span class="p_add">+</span>
<span class="p_add">+static inline unsigned long migration_entry_to_pfn(swp_entry_t entry)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static inline struct page *migration_entry_to_page(swp_entry_t entry)
 {
 	return NULL;
<span class="p_header">diff --git a/include/linux/vermagic.h b/include/linux/vermagic.h</span>
<span class="p_header">index bae807eb2933..853291714ae0 100644</span>
<span class="p_header">--- a/include/linux/vermagic.h</span>
<span class="p_header">+++ b/include/linux/vermagic.h</span>
<span class="p_chunk">@@ -31,11 +31,17 @@</span> <span class="p_context"></span>
 #else
 #define MODULE_RANDSTRUCT_PLUGIN
 #endif
<span class="p_add">+#ifdef RETPOLINE</span>
<span class="p_add">+#define MODULE_VERMAGIC_RETPOLINE &quot;retpoline &quot;</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define MODULE_VERMAGIC_RETPOLINE &quot;&quot;</span>
<span class="p_add">+#endif</span>
 
 #define VERMAGIC_STRING 						\
 	UTS_RELEASE &quot; &quot;							\
 	MODULE_VERMAGIC_SMP MODULE_VERMAGIC_PREEMPT 			\
 	MODULE_VERMAGIC_MODULE_UNLOAD MODULE_VERMAGIC_MODVERSIONS	\
 	MODULE_ARCH_VERMAGIC						\
<span class="p_del">-	MODULE_RANDSTRUCT_PLUGIN</span>
<span class="p_add">+	MODULE_RANDSTRUCT_PLUGIN					\</span>
<span class="p_add">+	MODULE_VERMAGIC_RETPOLINE</span>
 
<span class="p_header">diff --git a/kernel/delayacct.c b/kernel/delayacct.c</span>
<span class="p_header">index 4a1c33416b6a..e2764d767f18 100644</span>
<span class="p_header">--- a/kernel/delayacct.c</span>
<span class="p_header">+++ b/kernel/delayacct.c</span>
<span class="p_chunk">@@ -51,16 +51,16 @@</span> <span class="p_context"> void __delayacct_tsk_init(struct task_struct *tsk)</span>
  * Finish delay accounting for a statistic using its timestamps (@start),
  * accumalator (@total) and @count
  */
<span class="p_del">-static void delayacct_end(u64 *start, u64 *total, u32 *count)</span>
<span class="p_add">+static void delayacct_end(spinlock_t *lock, u64 *start, u64 *total, u32 *count)</span>
 {
 	s64 ns = ktime_get_ns() - *start;
 	unsigned long flags;
 
 	if (ns &gt; 0) {
<span class="p_del">-		spin_lock_irqsave(&amp;current-&gt;delays-&gt;lock, flags);</span>
<span class="p_add">+		spin_lock_irqsave(lock, flags);</span>
 		*total += ns;
 		(*count)++;
<span class="p_del">-		spin_unlock_irqrestore(&amp;current-&gt;delays-&gt;lock, flags);</span>
<span class="p_add">+		spin_unlock_irqrestore(lock, flags);</span>
 	}
 }
 
<span class="p_chunk">@@ -69,17 +69,25 @@</span> <span class="p_context"> void __delayacct_blkio_start(void)</span>
 	current-&gt;delays-&gt;blkio_start = ktime_get_ns();
 }
 
<span class="p_del">-void __delayacct_blkio_end(void)</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * We cannot rely on the `current` macro, as we haven&#39;t yet switched back to</span>
<span class="p_add">+ * the process being woken.</span>
<span class="p_add">+ */</span>
<span class="p_add">+void __delayacct_blkio_end(struct task_struct *p)</span>
 {
<span class="p_del">-	if (current-&gt;delays-&gt;flags &amp; DELAYACCT_PF_SWAPIN)</span>
<span class="p_del">-		/* Swapin block I/O */</span>
<span class="p_del">-		delayacct_end(&amp;current-&gt;delays-&gt;blkio_start,</span>
<span class="p_del">-			&amp;current-&gt;delays-&gt;swapin_delay,</span>
<span class="p_del">-			&amp;current-&gt;delays-&gt;swapin_count);</span>
<span class="p_del">-	else	/* Other block I/O */</span>
<span class="p_del">-		delayacct_end(&amp;current-&gt;delays-&gt;blkio_start,</span>
<span class="p_del">-			&amp;current-&gt;delays-&gt;blkio_delay,</span>
<span class="p_del">-			&amp;current-&gt;delays-&gt;blkio_count);</span>
<span class="p_add">+	struct task_delay_info *delays = p-&gt;delays;</span>
<span class="p_add">+	u64 *total;</span>
<span class="p_add">+	u32 *count;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (p-&gt;delays-&gt;flags &amp; DELAYACCT_PF_SWAPIN) {</span>
<span class="p_add">+		total = &amp;delays-&gt;swapin_delay;</span>
<span class="p_add">+		count = &amp;delays-&gt;swapin_count;</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		total = &amp;delays-&gt;blkio_delay;</span>
<span class="p_add">+		count = &amp;delays-&gt;blkio_count;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	delayacct_end(&amp;delays-&gt;lock, &amp;delays-&gt;blkio_start, total, count);</span>
 }
 
 int __delayacct_add_tsk(struct taskstats *d, struct task_struct *tsk)
<span class="p_chunk">@@ -153,8 +161,10 @@</span> <span class="p_context"> void __delayacct_freepages_start(void)</span>
 
 void __delayacct_freepages_end(void)
 {
<span class="p_del">-	delayacct_end(&amp;current-&gt;delays-&gt;freepages_start,</span>
<span class="p_del">-			&amp;current-&gt;delays-&gt;freepages_delay,</span>
<span class="p_del">-			&amp;current-&gt;delays-&gt;freepages_count);</span>
<span class="p_add">+	delayacct_end(</span>
<span class="p_add">+		&amp;current-&gt;delays-&gt;lock,</span>
<span class="p_add">+		&amp;current-&gt;delays-&gt;freepages_start,</span>
<span class="p_add">+		&amp;current-&gt;delays-&gt;freepages_delay,</span>
<span class="p_add">+		&amp;current-&gt;delays-&gt;freepages_count);</span>
 }
 
<span class="p_header">diff --git a/kernel/futex.c b/kernel/futex.c</span>
<span class="p_header">index 76ed5921117a..52b3f4703158 100644</span>
<span class="p_header">--- a/kernel/futex.c</span>
<span class="p_header">+++ b/kernel/futex.c</span>
<span class="p_chunk">@@ -1878,6 +1878,9 @@</span> <span class="p_context"> static int futex_requeue(u32 __user *uaddr1, unsigned int flags,</span>
 	struct futex_q *this, *next;
 	DEFINE_WAKE_Q(wake_q);
 
<span class="p_add">+	if (nr_wake &lt; 0 || nr_requeue &lt; 0)</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+</span>
 	/*
 	 * When PI not supported: return -ENOSYS if requeue_pi is true,
 	 * consequently the compiler knows requeue_pi is always false past
<span class="p_chunk">@@ -2294,21 +2297,17 @@</span> <span class="p_context"> static void unqueue_me_pi(struct futex_q *q)</span>
 	spin_unlock(q-&gt;lock_ptr);
 }
 
<span class="p_del">-/*</span>
<span class="p_del">- * Fixup the pi_state owner with the new owner.</span>
<span class="p_del">- *</span>
<span class="p_del">- * Must be called with hash bucket lock held and mm-&gt;sem held for non</span>
<span class="p_del">- * private futexes.</span>
<span class="p_del">- */</span>
 static int fixup_pi_state_owner(u32 __user *uaddr, struct futex_q *q,
<span class="p_del">-				struct task_struct *newowner)</span>
<span class="p_add">+				struct task_struct *argowner)</span>
 {
<span class="p_del">-	u32 newtid = task_pid_vnr(newowner) | FUTEX_WAITERS;</span>
 	struct futex_pi_state *pi_state = q-&gt;pi_state;
 	u32 uval, uninitialized_var(curval), newval;
<span class="p_del">-	struct task_struct *oldowner;</span>
<span class="p_add">+	struct task_struct *oldowner, *newowner;</span>
<span class="p_add">+	u32 newtid;</span>
 	int ret;
 
<span class="p_add">+	lockdep_assert_held(q-&gt;lock_ptr);</span>
<span class="p_add">+</span>
 	raw_spin_lock_irq(&amp;pi_state-&gt;pi_mutex.wait_lock);
 
 	oldowner = pi_state-&gt;owner;
<span class="p_chunk">@@ -2317,11 +2316,17 @@</span> <span class="p_context"> static int fixup_pi_state_owner(u32 __user *uaddr, struct futex_q *q,</span>
 		newtid |= FUTEX_OWNER_DIED;
 
 	/*
<span class="p_del">-	 * We are here either because we stole the rtmutex from the</span>
<span class="p_del">-	 * previous highest priority waiter or we are the highest priority</span>
<span class="p_del">-	 * waiter but have failed to get the rtmutex the first time.</span>
<span class="p_add">+	 * We are here because either:</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 *  - we stole the lock and pi_state-&gt;owner needs updating to reflect</span>
<span class="p_add">+	 *    that (@argowner == current),</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * or:</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 *  - someone stole our lock and we need to fix things to point to the</span>
<span class="p_add">+	 *    new owner (@argowner == NULL).</span>
 	 *
<span class="p_del">-	 * We have to replace the newowner TID in the user space variable.</span>
<span class="p_add">+	 * Either way, we have to replace the TID in the user space variable.</span>
 	 * This must be atomic as we have to preserve the owner died bit here.
 	 *
 	 * Note: We write the user space value _before_ changing the pi_state
<span class="p_chunk">@@ -2334,6 +2339,42 @@</span> <span class="p_context"> static int fixup_pi_state_owner(u32 __user *uaddr, struct futex_q *q,</span>
 	 * in the PID check in lookup_pi_state.
 	 */
 retry:
<span class="p_add">+	if (!argowner) {</span>
<span class="p_add">+		if (oldowner != current) {</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * We raced against a concurrent self; things are</span>
<span class="p_add">+			 * already fixed up. Nothing to do.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			ret = 0;</span>
<span class="p_add">+			goto out_unlock;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		if (__rt_mutex_futex_trylock(&amp;pi_state-&gt;pi_mutex)) {</span>
<span class="p_add">+			/* We got the lock after all, nothing to fix. */</span>
<span class="p_add">+			ret = 0;</span>
<span class="p_add">+			goto out_unlock;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Since we just failed the trylock; there must be an owner.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		newowner = rt_mutex_owner(&amp;pi_state-&gt;pi_mutex);</span>
<span class="p_add">+		BUG_ON(!newowner);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		WARN_ON_ONCE(argowner != current);</span>
<span class="p_add">+		if (oldowner == current) {</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * We raced against a concurrent self; things are</span>
<span class="p_add">+			 * already fixed up. Nothing to do.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			ret = 0;</span>
<span class="p_add">+			goto out_unlock;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		newowner = argowner;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	newtid = task_pid_vnr(newowner) | FUTEX_WAITERS;</span>
<span class="p_add">+</span>
 	if (get_futex_value_locked(&amp;uval, uaddr))
 		goto handle_fault;
 
<span class="p_chunk">@@ -2434,15 +2475,28 @@</span> <span class="p_context"> static int fixup_owner(u32 __user *uaddr, struct futex_q *q, int locked)</span>
 		 * Got the lock. We might not be the anticipated owner if we
 		 * did a lock-steal - fix up the PI-state in that case:
 		 *
<span class="p_del">-		 * We can safely read pi_state-&gt;owner without holding wait_lock</span>
<span class="p_del">-		 * because we now own the rt_mutex, only the owner will attempt</span>
<span class="p_del">-		 * to change it.</span>
<span class="p_add">+		 * Speculative pi_state-&gt;owner read (we don&#39;t hold wait_lock);</span>
<span class="p_add">+		 * since we own the lock pi_state-&gt;owner == current is the</span>
<span class="p_add">+		 * stable state, anything else needs more attention.</span>
 		 */
 		if (q-&gt;pi_state-&gt;owner != current)
 			ret = fixup_pi_state_owner(uaddr, q, current);
 		goto out;
 	}
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If we didn&#39;t get the lock; check if anybody stole it from us. In</span>
<span class="p_add">+	 * that case, we need to fix up the uval to point to them instead of</span>
<span class="p_add">+	 * us, otherwise bad things happen. [10]</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * Another speculative read; pi_state-&gt;owner == current is unstable</span>
<span class="p_add">+	 * but needs our attention.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (q-&gt;pi_state-&gt;owner == current) {</span>
<span class="p_add">+		ret = fixup_pi_state_owner(uaddr, q, NULL);</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	/*
 	 * Paranoia check. If we did not take the lock, then we should not be
 	 * the owner of the rt_mutex.
<span class="p_header">diff --git a/kernel/locking/rtmutex.c b/kernel/locking/rtmutex.c</span>
<span class="p_header">index 6f3dba6e4e9e..65cc0cb984e6 100644</span>
<span class="p_header">--- a/kernel/locking/rtmutex.c</span>
<span class="p_header">+++ b/kernel/locking/rtmutex.c</span>
<span class="p_chunk">@@ -1290,6 +1290,19 @@</span> <span class="p_context"> rt_mutex_slowlock(struct rt_mutex *lock, int state,</span>
 	return ret;
 }
 
<span class="p_add">+static inline int __rt_mutex_slowtrylock(struct rt_mutex *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int ret = try_to_take_rt_mutex(lock, current, NULL);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * try_to_take_rt_mutex() sets the lock waiters bit</span>
<span class="p_add">+	 * unconditionally. Clean this up.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	fixup_rt_mutex_waiters(lock);</span>
<span class="p_add">+</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /*
  * Slow path try-lock function:
  */
<span class="p_chunk">@@ -1312,13 +1325,7 @@</span> <span class="p_context"> static inline int rt_mutex_slowtrylock(struct rt_mutex *lock)</span>
 	 */
 	raw_spin_lock_irqsave(&amp;lock-&gt;wait_lock, flags);
 
<span class="p_del">-	ret = try_to_take_rt_mutex(lock, current, NULL);</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * try_to_take_rt_mutex() sets the lock waiters bit</span>
<span class="p_del">-	 * unconditionally. Clean this up.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	fixup_rt_mutex_waiters(lock);</span>
<span class="p_add">+	ret = __rt_mutex_slowtrylock(lock);</span>
 
 	raw_spin_unlock_irqrestore(&amp;lock-&gt;wait_lock, flags);
 
<span class="p_chunk">@@ -1505,6 +1512,11 @@</span> <span class="p_context"> int __sched rt_mutex_futex_trylock(struct rt_mutex *lock)</span>
 	return rt_mutex_slowtrylock(lock);
 }
 
<span class="p_add">+int __sched __rt_mutex_futex_trylock(struct rt_mutex *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __rt_mutex_slowtrylock(lock);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /**
  * rt_mutex_timed_lock - lock a rt_mutex interruptible
  *			the timeout structure is provided
<span class="p_header">diff --git a/kernel/locking/rtmutex_common.h b/kernel/locking/rtmutex_common.h</span>
<span class="p_header">index 124e98ca0b17..68686b3ec3c1 100644</span>
<span class="p_header">--- a/kernel/locking/rtmutex_common.h</span>
<span class="p_header">+++ b/kernel/locking/rtmutex_common.h</span>
<span class="p_chunk">@@ -148,6 +148,7 @@</span> <span class="p_context"> extern bool rt_mutex_cleanup_proxy_lock(struct rt_mutex *lock,</span>
 				 struct rt_mutex_waiter *waiter);
 
 extern int rt_mutex_futex_trylock(struct rt_mutex *l);
<span class="p_add">+extern int __rt_mutex_futex_trylock(struct rt_mutex *l);</span>
 
 extern void rt_mutex_futex_unlock(struct rt_mutex *lock);
 extern bool __rt_mutex_futex_unlock(struct rt_mutex *lock,
<span class="p_header">diff --git a/kernel/sched/core.c b/kernel/sched/core.c</span>
<span class="p_header">index 8fa7b6f9e19b..55062461b2fd 100644</span>
<span class="p_header">--- a/kernel/sched/core.c</span>
<span class="p_header">+++ b/kernel/sched/core.c</span>
<span class="p_chunk">@@ -2046,7 +2046,7 @@</span> <span class="p_context"> try_to_wake_up(struct task_struct *p, unsigned int state, int wake_flags)</span>
 	p-&gt;state = TASK_WAKING;
 
 	if (p-&gt;in_iowait) {
<span class="p_del">-		delayacct_blkio_end();</span>
<span class="p_add">+		delayacct_blkio_end(p);</span>
 		atomic_dec(&amp;task_rq(p)-&gt;nr_iowait);
 	}
 
<span class="p_chunk">@@ -2059,7 +2059,7 @@</span> <span class="p_context"> try_to_wake_up(struct task_struct *p, unsigned int state, int wake_flags)</span>
 #else /* CONFIG_SMP */
 
 	if (p-&gt;in_iowait) {
<span class="p_del">-		delayacct_blkio_end();</span>
<span class="p_add">+		delayacct_blkio_end(p);</span>
 		atomic_dec(&amp;task_rq(p)-&gt;nr_iowait);
 	}
 
<span class="p_chunk">@@ -2112,7 +2112,7 @@</span> <span class="p_context"> static void try_to_wake_up_local(struct task_struct *p, struct rq_flags *rf)</span>
 
 	if (!task_on_rq_queued(p)) {
 		if (p-&gt;in_iowait) {
<span class="p_del">-			delayacct_blkio_end();</span>
<span class="p_add">+			delayacct_blkio_end(p);</span>
 			atomic_dec(&amp;rq-&gt;nr_iowait);
 		}
 		ttwu_activate(rq, p, ENQUEUE_WAKEUP | ENQUEUE_NOCLOCK);
<span class="p_header">diff --git a/kernel/time/timer.c b/kernel/time/timer.c</span>
<span class="p_header">index 73e3cdbc61f1..db5e6daadd94 100644</span>
<span class="p_header">--- a/kernel/time/timer.c</span>
<span class="p_header">+++ b/kernel/time/timer.c</span>
<span class="p_chunk">@@ -1656,7 +1656,7 @@</span> <span class="p_context"> void run_local_timers(void)</span>
 	hrtimer_run_queues();
 	/* Raise the softirq only if required. */
 	if (time_before(jiffies, base-&gt;clk)) {
<span class="p_del">-		if (!IS_ENABLED(CONFIG_NO_HZ_COMMON) || !base-&gt;nohz_active)</span>
<span class="p_add">+		if (!IS_ENABLED(CONFIG_NO_HZ_COMMON))</span>
 			return;
 		/* CPU is awake, so check the deferrable base. */
 		base++;
<span class="p_header">diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c</span>
<span class="p_header">index 87468398b9ed..d53268a4e167 100644</span>
<span class="p_header">--- a/kernel/trace/trace_events.c</span>
<span class="p_header">+++ b/kernel/trace/trace_events.c</span>
<span class="p_chunk">@@ -2213,6 +2213,7 @@</span> <span class="p_context"> void trace_event_eval_update(struct trace_eval_map **map, int len)</span>
 {
 	struct trace_event_call *call, *p;
 	const char *last_system = NULL;
<span class="p_add">+	bool first = false;</span>
 	int last_i;
 	int i;
 
<span class="p_chunk">@@ -2220,15 +2221,28 @@</span> <span class="p_context"> void trace_event_eval_update(struct trace_eval_map **map, int len)</span>
 	list_for_each_entry_safe(call, p, &amp;ftrace_events, list) {
 		/* events are usually grouped together with systems */
 		if (!last_system || call-&gt;class-&gt;system != last_system) {
<span class="p_add">+			first = true;</span>
 			last_i = 0;
 			last_system = call-&gt;class-&gt;system;
 		}
 
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Since calls are grouped by systems, the likelyhood that the</span>
<span class="p_add">+		 * next call in the iteration belongs to the same system as the</span>
<span class="p_add">+		 * previous call is high. As an optimization, we skip seaching</span>
<span class="p_add">+		 * for a map[] that matches the call&#39;s system if the last call</span>
<span class="p_add">+		 * was from the same system. That&#39;s what last_i is for. If the</span>
<span class="p_add">+		 * call has the same system as the previous call, then last_i</span>
<span class="p_add">+		 * will be the index of the first map[] that has a matching</span>
<span class="p_add">+		 * system.</span>
<span class="p_add">+		 */</span>
 		for (i = last_i; i &lt; len; i++) {
 			if (call-&gt;class-&gt;system == map[i]-&gt;system) {
 				/* Save the first system if need be */
<span class="p_del">-				if (!last_i)</span>
<span class="p_add">+				if (first) {</span>
 					last_i = i;
<span class="p_add">+					first = false;</span>
<span class="p_add">+				}</span>
 				update_event_printk(call, map[i]);
 			}
 		}
<span class="p_header">diff --git a/kernel/workqueue.c b/kernel/workqueue.c</span>
<span class="p_header">index a2dccfe1acec..8365a52a74c5 100644</span>
<span class="p_header">--- a/kernel/workqueue.c</span>
<span class="p_header">+++ b/kernel/workqueue.c</span>
<span class="p_chunk">@@ -48,6 +48,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/nodemask.h&gt;
 #include &lt;linux/moduleparam.h&gt;
 #include &lt;linux/uaccess.h&gt;
<span class="p_add">+#include &lt;linux/nmi.h&gt;</span>
 
 #include &quot;workqueue_internal.h&quot;
 
<span class="p_chunk">@@ -4479,6 +4480,12 @@</span> <span class="p_context"> void show_workqueue_state(void)</span>
 			if (pwq-&gt;nr_active || !list_empty(&amp;pwq-&gt;delayed_works))
 				show_pwq(pwq);
 			spin_unlock_irqrestore(&amp;pwq-&gt;pool-&gt;lock, flags);
<span class="p_add">+			/*</span>
<span class="p_add">+			 * We could be printing a lot from atomic context, e.g.</span>
<span class="p_add">+			 * sysrq-t -&gt; show_workqueue_state(). Avoid triggering</span>
<span class="p_add">+			 * hard lockup.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			touch_nmi_watchdog();</span>
 		}
 	}
 
<span class="p_chunk">@@ -4506,6 +4513,12 @@</span> <span class="p_context"> void show_workqueue_state(void)</span>
 		pr_cont(&quot;\n&quot;);
 	next_pool:
 		spin_unlock_irqrestore(&amp;pool-&gt;lock, flags);
<span class="p_add">+		/*</span>
<span class="p_add">+		 * We could be printing a lot from atomic context, e.g.</span>
<span class="p_add">+		 * sysrq-t -&gt; show_workqueue_state(). Avoid triggering</span>
<span class="p_add">+		 * hard lockup.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		touch_nmi_watchdog();</span>
 	}
 
 	rcu_read_unlock_sched();
<span class="p_header">diff --git a/mm/page_vma_mapped.c b/mm/page_vma_mapped.c</span>
<span class="p_header">index d22b84310f6d..956015614395 100644</span>
<span class="p_header">--- a/mm/page_vma_mapped.c</span>
<span class="p_header">+++ b/mm/page_vma_mapped.c</span>
<span class="p_chunk">@@ -30,10 +30,29 @@</span> <span class="p_context"> static bool map_pte(struct page_vma_mapped_walk *pvmw)</span>
 	return true;
 }
 
<span class="p_add">+/**</span>
<span class="p_add">+ * check_pte - check if @pvmw-&gt;page is mapped at the @pvmw-&gt;pte</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * page_vma_mapped_walk() found a place where @pvmw-&gt;page is *potentially*</span>
<span class="p_add">+ * mapped. check_pte() has to validate this.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * @pvmw-&gt;pte may point to empty PTE, swap PTE or PTE pointing to arbitrary</span>
<span class="p_add">+ * page.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * If PVMW_MIGRATION flag is set, returns true if @pvmw-&gt;pte contains migration</span>
<span class="p_add">+ * entry that points to @pvmw-&gt;page or any subpage in case of THP.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * If PVMW_MIGRATION flag is not set, returns true if @pvmw-&gt;pte points to</span>
<span class="p_add">+ * @pvmw-&gt;page or any subpage in case of THP.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Otherwise, return false.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ */</span>
 static bool check_pte(struct page_vma_mapped_walk *pvmw)
 {
<span class="p_add">+	unsigned long pfn;</span>
<span class="p_add">+</span>
 	if (pvmw-&gt;flags &amp; PVMW_MIGRATION) {
<span class="p_del">-#ifdef CONFIG_MIGRATION</span>
 		swp_entry_t entry;
 		if (!is_swap_pte(*pvmw-&gt;pte))
 			return false;
<span class="p_chunk">@@ -41,37 +60,31 @@</span> <span class="p_context"> static bool check_pte(struct page_vma_mapped_walk *pvmw)</span>
 
 		if (!is_migration_entry(entry))
 			return false;
<span class="p_del">-		if (migration_entry_to_page(entry) - pvmw-&gt;page &gt;=</span>
<span class="p_del">-				hpage_nr_pages(pvmw-&gt;page)) {</span>
<span class="p_del">-			return false;</span>
<span class="p_del">-		}</span>
<span class="p_del">-		if (migration_entry_to_page(entry) &lt; pvmw-&gt;page)</span>
<span class="p_del">-			return false;</span>
<span class="p_del">-#else</span>
<span class="p_del">-		WARN_ON_ONCE(1);</span>
<span class="p_del">-#endif</span>
<span class="p_del">-	} else {</span>
<span class="p_del">-		if (is_swap_pte(*pvmw-&gt;pte)) {</span>
<span class="p_del">-			swp_entry_t entry;</span>
 
<span class="p_del">-			entry = pte_to_swp_entry(*pvmw-&gt;pte);</span>
<span class="p_del">-			if (is_device_private_entry(entry) &amp;&amp;</span>
<span class="p_del">-			    device_private_entry_to_page(entry) == pvmw-&gt;page)</span>
<span class="p_del">-				return true;</span>
<span class="p_del">-		}</span>
<span class="p_add">+		pfn = migration_entry_to_pfn(entry);</span>
<span class="p_add">+	} else if (is_swap_pte(*pvmw-&gt;pte)) {</span>
<span class="p_add">+		swp_entry_t entry;</span>
 
<span class="p_del">-		if (!pte_present(*pvmw-&gt;pte))</span>
<span class="p_add">+		/* Handle un-addressable ZONE_DEVICE memory */</span>
<span class="p_add">+		entry = pte_to_swp_entry(*pvmw-&gt;pte);</span>
<span class="p_add">+		if (!is_device_private_entry(entry))</span>
 			return false;
 
<span class="p_del">-		/* THP can be referenced by any subpage */</span>
<span class="p_del">-		if (pte_page(*pvmw-&gt;pte) - pvmw-&gt;page &gt;=</span>
<span class="p_del">-				hpage_nr_pages(pvmw-&gt;page)) {</span>
<span class="p_del">-			return false;</span>
<span class="p_del">-		}</span>
<span class="p_del">-		if (pte_page(*pvmw-&gt;pte) &lt; pvmw-&gt;page)</span>
<span class="p_add">+		pfn = device_private_entry_to_pfn(entry);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		if (!pte_present(*pvmw-&gt;pte))</span>
 			return false;
<span class="p_add">+</span>
<span class="p_add">+		pfn = pte_pfn(*pvmw-&gt;pte);</span>
 	}
 
<span class="p_add">+	if (pfn &lt; page_to_pfn(pvmw-&gt;page))</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* THP can be referenced by any subpage */</span>
<span class="p_add">+	if (pfn - page_to_pfn(pvmw-&gt;page) &gt;= hpage_nr_pages(pvmw-&gt;page))</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+</span>
 	return true;
 }
 
<span class="p_header">diff --git a/net/can/af_can.c b/net/can/af_can.c</span>
<span class="p_header">index ecd5c703d11e..e3626e8500c2 100644</span>
<span class="p_header">--- a/net/can/af_can.c</span>
<span class="p_header">+++ b/net/can/af_can.c</span>
<span class="p_chunk">@@ -721,20 +721,16 @@</span> <span class="p_context"> static int can_rcv(struct sk_buff *skb, struct net_device *dev,</span>
 {
 	struct canfd_frame *cfd = (struct canfd_frame *)skb-&gt;data;
 
<span class="p_del">-	if (WARN_ONCE(dev-&gt;type != ARPHRD_CAN ||</span>
<span class="p_del">-		      skb-&gt;len != CAN_MTU ||</span>
<span class="p_del">-		      cfd-&gt;len &gt; CAN_MAX_DLEN,</span>
<span class="p_del">-		      &quot;PF_CAN: dropped non conform CAN skbuf: &quot;</span>
<span class="p_del">-		      &quot;dev type %d, len %d, datalen %d\n&quot;,</span>
<span class="p_del">-		      dev-&gt;type, skb-&gt;len, cfd-&gt;len))</span>
<span class="p_del">-		goto drop;</span>
<span class="p_add">+	if (unlikely(dev-&gt;type != ARPHRD_CAN || skb-&gt;len != CAN_MTU ||</span>
<span class="p_add">+		     cfd-&gt;len &gt; CAN_MAX_DLEN)) {</span>
<span class="p_add">+		pr_warn_once(&quot;PF_CAN: dropped non conform CAN skbuf: dev type %d, len %d, datalen %d\n&quot;,</span>
<span class="p_add">+			     dev-&gt;type, skb-&gt;len, cfd-&gt;len);</span>
<span class="p_add">+		kfree_skb(skb);</span>
<span class="p_add">+		return NET_RX_DROP;</span>
<span class="p_add">+	}</span>
 
 	can_receive(skb, dev);
 	return NET_RX_SUCCESS;
<span class="p_del">-</span>
<span class="p_del">-drop:</span>
<span class="p_del">-	kfree_skb(skb);</span>
<span class="p_del">-	return NET_RX_DROP;</span>
 }
 
 static int canfd_rcv(struct sk_buff *skb, struct net_device *dev,
<span class="p_chunk">@@ -742,20 +738,16 @@</span> <span class="p_context"> static int canfd_rcv(struct sk_buff *skb, struct net_device *dev,</span>
 {
 	struct canfd_frame *cfd = (struct canfd_frame *)skb-&gt;data;
 
<span class="p_del">-	if (WARN_ONCE(dev-&gt;type != ARPHRD_CAN ||</span>
<span class="p_del">-		      skb-&gt;len != CANFD_MTU ||</span>
<span class="p_del">-		      cfd-&gt;len &gt; CANFD_MAX_DLEN,</span>
<span class="p_del">-		      &quot;PF_CAN: dropped non conform CAN FD skbuf: &quot;</span>
<span class="p_del">-		      &quot;dev type %d, len %d, datalen %d\n&quot;,</span>
<span class="p_del">-		      dev-&gt;type, skb-&gt;len, cfd-&gt;len))</span>
<span class="p_del">-		goto drop;</span>
<span class="p_add">+	if (unlikely(dev-&gt;type != ARPHRD_CAN || skb-&gt;len != CANFD_MTU ||</span>
<span class="p_add">+		     cfd-&gt;len &gt; CANFD_MAX_DLEN)) {</span>
<span class="p_add">+		pr_warn_once(&quot;PF_CAN: dropped non conform CAN FD skbuf: dev type %d, len %d, datalen %d\n&quot;,</span>
<span class="p_add">+			     dev-&gt;type, skb-&gt;len, cfd-&gt;len);</span>
<span class="p_add">+		kfree_skb(skb);</span>
<span class="p_add">+		return NET_RX_DROP;</span>
<span class="p_add">+	}</span>
 
 	can_receive(skb, dev);
 	return NET_RX_SUCCESS;
<span class="p_del">-</span>
<span class="p_del">-drop:</span>
<span class="p_del">-	kfree_skb(skb);</span>
<span class="p_del">-	return NET_RX_DROP;</span>
 }
 
 /*
<span class="p_header">diff --git a/net/key/af_key.c b/net/key/af_key.c</span>
<span class="p_header">index a00d607e7224..2ad693232f74 100644</span>
<span class="p_header">--- a/net/key/af_key.c</span>
<span class="p_header">+++ b/net/key/af_key.c</span>
<span class="p_chunk">@@ -401,6 +401,11 @@</span> <span class="p_context"> static int verify_address_len(const void *p)</span>
 #endif
 	int len;
 
<span class="p_add">+	if (sp-&gt;sadb_address_len &lt;</span>
<span class="p_add">+	    DIV_ROUND_UP(sizeof(*sp) + offsetofend(typeof(*addr), sa_family),</span>
<span class="p_add">+			 sizeof(uint64_t)))</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+</span>
 	switch (addr-&gt;sa_family) {
 	case AF_INET:
 		len = DIV_ROUND_UP(sizeof(*sp) + sizeof(*sin), sizeof(uint64_t));
<span class="p_chunk">@@ -511,6 +516,9 @@</span> <span class="p_context"> static int parse_exthdrs(struct sk_buff *skb, const struct sadb_msg *hdr, void *</span>
 		uint16_t ext_type;
 		int ext_len;
 
<span class="p_add">+		if (len &lt; sizeof(*ehdr))</span>
<span class="p_add">+			return -EINVAL;</span>
<span class="p_add">+</span>
 		ext_len  = ehdr-&gt;sadb_ext_len;
 		ext_len *= sizeof(uint64_t);
 		ext_type = ehdr-&gt;sadb_ext_type;
<span class="p_header">diff --git a/scripts/Makefile.build b/scripts/Makefile.build</span>
<span class="p_header">index e63af4e19382..6bed45dc2cb1 100644</span>
<span class="p_header">--- a/scripts/Makefile.build</span>
<span class="p_header">+++ b/scripts/Makefile.build</span>
<span class="p_chunk">@@ -270,12 +270,18 @@</span> <span class="p_context"> else</span>
 objtool_args += $(call cc-ifversion, -lt, 0405, --no-unreachable)
 endif
 
<span class="p_add">+ifdef CONFIG_MODVERSIONS</span>
<span class="p_add">+objtool_o = $(@D)/.tmp_$(@F)</span>
<span class="p_add">+else</span>
<span class="p_add">+objtool_o = $(@)</span>
<span class="p_add">+endif</span>
<span class="p_add">+</span>
 # &#39;OBJECT_FILES_NON_STANDARD := y&#39;: skip objtool checking for a directory
 # &#39;OBJECT_FILES_NON_STANDARD_foo.o := &#39;y&#39;: skip objtool checking for a file
 # &#39;OBJECT_FILES_NON_STANDARD_foo.o := &#39;n&#39;: override directory skip for a file
 cmd_objtool = $(if $(patsubst y%,, \
 	$(OBJECT_FILES_NON_STANDARD_$(basetarget).o)$(OBJECT_FILES_NON_STANDARD)n), \
<span class="p_del">-	$(__objtool_obj) $(objtool_args) &quot;$(@)&quot;;)</span>
<span class="p_add">+	$(__objtool_obj) $(objtool_args) &quot;$(objtool_o)&quot;;)</span>
 objtool_obj = $(if $(patsubst y%,, \
 	$(OBJECT_FILES_NON_STANDARD_$(basetarget).o)$(OBJECT_FILES_NON_STANDARD)n), \
 	$(__objtool_obj))
<span class="p_chunk">@@ -291,15 +297,15 @@</span> <span class="p_context"> objtool_dep = $(objtool_obj)					\</span>
 define rule_cc_o_c
 	$(call echo-cmd,checksrc) $(cmd_checksrc)			  \
 	$(call cmd_and_fixdep,cc_o_c)					  \
<span class="p_del">-	$(cmd_modversions_c)						  \</span>
 	$(call echo-cmd,objtool) $(cmd_objtool)				  \
<span class="p_add">+	$(cmd_modversions_c)						  \</span>
 	$(call echo-cmd,record_mcount) $(cmd_record_mcount)
 endef
 
 define rule_as_o_S
 	$(call cmd_and_fixdep,as_o_S)					  \
<span class="p_del">-	$(cmd_modversions_S)						  \</span>
<span class="p_del">-	$(call echo-cmd,objtool) $(cmd_objtool)</span>
<span class="p_add">+	$(call echo-cmd,objtool) $(cmd_objtool)				  \</span>
<span class="p_add">+	$(cmd_modversions_S)</span>
 endef
 
 # List module undefined symbols (or empty line if not enabled)
<span class="p_header">diff --git a/scripts/gdb/linux/tasks.py b/scripts/gdb/linux/tasks.py</span>
<span class="p_header">index 1bf949c43b76..f6ab3ccf698f 100644</span>
<span class="p_header">--- a/scripts/gdb/linux/tasks.py</span>
<span class="p_header">+++ b/scripts/gdb/linux/tasks.py</span>
<span class="p_chunk">@@ -96,6 +96,8 @@</span> <span class="p_context"> def get_thread_info(task):</span>
         thread_info_addr = task.address + ia64_task_size
         thread_info = thread_info_addr.cast(thread_info_ptr_type)
     else:
<span class="p_add">+        if task.type.fields()[0].type == thread_info_type.get_type():</span>
<span class="p_add">+            return task[&#39;thread_info&#39;]</span>
         thread_info = task[&#39;stack&#39;].cast(thread_info_ptr_type)
     return thread_info.dereference()
 
<span class="p_header">diff --git a/sound/core/pcm_lib.c b/sound/core/pcm_lib.c</span>
<span class="p_header">index db7894bb028c..faa67861cbc1 100644</span>
<span class="p_header">--- a/sound/core/pcm_lib.c</span>
<span class="p_header">+++ b/sound/core/pcm_lib.c</span>
<span class="p_chunk">@@ -560,7 +560,6 @@</span> <span class="p_context"> static inline unsigned int muldiv32(unsigned int a, unsigned int b,</span>
 {
 	u_int64_t n = (u_int64_t) a * b;
 	if (c == 0) {
<span class="p_del">-		snd_BUG_ON(!n);</span>
 		*r = 0;
 		return UINT_MAX;
 	}
<span class="p_header">diff --git a/sound/core/seq/seq_clientmgr.c b/sound/core/seq/seq_clientmgr.c</span>
<span class="p_header">index d10c780dfd54..ac30fc1ab98b 100644</span>
<span class="p_header">--- a/sound/core/seq/seq_clientmgr.c</span>
<span class="p_header">+++ b/sound/core/seq/seq_clientmgr.c</span>
<span class="p_chunk">@@ -221,6 +221,7 @@</span> <span class="p_context"> static struct snd_seq_client *seq_create_client1(int client_index, int poolsize)</span>
 	rwlock_init(&amp;client-&gt;ports_lock);
 	mutex_init(&amp;client-&gt;ports_mutex);
 	INIT_LIST_HEAD(&amp;client-&gt;ports_list_head);
<span class="p_add">+	mutex_init(&amp;client-&gt;ioctl_mutex);</span>
 
 	/* find free slot in the client table */
 	spin_lock_irqsave(&amp;clients_lock, flags);
<span class="p_chunk">@@ -2126,7 +2127,9 @@</span> <span class="p_context"> static long snd_seq_ioctl(struct file *file, unsigned int cmd,</span>
 			return -EFAULT;
 	}
 
<span class="p_add">+	mutex_lock(&amp;client-&gt;ioctl_mutex);</span>
 	err = handler-&gt;func(client, &amp;buf);
<span class="p_add">+	mutex_unlock(&amp;client-&gt;ioctl_mutex);</span>
 	if (err &gt;= 0) {
 		/* Some commands includes a bug in &#39;dir&#39; field. */
 		if (handler-&gt;cmd == SNDRV_SEQ_IOCTL_SET_QUEUE_CLIENT ||
<span class="p_header">diff --git a/sound/core/seq/seq_clientmgr.h b/sound/core/seq/seq_clientmgr.h</span>
<span class="p_header">index c6614254ef8a..0611e1e0ed5b 100644</span>
<span class="p_header">--- a/sound/core/seq/seq_clientmgr.h</span>
<span class="p_header">+++ b/sound/core/seq/seq_clientmgr.h</span>
<span class="p_chunk">@@ -61,6 +61,7 @@</span> <span class="p_context"> struct snd_seq_client {</span>
 	struct list_head ports_list_head;
 	rwlock_t ports_lock;
 	struct mutex ports_mutex;
<span class="p_add">+	struct mutex ioctl_mutex;</span>
 	int convert32;		/* convert 32-&gt;64bit */
 
 	/* output pool */
<span class="p_header">diff --git a/sound/pci/hda/patch_cirrus.c b/sound/pci/hda/patch_cirrus.c</span>
<span class="p_header">index 80bbadc83721..d6e079f4ec09 100644</span>
<span class="p_header">--- a/sound/pci/hda/patch_cirrus.c</span>
<span class="p_header">+++ b/sound/pci/hda/patch_cirrus.c</span>
<span class="p_chunk">@@ -408,6 +408,7 @@</span> <span class="p_context"> static const struct snd_pci_quirk cs420x_fixup_tbl[] = {</span>
 	/*SND_PCI_QUIRK(0x8086, 0x7270, &quot;IMac 27 Inch&quot;, CS420X_IMAC27),*/
 
 	/* codec SSID */
<span class="p_add">+	SND_PCI_QUIRK(0x106b, 0x0600, &quot;iMac 14,1&quot;, CS420X_IMAC27_122),</span>
 	SND_PCI_QUIRK(0x106b, 0x1c00, &quot;MacBookPro 8,1&quot;, CS420X_MBP81),
 	SND_PCI_QUIRK(0x106b, 0x2000, &quot;iMac 12,2&quot;, CS420X_IMAC27_122),
 	SND_PCI_QUIRK(0x106b, 0x2800, &quot;MacBookPro 10,1&quot;, CS420X_MBP101),
<span class="p_header">diff --git a/sound/pci/hda/patch_realtek.c b/sound/pci/hda/patch_realtek.c</span>
<span class="p_header">index acdb196ddb44..145e92d6ca94 100644</span>
<span class="p_header">--- a/sound/pci/hda/patch_realtek.c</span>
<span class="p_header">+++ b/sound/pci/hda/patch_realtek.c</span>
<span class="p_chunk">@@ -6173,6 +6173,7 @@</span> <span class="p_context"> static const struct snd_pci_quirk alc269_fixup_tbl[] = {</span>
 	SND_PCI_QUIRK(0x1028, 0x075b, &quot;Dell XPS 13 9360&quot;, ALC256_FIXUP_DELL_XPS_13_HEADPHONE_NOISE),
 	SND_PCI_QUIRK(0x1028, 0x075d, &quot;Dell AIO&quot;, ALC298_FIXUP_SPK_VOLUME),
 	SND_PCI_QUIRK(0x1028, 0x0798, &quot;Dell Inspiron 17 7000 Gaming&quot;, ALC256_FIXUP_DELL_INSPIRON_7559_SUBWOOFER),
<span class="p_add">+	SND_PCI_QUIRK(0x1028, 0x082a, &quot;Dell XPS 13 9360&quot;, ALC256_FIXUP_DELL_XPS_13_HEADPHONE_NOISE),</span>
 	SND_PCI_QUIRK(0x1028, 0x164a, &quot;Dell&quot;, ALC293_FIXUP_DELL1_MIC_NO_PRESENCE),
 	SND_PCI_QUIRK(0x1028, 0x164b, &quot;Dell&quot;, ALC293_FIXUP_DELL1_MIC_NO_PRESENCE),
 	SND_PCI_QUIRK(0x103c, 0x1586, &quot;HP&quot;, ALC269_FIXUP_HP_MUTE_LED_MIC2),
<span class="p_header">diff --git a/tools/objtool/Makefile b/tools/objtool/Makefile</span>
<span class="p_header">index ae0272f9a091..e6acc281dd37 100644</span>
<span class="p_header">--- a/tools/objtool/Makefile</span>
<span class="p_header">+++ b/tools/objtool/Makefile</span>
<span class="p_chunk">@@ -46,7 +46,7 @@</span> <span class="p_context"> $(OBJTOOL_IN): fixdep FORCE</span>
 	@$(MAKE) $(build)=objtool
 
 $(OBJTOOL): $(LIBSUBCMD) $(OBJTOOL_IN)
<span class="p_del">-	@./sync-check.sh</span>
<span class="p_add">+	@$(CONFIG_SHELL) ./sync-check.sh</span>
 	$(QUIET_LINK)$(CC) $(OBJTOOL_IN) $(LDFLAGS) -o $@
 
 
<span class="p_header">diff --git a/tools/objtool/arch/x86/decode.c b/tools/objtool/arch/x86/decode.c</span>
<span class="p_header">index 8acfc47af70e..540a209b78ab 100644</span>
<span class="p_header">--- a/tools/objtool/arch/x86/decode.c</span>
<span class="p_header">+++ b/tools/objtool/arch/x86/decode.c</span>
<span class="p_chunk">@@ -138,7 +138,7 @@</span> <span class="p_context"> int arch_decode_instruction(struct elf *elf, struct section *sec,</span>
 			*type = INSN_STACK;
 			op-&gt;src.type = OP_SRC_ADD;
 			op-&gt;src.reg = op_to_cfi_reg[modrm_reg][rex_r];
<span class="p_del">-			op-&gt;dest.type = OP_SRC_REG;</span>
<span class="p_add">+			op-&gt;dest.type = OP_DEST_REG;</span>
 			op-&gt;dest.reg = CFI_SP;
 		}
 		break;
<span class="p_header">diff --git a/tools/objtool/builtin-orc.c b/tools/objtool/builtin-orc.c</span>
<span class="p_header">index 4c6b5c9ef073..91e8e19ff5e0 100644</span>
<span class="p_header">--- a/tools/objtool/builtin-orc.c</span>
<span class="p_header">+++ b/tools/objtool/builtin-orc.c</span>
<span class="p_chunk">@@ -44,6 +44,9 @@</span> <span class="p_context"> int cmd_orc(int argc, const char **argv)</span>
 	const char *objname;
 
 	argc--; argv++;
<span class="p_add">+	if (argc &lt;= 0)</span>
<span class="p_add">+		usage_with_options(orc_usage, check_options);</span>
<span class="p_add">+</span>
 	if (!strncmp(argv[0], &quot;gen&quot;, 3)) {
 		argc = parse_options(argc, argv, check_options, orc_usage, 0);
 		if (argc != 1)
<span class="p_chunk">@@ -52,7 +55,6 @@</span> <span class="p_context"> int cmd_orc(int argc, const char **argv)</span>
 		objname = argv[0];
 
 		return check(objname, no_fp, no_unreachable, true);
<span class="p_del">-</span>
 	}
 
 	if (!strcmp(argv[0], &quot;dump&quot;)) {
<span class="p_header">diff --git a/tools/objtool/elf.c b/tools/objtool/elf.c</span>
<span class="p_header">index 24460155c82c..c1c338661699 100644</span>
<span class="p_header">--- a/tools/objtool/elf.c</span>
<span class="p_header">+++ b/tools/objtool/elf.c</span>
<span class="p_chunk">@@ -26,6 +26,7 @@</span> <span class="p_context"></span>
 #include &lt;stdlib.h&gt;
 #include &lt;string.h&gt;
 #include &lt;unistd.h&gt;
<span class="p_add">+#include &lt;errno.h&gt;</span>
 
 #include &quot;elf.h&quot;
 #include &quot;warn.h&quot;
<span class="p_chunk">@@ -358,7 +359,8 @@</span> <span class="p_context"> struct elf *elf_open(const char *name, int flags)</span>
 
 	elf-&gt;fd = open(name, flags);
 	if (elf-&gt;fd == -1) {
<span class="p_del">-		perror(&quot;open&quot;);</span>
<span class="p_add">+		fprintf(stderr, &quot;objtool: Can&#39;t open &#39;%s&#39;: %s\n&quot;,</span>
<span class="p_add">+			name, strerror(errno));</span>
 		goto err;
 	}
 
<span class="p_header">diff --git a/tools/objtool/orc_gen.c b/tools/objtool/orc_gen.c</span>
<span class="p_header">index e5ca31429c9b..e61fe703197b 100644</span>
<span class="p_header">--- a/tools/objtool/orc_gen.c</span>
<span class="p_header">+++ b/tools/objtool/orc_gen.c</span>
<span class="p_chunk">@@ -165,6 +165,8 @@</span> <span class="p_context"> int create_orc_sections(struct objtool_file *file)</span>
 
 	/* create .orc_unwind_ip and .rela.orc_unwind_ip sections */
 	sec = elf_create_section(file-&gt;elf, &quot;.orc_unwind_ip&quot;, sizeof(int), idx);
<span class="p_add">+	if (!sec)</span>
<span class="p_add">+		return -1;</span>
 
 	ip_relasec = elf_create_rela_section(file-&gt;elf, sec);
 	if (!ip_relasec)
<span class="p_header">diff --git a/virt/kvm/arm/mmu.c b/virt/kvm/arm/mmu.c</span>
<span class="p_header">index b4b69c2d1012..9dea96380339 100644</span>
<span class="p_header">--- a/virt/kvm/arm/mmu.c</span>
<span class="p_header">+++ b/virt/kvm/arm/mmu.c</span>
<span class="p_chunk">@@ -1310,7 +1310,7 @@</span> <span class="p_context"> static int user_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa,</span>
 		return -EFAULT;
 	}
 
<span class="p_del">-	if (is_vm_hugetlb_page(vma) &amp;&amp; !logging_active) {</span>
<span class="p_add">+	if (vma_kernel_pagesize(vma) == PMD_SIZE &amp;&amp; !logging_active) {</span>
 		hugetlb = true;
 		gfn = (fault_ipa &amp; PMD_MASK) &gt;&gt; PAGE_SHIFT;
 	} else {

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



