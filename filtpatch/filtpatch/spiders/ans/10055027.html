
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v4,2/4] KVM: X86: Add paravirt remote TLB flush - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v4,2/4] KVM: X86: Add paravirt remote TLB flush</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=104371">Wanpeng Li</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Nov. 13, 2017, 12:33 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1510533206-9821-3-git-send-email-wanpeng.li@hotmail.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10055027/mbox/"
   >mbox</a>
|
   <a href="/patch/10055027/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10055027/">/patch/10055027/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	192AD60325 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 13 Nov 2017 00:34:23 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 0AC9229249
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 13 Nov 2017 00:34:23 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id F35E12924E; Mon, 13 Nov 2017 00:34:22 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-7.0 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	DKIM_VALID, DKIM_VALID_AU, FREEMAIL_FROM,
	RCVD_IN_DNSWL_HI autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 82C6C29249
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 13 Nov 2017 00:34:22 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1751692AbdKMAeV (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Sun, 12 Nov 2017 19:34:21 -0500
Received: from mail-pg0-f68.google.com ([74.125.83.68]:50165 &quot;EHLO
	mail-pg0-f68.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1751335AbdKMAdf (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Sun, 12 Nov 2017 19:33:35 -0500
Received: by mail-pg0-f68.google.com with SMTP id g6so11447811pgn.6;
	Sun, 12 Nov 2017 16:33:34 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=gmail.com; s=20161025;
	h=from:to:cc:subject:date:message-id:in-reply-to:references
	:mime-version:content-transfer-encoding;
	bh=HO0gqI5FDGaKKNA8ucnFojGfWq3L5oSSutbqrHQNOeo=;
	b=MNI80J+6Kmm+GcRGC5KQrsLb9XN0rJKSgKgi3iCxUyhw5eMhLcYDcJ8R13D2EeZonc
	xnSsBp7I7gFQKbq1oEVdy2CsL30wBBpD2Bfn1VuUsCbD2VoCP33447UeprdqqWh8MuEH
	92el/WmvatqEzFJbjDWDnu/0AB3/NPE5n5QLWRk1/ypuoT/luSmUh6lxoADhz6CqlbLQ
	dv3xi4ue3Qk05qaorJsrOeJ2Q80dnSTnrV2Sk/LEZBAl93bFjiKz9NEkXlNPOuMY7Qk7
	sAh+yDBpJ9epC2pk5cpoHPuxB0PfDGWDafish0wMlFdWjt7tE3uMaenlrENTNlsLWYXE
	2JTw==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20161025;
	h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
	:references:mime-version:content-transfer-encoding;
	bh=HO0gqI5FDGaKKNA8ucnFojGfWq3L5oSSutbqrHQNOeo=;
	b=Db3BiA27ZiyAQTEiT+xnNFnnIIsLi+MLGGmL2wM4UtmgLnn9lGr4PR7bgzQgm3noLB
	yFbNa+VPSore+jqu/zzH7eCbOfxYQInF0C1kc18zHkWpTt5u5VB4H9s6JLXucFlErffU
	sOLOvEed4qv/KlrPS9MUSB3XmraZuL0SEz8zAorQqW1MLkOakneBDE9kvJaUIf8qhVlb
	6Onf3RigZbP2MzLEbyFeivmzZQJU4BSnoMHmBX3QGvOEY0YL3nUYQrqeNKd+E0EiRcVk
	6k8mEruBqxWqb9CGrdddUkdIgq2wUAaUAXiyeh/BpMS60i0w8+PDACHd++dMFM3G3XTo
	B/NQ==
X-Gm-Message-State: AJaThX4xq/aD2B/P8uAmF6FGP/uUdolXPgu5ARHhJOlHnRvA7maBOZ0C
	u5PB4rp/QTv0fnZEBlojg3T+tw==
X-Google-Smtp-Source: AGs4zMbHfoNsJt63PmU6wvbGV9jqaH2RyUy3BP8cIfLKyjVvJIVTcpGMdBbHl+qWUu77WEjv+aqdsQ==
X-Received: by 10.99.104.6 with SMTP id d6mr6966849pgc.319.1510533214255;
	Sun, 12 Nov 2017 16:33:34 -0800 (PST)
Received: from localhost ([203.205.141.123])
	by smtp.gmail.com with ESMTPSA id
	a4sm31020870pfj.72.2017.11.12.16.33.33
	(version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
	Sun, 12 Nov 2017 16:33:33 -0800 (PST)
From: Wanpeng Li &lt;kernellwp@gmail.com&gt;
X-Google-Original-From: Wanpeng Li &lt;wanpeng.li@hotmail.com&gt;
To: linux-kernel@vger.kernel.org, kvm@vger.kernel.org
Cc: Paolo Bonzini &lt;pbonzini@redhat.com&gt;,
	=?UTF-8?q?Radim=20Kr=C4=8Dm=C3=A1=C5=99?= &lt;rkrcmar@redhat.com&gt;,
	Peter Zijlstra &lt;peterz@infradead.org&gt;,
	Wanpeng Li &lt;wanpeng.li@hotmail.com&gt;
Subject: [PATCH v4 2/4] KVM: X86: Add paravirt remote TLB flush
Date: Sun, 12 Nov 2017 16:33:24 -0800
Message-Id: &lt;1510533206-9821-3-git-send-email-wanpeng.li@hotmail.com&gt;
X-Mailer: git-send-email 2.7.4
In-Reply-To: &lt;1510533206-9821-1-git-send-email-wanpeng.li@hotmail.com&gt;
References: &lt;1510533206-9821-1-git-send-email-wanpeng.li@hotmail.com&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=104371">Wanpeng Li</a> - Nov. 13, 2017, 12:33 a.m.</div>
<pre class="content">
<span class="from">From: Wanpeng Li &lt;wanpeng.li@hotmail.com&gt;</span>

Remote flushing api&#39;s does a busy wait which is fine in bare-metal
scenario. But with-in the guest, the vcpus might have been pre-empted
or blocked. In this scenario, the initator vcpu would end up
busy-waiting for a long amount of time.

This patch set implements para-virt flush tlbs making sure that it does 
not wait for vcpus that are sleeping. And all the sleeping vcpus flush 
the tlb on guest enter.

The best result is achieved when we&#39;re overcommiting the host by running 
multiple vCPUs on each pCPU. In this case PV tlb flush avoids touching 
vCPUs which are not scheduled and avoid the wait on the main CPU.

Test on a Haswell i7 desktop 4 cores (2HT), so 8 pCPUs, running ebizzy 
in one linux guest.

ebizzy -M 
              vanilla    optimized     boost
 8 vCPUs       10152       10083       -0.68% 
16 vCPUs        1224        4866       297.5% 
24 vCPUs        1109        3871       249%
32 vCPUs        1025        3375       229.3% 

Cc: Paolo Bonzini &lt;pbonzini@redhat.com&gt;
Cc: Radim Krčmář &lt;rkrcmar@redhat.com&gt;
Cc: Peter Zijlstra &lt;peterz@infradead.org&gt;
<span class="signed-off-by">Signed-off-by: Wanpeng Li &lt;wanpeng.li@hotmail.com&gt;</span>
---
 Documentation/virtual/kvm/cpuid.txt  |  4 ++++
 arch/x86/include/uapi/asm/kvm_para.h |  2 ++
 arch/x86/kernel/kvm.c                | 42 +++++++++++++++++++++++++++++++++++-
 3 files changed, 47 insertions(+), 1 deletion(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - Nov. 13, 2017, 7:59 a.m.</div>
<pre class="content">
On Sun, Nov 12, 2017 at 04:33:24PM -0800, Wanpeng Li wrote:
<span class="quote">&gt; +static void kvm_flush_tlb_others(const struct cpumask *cpumask,</span>
<span class="quote">&gt; +			const struct flush_tlb_info *info)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	u8 state;</span>
<span class="quote">&gt; +	int cpu;</span>
<span class="quote">&gt; +	struct kvm_steal_time *src;</span>
<span class="quote">&gt; +	struct cpumask *flushmask = this_cpu_cpumask_var_ptr(__pv_tlb_mask);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (unlikely(!flushmask))</span>
<span class="quote">&gt; +		return;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	cpumask_copy(flushmask, cpumask);</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * We have to call flush only on online vCPUs. And</span>
<span class="quote">&gt; +	 * queue flush_on_enter for pre-empted vCPUs</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	for_each_cpu(cpu, cpumask) {</span>

Should this not iterate flushmask? Its far too early to think, so I&#39;m
not sure this is an actual problem, but it does seem weird.
<span class="quote">
&gt; +		src = &amp;per_cpu(steal_time, cpu);</span>
<span class="quote">&gt; +		state = READ_ONCE(src-&gt;preempted);</span>
<span class="quote">&gt; +		if ((state &amp; KVM_VCPU_PREEMPTED)) {</span>
<span class="quote">&gt; +			if (try_cmpxchg(&amp;src-&gt;preempted, &amp;state,</span>
<span class="quote">&gt; +				state | KVM_VCPU_SHOULD_FLUSH))</span>
<span class="quote">&gt; +				__cpumask_clear_cpu(cpu, flushmask);</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	native_flush_tlb_others(flushmask, info);</span>
<span class="quote">&gt; +}</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - Nov. 13, 2017, 8:04 a.m.</div>
<pre class="content">
On Sun, Nov 12, 2017 at 04:33:24PM -0800, Wanpeng Li wrote:
<span class="quote">&gt; +static void kvm_flush_tlb_others(const struct cpumask *cpumask,</span>
<span class="quote">&gt; +			const struct flush_tlb_info *info)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	u8 state;</span>
<span class="quote">&gt; +	int cpu;</span>
<span class="quote">&gt; +	struct kvm_steal_time *src;</span>
<span class="quote">&gt; +	struct cpumask *flushmask = this_cpu_cpumask_var_ptr(__pv_tlb_mask);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (unlikely(!flushmask))</span>
<span class="quote">&gt; +		return;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	cpumask_copy(flushmask, cpumask);</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * We have to call flush only on online vCPUs. And</span>
<span class="quote">&gt; +	 * queue flush_on_enter for pre-empted vCPUs</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	for_each_cpu(cpu, cpumask) {</span>
<span class="quote">&gt; +		src = &amp;per_cpu(steal_time, cpu);</span>
<span class="quote">&gt; +		state = READ_ONCE(src-&gt;preempted);</span>
<span class="quote">&gt; +		if ((state &amp; KVM_VCPU_PREEMPTED)) {</span>
<span class="quote">&gt; +			if (try_cmpxchg(&amp;src-&gt;preempted, &amp;state,</span>
<span class="quote">&gt; +				state | KVM_VCPU_SHOULD_FLUSH))</span>
<span class="quote">&gt; +				__cpumask_clear_cpu(cpu, flushmask);</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +	}</span>

So if at this point a vCPU gets preempted we&#39;ll still spin-wait for it,
which is sub-optimal.

I think we can come up with something to get around that &#39;problem&#39; if
indeed it is a problem. But we can easily do that as follow up patches.
Just let me know if you think its worth spending more time on.
<span class="quote">
&gt; +	native_flush_tlb_others(flushmask, info);</span>
<span class="quote">&gt; +}</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=104371">Wanpeng Li</a> - Nov. 13, 2017, 8:12 a.m.</div>
<pre class="content">
2017-11-13 15:59 GMT+08:00 Peter Zijlstra &lt;peterz@infradead.org&gt;:
<span class="quote">&gt; On Sun, Nov 12, 2017 at 04:33:24PM -0800, Wanpeng Li wrote:</span>
<span class="quote">&gt;&gt; +static void kvm_flush_tlb_others(const struct cpumask *cpumask,</span>
<span class="quote">&gt;&gt; +                     const struct flush_tlb_info *info)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +     u8 state;</span>
<span class="quote">&gt;&gt; +     int cpu;</span>
<span class="quote">&gt;&gt; +     struct kvm_steal_time *src;</span>
<span class="quote">&gt;&gt; +     struct cpumask *flushmask = this_cpu_cpumask_var_ptr(__pv_tlb_mask);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +     if (unlikely(!flushmask))</span>
<span class="quote">&gt;&gt; +             return;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +     cpumask_copy(flushmask, cpumask);</span>
<span class="quote">&gt;&gt; +     /*</span>
<span class="quote">&gt;&gt; +      * We have to call flush only on online vCPUs. And</span>
<span class="quote">&gt;&gt; +      * queue flush_on_enter for pre-empted vCPUs</span>
<span class="quote">&gt;&gt; +      */</span>
<span class="quote">&gt;&gt; +     for_each_cpu(cpu, cpumask) {</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Should this not iterate flushmask? Its far too early to think, so I&#39;m</span>
<span class="quote">&gt; not sure this is an actual problem, but it does seem weird.</span>

Agreed, should be flushmask in next version. :)

Regards,
Wanpeng Li
<span class="quote">
&gt;</span>
<span class="quote">&gt;&gt; +             src = &amp;per_cpu(steal_time, cpu);</span>
<span class="quote">&gt;&gt; +             state = READ_ONCE(src-&gt;preempted);</span>
<span class="quote">&gt;&gt; +             if ((state &amp; KVM_VCPU_PREEMPTED)) {</span>
<span class="quote">&gt;&gt; +                     if (try_cmpxchg(&amp;src-&gt;preempted, &amp;state,</span>
<span class="quote">&gt;&gt; +                             state | KVM_VCPU_SHOULD_FLUSH))</span>
<span class="quote">&gt;&gt; +                             __cpumask_clear_cpu(cpu, flushmask);</span>
<span class="quote">&gt;&gt; +             }</span>
<span class="quote">&gt;&gt; +     }</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +     native_flush_tlb_others(flushmask, info);</span>
<span class="quote">&gt;&gt; +}</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=104371">Wanpeng Li</a> - Nov. 13, 2017, 8:26 a.m.</div>
<pre class="content">
2017-11-13 16:04 GMT+08:00 Peter Zijlstra &lt;peterz@infradead.org&gt;:
<span class="quote">&gt; On Sun, Nov 12, 2017 at 04:33:24PM -0800, Wanpeng Li wrote:</span>
<span class="quote">&gt;&gt; +static void kvm_flush_tlb_others(const struct cpumask *cpumask,</span>
<span class="quote">&gt;&gt; +                     const struct flush_tlb_info *info)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +     u8 state;</span>
<span class="quote">&gt;&gt; +     int cpu;</span>
<span class="quote">&gt;&gt; +     struct kvm_steal_time *src;</span>
<span class="quote">&gt;&gt; +     struct cpumask *flushmask = this_cpu_cpumask_var_ptr(__pv_tlb_mask);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +     if (unlikely(!flushmask))</span>
<span class="quote">&gt;&gt; +             return;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +     cpumask_copy(flushmask, cpumask);</span>
<span class="quote">&gt;&gt; +     /*</span>
<span class="quote">&gt;&gt; +      * We have to call flush only on online vCPUs. And</span>
<span class="quote">&gt;&gt; +      * queue flush_on_enter for pre-empted vCPUs</span>
<span class="quote">&gt;&gt; +      */</span>
<span class="quote">&gt;&gt; +     for_each_cpu(cpu, cpumask) {</span>
<span class="quote">&gt;&gt; +             src = &amp;per_cpu(steal_time, cpu);</span>
<span class="quote">&gt;&gt; +             state = READ_ONCE(src-&gt;preempted);</span>
<span class="quote">&gt;&gt; +             if ((state &amp; KVM_VCPU_PREEMPTED)) {</span>
<span class="quote">&gt;&gt; +                     if (try_cmpxchg(&amp;src-&gt;preempted, &amp;state,</span>
<span class="quote">&gt;&gt; +                             state | KVM_VCPU_SHOULD_FLUSH))</span>
<span class="quote">&gt;&gt; +                             __cpumask_clear_cpu(cpu, flushmask);</span>
<span class="quote">&gt;&gt; +             }</span>
<span class="quote">&gt;&gt; +     }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; So if at this point a vCPU gets preempted we&#39;ll still spin-wait for it,</span>
<span class="quote">&gt; which is sub-optimal.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I think we can come up with something to get around that &#39;problem&#39; if</span>
<span class="quote">&gt; indeed it is a problem. But we can easily do that as follow up patches.</span>
<span class="quote">&gt; Just let me know if you think its worth spending more time on.</span>

You can post your idea, it is always smart. :) Then we can evaluate
the complexity and gains.

Regards,
Wanpeng Li
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - Nov. 13, 2017, 10:46 a.m.</div>
<pre class="content">
On Mon, Nov 13, 2017 at 04:26:57PM +0800, Wanpeng Li wrote:
<span class="quote">&gt; 2017-11-13 16:04 GMT+08:00 Peter Zijlstra &lt;peterz@infradead.org&gt;:</span>
<span class="quote">
&gt; &gt; So if at this point a vCPU gets preempted we&#39;ll still spin-wait for it,</span>
<span class="quote">&gt; &gt; which is sub-optimal.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; I think we can come up with something to get around that &#39;problem&#39; if</span>
<span class="quote">&gt; &gt; indeed it is a problem. But we can easily do that as follow up patches.</span>
<span class="quote">&gt; &gt; Just let me know if you think its worth spending more time on.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; You can post your idea, it is always smart. :) Then we can evaluate</span>
<span class="quote">&gt; the complexity and gains.</span>

I&#39;m not sure I have a fully baked idea just yet, but the general idea
would be something like:

 - switch (back) to a dedicated TLB invalidate IPI

 - introduce KVM_VCPU_IPI_PENDING

 - change flush_tlb_others() into something like:

   for_each_cpu(cpu, flushmask) {
	 src = &amp;per_cpu(steal_time, cpu);
	 state = READ_ONCE(src-&gt;preempted);
	 do {
		 if (state &amp; KVM_VCPU_PREEMPTED) {
			 if (try_cmpxchg(&amp;src-&gt;preempted, &amp;state,
						 state | KVM_VCPU_SHOULD_FLUSH)) {
				 __cpumask_clear_cpu(cpu, flushmask);
				 break;
			 }
		 }
	 } while (!try_cmpxchg(&amp;src-&gt;preempted, &amp;state,
				 state | KVM_VCPU_IPI_PENDING));
   }

   apic-&gt;send_IPI_mask(flushmask, CALL_TLB_VECTOR);

   for_each_cpu(cpu, flushmask) {
	 src = &amp;per_cpu(steal_time, cpu);
	 smp_cond_load_acquire(&amp;src-&gt;preempted, !(VAL &amp; KVM_VCPU_IPI_PENDING);
   }


 - have the TLB invalidate handler do something like:

   state = READ_ONCE(src-&gt;preempted);
   if (!(state &amp; KVM_VCPU_IPI_PENDING))
	   return;

   local_flush_tlb();

   do {
   } while (!try_cmpxchg(&amp;src-&gt;preempted, &amp;state,
			 state &amp; ~KVM_VCPU_IPI_PENDING));

 - then at VMEXIT time do something like:

   state = READ_ONCE(src-&gt;preempted);
   do {
	if (!(state &amp; KVM_VCPU_IPI_PENDING))
		break;
   } while (!try_cmpxchg(&amp;src-&gt;preempted, state,
			 (state &amp; ~KVM_VCPU_IPI_PENDING) |
			 KVM_VCPU_SHOULD_FLUSH));

   and clear any possible pending TLB_VECTOR in the guest state to avoid
   raising that IPI spuriously on enter again.


This way the preemption will clear the IPI_PENDING and the
flush_others() wait loop will terminate.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - Nov. 13, 2017, 1:02 p.m.</div>
<pre class="content">
On Mon, Nov 13, 2017 at 11:46:34AM +0100, Peter Zijlstra wrote:
<span class="quote">&gt; On Mon, Nov 13, 2017 at 04:26:57PM +0800, Wanpeng Li wrote:</span>
<span class="quote">&gt; &gt; 2017-11-13 16:04 GMT+08:00 Peter Zijlstra &lt;peterz@infradead.org&gt;:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; &gt; So if at this point a vCPU gets preempted we&#39;ll still spin-wait for it,</span>
<span class="quote">&gt; &gt; &gt; which is sub-optimal.</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; I think we can come up with something to get around that &#39;problem&#39; if</span>
<span class="quote">&gt; &gt; &gt; indeed it is a problem. But we can easily do that as follow up patches.</span>
<span class="quote">&gt; &gt; &gt; Just let me know if you think its worth spending more time on.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; You can post your idea, it is always smart. :) Then we can evaluate</span>
<span class="quote">&gt; &gt; the complexity and gains.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I&#39;m not sure I have a fully baked idea just yet, but the general idea</span>
<span class="quote">&gt; would be something like:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  - switch (back) to a dedicated TLB invalidate IPI</span>

Just for PV that is; the !PV code can continue doing what it does today.
<span class="quote">
&gt;  - introduce KVM_VCPU_IPI_PENDING</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  - change flush_tlb_others() into something like:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;    for_each_cpu(cpu, flushmask) {</span>
<span class="quote">&gt; 	 src = &amp;per_cpu(steal_time, cpu);</span>
<span class="quote">&gt; 	 state = READ_ONCE(src-&gt;preempted);</span>
<span class="quote">&gt; 	 do {</span>
<span class="quote">&gt; 		 if (state &amp; KVM_VCPU_PREEMPTED) {</span>
<span class="quote">&gt; 			 if (try_cmpxchg(&amp;src-&gt;preempted, &amp;state,</span>
<span class="quote">&gt; 						 state | KVM_VCPU_SHOULD_FLUSH)) {</span>
<span class="quote">&gt; 				 __cpumask_clear_cpu(cpu, flushmask);</span>
<span class="quote">&gt; 				 break;</span>
<span class="quote">&gt; 			 }</span>
<span class="quote">&gt; 		 }</span>
<span class="quote">&gt; 	 } while (!try_cmpxchg(&amp;src-&gt;preempted, &amp;state,</span>
<span class="quote">&gt; 				 state | KVM_VCPU_IPI_PENDING));</span>

That can be written like:

	do {
		if (state &amp; KVM_VCPU_PREEMPTED)
			new_state = state | KVM_VCPU_SHOULD_FLUSH;
		else
			new_state = state | KVM_VCPU_IPI_PENDING;
	} while (!try_cmpxchg(&amp;src-&gt;preempted, state, new_state);

	if (new_state &amp; KVM_VCPU_IPI_PENDING)
		__cpumask_clear_cpu(cpu, flushmask);
<span class="quote">
&gt;    }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;    apic-&gt;send_IPI_mask(flushmask, CALL_TLB_VECTOR);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;    for_each_cpu(cpu, flushmask) {</span>
<span class="quote">&gt; 	 src = &amp;per_cpu(steal_time, cpu);</span>

	/*
	 * The ACQUIRE pairs with the cmpxchg clearing IPI_PENDING,
	 * which is either the TLB IPI handler, or the VMEXIT path.
	 * It ensure that the invalidate happens-before.
	 */
<span class="quote">&gt; 	 smp_cond_load_acquire(&amp;src-&gt;preempted, !(VAL &amp; KVM_VCPU_IPI_PENDING);</span>
<span class="quote">&gt;    }</span>

And here we wait for completion of the invalidate; but because of the
VMEXIT change below, this will never stall on a !running vCPU.

Note that PLE will not help (much) here, without this extra IPI_PENDING
state and the VMEXIT transferring it to SHOULD_FLUSH this vCPU&#39;s progress
will be held up until all vCPU&#39;s you&#39;ve IPI&#39;d will have ran the IPI
handler, which in the worst case is still a very long time.
<span class="quote">
&gt;  - have the TLB invalidate handler do something like:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;    state = READ_ONCE(src-&gt;preempted);</span>
<span class="quote">&gt;    if (!(state &amp; KVM_VCPU_IPI_PENDING))</span>
<span class="quote">&gt; 	   return;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;    local_flush_tlb();</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;    do {</span>
<span class="quote">&gt;    } while (!try_cmpxchg(&amp;src-&gt;preempted, &amp;state,</span>
<span class="quote">&gt; 			 state &amp; ~KVM_VCPU_IPI_PENDING));</span>

That needs to be:

	/*
	 * Clear KVM_VCPU_IPI_PENDING to &#39;complete&#39; flush_tlb_others().
	 */
	do {
		/*
		 * VMEXIT could have cleared this for us, in which case
		 * we&#39;re done.
		 */
		if (!(state &amp; KVM_VCPU_IPI_PENDING))
			return;

	} while (!try_cmpxchg(&amp;src-&gt;preempted, state,
				state &amp; ~KVM_VCPU_IPI_PENDING));
<span class="quote">
&gt;  - then at VMEXIT time do something like:</span>
<span class="quote">&gt; </span>
	/*
	 * If we have IPI_PENDING set at VMEXIT time, transfer it to
	 * SHOULD_FLUSH. Clearing IPI_PENDING here allows the
	 * flush_others() vCPU to continue while the SHOULD_FLUSH
	 * guarantees this vCPU will flush TLBs before it continues
	 * execution.
	 */
<span class="quote">
&gt;    state = READ_ONCE(src-&gt;preempted);</span>
<span class="quote">&gt;    do {</span>
<span class="quote">&gt; 	if (!(state &amp; KVM_VCPU_IPI_PENDING))</span>
<span class="quote">&gt; 		break;</span>
<span class="quote">&gt;    } while (!try_cmpxchg(&amp;src-&gt;preempted, state,</span>
<span class="quote">&gt; 			 (state &amp; ~KVM_VCPU_IPI_PENDING) |</span>
<span class="quote">&gt; 			 KVM_VCPU_SHOULD_FLUSH));</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;    and clear any possible pending TLB_VECTOR in the guest state to avoid</span>
<span class="quote">&gt;    raising that IPI spuriously on enter again.</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=104371">Wanpeng Li</a> - Nov. 14, 2017, 6:10 a.m.</div>
<pre class="content">
2017-11-13 21:02 GMT+08:00 Peter Zijlstra &lt;peterz@infradead.org&gt;:
<span class="quote">&gt; On Mon, Nov 13, 2017 at 11:46:34AM +0100, Peter Zijlstra wrote:</span>
<span class="quote">&gt;&gt; On Mon, Nov 13, 2017 at 04:26:57PM +0800, Wanpeng Li wrote:</span>
<span class="quote">&gt;&gt; &gt; 2017-11-13 16:04 GMT+08:00 Peter Zijlstra &lt;peterz@infradead.org&gt;:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; &gt; &gt; So if at this point a vCPU gets preempted we&#39;ll still spin-wait for it,</span>
<span class="quote">&gt;&gt; &gt; &gt; which is sub-optimal.</span>
<span class="quote">&gt;&gt; &gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; &gt; I think we can come up with something to get around that &#39;problem&#39; if</span>
<span class="quote">&gt;&gt; &gt; &gt; indeed it is a problem. But we can easily do that as follow up patches.</span>
<span class="quote">&gt;&gt; &gt; &gt; Just let me know if you think its worth spending more time on.</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; You can post your idea, it is always smart. :) Then we can evaluate</span>
<span class="quote">&gt;&gt; &gt; the complexity and gains.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; I&#39;m not sure I have a fully baked idea just yet, but the general idea</span>
<span class="quote">&gt;&gt; would be something like:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;  - switch (back) to a dedicated TLB invalidate IPI</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Just for PV that is; the !PV code can continue doing what it does today.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt;  - introduce KVM_VCPU_IPI_PENDING</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;  - change flush_tlb_others() into something like:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;    for_each_cpu(cpu, flushmask) {</span>
<span class="quote">&gt;&gt;        src = &amp;per_cpu(steal_time, cpu);</span>
<span class="quote">&gt;&gt;        state = READ_ONCE(src-&gt;preempted);</span>
<span class="quote">&gt;&gt;        do {</span>
<span class="quote">&gt;&gt;                if (state &amp; KVM_VCPU_PREEMPTED) {</span>
<span class="quote">&gt;&gt;                        if (try_cmpxchg(&amp;src-&gt;preempted, &amp;state,</span>
<span class="quote">&gt;&gt;                                                state | KVM_VCPU_SHOULD_FLUSH)) {</span>
<span class="quote">&gt;&gt;                                __cpumask_clear_cpu(cpu, flushmask);</span>
<span class="quote">&gt;&gt;                                break;</span>
<span class="quote">&gt;&gt;                        }</span>
<span class="quote">&gt;&gt;                }</span>
<span class="quote">&gt;&gt;        } while (!try_cmpxchg(&amp;src-&gt;preempted, &amp;state,</span>
<span class="quote">&gt;&gt;                                state | KVM_VCPU_IPI_PENDING));</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; That can be written like:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         do {</span>
<span class="quote">&gt;                 if (state &amp; KVM_VCPU_PREEMPTED)</span>
<span class="quote">&gt;                         new_state = state | KVM_VCPU_SHOULD_FLUSH;</span>
<span class="quote">&gt;                 else</span>
<span class="quote">&gt;                         new_state = state | KVM_VCPU_IPI_PENDING;</span>
<span class="quote">&gt;         } while (!try_cmpxchg(&amp;src-&gt;preempted, state, new_state);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         if (new_state &amp; KVM_VCPU_IPI_PENDING)</span>

Should be new_state &amp; KVM_VCPU_SHOULD_FLUSH I think.

Regards,
Wanpeng Li
<span class="quote">
&gt;                 __cpumask_clear_cpu(cpu, flushmask);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt;    }</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;    apic-&gt;send_IPI_mask(flushmask, CALL_TLB_VECTOR);</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;    for_each_cpu(cpu, flushmask) {</span>
<span class="quote">&gt;&gt;        src = &amp;per_cpu(steal_time, cpu);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         /*</span>
<span class="quote">&gt;          * The ACQUIRE pairs with the cmpxchg clearing IPI_PENDING,</span>
<span class="quote">&gt;          * which is either the TLB IPI handler, or the VMEXIT path.</span>
<span class="quote">&gt;          * It ensure that the invalidate happens-before.</span>
<span class="quote">&gt;          */</span>
<span class="quote">&gt;&gt;        smp_cond_load_acquire(&amp;src-&gt;preempted, !(VAL &amp; KVM_VCPU_IPI_PENDING);</span>
<span class="quote">&gt;&gt;    }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; And here we wait for completion of the invalidate; but because of the</span>
<span class="quote">&gt; VMEXIT change below, this will never stall on a !running vCPU.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Note that PLE will not help (much) here, without this extra IPI_PENDING</span>
<span class="quote">&gt; state and the VMEXIT transferring it to SHOULD_FLUSH this vCPU&#39;s progress</span>
<span class="quote">&gt; will be held up until all vCPU&#39;s you&#39;ve IPI&#39;d will have ran the IPI</span>
<span class="quote">&gt; handler, which in the worst case is still a very long time.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt;  - have the TLB invalidate handler do something like:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;    state = READ_ONCE(src-&gt;preempted);</span>
<span class="quote">&gt;&gt;    if (!(state &amp; KVM_VCPU_IPI_PENDING))</span>
<span class="quote">&gt;&gt;          return;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;    local_flush_tlb();</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;    do {</span>
<span class="quote">&gt;&gt;    } while (!try_cmpxchg(&amp;src-&gt;preempted, &amp;state,</span>
<span class="quote">&gt;&gt;                        state &amp; ~KVM_VCPU_IPI_PENDING));</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; That needs to be:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         /*</span>
<span class="quote">&gt;          * Clear KVM_VCPU_IPI_PENDING to &#39;complete&#39; flush_tlb_others().</span>
<span class="quote">&gt;          */</span>
<span class="quote">&gt;         do {</span>
<span class="quote">&gt;                 /*</span>
<span class="quote">&gt;                  * VMEXIT could have cleared this for us, in which case</span>
<span class="quote">&gt;                  * we&#39;re done.</span>
<span class="quote">&gt;                  */</span>
<span class="quote">&gt;                 if (!(state &amp; KVM_VCPU_IPI_PENDING))</span>
<span class="quote">&gt;                         return;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         } while (!try_cmpxchg(&amp;src-&gt;preempted, state,</span>
<span class="quote">&gt;                                 state &amp; ~KVM_VCPU_IPI_PENDING));</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt;  - then at VMEXIT time do something like:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;         /*</span>
<span class="quote">&gt;          * If we have IPI_PENDING set at VMEXIT time, transfer it to</span>
<span class="quote">&gt;          * SHOULD_FLUSH. Clearing IPI_PENDING here allows the</span>
<span class="quote">&gt;          * flush_others() vCPU to continue while the SHOULD_FLUSH</span>
<span class="quote">&gt;          * guarantees this vCPU will flush TLBs before it continues</span>
<span class="quote">&gt;          * execution.</span>
<span class="quote">&gt;          */</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt;    state = READ_ONCE(src-&gt;preempted);</span>
<span class="quote">&gt;&gt;    do {</span>
<span class="quote">&gt;&gt;       if (!(state &amp; KVM_VCPU_IPI_PENDING))</span>
<span class="quote">&gt;&gt;               break;</span>
<span class="quote">&gt;&gt;    } while (!try_cmpxchg(&amp;src-&gt;preempted, state,</span>
<span class="quote">&gt;&gt;                        (state &amp; ~KVM_VCPU_IPI_PENDING) |</span>
<span class="quote">&gt;&gt;                        KVM_VCPU_SHOULD_FLUSH));</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;    and clear any possible pending TLB_VECTOR in the guest state to avoid</span>
<span class="quote">&gt;&gt;    raising that IPI spuriously on enter again.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=104371">Wanpeng Li</a> - Nov. 14, 2017, 6:28 a.m.</div>
<pre class="content">
2017-11-13 18:46 GMT+08:00 Peter Zijlstra &lt;peterz@infradead.org&gt;:
<span class="quote">&gt; On Mon, Nov 13, 2017 at 04:26:57PM +0800, Wanpeng Li wrote:</span>
<span class="quote">&gt;&gt; 2017-11-13 16:04 GMT+08:00 Peter Zijlstra &lt;peterz@infradead.org&gt;:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; &gt; So if at this point a vCPU gets preempted we&#39;ll still spin-wait for it,</span>
<span class="quote">&gt;&gt; &gt; which is sub-optimal.</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; I think we can come up with something to get around that &#39;problem&#39; if</span>
<span class="quote">&gt;&gt; &gt; indeed it is a problem. But we can easily do that as follow up patches.</span>
<span class="quote">&gt;&gt; &gt; Just let me know if you think its worth spending more time on.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; You can post your idea, it is always smart. :) Then we can evaluate</span>
<span class="quote">&gt;&gt; the complexity and gains.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I&#39;m not sure I have a fully baked idea just yet, but the general idea</span>
<span class="quote">&gt; would be something like:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  - switch (back) to a dedicated TLB invalidate IPI</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  - introduce KVM_VCPU_IPI_PENDING</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  - change flush_tlb_others() into something like:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;    for_each_cpu(cpu, flushmask) {</span>
<span class="quote">&gt;          src = &amp;per_cpu(steal_time, cpu);</span>
<span class="quote">&gt;          state = READ_ONCE(src-&gt;preempted);</span>
<span class="quote">&gt;          do {</span>
<span class="quote">&gt;                  if (state &amp; KVM_VCPU_PREEMPTED) {</span>
<span class="quote">&gt;                          if (try_cmpxchg(&amp;src-&gt;preempted, &amp;state,</span>
<span class="quote">&gt;                                                  state | KVM_VCPU_SHOULD_FLUSH)) {</span>
<span class="quote">&gt;                                  __cpumask_clear_cpu(cpu, flushmask);</span>
<span class="quote">&gt;                                  break;</span>
<span class="quote">&gt;                          }</span>
<span class="quote">&gt;                  }</span>
<span class="quote">&gt;          } while (!try_cmpxchg(&amp;src-&gt;preempted, &amp;state,</span>
<span class="quote">&gt;                                  state | KVM_VCPU_IPI_PENDING));</span>
<span class="quote">&gt;    }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;    apic-&gt;send_IPI_mask(flushmask, CALL_TLB_VECTOR);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;    for_each_cpu(cpu, flushmask) {</span>
<span class="quote">&gt;          src = &amp;per_cpu(steal_time, cpu);</span>
<span class="quote">&gt;          smp_cond_load_acquire(&amp;src-&gt;preempted, !(VAL &amp; KVM_VCPU_IPI_PENDING);</span>
<span class="quote">&gt;    }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  - have the TLB invalidate handler do something like:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;    state = READ_ONCE(src-&gt;preempted);</span>
<span class="quote">&gt;    if (!(state &amp; KVM_VCPU_IPI_PENDING))</span>
<span class="quote">&gt;            return;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;    local_flush_tlb();</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;    do {</span>
<span class="quote">&gt;    } while (!try_cmpxchg(&amp;src-&gt;preempted, &amp;state,</span>
<span class="quote">&gt;                          state &amp; ~KVM_VCPU_IPI_PENDING));</span>

There are a lot of cases handled by flush_tlb_func_remote() -&gt;
flush_tlb_function_common(), so I&#39;m afraid to have hole.

Regards,
Wanpeng Li
<span class="quote">
&gt;</span>
<span class="quote">&gt;  - then at VMEXIT time do something like:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;    state = READ_ONCE(src-&gt;preempted);</span>
<span class="quote">&gt;    do {</span>
<span class="quote">&gt;         if (!(state &amp; KVM_VCPU_IPI_PENDING))</span>
<span class="quote">&gt;                 break;</span>
<span class="quote">&gt;    } while (!try_cmpxchg(&amp;src-&gt;preempted, state,</span>
<span class="quote">&gt;                          (state &amp; ~KVM_VCPU_IPI_PENDING) |</span>
<span class="quote">&gt;                          KVM_VCPU_SHOULD_FLUSH));</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;    and clear any possible pending TLB_VECTOR in the guest state to avoid</span>
<span class="quote">&gt;    raising that IPI spuriously on enter again.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; This way the preemption will clear the IPI_PENDING and the</span>
<span class="quote">&gt; flush_others() wait loop will terminate.</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - Nov. 14, 2017, 7:35 a.m.</div>
<pre class="content">
On Tue, Nov 14, 2017 at 02:10:16PM +0800, Wanpeng Li wrote:
<span class="quote">&gt; 2017-11-13 21:02 GMT+08:00 Peter Zijlstra &lt;peterz@infradead.org&gt;:</span>
<span class="quote">&gt; &gt; That can be written like:</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;         do {</span>
<span class="quote">&gt; &gt;                 if (state &amp; KVM_VCPU_PREEMPTED)</span>
<span class="quote">&gt; &gt;                         new_state = state | KVM_VCPU_SHOULD_FLUSH;</span>
<span class="quote">&gt; &gt;                 else</span>
<span class="quote">&gt; &gt;                         new_state = state | KVM_VCPU_IPI_PENDING;</span>
<span class="quote">&gt; &gt;         } while (!try_cmpxchg(&amp;src-&gt;preempted, state, new_state);</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;         if (new_state &amp; KVM_VCPU_IPI_PENDING)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Should be new_state &amp; KVM_VCPU_SHOULD_FLUSH I think.</span>

Quite so indeed.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - Nov. 14, 2017, 7:56 a.m.</div>
<pre class="content">
On Tue, Nov 14, 2017 at 02:28:56PM +0800, Wanpeng Li wrote:
<span class="quote">&gt; &gt;  - have the TLB invalidate handler do something like:</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;    state = READ_ONCE(src-&gt;preempted);</span>
<span class="quote">&gt; &gt;    if (!(state &amp; KVM_VCPU_IPI_PENDING))</span>
<span class="quote">&gt; &gt;            return;</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;    local_flush_tlb();</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;    do {</span>
<span class="quote">&gt; &gt;    } while (!try_cmpxchg(&amp;src-&gt;preempted, &amp;state,</span>
<span class="quote">&gt; &gt;                          state &amp; ~KVM_VCPU_IPI_PENDING));</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; There are a lot of cases handled by flush_tlb_func_remote() -&gt;</span>
<span class="quote">&gt; flush_tlb_function_common(), so I&#39;m afraid to have hole.</span>

Sure, just fix the handler to do what must be done. The above was merely
a sketch. The important part is to only clear IPI_PENDING after we do
the actual flushing, since the caller is waiting for that bit.


So flush_tlb_others() has two callers:

 - arch_tlbbatch_flush() -- info::end = TLB_FLUSH_ALL
 - flush_tlb_mm_range()  -- info::mm = mm

native_flush_tlb_others() does smp_call_function_many(
.func=flush_tlb_func_remote) which in turn calls flush_tlb_func_common(
.local=false, .reason=TLB_REMOTE_SHOOTDOWN).


So something like:

	struct flush_tlb_info info = {
		.start = 0,
		.end = TLB_FLUSH_ALL,
	};

	flush_tlb_func_common(&amp;info, false, TLB_REMOTE_SHOOTDOWN);

should work for the new IPI. It &#39;upgrades&#39; all ranges to full flushes,
but meh.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/Documentation/virtual/kvm/cpuid.txt b/Documentation/virtual/kvm/cpuid.txt</span>
<span class="p_header">index 117066a..9693fcc 100644</span>
<span class="p_header">--- a/Documentation/virtual/kvm/cpuid.txt</span>
<span class="p_header">+++ b/Documentation/virtual/kvm/cpuid.txt</span>
<span class="p_chunk">@@ -60,6 +60,10 @@</span> <span class="p_context"> KVM_FEATURE_PV_DEDICATED           ||     8 || guest checks this feature bit</span>
                                    ||       || mizations such as usage of
                                    ||       || qspinlocks.
 ------------------------------------------------------------------------------
<span class="p_add">+KVM_FEATURE_PV_TLB_FLUSH           ||     9 || guest checks this feature bit</span>
<span class="p_add">+                                   ||       || before enabling paravirtualized</span>
<span class="p_add">+                                   ||       || tlb flush.</span>
<span class="p_add">+------------------------------------------------------------------------------</span>
 KVM_FEATURE_CLOCKSOURCE_STABLE_BIT ||    24 || host will warn if no guest-side
                                    ||       || per-cpu warps are expected in
                                    ||       || kvmclock.
<span class="p_header">diff --git a/arch/x86/include/uapi/asm/kvm_para.h b/arch/x86/include/uapi/asm/kvm_para.h</span>
<span class="p_header">index 6d66556..e267d83 100644</span>
<span class="p_header">--- a/arch/x86/include/uapi/asm/kvm_para.h</span>
<span class="p_header">+++ b/arch/x86/include/uapi/asm/kvm_para.h</span>
<span class="p_chunk">@@ -26,6 +26,7 @@</span> <span class="p_context"></span>
 #define KVM_FEATURE_PV_EOI		6
 #define KVM_FEATURE_PV_UNHALT		7
 #define KVM_FEATURE_PV_DEDICATED	8
<span class="p_add">+#define KVM_FEATURE_PV_TLB_FLUSH	9</span>
 
 /* The last 8 bits are used to indicate how to interpret the flags field
  * in pvclock structure. If no bits are set, all flags are ignored.
<span class="p_chunk">@@ -54,6 +55,7 @@</span> <span class="p_context"> struct kvm_steal_time {</span>
 
 #define KVM_VCPU_NOT_PREEMPTED      (0 &lt;&lt; 0)
 #define KVM_VCPU_PREEMPTED          (1 &lt;&lt; 0)
<span class="p_add">+#define KVM_VCPU_SHOULD_FLUSH       (1 &lt;&lt; 1)</span>
 
 #define KVM_CLOCK_PAIRING_WALLCLOCK 0
 struct kvm_clock_pairing {
<span class="p_header">diff --git a/arch/x86/kernel/kvm.c b/arch/x86/kernel/kvm.c</span>
<span class="p_header">index 66ed3bc..78794c1 100644</span>
<span class="p_header">--- a/arch/x86/kernel/kvm.c</span>
<span class="p_header">+++ b/arch/x86/kernel/kvm.c</span>
<span class="p_chunk">@@ -465,9 +465,40 @@</span> <span class="p_context"> static void __init kvm_apf_trap_init(void)</span>
 	update_intr_gate(X86_TRAP_PF, async_page_fault);
 }
 
<span class="p_add">+static DEFINE_PER_CPU(cpumask_var_t, __pv_tlb_mask);</span>
<span class="p_add">+</span>
<span class="p_add">+static void kvm_flush_tlb_others(const struct cpumask *cpumask,</span>
<span class="p_add">+			const struct flush_tlb_info *info)</span>
<span class="p_add">+{</span>
<span class="p_add">+	u8 state;</span>
<span class="p_add">+	int cpu;</span>
<span class="p_add">+	struct kvm_steal_time *src;</span>
<span class="p_add">+	struct cpumask *flushmask = this_cpu_cpumask_var_ptr(__pv_tlb_mask);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (unlikely(!flushmask))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	cpumask_copy(flushmask, cpumask);</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We have to call flush only on online vCPUs. And</span>
<span class="p_add">+	 * queue flush_on_enter for pre-empted vCPUs</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	for_each_cpu(cpu, cpumask) {</span>
<span class="p_add">+		src = &amp;per_cpu(steal_time, cpu);</span>
<span class="p_add">+		state = READ_ONCE(src-&gt;preempted);</span>
<span class="p_add">+		if ((state &amp; KVM_VCPU_PREEMPTED)) {</span>
<span class="p_add">+			if (try_cmpxchg(&amp;src-&gt;preempted, &amp;state,</span>
<span class="p_add">+				state | KVM_VCPU_SHOULD_FLUSH))</span>
<span class="p_add">+				__cpumask_clear_cpu(cpu, flushmask);</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	native_flush_tlb_others(flushmask, info);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 void __init kvm_guest_init(void)
 {
<span class="p_del">-	int i;</span>
<span class="p_add">+	int i, cpu;</span>
 
 	if (!kvm_para_available())
 		return;
<span class="p_chunk">@@ -484,6 +515,15 @@</span> <span class="p_context"> void __init kvm_guest_init(void)</span>
 		pv_time_ops.steal_clock = kvm_steal_clock;
 	}
 
<span class="p_add">+	if (kvm_para_has_feature(KVM_FEATURE_PV_TLB_FLUSH) &amp;&amp;</span>
<span class="p_add">+		!kvm_para_has_feature(KVM_FEATURE_PV_DEDICATED)) {</span>
<span class="p_add">+		for_each_possible_cpu(cpu) {</span>
<span class="p_add">+			zalloc_cpumask_var_node(per_cpu_ptr(&amp;__pv_tlb_mask, cpu),</span>
<span class="p_add">+					GFP_KERNEL, cpu_to_node(cpu));</span>
<span class="p_add">+		}</span>
<span class="p_add">+		pv_mmu_ops.flush_tlb_others = kvm_flush_tlb_others;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	if (kvm_para_has_feature(KVM_FEATURE_PV_EOI))
 		apic_set_eoi_write(kvm_guest_apic_eoi_write);
 

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



