
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[RFC] Add support for eXclusive Page Frame Ownership (XPFO) - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [RFC] Add support for eXclusive Page Frame Ownership (XPFO)</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=153531">Juerg Haefliger</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Feb. 26, 2016, 2:21 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1456496467-14247-1-git-send-email-juerg.haefliger@hpe.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/8437451/mbox/"
   >mbox</a>
|
   <a href="/patch/8437451/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/8437451/">/patch/8437451/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork2.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork2.web.kernel.org (Postfix) with ESMTP id 7323DC0553
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 26 Feb 2016 14:21:22 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id D9059203AC
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 26 Feb 2016 14:21:20 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id 4F4BE203AA
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 26 Feb 2016 14:21:17 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1754057AbcBZOVO (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Fri, 26 Feb 2016 09:21:14 -0500
Received: from g1t6225.austin.hp.com ([15.73.96.126]:52211 &quot;EHLO
	g1t6225.austin.hp.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1752174AbcBZOVM (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Fri, 26 Feb 2016 09:21:12 -0500
Received: from g2t4622.austin.hp.com (g2t4622.austin.hp.com [15.73.212.79])
	(using TLSv1.2 with cipher DHE-RSA-AES256-GCM-SHA384 (256/256
	bits)) (No client certificate requested)
	by g1t6225.austin.hp.com (Postfix) with ESMTPS id D74417A47
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Fri, 26 Feb 2016 14:21:11 +0000 (UTC)
Received: from smtp1.hp.com (unknown [16.29.138.57])
	(using TLSv1.2 with cipher AES128-SHA256 (128/128 bits))
	(No client certificate requested)
	by g2t4622.austin.hp.com (Postfix) with ESMTPS id 9D9D79C;
	Fri, 26 Feb 2016 14:21:09 +0000 (UTC)
From: Juerg Haefliger &lt;juerg.haefliger@hpe.com&gt;
To: linux-kernel@vger.kernel.org, linux-mm@kvack.org
Cc: vpk@cs.brown.edu, juerg.haefliger@hpe.com
Subject: [RFC PATCH] Add support for eXclusive Page Frame Ownership (XPFO)
Date: Fri, 26 Feb 2016 15:21:07 +0100
Message-Id: &lt;1456496467-14247-1-git-send-email-juerg.haefliger@hpe.com&gt;
X-Mailer: git-send-email 2.1.4
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-6.9 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=153531">Juerg Haefliger</a> - Feb. 26, 2016, 2:21 p.m.</div>
<pre class="content">
This patch adds support for XPFO which protects against &#39;ret2dir&#39; kernel
attacks. The basic idea is to enforce exclusive ownership of page frames
by either the kernel or userland, unless explicitly requested by the
kernel. Whenever a page destined for userland is allocated, it is
unmapped from physmap. When such a page is reclaimed from userland, it is
mapped back to physmap.

Mapping/unmapping from physmap is accomplished by modifying the PTE
permission bits to allow/disallow access to the page.

Additional fields are added to the page struct for XPFO housekeeping.
Specifically a flags field to distinguish user vs. kernel pages, a
reference counter to track physmap map/unmap operations and a lock to
protect the XPFO fields.

Known issues/limitations:
  - Only supported on x86-64.
  - Only supports 4k pages.
  - Adds additional data to the page struct.
  - There are most likely some additional and legitimate uses cases where
    the kernel needs to access userspace. Those need to be identified and
    made XPFO-aware.
  - There&#39;s a performance impact if XPFO is turned on. Per the paper
    referenced below it&#39;s in the 1-3% ballpark. More performance testing
    wouldn&#39;t hurt. What tests to run though?

Reference paper by the original patch authors:
  http://www.cs.columbia.edu/~vpk/papers/ret2dir.sec14.pdf

Suggested-by: Vasileios P. Kemerlis &lt;vpk@cs.brown.edu&gt;
<span class="signed-off-by">Signed-off-by: Juerg Haefliger &lt;juerg.haefliger@hpe.com&gt;</span>
---
 arch/x86/Kconfig         |   2 +-
 arch/x86/Kconfig.debug   |  17 +++++
 arch/x86/mm/Makefile     |   2 +
 arch/x86/mm/init.c       |   3 +-
 arch/x86/mm/xpfo.c       | 176 +++++++++++++++++++++++++++++++++++++++++++++++
 block/blk-map.c          |   7 +-
 include/linux/highmem.h  |  23 +++++--
 include/linux/mm_types.h |   4 ++
 include/linux/xpfo.h     |  88 ++++++++++++++++++++++++
 lib/swiotlb.c            |   3 +-
 mm/page_alloc.c          |   7 +-
 11 files changed, 323 insertions(+), 9 deletions(-)
 create mode 100644 arch/x86/mm/xpfo.c
 create mode 100644 include/linux/xpfo.h
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=130411">Laura Abbott</a> - March 1, 2016, 1:31 a.m.</div>
<pre class="content">
On 02/26/2016 06:21 AM, Juerg Haefliger wrote:
<span class="quote">&gt; This patch adds support for XPFO which protects against &#39;ret2dir&#39; kernel</span>
<span class="quote">&gt; attacks. The basic idea is to enforce exclusive ownership of page frames</span>
<span class="quote">&gt; by either the kernel or userland, unless explicitly requested by the</span>
<span class="quote">&gt; kernel. Whenever a page destined for userland is allocated, it is</span>
<span class="quote">&gt; unmapped from physmap. When such a page is reclaimed from userland, it is</span>
<span class="quote">&gt; mapped back to physmap.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Mapping/unmapping from physmap is accomplished by modifying the PTE</span>
<span class="quote">&gt; permission bits to allow/disallow access to the page.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Additional fields are added to the page struct for XPFO housekeeping.</span>
<span class="quote">&gt; Specifically a flags field to distinguish user vs. kernel pages, a</span>
<span class="quote">&gt; reference counter to track physmap map/unmap operations and a lock to</span>
<span class="quote">&gt; protect the XPFO fields.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Known issues/limitations:</span>
<span class="quote">&gt;    - Only supported on x86-64.</span>
<span class="quote">&gt;    - Only supports 4k pages.</span>
<span class="quote">&gt;    - Adds additional data to the page struct.</span>
<span class="quote">&gt;    - There are most likely some additional and legitimate uses cases where</span>
<span class="quote">&gt;      the kernel needs to access userspace. Those need to be identified and</span>
<span class="quote">&gt;      made XPFO-aware.</span>
<span class="quote">&gt;    - There&#39;s a performance impact if XPFO is turned on. Per the paper</span>
<span class="quote">&gt;      referenced below it&#39;s in the 1-3% ballpark. More performance testing</span>
<span class="quote">&gt;      wouldn&#39;t hurt. What tests to run though?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Reference paper by the original patch authors:</span>
<span class="quote">&gt;    http://www.cs.columbia.edu/~vpk/papers/ret2dir.sec14.pdf</span>
<span class="quote">&gt;</span>

General note: Make sure to cc the x86 maintainers on the next version of
the patch. I&#39;d also recommend ccing the kernel hardening list (see the wiki
page http://kernsec.org/wiki/index.php/Kernel_Self_Protection_Project for
details)

If you can find a way to break this up into x86 specific vs. generic patches
that would be better. Perhaps move the Kconfig for XPFO to the generic
Kconfig layer and make it depend on ARCH_HAS_XPFO? x86 can then select
ARCH_HAS_XPFO as the last option.

There also isn&#39;t much that&#39;s actually x86 specific here except for
some of the page table manipulation functions and even those can probably
be abstracted away. It would be good to get more of this out of x86 to
let other arches take advantage of it. The arm64 implementation would
look pretty similar if you save the old kernel mapping and restore
it on free.
<span class="quote">
  
&gt; Suggested-by: Vasileios P. Kemerlis &lt;vpk@cs.brown.edu&gt;</span>
<span class="quote">&gt; Signed-off-by: Juerg Haefliger &lt;juerg.haefliger@hpe.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;   arch/x86/Kconfig         |   2 +-</span>
<span class="quote">&gt;   arch/x86/Kconfig.debug   |  17 +++++</span>
<span class="quote">&gt;   arch/x86/mm/Makefile     |   2 +</span>
<span class="quote">&gt;   arch/x86/mm/init.c       |   3 +-</span>
<span class="quote">&gt;   arch/x86/mm/xpfo.c       | 176 +++++++++++++++++++++++++++++++++++++++++++++++</span>
<span class="quote">&gt;   block/blk-map.c          |   7 +-</span>
<span class="quote">&gt;   include/linux/highmem.h  |  23 +++++--</span>
<span class="quote">&gt;   include/linux/mm_types.h |   4 ++</span>
<span class="quote">&gt;   include/linux/xpfo.h     |  88 ++++++++++++++++++++++++</span>
<span class="quote">&gt;   lib/swiotlb.c            |   3 +-</span>
<span class="quote">&gt;   mm/page_alloc.c          |   7 +-</span>
<span class="quote">&gt;   11 files changed, 323 insertions(+), 9 deletions(-)</span>
<span class="quote">&gt;   create mode 100644 arch/x86/mm/xpfo.c</span>
<span class="quote">&gt;   create mode 100644 include/linux/xpfo.h</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig</span>
<span class="quote">&gt; index c46662f..9d32b4a 100644</span>
<span class="quote">&gt; --- a/arch/x86/Kconfig</span>
<span class="quote">&gt; +++ b/arch/x86/Kconfig</span>
<span class="quote">&gt; @@ -1343,7 +1343,7 @@ config ARCH_DMA_ADDR_T_64BIT</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;   config X86_DIRECT_GBPAGES</span>
<span class="quote">&gt;   	def_bool y</span>
<span class="quote">&gt; -	depends on X86_64 &amp;&amp; !DEBUG_PAGEALLOC &amp;&amp; !KMEMCHECK</span>
<span class="quote">&gt; +	depends on X86_64 &amp;&amp; !DEBUG_PAGEALLOC &amp;&amp; !KMEMCHECK &amp;&amp; !XPFO</span>
<span class="quote">&gt;   	---help---</span>
<span class="quote">&gt;   	  Certain kernel features effectively disable kernel</span>
<span class="quote">&gt;   	  linear 1 GB mappings (even if the CPU otherwise</span>
<span class="quote">&gt; diff --git a/arch/x86/Kconfig.debug b/arch/x86/Kconfig.debug</span>
<span class="quote">&gt; index 9b18ed9..1331da5 100644</span>
<span class="quote">&gt; --- a/arch/x86/Kconfig.debug</span>
<span class="quote">&gt; +++ b/arch/x86/Kconfig.debug</span>
<span class="quote">&gt; @@ -5,6 +5,23 @@ config TRACE_IRQFLAGS_SUPPORT</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;   source &quot;lib/Kconfig.debug&quot;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; +config XPFO</span>
<span class="quote">&gt; +	bool &quot;Enable eXclusive Page Frame Ownership (XPFO)&quot;</span>
<span class="quote">&gt; +	default n</span>
<span class="quote">&gt; +	depends on DEBUG_KERNEL</span>
<span class="quote">&gt; +	depends on X86_64</span>
<span class="quote">&gt; +	select DEBUG_TLBFLUSH</span>
<span class="quote">&gt; +	---help---</span>
<span class="quote">&gt; +	  This option offers protection against &#39;ret2dir&#39; (kernel) attacks.</span>
<span class="quote">&gt; +	  When enabled, every time a page frame is allocated to user space, it</span>
<span class="quote">&gt; +	  is unmapped from the direct mapped RAM region in kernel space</span>
<span class="quote">&gt; +	  (physmap). Similarly, whenever page frames are freed/reclaimed, they</span>
<span class="quote">&gt; +	  are mapped back to physmap. Special care is taken to minimize the</span>
<span class="quote">&gt; +	  impact on performance by reducing TLB shootdowns and unnecessary page</span>
<span class="quote">&gt; +	  zero fills.</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	  If in doubt, say &quot;N&quot;.</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;   config X86_VERBOSE_BOOTUP</span>
<span class="quote">&gt;   	bool &quot;Enable verbose x86 bootup info messages&quot;</span>
<span class="quote">&gt;   	default y</span>
<span class="quote">&gt; diff --git a/arch/x86/mm/Makefile b/arch/x86/mm/Makefile</span>
<span class="quote">&gt; index f9d38a4..8bf52b6 100644</span>
<span class="quote">&gt; --- a/arch/x86/mm/Makefile</span>
<span class="quote">&gt; +++ b/arch/x86/mm/Makefile</span>
<span class="quote">&gt; @@ -34,3 +34,5 @@ obj-$(CONFIG_ACPI_NUMA)		+= srat.o</span>
<span class="quote">&gt;   obj-$(CONFIG_NUMA_EMU)		+= numa_emulation.o</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;   obj-$(CONFIG_X86_INTEL_MPX)	+= mpx.o</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +obj-$(CONFIG_XPFO)		+= xpfo.o</span>
<span class="quote">&gt; diff --git a/arch/x86/mm/init.c b/arch/x86/mm/init.c</span>
<span class="quote">&gt; index 493f541..27fc8a6 100644</span>
<span class="quote">&gt; --- a/arch/x86/mm/init.c</span>
<span class="quote">&gt; +++ b/arch/x86/mm/init.c</span>
<span class="quote">&gt; @@ -150,7 +150,8 @@ static int page_size_mask;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;   static void __init probe_page_size_mask(void)</span>
<span class="quote">&gt;   {</span>
<span class="quote">&gt; -#if !defined(CONFIG_DEBUG_PAGEALLOC) &amp;&amp; !defined(CONFIG_KMEMCHECK)</span>
<span class="quote">&gt; +#if !defined(CONFIG_DEBUG_PAGEALLOC) &amp;&amp; !defined(CONFIG_KMEMCHECK) &amp;&amp; \</span>
<span class="quote">&gt; +	!defined(CONFIG_XPFO)</span>
<span class="quote">&gt;   	/*</span>
<span class="quote">&gt;   	 * For CONFIG_DEBUG_PAGEALLOC, identity mapping will use small pages.</span>
<span class="quote">&gt;   	 * This will simplify cpa(), which otherwise needs to support splitting</span>
<span class="quote">&gt; diff --git a/arch/x86/mm/xpfo.c b/arch/x86/mm/xpfo.c</span>
<span class="quote">&gt; new file mode 100644</span>
<span class="quote">&gt; index 0000000..6bc24d3</span>
<span class="quote">&gt; --- /dev/null</span>
<span class="quote">&gt; +++ b/arch/x86/mm/xpfo.c</span>
<span class="quote">&gt; @@ -0,0 +1,176 @@</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * Copyright (C) 2016 Brown University. All rights reserved.</span>
<span class="quote">&gt; + * Copyright (C) 2016 Hewlett Packard Enterprise Development, L.P.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Authors:</span>
<span class="quote">&gt; + *   Vasileios P. Kemerlis &lt;vpk@cs.brown.edu&gt;</span>
<span class="quote">&gt; + *   Juerg Haefliger &lt;juerg.haefliger@hpe.com&gt;</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * This program is free software; you can redistribute it and/or modify it</span>
<span class="quote">&gt; + * under the terms of the GNU General Public License version 2 as published by</span>
<span class="quote">&gt; + * the Free Software Foundation.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#include &lt;linux/mm.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/module.h&gt;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#include &lt;asm/pgtable.h&gt;</span>
<span class="quote">&gt; +#include &lt;asm/tlbflush.h&gt;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define TEST_XPFO_FLAG(flag, page) \</span>
<span class="quote">&gt; +	test_bit(PG_XPFO_##flag, &amp;(page)-&gt;xpfo.flags)</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define SET_XPFO_FLAG(flag, page)			\</span>
<span class="quote">&gt; +	__set_bit(PG_XPFO_##flag, &amp;(page)-&gt;xpfo.flags)</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define CLEAR_XPFO_FLAG(flag, page)			\</span>
<span class="quote">&gt; +	__clear_bit(PG_XPFO_##flag, &amp;(page)-&gt;xpfo.flags)</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define TEST_AND_CLEAR_XPFO_FLAG(flag, page)			\</span>
<span class="quote">&gt; +	__test_and_clear_bit(PG_XPFO_##flag, &amp;(page)-&gt;xpfo.flags)</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * Update a single kernel page table entry</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +static inline void set_kpte(struct page *page, unsigned long kaddr,</span>
<span class="quote">&gt; +			    pgprot_t prot) {</span>
<span class="quote">&gt; +	unsigned int level;</span>
<span class="quote">&gt; +	pte_t *kpte = lookup_address(kaddr, &amp;level);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/* We only support 4k pages for now */</span>
<span class="quote">&gt; +	BUG_ON(!kpte || level != PG_LEVEL_4K);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	set_pte_atomic(kpte, pfn_pte(page_to_pfn(page), canon_pgprot(prot)));</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +inline void xpfo_clear_zap(struct page *page, int order)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	int i;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	for (i = 0; i &lt; (1 &lt;&lt; order); i++)</span>
<span class="quote">&gt; +		CLEAR_XPFO_FLAG(zap, page + i);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +inline int xpfo_test_and_clear_zap(struct page *page)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return TEST_AND_CLEAR_XPFO_FLAG(zap, page);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +inline int xpfo_test_kernel(struct page *page)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return TEST_XPFO_FLAG(kernel, page);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +inline int xpfo_test_user(struct page *page)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return TEST_XPFO_FLAG(user, page);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +void xpfo_alloc_page(struct page *page, int order, gfp_t gfp)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	int i, tlb_shoot = 0;</span>
<span class="quote">&gt; +	unsigned long kaddr;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	for (i = 0; i &lt; (1 &lt;&lt; order); i++)  {</span>
<span class="quote">&gt; +		WARN_ON(TEST_XPFO_FLAG(user_fp, page + i) ||</span>
<span class="quote">&gt; +			TEST_XPFO_FLAG(user, page + i));</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		if (gfp &amp; GFP_HIGHUSER) {</span>

This check doesn&#39;t seem right. If the GFP flags have _any_ in common with
GFP_HIGHUSER it will be marked as a user page so GFP_KERNEL will be marked
as well.
<span class="quote">
&gt; +			/* Initialize the xpfo lock and map counter */</span>
<span class="quote">&gt; +			spin_lock_init(&amp;(page + i)-&gt;xpfo.lock);</span>

This is initializing the spin_lock every time. That&#39;s not really necessary.
<span class="quote">
&gt; +			atomic_set(&amp;(page + i)-&gt;xpfo.mapcount, 0);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			/* Mark it as a user page */</span>
<span class="quote">&gt; +			SET_XPFO_FLAG(user_fp, page + i);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			/*</span>
<span class="quote">&gt; +			 * Shoot the TLB if the page was previously allocated</span>
<span class="quote">&gt; +			 * to kernel space</span>
<span class="quote">&gt; +			 */</span>
<span class="quote">&gt; +			if (TEST_AND_CLEAR_XPFO_FLAG(kernel, page + i))</span>
<span class="quote">&gt; +				tlb_shoot = 1;</span>
<span class="quote">&gt; +		} else {</span>
<span class="quote">&gt; +			/* Mark it as a kernel page */</span>
<span class="quote">&gt; +			SET_XPFO_FLAG(kernel, page + i);</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (tlb_shoot) {</span>
<span class="quote">&gt; +		kaddr = (unsigned long)page_address(page);</span>
<span class="quote">&gt; +		flush_tlb_kernel_range(kaddr, kaddr + (1 &lt;&lt; order) *</span>
<span class="quote">&gt; +				       PAGE_SIZE);</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +void xpfo_free_page(struct page *page, int order)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	int i;</span>
<span class="quote">&gt; +	unsigned long kaddr;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	for (i = 0; i &lt; (1 &lt;&lt; order); i++) {</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		/* The page frame was previously allocated to user space */</span>
<span class="quote">&gt; +		if (TEST_AND_CLEAR_XPFO_FLAG(user, page + i)) {</span>
<span class="quote">&gt; +			kaddr = (unsigned long)page_address(page + i);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			/* Clear the page and mark it accordingly */</span>
<span class="quote">&gt; +			clear_page((void *)kaddr);</span>

Clearing the page isn&#39;t related to XPFO. There&#39;s other work ongoing to
do clearing of the page on free.
<span class="quote">
&gt; +			SET_XPFO_FLAG(zap, page + i);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			/* Map it back to kernel space */</span>
<span class="quote">&gt; +			set_kpte(page + i,  kaddr, __pgprot(__PAGE_KERNEL));</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			/* No TLB update */</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		/* Clear the xpfo fast-path flag */</span>
<span class="quote">&gt; +		CLEAR_XPFO_FLAG(user_fp, page + i);</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +void xpfo_kmap(void *kaddr, struct page *page)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	unsigned long flags;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/* The page is allocated to kernel space, so nothing to do */</span>
<span class="quote">&gt; +	if (TEST_XPFO_FLAG(kernel, page))</span>
<span class="quote">&gt; +		return;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	spin_lock_irqsave(&amp;page-&gt;xpfo.lock, flags);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * The page was previously allocated to user space, so map it back</span>
<span class="quote">&gt; +	 * into the kernel. No TLB update required.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	if ((atomic_inc_return(&amp;page-&gt;xpfo.mapcount) == 1) &amp;&amp;</span>
<span class="quote">&gt; +	    TEST_XPFO_FLAG(user, page))</span>
<span class="quote">&gt; +		set_kpte(page, (unsigned long)kaddr, __pgprot(__PAGE_KERNEL));</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	spin_unlock_irqrestore(&amp;page-&gt;xpfo.lock, flags);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +EXPORT_SYMBOL(xpfo_kmap);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +void xpfo_kunmap(void *kaddr, struct page *page)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	unsigned long flags;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/* The page is allocated to kernel space, so nothing to do */</span>
<span class="quote">&gt; +	if (TEST_XPFO_FLAG(kernel, page))</span>
<span class="quote">&gt; +		return;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	spin_lock_irqsave(&amp;page-&gt;xpfo.lock, flags);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * The page frame is to be allocated back to user space. So unmap it</span>
<span class="quote">&gt; +	 * from the kernel, update the TLB and mark it as a user page.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	if ((atomic_dec_return(&amp;page-&gt;xpfo.mapcount) == 0) &amp;&amp;</span>
<span class="quote">&gt; +	    (TEST_XPFO_FLAG(user_fp, page) || TEST_XPFO_FLAG(user, page))) {</span>
<span class="quote">&gt; +		set_kpte(page, (unsigned long)kaddr, __pgprot(0));</span>
<span class="quote">&gt; +		__flush_tlb_one((unsigned long)kaddr);</span>
<span class="quote">&gt; +		SET_XPFO_FLAG(user, page);</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	spin_unlock_irqrestore(&amp;page-&gt;xpfo.lock, flags);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +EXPORT_SYMBOL(xpfo_kunmap);</span>

I&#39;m confused by the checks in kmap/kunmap here. It looks like once the
page is allocated there is no changing of flags between user and
kernel mode so the checks for if the page is user seem redundant.
<span class="quote">
&gt; diff --git a/block/blk-map.c b/block/blk-map.c</span>
<span class="quote">&gt; index f565e11..b7b8302 100644</span>
<span class="quote">&gt; --- a/block/blk-map.c</span>
<span class="quote">&gt; +++ b/block/blk-map.c</span>
<span class="quote">&gt; @@ -107,7 +107,12 @@ int blk_rq_map_user_iov(struct request_queue *q, struct request *rq,</span>
<span class="quote">&gt;   		prv.iov_len = iov.iov_len;</span>
<span class="quote">&gt;   	}</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -	if (unaligned || (q-&gt;dma_pad_mask &amp; iter-&gt;count) || map_data)</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * juergh: Temporary hack to force the use of a bounce buffer if XPFO</span>
<span class="quote">&gt; +	 * is enabled. Results in an XPFO page fault otherwise.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	if (unaligned || (q-&gt;dma_pad_mask &amp; iter-&gt;count) || map_data ||</span>
<span class="quote">&gt; +	    IS_ENABLED(CONFIG_XPFO))</span>
<span class="quote">&gt;   		bio = bio_copy_user_iov(q, map_data, iter, gfp_mask);</span>
<span class="quote">&gt;   	else</span>
<span class="quote">&gt;   		bio = bio_map_user_iov(q, iter, gfp_mask);</span>
<span class="quote">&gt; diff --git a/include/linux/highmem.h b/include/linux/highmem.h</span>
<span class="quote">&gt; index bb3f329..0ca9130 100644</span>
<span class="quote">&gt; --- a/include/linux/highmem.h</span>
<span class="quote">&gt; +++ b/include/linux/highmem.h</span>
<span class="quote">&gt; @@ -55,24 +55,37 @@ static inline struct page *kmap_to_page(void *addr)</span>
<span class="quote">&gt;   #ifndef ARCH_HAS_KMAP</span>
<span class="quote">&gt;   static inline void *kmap(struct page *page)</span>
<span class="quote">&gt;   {</span>
<span class="quote">&gt; +	void *kaddr;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;   	might_sleep();</span>
<span class="quote">&gt; -	return page_address(page);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	kaddr = page_address(page);</span>
<span class="quote">&gt; +	xpfo_kmap(kaddr, page);</span>
<span class="quote">&gt; +	return kaddr;</span>
<span class="quote">&gt;   }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;   static inline void kunmap(struct page *page)</span>
<span class="quote">&gt;   {</span>
<span class="quote">&gt; +	xpfo_kunmap(page_address(page), page);</span>
<span class="quote">&gt;   }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;   static inline void *kmap_atomic(struct page *page)</span>
<span class="quote">&gt;   {</span>
<span class="quote">&gt; +	void *kaddr;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;   	preempt_disable();</span>
<span class="quote">&gt;   	pagefault_disable();</span>
<span class="quote">&gt; -	return page_address(page);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	kaddr = page_address(page);</span>
<span class="quote">&gt; +	xpfo_kmap(kaddr, page);</span>
<span class="quote">&gt; +	return kaddr;</span>
<span class="quote">&gt;   }</span>
<span class="quote">&gt;   #define kmap_atomic_prot(page, prot)	kmap_atomic(page)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;   static inline void __kunmap_atomic(void *addr)</span>
<span class="quote">&gt;   {</span>
<span class="quote">&gt; +	xpfo_kunmap(addr, virt_to_page(addr));</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;   	pagefault_enable();</span>
<span class="quote">&gt;   	preempt_enable();</span>
<span class="quote">&gt;   }</span>
<span class="quote">&gt; @@ -133,7 +146,8 @@ do {                                                            \</span>
<span class="quote">&gt;   static inline void clear_user_highpage(struct page *page, unsigned long vaddr)</span>
<span class="quote">&gt;   {</span>
<span class="quote">&gt;   	void *addr = kmap_atomic(page);</span>
<span class="quote">&gt; -	clear_user_page(addr, vaddr, page);</span>
<span class="quote">&gt; +	if (!xpfo_test_and_clear_zap(page))</span>
<span class="quote">&gt; +		clear_user_page(addr, vaddr, page);</span>
<span class="quote">&gt;   	kunmap_atomic(addr);</span>
<span class="quote">&gt;   }</span>
<span class="quote">&gt;   #endif</span>
<span class="quote">&gt; @@ -186,7 +200,8 @@ alloc_zeroed_user_highpage_movable(struct vm_area_struct *vma,</span>
<span class="quote">&gt;   static inline void clear_highpage(struct page *page)</span>
<span class="quote">&gt;   {</span>
<span class="quote">&gt;   	void *kaddr = kmap_atomic(page);</span>
<span class="quote">&gt; -	clear_page(kaddr);</span>
<span class="quote">&gt; +	if (!xpfo_test_and_clear_zap(page))</span>
<span class="quote">&gt; +		clear_page(kaddr);</span>
<span class="quote">&gt;   	kunmap_atomic(kaddr);</span>
<span class="quote">&gt;   }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h</span>
<span class="quote">&gt; index 624b78b..71c95aa 100644</span>
<span class="quote">&gt; --- a/include/linux/mm_types.h</span>
<span class="quote">&gt; +++ b/include/linux/mm_types.h</span>
<span class="quote">&gt; @@ -12,6 +12,7 @@</span>
<span class="quote">&gt;   #include &lt;linux/cpumask.h&gt;</span>
<span class="quote">&gt;   #include &lt;linux/uprobes.h&gt;</span>
<span class="quote">&gt;   #include &lt;linux/page-flags-layout.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/xpfo.h&gt;</span>
<span class="quote">&gt;   #include &lt;asm/page.h&gt;</span>
<span class="quote">&gt;   #include &lt;asm/mmu.h&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; @@ -215,6 +216,9 @@ struct page {</span>
<span class="quote">&gt;   #ifdef LAST_CPUPID_NOT_IN_PAGE_FLAGS</span>
<span class="quote">&gt;   	int _last_cpupid;</span>
<span class="quote">&gt;   #endif</span>
<span class="quote">&gt; +#ifdef CONFIG_XPFO</span>
<span class="quote">&gt; +	struct xpfo_info xpfo;</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt;   }</span>
<span class="quote">&gt;   /*</span>
<span class="quote">&gt;    * The struct page can be forced to be double word aligned so that atomic ops</span>
<span class="quote">&gt; diff --git a/include/linux/xpfo.h b/include/linux/xpfo.h</span>
<span class="quote">&gt; new file mode 100644</span>
<span class="quote">&gt; index 0000000..c4f0871</span>
<span class="quote">&gt; --- /dev/null</span>
<span class="quote">&gt; +++ b/include/linux/xpfo.h</span>
<span class="quote">&gt; @@ -0,0 +1,88 @@</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * Copyright (C) 2016 Brown University. All rights reserved.</span>
<span class="quote">&gt; + * Copyright (C) 2016 Hewlett Packard Enterprise Development, L.P.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Authors:</span>
<span class="quote">&gt; + *   Vasileios P. Kemerlis &lt;vpk@cs.brown.edu&gt;</span>
<span class="quote">&gt; + *   Juerg Haefliger &lt;juerg.haefliger@hpe.com&gt;</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * This program is free software; you can redistribute it and/or modify it</span>
<span class="quote">&gt; + * under the terms of the GNU General Public License version 2 as published by</span>
<span class="quote">&gt; + * the Free Software Foundation.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#ifndef _LINUX_XPFO_H</span>
<span class="quote">&gt; +#define _LINUX_XPFO_H</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#ifdef CONFIG_XPFO</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * XPFO page flags:</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * PG_XPFO_user_fp denotes that the page is allocated to user space. This flag</span>
<span class="quote">&gt; + * is used in the fast path, where the page is marked accordingly but *not*</span>
<span class="quote">&gt; + * unmapped from the kernel. In most cases, the kernel will need access to the</span>
<span class="quote">&gt; + * page immediately after its acquisition so an unnecessary mapping operation</span>
<span class="quote">&gt; + * is avoided.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * PG_XPFO_user denotes that the page is destined for user space. This flag is</span>
<span class="quote">&gt; + * used in the slow path, where the page needs to be mapped/unmapped when the</span>
<span class="quote">&gt; + * kernel wants to access it. If a page is deallocated and this flag is set,</span>
<span class="quote">&gt; + * the page is cleared and mapped back into the kernel.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * PG_XPFO_kernel denotes a page that is destined to kernel space. This is used</span>
<span class="quote">&gt; + * for identifying pages that are first assigned to kernel space and then freed</span>
<span class="quote">&gt; + * and mapped to user space. In such cases, an expensive TLB shootdown is</span>
<span class="quote">&gt; + * necessary. Pages allocated to user space, freed, and subsequently allocated</span>
<span class="quote">&gt; + * to user space again, require only local TLB invalidation.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * PG_XPFO_zap indicates that the page has been zapped. This flag is used to</span>
<span class="quote">&gt; + * avoid zapping pages multiple times. Whenever a page is freed and was</span>
<span class="quote">&gt; + * previously mapped to user space, it needs to be zapped before mapped back</span>
<span class="quote">&gt; + * in to the kernel.</span>
<span class="quote">&gt; + */</span>

&#39;zap&#39; doesn&#39;t really indicate what is actually happening with the page. Can you
be a bit more descriptive about what this actually does?
<span class="quote">
&gt; +</span>
<span class="quote">&gt; +enum xpfo_pageflags {</span>
<span class="quote">&gt; +	PG_XPFO_user_fp,</span>
<span class="quote">&gt; +	PG_XPFO_user,</span>
<span class="quote">&gt; +	PG_XPFO_kernel,</span>
<span class="quote">&gt; +	PG_XPFO_zap,</span>
<span class="quote">&gt; +};</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +struct xpfo_info {</span>
<span class="quote">&gt; +	unsigned long flags;	/* Flags for tracking the page&#39;s XPFO state */</span>
<span class="quote">&gt; +	atomic_t mapcount;	/* Counter for balancing page map/unmap</span>
<span class="quote">&gt; +				 * requests. Only the first map request maps</span>
<span class="quote">&gt; +				 * the page back to kernel space. Likewise,</span>
<span class="quote">&gt; +				 * only the last unmap request unmaps the page.</span>
<span class="quote">&gt; +				 */</span>
<span class="quote">&gt; +	spinlock_t lock;	/* Lock to serialize concurrent map/unmap</span>
<span class="quote">&gt; +				 * requests.</span>
<span class="quote">&gt; +				 */</span>
<span class="quote">&gt; +};</span>

Can you change this to use the page_ext implementation? See what
mm/page_owner.c does. This might lessen the impact of the extra
page metadata. This metadata still feels like a copy of what
mm/highmem.c is trying to do though.
<span class="quote">
&gt; +</span>
<span class="quote">&gt; +extern void xpfo_clear_zap(struct page *page, int order);</span>
<span class="quote">&gt; +extern int xpfo_test_and_clear_zap(struct page *page);</span>
<span class="quote">&gt; +extern int xpfo_test_kernel(struct page *page);</span>
<span class="quote">&gt; +extern int xpfo_test_user(struct page *page);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +extern void xpfo_kmap(void *kaddr, struct page *page);</span>
<span class="quote">&gt; +extern void xpfo_kunmap(void *kaddr, struct page *page);</span>
<span class="quote">&gt; +extern void xpfo_alloc_page(struct page *page, int order, gfp_t gfp);</span>
<span class="quote">&gt; +extern void xpfo_free_page(struct page *page, int order);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#else /* ifdef CONFIG_XPFO */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline void xpfo_clear_zap(struct page *page, int order) { }</span>
<span class="quote">&gt; +static inline int xpfo_test_and_clear_zap(struct page *page) { return 0; }</span>
<span class="quote">&gt; +static inline int xpfo_test_kernel(struct page *page) { return 0; }</span>
<span class="quote">&gt; +static inline int xpfo_test_user(struct page *page) { return 0; }</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline void xpfo_kmap(void *kaddr, struct page *page) { }</span>
<span class="quote">&gt; +static inline void xpfo_kunmap(void *kaddr, struct page *page) { }</span>
<span class="quote">&gt; +static inline void xpfo_alloc_page(struct page *page, int order, gfp_t gfp) { }</span>
<span class="quote">&gt; +static inline void xpfo_free_page(struct page *page, int order) { }</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#endif /* ifdef CONFIG_XPFO */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#endif /* ifndef _LINUX_XPFO_H */</span>
<span class="quote">&gt; diff --git a/lib/swiotlb.c b/lib/swiotlb.c</span>
<span class="quote">&gt; index 76f29ec..cf57ee9 100644</span>
<span class="quote">&gt; --- a/lib/swiotlb.c</span>
<span class="quote">&gt; +++ b/lib/swiotlb.c</span>
<span class="quote">&gt; @@ -390,8 +390,9 @@ static void swiotlb_bounce(phys_addr_t orig_addr, phys_addr_t tlb_addr,</span>
<span class="quote">&gt;   {</span>
<span class="quote">&gt;   	unsigned long pfn = PFN_DOWN(orig_addr);</span>
<span class="quote">&gt;   	unsigned char *vaddr = phys_to_virt(tlb_addr);</span>
<span class="quote">&gt; +	struct page *page = pfn_to_page(pfn);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -	if (PageHighMem(pfn_to_page(pfn))) {</span>
<span class="quote">&gt; +	if (PageHighMem(page) || xpfo_test_user(page)) {</span>
<span class="quote">&gt;   		/* The buffer does not have a mapping.  Map it in and copy */</span>
<span class="quote">&gt;   		unsigned int offset = orig_addr &amp; ~PAGE_MASK;</span>
<span class="quote">&gt;   		char *buffer;</span>
<span class="quote">&gt; diff --git a/mm/page_alloc.c b/mm/page_alloc.c</span>
<span class="quote">&gt; index 838ca8bb..47b42a3 100644</span>
<span class="quote">&gt; --- a/mm/page_alloc.c</span>
<span class="quote">&gt; +++ b/mm/page_alloc.c</span>
<span class="quote">&gt; @@ -1003,6 +1003,7 @@ static bool free_pages_prepare(struct page *page, unsigned int order)</span>
<span class="quote">&gt;   	}</span>
<span class="quote">&gt;   	arch_free_page(page, order);</span>
<span class="quote">&gt;   	kernel_map_pages(page, 1 &lt;&lt; order, 0);</span>
<span class="quote">&gt; +	xpfo_free_page(page, order);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;   	return true;</span>
<span class="quote">&gt;   }</span>
<span class="quote">&gt; @@ -1398,10 +1399,13 @@ static int prep_new_page(struct page *page, unsigned int order, gfp_t gfp_flags,</span>
<span class="quote">&gt;   	arch_alloc_page(page, order);</span>
<span class="quote">&gt;   	kernel_map_pages(page, 1 &lt;&lt; order, 1);</span>
<span class="quote">&gt;   	kasan_alloc_pages(page, order);</span>
<span class="quote">&gt; +	xpfo_alloc_page(page, order, gfp_flags);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;   	if (gfp_flags &amp; __GFP_ZERO)</span>
<span class="quote">&gt;   		for (i = 0; i &lt; (1 &lt;&lt; order); i++)</span>
<span class="quote">&gt;   			clear_highpage(page + i);</span>
<span class="quote">&gt; +	else</span>
<span class="quote">&gt; +		xpfo_clear_zap(page, order);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;   	if (order &amp;&amp; (gfp_flags &amp; __GFP_COMP))</span>
<span class="quote">&gt;   		prep_compound_page(page, order);</span>
<span class="quote">&gt; @@ -2072,10 +2076,11 @@ void free_hot_cold_page(struct page *page, bool cold)</span>
<span class="quote">&gt;   	}</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;   	pcp = &amp;this_cpu_ptr(zone-&gt;pageset)-&gt;pcp;</span>
<span class="quote">&gt; -	if (!cold)</span>
<span class="quote">&gt; +	if (!cold &amp;&amp; !xpfo_test_kernel(page))</span>
<span class="quote">&gt;   		list_add(&amp;page-&gt;lru, &amp;pcp-&gt;lists[migratetype]);</span>
<span class="quote">&gt;   	else</span>
<span class="quote">&gt;   		list_add_tail(&amp;page-&gt;lru, &amp;pcp-&gt;lists[migratetype]);</span>
<span class="quote">&gt; +</span>

What&#39;s the advantage of this?
<span class="quote">
&gt;   	pcp-&gt;count++;</span>
<span class="quote">&gt;   	if (pcp-&gt;count &gt;= pcp-&gt;high) {</span>
<span class="quote">&gt;   		unsigned long batch = READ_ONCE(pcp-&gt;batch);</span>
<span class="quote">&gt;</span>

Thanks,
Laura
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=4745">Balbir Singh</a> - March 1, 2016, 2:10 a.m.</div>
<pre class="content">
On 27/02/16 01:21, Juerg Haefliger wrote:
<span class="quote">&gt; This patch adds support for XPFO which protects against &#39;ret2dir&#39; kernel</span>
<span class="quote">&gt; attacks. The basic idea is to enforce exclusive ownership of page frames</span>
<span class="quote">&gt; by either the kernel or userland, unless explicitly requested by the</span>
<span class="quote">&gt; kernel. Whenever a page destined for userland is allocated, it is</span>
<span class="quote">&gt; unmapped from physmap. When such a page is reclaimed from userland, it is</span>
<span class="quote">&gt; mapped back to physmap.</span>
physmap == xen physmap? Please clarify
<span class="quote">&gt; Mapping/unmapping from physmap is accomplished by modifying the PTE</span>
<span class="quote">&gt; permission bits to allow/disallow access to the page.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Additional fields are added to the page struct for XPFO housekeeping.</span>
<span class="quote">&gt; Specifically a flags field to distinguish user vs. kernel pages, a</span>
<span class="quote">&gt; reference counter to track physmap map/unmap operations and a lock to</span>
<span class="quote">&gt; protect the XPFO fields.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Known issues/limitations:</span>
<span class="quote">&gt;   - Only supported on x86-64.</span>
Is it due to lack of porting or a design limitation?
<span class="quote">&gt;   - Only supports 4k pages.</span>
<span class="quote">&gt;   - Adds additional data to the page struct.</span>
<span class="quote">&gt;   - There are most likely some additional and legitimate uses cases where</span>
<span class="quote">&gt;     the kernel needs to access userspace. Those need to be identified and</span>
<span class="quote">&gt;     made XPFO-aware.</span>
Why not build an audit mode for it?
<span class="quote">&gt;   - There&#39;s a performance impact if XPFO is turned on. Per the paper</span>
<span class="quote">&gt;     referenced below it&#39;s in the 1-3% ballpark. More performance testing</span>
<span class="quote">&gt;     wouldn&#39;t hurt. What tests to run though?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Reference paper by the original patch authors:</span>
<span class="quote">&gt;   http://www.cs.columbia.edu/~vpk/papers/ret2dir.sec14.pdf</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Suggested-by: Vasileios P. Kemerlis &lt;vpk@cs.brown.edu&gt;</span>
<span class="quote">&gt; Signed-off-by: Juerg Haefliger &lt;juerg.haefliger@hpe.com&gt;</span>
This patch needs to be broken down into smaller patches - a series
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  arch/x86/Kconfig         |   2 +-</span>
<span class="quote">&gt;  arch/x86/Kconfig.debug   |  17 +++++</span>
<span class="quote">&gt;  arch/x86/mm/Makefile     |   2 +</span>
<span class="quote">&gt;  arch/x86/mm/init.c       |   3 +-</span>
<span class="quote">&gt;  arch/x86/mm/xpfo.c       | 176 +++++++++++++++++++++++++++++++++++++++++++++++</span>
<span class="quote">&gt;  block/blk-map.c          |   7 +-</span>
<span class="quote">&gt;  include/linux/highmem.h  |  23 +++++--</span>
<span class="quote">&gt;  include/linux/mm_types.h |   4 ++</span>
<span class="quote">&gt;  include/linux/xpfo.h     |  88 ++++++++++++++++++++++++</span>
<span class="quote">&gt;  lib/swiotlb.c            |   3 +-</span>
<span class="quote">&gt;  mm/page_alloc.c          |   7 +-</span>
<span class="quote">&gt;  11 files changed, 323 insertions(+), 9 deletions(-)</span>
<span class="quote">&gt;  create mode 100644 arch/x86/mm/xpfo.c</span>
<span class="quote">&gt;  create mode 100644 include/linux/xpfo.h</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig</span>
<span class="quote">&gt; index c46662f..9d32b4a 100644</span>
<span class="quote">&gt; --- a/arch/x86/Kconfig</span>
<span class="quote">&gt; +++ b/arch/x86/Kconfig</span>
<span class="quote">&gt; @@ -1343,7 +1343,7 @@ config ARCH_DMA_ADDR_T_64BIT</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  config X86_DIRECT_GBPAGES</span>
<span class="quote">&gt;  	def_bool y</span>
<span class="quote">&gt; -	depends on X86_64 &amp;&amp; !DEBUG_PAGEALLOC &amp;&amp; !KMEMCHECK</span>
<span class="quote">&gt; +	depends on X86_64 &amp;&amp; !DEBUG_PAGEALLOC &amp;&amp; !KMEMCHECK &amp;&amp; !XPFO</span>
<span class="quote">&gt;  	---help---</span>
<span class="quote">&gt;  	  Certain kernel features effectively disable kernel</span>
<span class="quote">&gt;  	  linear 1 GB mappings (even if the CPU otherwise</span>
<span class="quote">&gt; diff --git a/arch/x86/Kconfig.debug b/arch/x86/Kconfig.debug</span>
<span class="quote">&gt; index 9b18ed9..1331da5 100644</span>
<span class="quote">&gt; --- a/arch/x86/Kconfig.debug</span>
<span class="quote">&gt; +++ b/arch/x86/Kconfig.debug</span>
<span class="quote">&gt; @@ -5,6 +5,23 @@ config TRACE_IRQFLAGS_SUPPORT</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  source &quot;lib/Kconfig.debug&quot;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +config XPFO</span>
<span class="quote">&gt; +	bool &quot;Enable eXclusive Page Frame Ownership (XPFO)&quot;</span>
<span class="quote">&gt; +	default n</span>
<span class="quote">&gt; +	depends on DEBUG_KERNEL</span>
<span class="quote">&gt; +	depends on X86_64</span>
<span class="quote">&gt; +	select DEBUG_TLBFLUSH</span>
<span class="quote">&gt; +	---help---</span>
<span class="quote">&gt; +	  This option offers protection against &#39;ret2dir&#39; (kernel) attacks.</span>
<span class="quote">&gt; +	  When enabled, every time a page frame is allocated to user space, it</span>
<span class="quote">&gt; +	  is unmapped from the direct mapped RAM region in kernel space</span>
<span class="quote">&gt; +	  (physmap). Similarly, whenever page frames are freed/reclaimed, they</span>
<span class="quote">&gt; +	  are mapped back to physmap. Special care is taken to minimize the</span>
<span class="quote">&gt; +	  impact on performance by reducing TLB shootdowns and unnecessary page</span>
<span class="quote">&gt; +	  zero fills.</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	  If in doubt, say &quot;N&quot;.</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  config X86_VERBOSE_BOOTUP</span>
<span class="quote">&gt;  	bool &quot;Enable verbose x86 bootup info messages&quot;</span>
<span class="quote">&gt;  	default y</span>
<span class="quote">&gt; diff --git a/arch/x86/mm/Makefile b/arch/x86/mm/Makefile</span>
<span class="quote">&gt; index f9d38a4..8bf52b6 100644</span>
<span class="quote">&gt; --- a/arch/x86/mm/Makefile</span>
<span class="quote">&gt; +++ b/arch/x86/mm/Makefile</span>
<span class="quote">&gt; @@ -34,3 +34,5 @@ obj-$(CONFIG_ACPI_NUMA)		+= srat.o</span>
<span class="quote">&gt;  obj-$(CONFIG_NUMA_EMU)		+= numa_emulation.o</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  obj-$(CONFIG_X86_INTEL_MPX)	+= mpx.o</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +obj-$(CONFIG_XPFO)		+= xpfo.o</span>
<span class="quote">&gt; diff --git a/arch/x86/mm/init.c b/arch/x86/mm/init.c</span>
<span class="quote">&gt; index 493f541..27fc8a6 100644</span>
<span class="quote">&gt; --- a/arch/x86/mm/init.c</span>
<span class="quote">&gt; +++ b/arch/x86/mm/init.c</span>
<span class="quote">&gt; @@ -150,7 +150,8 @@ static int page_size_mask;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static void __init probe_page_size_mask(void)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -#if !defined(CONFIG_DEBUG_PAGEALLOC) &amp;&amp; !defined(CONFIG_KMEMCHECK)</span>
<span class="quote">&gt; +#if !defined(CONFIG_DEBUG_PAGEALLOC) &amp;&amp; !defined(CONFIG_KMEMCHECK) &amp;&amp; \</span>
<span class="quote">&gt; +	!defined(CONFIG_XPFO)</span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt;  	 * For CONFIG_DEBUG_PAGEALLOC, identity mapping will use small pages.</span>
<span class="quote">&gt;  	 * This will simplify cpa(), which otherwise needs to support splitting</span>
<span class="quote">&gt; diff --git a/arch/x86/mm/xpfo.c b/arch/x86/mm/xpfo.c</span>
<span class="quote">&gt; new file mode 100644</span>
<span class="quote">&gt; index 0000000..6bc24d3</span>
<span class="quote">&gt; --- /dev/null</span>
<span class="quote">&gt; +++ b/arch/x86/mm/xpfo.c</span>
<span class="quote">&gt; @@ -0,0 +1,176 @@</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * Copyright (C) 2016 Brown University. All rights reserved.</span>
<span class="quote">&gt; + * Copyright (C) 2016 Hewlett Packard Enterprise Development, L.P.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Authors:</span>
<span class="quote">&gt; + *   Vasileios P. Kemerlis &lt;vpk@cs.brown.edu&gt;</span>
<span class="quote">&gt; + *   Juerg Haefliger &lt;juerg.haefliger@hpe.com&gt;</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * This program is free software; you can redistribute it and/or modify it</span>
<span class="quote">&gt; + * under the terms of the GNU General Public License version 2 as published by</span>
<span class="quote">&gt; + * the Free Software Foundation.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#include &lt;linux/mm.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/module.h&gt;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#include &lt;asm/pgtable.h&gt;</span>
<span class="quote">&gt; +#include &lt;asm/tlbflush.h&gt;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define TEST_XPFO_FLAG(flag, page) \</span>
<span class="quote">&gt; +	test_bit(PG_XPFO_##flag, &amp;(page)-&gt;xpfo.flags)</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define SET_XPFO_FLAG(flag, page)			\</span>
<span class="quote">&gt; +	__set_bit(PG_XPFO_##flag, &amp;(page)-&gt;xpfo.flags)</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define CLEAR_XPFO_FLAG(flag, page)			\</span>
<span class="quote">&gt; +	__clear_bit(PG_XPFO_##flag, &amp;(page)-&gt;xpfo.flags)</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define TEST_AND_CLEAR_XPFO_FLAG(flag, page)			\</span>
<span class="quote">&gt; +	__test_and_clear_bit(PG_XPFO_##flag, &amp;(page)-&gt;xpfo.flags)</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * Update a single kernel page table entry</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +static inline void set_kpte(struct page *page, unsigned long kaddr,</span>
<span class="quote">&gt; +			    pgprot_t prot) {</span>
<span class="quote">&gt; +	unsigned int level;</span>
<span class="quote">&gt; +	pte_t *kpte = lookup_address(kaddr, &amp;level);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/* We only support 4k pages for now */</span>
<span class="quote">&gt; +	BUG_ON(!kpte || level != PG_LEVEL_4K);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	set_pte_atomic(kpte, pfn_pte(page_to_pfn(page), canon_pgprot(prot)));</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +inline void xpfo_clear_zap(struct page *page, int order)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	int i;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	for (i = 0; i &lt; (1 &lt;&lt; order); i++)</span>
<span class="quote">&gt; +		CLEAR_XPFO_FLAG(zap, page + i);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +inline int xpfo_test_and_clear_zap(struct page *page)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return TEST_AND_CLEAR_XPFO_FLAG(zap, page);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +inline int xpfo_test_kernel(struct page *page)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return TEST_XPFO_FLAG(kernel, page);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +inline int xpfo_test_user(struct page *page)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return TEST_XPFO_FLAG(user, page);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +void xpfo_alloc_page(struct page *page, int order, gfp_t gfp)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	int i, tlb_shoot = 0;</span>
<span class="quote">&gt; +	unsigned long kaddr;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	for (i = 0; i &lt; (1 &lt;&lt; order); i++)  {</span>
<span class="quote">&gt; +		WARN_ON(TEST_XPFO_FLAG(user_fp, page + i) ||</span>
<span class="quote">&gt; +			TEST_XPFO_FLAG(user, page + i));</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		if (gfp &amp; GFP_HIGHUSER) {</span>
Why GFP_HIGHUSER?
<span class="quote">&gt; +			/* Initialize the xpfo lock and map counter */</span>
<span class="quote">&gt; +			spin_lock_init(&amp;(page + i)-&gt;xpfo.lock);</span>
<span class="quote">&gt; +			atomic_set(&amp;(page + i)-&gt;xpfo.mapcount, 0);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			/* Mark it as a user page */</span>
<span class="quote">&gt; +			SET_XPFO_FLAG(user_fp, page + i);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			/*</span>
<span class="quote">&gt; +			 * Shoot the TLB if the page was previously allocated</span>
<span class="quote">&gt; +			 * to kernel space</span>
<span class="quote">&gt; +			 */</span>
<span class="quote">&gt; +			if (TEST_AND_CLEAR_XPFO_FLAG(kernel, page + i))</span>
<span class="quote">&gt; +				tlb_shoot = 1;</span>
<span class="quote">&gt; +		} else {</span>
<span class="quote">&gt; +			/* Mark it as a kernel page */</span>
<span class="quote">&gt; +			SET_XPFO_FLAG(kernel, page + i);</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (tlb_shoot) {</span>
<span class="quote">&gt; +		kaddr = (unsigned long)page_address(page);</span>
<span class="quote">&gt; +		flush_tlb_kernel_range(kaddr, kaddr + (1 &lt;&lt; order) *</span>
<span class="quote">&gt; +				       PAGE_SIZE);</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +void xpfo_free_page(struct page *page, int order)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	int i;</span>
<span class="quote">&gt; +	unsigned long kaddr;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	for (i = 0; i &lt; (1 &lt;&lt; order); i++) {</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		/* The page frame was previously allocated to user space */</span>
<span class="quote">&gt; +		if (TEST_AND_CLEAR_XPFO_FLAG(user, page + i)) {</span>
<span class="quote">&gt; +			kaddr = (unsigned long)page_address(page + i);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			/* Clear the page and mark it accordingly */</span>
<span class="quote">&gt; +			clear_page((void *)kaddr);</span>
<span class="quote">&gt; +			SET_XPFO_FLAG(zap, page + i);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			/* Map it back to kernel space */</span>
<span class="quote">&gt; +			set_kpte(page + i,  kaddr, __pgprot(__PAGE_KERNEL));</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			/* No TLB update */</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		/* Clear the xpfo fast-path flag */</span>
<span class="quote">&gt; +		CLEAR_XPFO_FLAG(user_fp, page + i);</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +void xpfo_kmap(void *kaddr, struct page *page)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	unsigned long flags;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/* The page is allocated to kernel space, so nothing to do */</span>
<span class="quote">&gt; +	if (TEST_XPFO_FLAG(kernel, page))</span>
<span class="quote">&gt; +		return;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	spin_lock_irqsave(&amp;page-&gt;xpfo.lock, flags);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * The page was previously allocated to user space, so map it back</span>
<span class="quote">&gt; +	 * into the kernel. No TLB update required.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	if ((atomic_inc_return(&amp;page-&gt;xpfo.mapcount) == 1) &amp;&amp;</span>
<span class="quote">&gt; +	    TEST_XPFO_FLAG(user, page))</span>
<span class="quote">&gt; +		set_kpte(page, (unsigned long)kaddr, __pgprot(__PAGE_KERNEL));</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	spin_unlock_irqrestore(&amp;page-&gt;xpfo.lock, flags);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +EXPORT_SYMBOL(xpfo_kmap);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +void xpfo_kunmap(void *kaddr, struct page *page)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	unsigned long flags;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/* The page is allocated to kernel space, so nothing to do */</span>
<span class="quote">&gt; +	if (TEST_XPFO_FLAG(kernel, page))</span>
<span class="quote">&gt; +		return;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	spin_lock_irqsave(&amp;page-&gt;xpfo.lock, flags);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * The page frame is to be allocated back to user space. So unmap it</span>
<span class="quote">&gt; +	 * from the kernel, update the TLB and mark it as a user page.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	if ((atomic_dec_return(&amp;page-&gt;xpfo.mapcount) == 0) &amp;&amp;</span>
<span class="quote">&gt; +	    (TEST_XPFO_FLAG(user_fp, page) || TEST_XPFO_FLAG(user, page))) {</span>
<span class="quote">&gt; +		set_kpte(page, (unsigned long)kaddr, __pgprot(0));</span>
<span class="quote">&gt; +		__flush_tlb_one((unsigned long)kaddr);</span>
<span class="quote">&gt; +		SET_XPFO_FLAG(user, page);</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	spin_unlock_irqrestore(&amp;page-&gt;xpfo.lock, flags);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +EXPORT_SYMBOL(xpfo_kunmap);</span>
<span class="quote">&gt; diff --git a/block/blk-map.c b/block/blk-map.c</span>
<span class="quote">&gt; index f565e11..b7b8302 100644</span>
<span class="quote">&gt; --- a/block/blk-map.c</span>
<span class="quote">&gt; +++ b/block/blk-map.c</span>
<span class="quote">&gt; @@ -107,7 +107,12 @@ int blk_rq_map_user_iov(struct request_queue *q, struct request *rq,</span>
<span class="quote">&gt;  		prv.iov_len = iov.iov_len;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (unaligned || (q-&gt;dma_pad_mask &amp; iter-&gt;count) || map_data)</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * juergh: Temporary hack to force the use of a bounce buffer if XPFO</span>
<span class="quote">&gt; +	 * is enabled. Results in an XPFO page fault otherwise.</span>
<span class="quote">&gt; +	 */</span>
This does look like it might add a bunch of overhead
<span class="quote">&gt; +	if (unaligned || (q-&gt;dma_pad_mask &amp; iter-&gt;count) || map_data ||</span>
<span class="quote">&gt; +	    IS_ENABLED(CONFIG_XPFO))</span>
<span class="quote">&gt;  		bio = bio_copy_user_iov(q, map_data, iter, gfp_mask);</span>
<span class="quote">&gt;  	else</span>
<span class="quote">&gt;  		bio = bio_map_user_iov(q, iter, gfp_mask);</span>
<span class="quote">&gt; diff --git a/include/linux/highmem.h b/include/linux/highmem.h</span>
<span class="quote">&gt; index bb3f329..0ca9130 100644</span>
<span class="quote">&gt; --- a/include/linux/highmem.h</span>
<span class="quote">&gt; +++ b/include/linux/highmem.h</span>
<span class="quote">&gt; @@ -55,24 +55,37 @@ static inline struct page *kmap_to_page(void *addr)</span>
<span class="quote">&gt;  #ifndef ARCH_HAS_KMAP</span>
<span class="quote">&gt;  static inline void *kmap(struct page *page)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; +	void *kaddr;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	might_sleep();</span>
<span class="quote">&gt; -	return page_address(page);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	kaddr = page_address(page);</span>
<span class="quote">&gt; +	xpfo_kmap(kaddr, page);</span>
<span class="quote">&gt; +	return kaddr;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static inline void kunmap(struct page *page)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; +	xpfo_kunmap(page_address(page), page);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static inline void *kmap_atomic(struct page *page)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; +	void *kaddr;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	preempt_disable();</span>
<span class="quote">&gt;  	pagefault_disable();</span>
<span class="quote">&gt; -	return page_address(page);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	kaddr = page_address(page);</span>
<span class="quote">&gt; +	xpfo_kmap(kaddr, page);</span>
<span class="quote">&gt; +	return kaddr;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  #define kmap_atomic_prot(page, prot)	kmap_atomic(page)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static inline void __kunmap_atomic(void *addr)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; +	xpfo_kunmap(addr, virt_to_page(addr));</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	pagefault_enable();</span>
<span class="quote">&gt;  	preempt_enable();</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; @@ -133,7 +146,8 @@ do {                                                            \</span>
<span class="quote">&gt;  static inline void clear_user_highpage(struct page *page, unsigned long vaddr)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	void *addr = kmap_atomic(page);</span>
<span class="quote">&gt; -	clear_user_page(addr, vaddr, page);</span>
<span class="quote">&gt; +	if (!xpfo_test_and_clear_zap(page))</span>
<span class="quote">&gt; +		clear_user_page(addr, vaddr, page);</span>
<span class="quote">&gt;  	kunmap_atomic(addr);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt; @@ -186,7 +200,8 @@ alloc_zeroed_user_highpage_movable(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  static inline void clear_highpage(struct page *page)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	void *kaddr = kmap_atomic(page);</span>
<span class="quote">&gt; -	clear_page(kaddr);</span>
<span class="quote">&gt; +	if (!xpfo_test_and_clear_zap(page))</span>
<span class="quote">&gt; +		clear_page(kaddr);</span>
<span class="quote">&gt;  	kunmap_atomic(kaddr);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h</span>
<span class="quote">&gt; index 624b78b..71c95aa 100644</span>
<span class="quote">&gt; --- a/include/linux/mm_types.h</span>
<span class="quote">&gt; +++ b/include/linux/mm_types.h</span>
<span class="quote">&gt; @@ -12,6 +12,7 @@</span>
<span class="quote">&gt;  #include &lt;linux/cpumask.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/uprobes.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/page-flags-layout.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/xpfo.h&gt;</span>
<span class="quote">&gt;  #include &lt;asm/page.h&gt;</span>
<span class="quote">&gt;  #include &lt;asm/mmu.h&gt;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -215,6 +216,9 @@ struct page {</span>
<span class="quote">&gt;  #ifdef LAST_CPUPID_NOT_IN_PAGE_FLAGS</span>
<span class="quote">&gt;  	int _last_cpupid;</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt; +#ifdef CONFIG_XPFO</span>
<span class="quote">&gt; +	struct xpfo_info xpfo;</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt;   * The struct page can be forced to be double word aligned so that atomic ops</span>
<span class="quote">&gt; diff --git a/include/linux/xpfo.h b/include/linux/xpfo.h</span>
<span class="quote">&gt; new file mode 100644</span>
<span class="quote">&gt; index 0000000..c4f0871</span>
<span class="quote">&gt; --- /dev/null</span>
<span class="quote">&gt; +++ b/include/linux/xpfo.h</span>
<span class="quote">&gt; @@ -0,0 +1,88 @@</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * Copyright (C) 2016 Brown University. All rights reserved.</span>
<span class="quote">&gt; + * Copyright (C) 2016 Hewlett Packard Enterprise Development, L.P.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Authors:</span>
<span class="quote">&gt; + *   Vasileios P. Kemerlis &lt;vpk@cs.brown.edu&gt;</span>
<span class="quote">&gt; + *   Juerg Haefliger &lt;juerg.haefliger@hpe.com&gt;</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * This program is free software; you can redistribute it and/or modify it</span>
<span class="quote">&gt; + * under the terms of the GNU General Public License version 2 as published by</span>
<span class="quote">&gt; + * the Free Software Foundation.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#ifndef _LINUX_XPFO_H</span>
<span class="quote">&gt; +#define _LINUX_XPFO_H</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#ifdef CONFIG_XPFO</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * XPFO page flags:</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * PG_XPFO_user_fp denotes that the page is allocated to user space. This flag</span>
<span class="quote">&gt; + * is used in the fast path, where the page is marked accordingly but *not*</span>
<span class="quote">&gt; + * unmapped from the kernel. In most cases, the kernel will need access to the</span>
<span class="quote">&gt; + * page immediately after its acquisition so an unnecessary mapping operation</span>
<span class="quote">&gt; + * is avoided.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * PG_XPFO_user denotes that the page is destined for user space. This flag is</span>
<span class="quote">&gt; + * used in the slow path, where the page needs to be mapped/unmapped when the</span>
<span class="quote">&gt; + * kernel wants to access it. If a page is deallocated and this flag is set,</span>
<span class="quote">&gt; + * the page is cleared and mapped back into the kernel.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * PG_XPFO_kernel denotes a page that is destined to kernel space. This is used</span>
<span class="quote">&gt; + * for identifying pages that are first assigned to kernel space and then freed</span>
<span class="quote">&gt; + * and mapped to user space. In such cases, an expensive TLB shootdown is</span>
<span class="quote">&gt; + * necessary. Pages allocated to user space, freed, and subsequently allocated</span>
<span class="quote">&gt; + * to user space again, require only local TLB invalidation.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * PG_XPFO_zap indicates that the page has been zapped. This flag is used to</span>
<span class="quote">&gt; + * avoid zapping pages multiple times. Whenever a page is freed and was</span>
<span class="quote">&gt; + * previously mapped to user space, it needs to be zapped before mapped back</span>
<span class="quote">&gt; + * in to the kernel.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +enum xpfo_pageflags {</span>
<span class="quote">&gt; +	PG_XPFO_user_fp,</span>
<span class="quote">&gt; +	PG_XPFO_user,</span>
<span class="quote">&gt; +	PG_XPFO_kernel,</span>
<span class="quote">&gt; +	PG_XPFO_zap,</span>
<span class="quote">&gt; +};</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +struct xpfo_info {</span>
<span class="quote">&gt; +	unsigned long flags;	/* Flags for tracking the page&#39;s XPFO state */</span>
<span class="quote">&gt; +	atomic_t mapcount;	/* Counter for balancing page map/unmap</span>
<span class="quote">&gt; +				 * requests. Only the first map request maps</span>
<span class="quote">&gt; +				 * the page back to kernel space. Likewise,</span>
<span class="quote">&gt; +				 * only the last unmap request unmaps the page.</span>
<span class="quote">&gt; +				 */</span>
<span class="quote">&gt; +	spinlock_t lock;	/* Lock to serialize concurrent map/unmap</span>
<span class="quote">&gt; +				 * requests.</span>
<span class="quote">&gt; +				 */</span>
<span class="quote">&gt; +};</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +extern void xpfo_clear_zap(struct page *page, int order);</span>
<span class="quote">&gt; +extern int xpfo_test_and_clear_zap(struct page *page);</span>
<span class="quote">&gt; +extern int xpfo_test_kernel(struct page *page);</span>
<span class="quote">&gt; +extern int xpfo_test_user(struct page *page);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +extern void xpfo_kmap(void *kaddr, struct page *page);</span>
<span class="quote">&gt; +extern void xpfo_kunmap(void *kaddr, struct page *page);</span>
<span class="quote">&gt; +extern void xpfo_alloc_page(struct page *page, int order, gfp_t gfp);</span>
<span class="quote">&gt; +extern void xpfo_free_page(struct page *page, int order);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#else /* ifdef CONFIG_XPFO */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline void xpfo_clear_zap(struct page *page, int order) { }</span>
<span class="quote">&gt; +static inline int xpfo_test_and_clear_zap(struct page *page) { return 0; }</span>
<span class="quote">&gt; +static inline int xpfo_test_kernel(struct page *page) { return 0; }</span>
<span class="quote">&gt; +static inline int xpfo_test_user(struct page *page) { return 0; }</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline void xpfo_kmap(void *kaddr, struct page *page) { }</span>
<span class="quote">&gt; +static inline void xpfo_kunmap(void *kaddr, struct page *page) { }</span>
<span class="quote">&gt; +static inline void xpfo_alloc_page(struct page *page, int order, gfp_t gfp) { }</span>
<span class="quote">&gt; +static inline void xpfo_free_page(struct page *page, int order) { }</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#endif /* ifdef CONFIG_XPFO */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#endif /* ifndef _LINUX_XPFO_H */</span>
<span class="quote">&gt; diff --git a/lib/swiotlb.c b/lib/swiotlb.c</span>
<span class="quote">&gt; index 76f29ec..cf57ee9 100644</span>
<span class="quote">&gt; --- a/lib/swiotlb.c</span>
<span class="quote">&gt; +++ b/lib/swiotlb.c</span>
<span class="quote">&gt; @@ -390,8 +390,9 @@ static void swiotlb_bounce(phys_addr_t orig_addr, phys_addr_t tlb_addr,</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	unsigned long pfn = PFN_DOWN(orig_addr);</span>
<span class="quote">&gt;  	unsigned char *vaddr = phys_to_virt(tlb_addr);</span>
<span class="quote">&gt; +	struct page *page = pfn_to_page(pfn);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (PageHighMem(pfn_to_page(pfn))) {</span>
<span class="quote">&gt; +	if (PageHighMem(page) || xpfo_test_user(page)) {</span>
<span class="quote">&gt;  		/* The buffer does not have a mapping.  Map it in and copy */</span>
<span class="quote">&gt;  		unsigned int offset = orig_addr &amp; ~PAGE_MASK;</span>
<span class="quote">&gt;  		char *buffer;</span>
<span class="quote">&gt; diff --git a/mm/page_alloc.c b/mm/page_alloc.c</span>
<span class="quote">&gt; index 838ca8bb..47b42a3 100644</span>
<span class="quote">&gt; --- a/mm/page_alloc.c</span>
<span class="quote">&gt; +++ b/mm/page_alloc.c</span>
<span class="quote">&gt; @@ -1003,6 +1003,7 @@ static bool free_pages_prepare(struct page *page, unsigned int order)</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  	arch_free_page(page, order);</span>
<span class="quote">&gt;  	kernel_map_pages(page, 1 &lt;&lt; order, 0);</span>
<span class="quote">&gt; +	xpfo_free_page(page, order);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	return true;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; @@ -1398,10 +1399,13 @@ static int prep_new_page(struct page *page, unsigned int order, gfp_t gfp_flags,</span>
<span class="quote">&gt;  	arch_alloc_page(page, order);</span>
<span class="quote">&gt;  	kernel_map_pages(page, 1 &lt;&lt; order, 1);</span>
<span class="quote">&gt;  	kasan_alloc_pages(page, order);</span>
<span class="quote">&gt; +	xpfo_alloc_page(page, order, gfp_flags);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (gfp_flags &amp; __GFP_ZERO)</span>
<span class="quote">&gt;  		for (i = 0; i &lt; (1 &lt;&lt; order); i++)</span>
<span class="quote">&gt;  			clear_highpage(page + i);</span>
<span class="quote">&gt; +	else</span>
<span class="quote">&gt; +		xpfo_clear_zap(page, order);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (order &amp;&amp; (gfp_flags &amp; __GFP_COMP))</span>
<span class="quote">&gt;  		prep_compound_page(page, order);</span>
<span class="quote">&gt; @@ -2072,10 +2076,11 @@ void free_hot_cold_page(struct page *page, bool cold)</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	pcp = &amp;this_cpu_ptr(zone-&gt;pageset)-&gt;pcp;</span>
<span class="quote">&gt; -	if (!cold)</span>
<span class="quote">&gt; +	if (!cold &amp;&amp; !xpfo_test_kernel(page))</span>
<span class="quote">&gt;  		list_add(&amp;page-&gt;lru, &amp;pcp-&gt;lists[migratetype]);</span>
<span class="quote">&gt;  	else</span>
<span class="quote">&gt;  		list_add_tail(&amp;page-&gt;lru, &amp;pcp-&gt;lists[migratetype]);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	pcp-&gt;count++;</span>
<span class="quote">&gt;  	if (pcp-&gt;count &gt;= pcp-&gt;high) {</span>
<span class="quote">&gt;  		unsigned long batch = READ_ONCE(pcp-&gt;batch);</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=153531">Juerg Haefliger</a> - March 21, 2016, 8:37 a.m.</div>
<pre class="content">
Hi Laura,

Sorry for the late reply. I was on FTO and then traveling for the past couple of
days.


On 03/01/2016 02:31 AM, Laura Abbott wrote:
<span class="quote">&gt; On 02/26/2016 06:21 AM, Juerg Haefliger wrote:</span>
<span class="quote">&gt;&gt; This patch adds support for XPFO which protects against &#39;ret2dir&#39; kernel</span>
<span class="quote">&gt;&gt; attacks. The basic idea is to enforce exclusive ownership of page frames</span>
<span class="quote">&gt;&gt; by either the kernel or userland, unless explicitly requested by the</span>
<span class="quote">&gt;&gt; kernel. Whenever a page destined for userland is allocated, it is</span>
<span class="quote">&gt;&gt; unmapped from physmap. When such a page is reclaimed from userland, it is</span>
<span class="quote">&gt;&gt; mapped back to physmap.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Mapping/unmapping from physmap is accomplished by modifying the PTE</span>
<span class="quote">&gt;&gt; permission bits to allow/disallow access to the page.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Additional fields are added to the page struct for XPFO housekeeping.</span>
<span class="quote">&gt;&gt; Specifically a flags field to distinguish user vs. kernel pages, a</span>
<span class="quote">&gt;&gt; reference counter to track physmap map/unmap operations and a lock to</span>
<span class="quote">&gt;&gt; protect the XPFO fields.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Known issues/limitations:</span>
<span class="quote">&gt;&gt;    - Only supported on x86-64.</span>
<span class="quote">&gt;&gt;    - Only supports 4k pages.</span>
<span class="quote">&gt;&gt;    - Adds additional data to the page struct.</span>
<span class="quote">&gt;&gt;    - There are most likely some additional and legitimate uses cases where</span>
<span class="quote">&gt;&gt;      the kernel needs to access userspace. Those need to be identified and</span>
<span class="quote">&gt;&gt;      made XPFO-aware.</span>
<span class="quote">&gt;&gt;    - There&#39;s a performance impact if XPFO is turned on. Per the paper</span>
<span class="quote">&gt;&gt;      referenced below it&#39;s in the 1-3% ballpark. More performance testing</span>
<span class="quote">&gt;&gt;      wouldn&#39;t hurt. What tests to run though?</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Reference paper by the original patch authors:</span>
<span class="quote">&gt;&gt;    http://www.cs.columbia.edu/~vpk/papers/ret2dir.sec14.pdf</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; General note: Make sure to cc the x86 maintainers on the next version of</span>
<span class="quote">&gt; the patch. I&#39;d also recommend ccing the kernel hardening list (see the wiki</span>
<span class="quote">&gt; page http://kernsec.org/wiki/index.php/Kernel_Self_Protection_Project for</span>
<span class="quote">&gt; details)</span>

Good idea. Thanks for the suggestion.
<span class="quote">

&gt; If you can find a way to break this up into x86 specific vs. generic patches</span>
<span class="quote">&gt; that would be better. Perhaps move the Kconfig for XPFO to the generic</span>
<span class="quote">&gt; Kconfig layer and make it depend on ARCH_HAS_XPFO? x86 can then select</span>
<span class="quote">&gt; ARCH_HAS_XPFO as the last option.</span>

Good idea.
<span class="quote">

&gt; There also isn&#39;t much that&#39;s actually x86 specific here except for</span>
<span class="quote">&gt; some of the page table manipulation functions and even those can probably</span>
<span class="quote">&gt; be abstracted away. It would be good to get more of this out of x86 to</span>
<span class="quote">&gt; let other arches take advantage of it. The arm64 implementation would</span>
<span class="quote">&gt; look pretty similar if you save the old kernel mapping and restore</span>
<span class="quote">&gt; it on free.</span>

OK. I need to familiarize myself with ARM to figure out which pieces can move
out of the arch subdir.
<span class="quote">

&gt;  </span>
<span class="quote">&gt;&gt; Suggested-by: Vasileios P. Kemerlis &lt;vpk@cs.brown.edu&gt;</span>
<span class="quote">&gt;&gt; Signed-off-by: Juerg Haefliger &lt;juerg.haefliger@hpe.com&gt;</span>
<span class="quote">&gt;&gt; ---</span>
<span class="quote">&gt;&gt;   arch/x86/Kconfig         |   2 +-</span>
<span class="quote">&gt;&gt;   arch/x86/Kconfig.debug   |  17 +++++</span>
<span class="quote">&gt;&gt;   arch/x86/mm/Makefile     |   2 +</span>
<span class="quote">&gt;&gt;   arch/x86/mm/init.c       |   3 +-</span>
<span class="quote">&gt;&gt;   arch/x86/mm/xpfo.c       | 176 +++++++++++++++++++++++++++++++++++++++++++++++</span>
<span class="quote">&gt;&gt;   block/blk-map.c          |   7 +-</span>
<span class="quote">&gt;&gt;   include/linux/highmem.h  |  23 +++++--</span>
<span class="quote">&gt;&gt;   include/linux/mm_types.h |   4 ++</span>
<span class="quote">&gt;&gt;   include/linux/xpfo.h     |  88 ++++++++++++++++++++++++</span>
<span class="quote">&gt;&gt;   lib/swiotlb.c            |   3 +-</span>
<span class="quote">&gt;&gt;   mm/page_alloc.c          |   7 +-</span>
<span class="quote">&gt;&gt;   11 files changed, 323 insertions(+), 9 deletions(-)</span>
<span class="quote">&gt;&gt;   create mode 100644 arch/x86/mm/xpfo.c</span>
<span class="quote">&gt;&gt;   create mode 100644 include/linux/xpfo.h</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig</span>
<span class="quote">&gt;&gt; index c46662f..9d32b4a 100644</span>
<span class="quote">&gt;&gt; --- a/arch/x86/Kconfig</span>
<span class="quote">&gt;&gt; +++ b/arch/x86/Kconfig</span>
<span class="quote">&gt;&gt; @@ -1343,7 +1343,7 @@ config ARCH_DMA_ADDR_T_64BIT</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;   config X86_DIRECT_GBPAGES</span>
<span class="quote">&gt;&gt;       def_bool y</span>
<span class="quote">&gt;&gt; -    depends on X86_64 &amp;&amp; !DEBUG_PAGEALLOC &amp;&amp; !KMEMCHECK</span>
<span class="quote">&gt;&gt; +    depends on X86_64 &amp;&amp; !DEBUG_PAGEALLOC &amp;&amp; !KMEMCHECK &amp;&amp; !XPFO</span>
<span class="quote">&gt;&gt;       ---help---</span>
<span class="quote">&gt;&gt;         Certain kernel features effectively disable kernel</span>
<span class="quote">&gt;&gt;         linear 1 GB mappings (even if the CPU otherwise</span>
<span class="quote">&gt;&gt; diff --git a/arch/x86/Kconfig.debug b/arch/x86/Kconfig.debug</span>
<span class="quote">&gt;&gt; index 9b18ed9..1331da5 100644</span>
<span class="quote">&gt;&gt; --- a/arch/x86/Kconfig.debug</span>
<span class="quote">&gt;&gt; +++ b/arch/x86/Kconfig.debug</span>
<span class="quote">&gt;&gt; @@ -5,6 +5,23 @@ config TRACE_IRQFLAGS_SUPPORT</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;   source &quot;lib/Kconfig.debug&quot;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; +config XPFO</span>
<span class="quote">&gt;&gt; +    bool &quot;Enable eXclusive Page Frame Ownership (XPFO)&quot;</span>
<span class="quote">&gt;&gt; +    default n</span>
<span class="quote">&gt;&gt; +    depends on DEBUG_KERNEL</span>
<span class="quote">&gt;&gt; +    depends on X86_64</span>
<span class="quote">&gt;&gt; +    select DEBUG_TLBFLUSH</span>
<span class="quote">&gt;&gt; +    ---help---</span>
<span class="quote">&gt;&gt; +      This option offers protection against &#39;ret2dir&#39; (kernel) attacks.</span>
<span class="quote">&gt;&gt; +      When enabled, every time a page frame is allocated to user space, it</span>
<span class="quote">&gt;&gt; +      is unmapped from the direct mapped RAM region in kernel space</span>
<span class="quote">&gt;&gt; +      (physmap). Similarly, whenever page frames are freed/reclaimed, they</span>
<span class="quote">&gt;&gt; +      are mapped back to physmap. Special care is taken to minimize the</span>
<span class="quote">&gt;&gt; +      impact on performance by reducing TLB shootdowns and unnecessary page</span>
<span class="quote">&gt;&gt; +      zero fills.</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +      If in doubt, say &quot;N&quot;.</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt;   config X86_VERBOSE_BOOTUP</span>
<span class="quote">&gt;&gt;       bool &quot;Enable verbose x86 bootup info messages&quot;</span>
<span class="quote">&gt;&gt;       default y</span>
<span class="quote">&gt;&gt; diff --git a/arch/x86/mm/Makefile b/arch/x86/mm/Makefile</span>
<span class="quote">&gt;&gt; index f9d38a4..8bf52b6 100644</span>
<span class="quote">&gt;&gt; --- a/arch/x86/mm/Makefile</span>
<span class="quote">&gt;&gt; +++ b/arch/x86/mm/Makefile</span>
<span class="quote">&gt;&gt; @@ -34,3 +34,5 @@ obj-$(CONFIG_ACPI_NUMA)        += srat.o</span>
<span class="quote">&gt;&gt;   obj-$(CONFIG_NUMA_EMU)        += numa_emulation.o</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;   obj-$(CONFIG_X86_INTEL_MPX)    += mpx.o</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +obj-$(CONFIG_XPFO)        += xpfo.o</span>
<span class="quote">&gt;&gt; diff --git a/arch/x86/mm/init.c b/arch/x86/mm/init.c</span>
<span class="quote">&gt;&gt; index 493f541..27fc8a6 100644</span>
<span class="quote">&gt;&gt; --- a/arch/x86/mm/init.c</span>
<span class="quote">&gt;&gt; +++ b/arch/x86/mm/init.c</span>
<span class="quote">&gt;&gt; @@ -150,7 +150,8 @@ static int page_size_mask;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;   static void __init probe_page_size_mask(void)</span>
<span class="quote">&gt;&gt;   {</span>
<span class="quote">&gt;&gt; -#if !defined(CONFIG_DEBUG_PAGEALLOC) &amp;&amp; !defined(CONFIG_KMEMCHECK)</span>
<span class="quote">&gt;&gt; +#if !defined(CONFIG_DEBUG_PAGEALLOC) &amp;&amp; !defined(CONFIG_KMEMCHECK) &amp;&amp; \</span>
<span class="quote">&gt;&gt; +    !defined(CONFIG_XPFO)</span>
<span class="quote">&gt;&gt;       /*</span>
<span class="quote">&gt;&gt;        * For CONFIG_DEBUG_PAGEALLOC, identity mapping will use small pages.</span>
<span class="quote">&gt;&gt;        * This will simplify cpa(), which otherwise needs to support splitting</span>
<span class="quote">&gt;&gt; diff --git a/arch/x86/mm/xpfo.c b/arch/x86/mm/xpfo.c</span>
<span class="quote">&gt;&gt; new file mode 100644</span>
<span class="quote">&gt;&gt; index 0000000..6bc24d3</span>
<span class="quote">&gt;&gt; --- /dev/null</span>
<span class="quote">&gt;&gt; +++ b/arch/x86/mm/xpfo.c</span>
<span class="quote">&gt;&gt; @@ -0,0 +1,176 @@</span>
<span class="quote">&gt;&gt; +/*</span>
<span class="quote">&gt;&gt; + * Copyright (C) 2016 Brown University. All rights reserved.</span>
<span class="quote">&gt;&gt; + * Copyright (C) 2016 Hewlett Packard Enterprise Development, L.P.</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + * Authors:</span>
<span class="quote">&gt;&gt; + *   Vasileios P. Kemerlis &lt;vpk@cs.brown.edu&gt;</span>
<span class="quote">&gt;&gt; + *   Juerg Haefliger &lt;juerg.haefliger@hpe.com&gt;</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + * This program is free software; you can redistribute it and/or modify it</span>
<span class="quote">&gt;&gt; + * under the terms of the GNU General Public License version 2 as published by</span>
<span class="quote">&gt;&gt; + * the Free Software Foundation.</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#include &lt;linux/mm.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;linux/module.h&gt;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#include &lt;asm/pgtable.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;asm/tlbflush.h&gt;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#define TEST_XPFO_FLAG(flag, page) \</span>
<span class="quote">&gt;&gt; +    test_bit(PG_XPFO_##flag, &amp;(page)-&gt;xpfo.flags)</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#define SET_XPFO_FLAG(flag, page)            \</span>
<span class="quote">&gt;&gt; +    __set_bit(PG_XPFO_##flag, &amp;(page)-&gt;xpfo.flags)</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#define CLEAR_XPFO_FLAG(flag, page)            \</span>
<span class="quote">&gt;&gt; +    __clear_bit(PG_XPFO_##flag, &amp;(page)-&gt;xpfo.flags)</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#define TEST_AND_CLEAR_XPFO_FLAG(flag, page)            \</span>
<span class="quote">&gt;&gt; +    __test_and_clear_bit(PG_XPFO_##flag, &amp;(page)-&gt;xpfo.flags)</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +/*</span>
<span class="quote">&gt;&gt; + * Update a single kernel page table entry</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;&gt; +static inline void set_kpte(struct page *page, unsigned long kaddr,</span>
<span class="quote">&gt;&gt; +                pgprot_t prot) {</span>
<span class="quote">&gt;&gt; +    unsigned int level;</span>
<span class="quote">&gt;&gt; +    pte_t *kpte = lookup_address(kaddr, &amp;level);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +    /* We only support 4k pages for now */</span>
<span class="quote">&gt;&gt; +    BUG_ON(!kpte || level != PG_LEVEL_4K);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +    set_pte_atomic(kpte, pfn_pte(page_to_pfn(page), canon_pgprot(prot)));</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +inline void xpfo_clear_zap(struct page *page, int order)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +    int i;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +    for (i = 0; i &lt; (1 &lt;&lt; order); i++)</span>
<span class="quote">&gt;&gt; +        CLEAR_XPFO_FLAG(zap, page + i);</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +inline int xpfo_test_and_clear_zap(struct page *page)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +    return TEST_AND_CLEAR_XPFO_FLAG(zap, page);</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +inline int xpfo_test_kernel(struct page *page)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +    return TEST_XPFO_FLAG(kernel, page);</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +inline int xpfo_test_user(struct page *page)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +    return TEST_XPFO_FLAG(user, page);</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +void xpfo_alloc_page(struct page *page, int order, gfp_t gfp)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +    int i, tlb_shoot = 0;</span>
<span class="quote">&gt;&gt; +    unsigned long kaddr;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +    for (i = 0; i &lt; (1 &lt;&lt; order); i++)  {</span>
<span class="quote">&gt;&gt; +        WARN_ON(TEST_XPFO_FLAG(user_fp, page + i) ||</span>
<span class="quote">&gt;&gt; +            TEST_XPFO_FLAG(user, page + i));</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +        if (gfp &amp; GFP_HIGHUSER) {</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This check doesn&#39;t seem right. If the GFP flags have _any_ in common with</span>
<span class="quote">&gt; GFP_HIGHUSER it will be marked as a user page so GFP_KERNEL will be marked</span>
<span class="quote">&gt; as well.</span>

Duh. You&#39;re right. I broke this when I cleaned up the original patch. It should be:
(gfp &amp; GFP_HIGHUSER) == GFP_HIGHUSER
<span class="quote">


&gt;&gt; +            /* Initialize the xpfo lock and map counter */</span>
<span class="quote">&gt;&gt; +            spin_lock_init(&amp;(page + i)-&gt;xpfo.lock);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This is initializing the spin_lock every time. That&#39;s not really necessary.</span>

Correct. The initialization should probably be done when the page struct is
first allocated. But I haven&#39;t been able to find that piece of code quickly.
Will look again.
<span class="quote">

&gt;&gt; +            atomic_set(&amp;(page + i)-&gt;xpfo.mapcount, 0);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +            /* Mark it as a user page */</span>
<span class="quote">&gt;&gt; +            SET_XPFO_FLAG(user_fp, page + i);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +            /*</span>
<span class="quote">&gt;&gt; +             * Shoot the TLB if the page was previously allocated</span>
<span class="quote">&gt;&gt; +             * to kernel space</span>
<span class="quote">&gt;&gt; +             */</span>
<span class="quote">&gt;&gt; +            if (TEST_AND_CLEAR_XPFO_FLAG(kernel, page + i))</span>
<span class="quote">&gt;&gt; +                tlb_shoot = 1;</span>
<span class="quote">&gt;&gt; +        } else {</span>
<span class="quote">&gt;&gt; +            /* Mark it as a kernel page */</span>
<span class="quote">&gt;&gt; +            SET_XPFO_FLAG(kernel, page + i);</span>
<span class="quote">&gt;&gt; +        }</span>
<span class="quote">&gt;&gt; +    }</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +    if (tlb_shoot) {</span>
<span class="quote">&gt;&gt; +        kaddr = (unsigned long)page_address(page);</span>
<span class="quote">&gt;&gt; +        flush_tlb_kernel_range(kaddr, kaddr + (1 &lt;&lt; order) *</span>
<span class="quote">&gt;&gt; +                       PAGE_SIZE);</span>
<span class="quote">&gt;&gt; +    }</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +void xpfo_free_page(struct page *page, int order)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +    int i;</span>
<span class="quote">&gt;&gt; +    unsigned long kaddr;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +    for (i = 0; i &lt; (1 &lt;&lt; order); i++) {</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +        /* The page frame was previously allocated to user space */</span>
<span class="quote">&gt;&gt; +        if (TEST_AND_CLEAR_XPFO_FLAG(user, page + i)) {</span>
<span class="quote">&gt;&gt; +            kaddr = (unsigned long)page_address(page + i);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +            /* Clear the page and mark it accordingly */</span>
<span class="quote">&gt;&gt; +            clear_page((void *)kaddr);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Clearing the page isn&#39;t related to XPFO. There&#39;s other work ongoing to</span>
<span class="quote">&gt; do clearing of the page on free.</span>

It&#39;s not strictly related to XPFO but adds another layer of security. Do you
happen to have a pointer to the ongoing work that you mentioned?
<span class="quote">

&gt;&gt; +            SET_XPFO_FLAG(zap, page + i);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +            /* Map it back to kernel space */</span>
<span class="quote">&gt;&gt; +            set_kpte(page + i,  kaddr, __pgprot(__PAGE_KERNEL));</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +            /* No TLB update */</span>
<span class="quote">&gt;&gt; +        }</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +        /* Clear the xpfo fast-path flag */</span>
<span class="quote">&gt;&gt; +        CLEAR_XPFO_FLAG(user_fp, page + i);</span>
<span class="quote">&gt;&gt; +    }</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +void xpfo_kmap(void *kaddr, struct page *page)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +    unsigned long flags;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +    /* The page is allocated to kernel space, so nothing to do */</span>
<span class="quote">&gt;&gt; +    if (TEST_XPFO_FLAG(kernel, page))</span>
<span class="quote">&gt;&gt; +        return;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +    spin_lock_irqsave(&amp;page-&gt;xpfo.lock, flags);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +    /*</span>
<span class="quote">&gt;&gt; +     * The page was previously allocated to user space, so map it back</span>
<span class="quote">&gt;&gt; +     * into the kernel. No TLB update required.</span>
<span class="quote">&gt;&gt; +     */</span>
<span class="quote">&gt;&gt; +    if ((atomic_inc_return(&amp;page-&gt;xpfo.mapcount) == 1) &amp;&amp;</span>
<span class="quote">&gt;&gt; +        TEST_XPFO_FLAG(user, page))</span>
<span class="quote">&gt;&gt; +        set_kpte(page, (unsigned long)kaddr, __pgprot(__PAGE_KERNEL));</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +    spin_unlock_irqrestore(&amp;page-&gt;xpfo.lock, flags);</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +EXPORT_SYMBOL(xpfo_kmap);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +void xpfo_kunmap(void *kaddr, struct page *page)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +    unsigned long flags;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +    /* The page is allocated to kernel space, so nothing to do */</span>
<span class="quote">&gt;&gt; +    if (TEST_XPFO_FLAG(kernel, page))</span>
<span class="quote">&gt;&gt; +        return;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +    spin_lock_irqsave(&amp;page-&gt;xpfo.lock, flags);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +    /*</span>
<span class="quote">&gt;&gt; +     * The page frame is to be allocated back to user space. So unmap it</span>
<span class="quote">&gt;&gt; +     * from the kernel, update the TLB and mark it as a user page.</span>
<span class="quote">&gt;&gt; +     */</span>
<span class="quote">&gt;&gt; +    if ((atomic_dec_return(&amp;page-&gt;xpfo.mapcount) == 0) &amp;&amp;</span>
<span class="quote">&gt;&gt; +        (TEST_XPFO_FLAG(user_fp, page) || TEST_XPFO_FLAG(user, page))) {</span>
<span class="quote">&gt;&gt; +        set_kpte(page, (unsigned long)kaddr, __pgprot(0));</span>
<span class="quote">&gt;&gt; +        __flush_tlb_one((unsigned long)kaddr);</span>
<span class="quote">&gt;&gt; +        SET_XPFO_FLAG(user, page);</span>
<span class="quote">&gt;&gt; +    }</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +    spin_unlock_irqrestore(&amp;page-&gt;xpfo.lock, flags);</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +EXPORT_SYMBOL(xpfo_kunmap);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I&#39;m confused by the checks in kmap/kunmap here. It looks like once the</span>
<span class="quote">&gt; page is allocated there is no changing of flags between user and</span>
<span class="quote">&gt; kernel mode so the checks for if the page is user seem redundant.</span>

Hmm... I think you&#39;re partially right. In xpfo_kmap we need to distinguish
between user and user_fp, so the check for &#39;user&#39; is necessary. However, in
kunmap we can drop the check for &#39;user&#39; || &#39;user_fp&#39;.
<span class="quote">

&gt;&gt; diff --git a/block/blk-map.c b/block/blk-map.c</span>
<span class="quote">&gt;&gt; index f565e11..b7b8302 100644</span>
<span class="quote">&gt;&gt; --- a/block/blk-map.c</span>
<span class="quote">&gt;&gt; +++ b/block/blk-map.c</span>
<span class="quote">&gt;&gt; @@ -107,7 +107,12 @@ int blk_rq_map_user_iov(struct request_queue *q, struct</span>
<span class="quote">&gt;&gt; request *rq,</span>
<span class="quote">&gt;&gt;           prv.iov_len = iov.iov_len;</span>
<span class="quote">&gt;&gt;       }</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; -    if (unaligned || (q-&gt;dma_pad_mask &amp; iter-&gt;count) || map_data)</span>
<span class="quote">&gt;&gt; +    /*</span>
<span class="quote">&gt;&gt; +     * juergh: Temporary hack to force the use of a bounce buffer if XPFO</span>
<span class="quote">&gt;&gt; +     * is enabled. Results in an XPFO page fault otherwise.</span>
<span class="quote">&gt;&gt; +     */</span>
<span class="quote">&gt;&gt; +    if (unaligned || (q-&gt;dma_pad_mask &amp; iter-&gt;count) || map_data ||</span>
<span class="quote">&gt;&gt; +        IS_ENABLED(CONFIG_XPFO))</span>
<span class="quote">&gt;&gt;           bio = bio_copy_user_iov(q, map_data, iter, gfp_mask);</span>
<span class="quote">&gt;&gt;       else</span>
<span class="quote">&gt;&gt;           bio = bio_map_user_iov(q, iter, gfp_mask);</span>
<span class="quote">&gt;&gt; diff --git a/include/linux/highmem.h b/include/linux/highmem.h</span>
<span class="quote">&gt;&gt; index bb3f329..0ca9130 100644</span>
<span class="quote">&gt;&gt; --- a/include/linux/highmem.h</span>
<span class="quote">&gt;&gt; +++ b/include/linux/highmem.h</span>
<span class="quote">&gt;&gt; @@ -55,24 +55,37 @@ static inline struct page *kmap_to_page(void *addr)</span>
<span class="quote">&gt;&gt;   #ifndef ARCH_HAS_KMAP</span>
<span class="quote">&gt;&gt;   static inline void *kmap(struct page *page)</span>
<span class="quote">&gt;&gt;   {</span>
<span class="quote">&gt;&gt; +    void *kaddr;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt;       might_sleep();</span>
<span class="quote">&gt;&gt; -    return page_address(page);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +    kaddr = page_address(page);</span>
<span class="quote">&gt;&gt; +    xpfo_kmap(kaddr, page);</span>
<span class="quote">&gt;&gt; +    return kaddr;</span>
<span class="quote">&gt;&gt;   }</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;   static inline void kunmap(struct page *page)</span>
<span class="quote">&gt;&gt;   {</span>
<span class="quote">&gt;&gt; +    xpfo_kunmap(page_address(page), page);</span>
<span class="quote">&gt;&gt;   }</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;   static inline void *kmap_atomic(struct page *page)</span>
<span class="quote">&gt;&gt;   {</span>
<span class="quote">&gt;&gt; +    void *kaddr;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt;       preempt_disable();</span>
<span class="quote">&gt;&gt;       pagefault_disable();</span>
<span class="quote">&gt;&gt; -    return page_address(page);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +    kaddr = page_address(page);</span>
<span class="quote">&gt;&gt; +    xpfo_kmap(kaddr, page);</span>
<span class="quote">&gt;&gt; +    return kaddr;</span>
<span class="quote">&gt;&gt;   }</span>
<span class="quote">&gt;&gt;   #define kmap_atomic_prot(page, prot)    kmap_atomic(page)</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;   static inline void __kunmap_atomic(void *addr)</span>
<span class="quote">&gt;&gt;   {</span>
<span class="quote">&gt;&gt; +    xpfo_kunmap(addr, virt_to_page(addr));</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt;       pagefault_enable();</span>
<span class="quote">&gt;&gt;       preempt_enable();</span>
<span class="quote">&gt;&gt;   }</span>
<span class="quote">&gt;&gt; @@ -133,7 +146,8 @@ do</span>
<span class="quote">&gt;&gt; {                                                            \</span>
<span class="quote">&gt;&gt;   static inline void clear_user_highpage(struct page *page, unsigned long vaddr)</span>
<span class="quote">&gt;&gt;   {</span>
<span class="quote">&gt;&gt;       void *addr = kmap_atomic(page);</span>
<span class="quote">&gt;&gt; -    clear_user_page(addr, vaddr, page);</span>
<span class="quote">&gt;&gt; +    if (!xpfo_test_and_clear_zap(page))</span>
<span class="quote">&gt;&gt; +        clear_user_page(addr, vaddr, page);</span>
<span class="quote">&gt;&gt;       kunmap_atomic(addr);</span>
<span class="quote">&gt;&gt;   }</span>
<span class="quote">&gt;&gt;   #endif</span>
<span class="quote">&gt;&gt; @@ -186,7 +200,8 @@ alloc_zeroed_user_highpage_movable(struct vm_area_struct</span>
<span class="quote">&gt;&gt; *vma,</span>
<span class="quote">&gt;&gt;   static inline void clear_highpage(struct page *page)</span>
<span class="quote">&gt;&gt;   {</span>
<span class="quote">&gt;&gt;       void *kaddr = kmap_atomic(page);</span>
<span class="quote">&gt;&gt; -    clear_page(kaddr);</span>
<span class="quote">&gt;&gt; +    if (!xpfo_test_and_clear_zap(page))</span>
<span class="quote">&gt;&gt; +        clear_page(kaddr);</span>
<span class="quote">&gt;&gt;       kunmap_atomic(kaddr);</span>
<span class="quote">&gt;&gt;   }</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h</span>
<span class="quote">&gt;&gt; index 624b78b..71c95aa 100644</span>
<span class="quote">&gt;&gt; --- a/include/linux/mm_types.h</span>
<span class="quote">&gt;&gt; +++ b/include/linux/mm_types.h</span>
<span class="quote">&gt;&gt; @@ -12,6 +12,7 @@</span>
<span class="quote">&gt;&gt;   #include &lt;linux/cpumask.h&gt;</span>
<span class="quote">&gt;&gt;   #include &lt;linux/uprobes.h&gt;</span>
<span class="quote">&gt;&gt;   #include &lt;linux/page-flags-layout.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;linux/xpfo.h&gt;</span>
<span class="quote">&gt;&gt;   #include &lt;asm/page.h&gt;</span>
<span class="quote">&gt;&gt;   #include &lt;asm/mmu.h&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; @@ -215,6 +216,9 @@ struct page {</span>
<span class="quote">&gt;&gt;   #ifdef LAST_CPUPID_NOT_IN_PAGE_FLAGS</span>
<span class="quote">&gt;&gt;       int _last_cpupid;</span>
<span class="quote">&gt;&gt;   #endif</span>
<span class="quote">&gt;&gt; +#ifdef CONFIG_XPFO</span>
<span class="quote">&gt;&gt; +    struct xpfo_info xpfo;</span>
<span class="quote">&gt;&gt; +#endif</span>
<span class="quote">&gt;&gt;   }</span>
<span class="quote">&gt;&gt;   /*</span>
<span class="quote">&gt;&gt;    * The struct page can be forced to be double word aligned so that atomic ops</span>
<span class="quote">&gt;&gt; diff --git a/include/linux/xpfo.h b/include/linux/xpfo.h</span>
<span class="quote">&gt;&gt; new file mode 100644</span>
<span class="quote">&gt;&gt; index 0000000..c4f0871</span>
<span class="quote">&gt;&gt; --- /dev/null</span>
<span class="quote">&gt;&gt; +++ b/include/linux/xpfo.h</span>
<span class="quote">&gt;&gt; @@ -0,0 +1,88 @@</span>
<span class="quote">&gt;&gt; +/*</span>
<span class="quote">&gt;&gt; + * Copyright (C) 2016 Brown University. All rights reserved.</span>
<span class="quote">&gt;&gt; + * Copyright (C) 2016 Hewlett Packard Enterprise Development, L.P.</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + * Authors:</span>
<span class="quote">&gt;&gt; + *   Vasileios P. Kemerlis &lt;vpk@cs.brown.edu&gt;</span>
<span class="quote">&gt;&gt; + *   Juerg Haefliger &lt;juerg.haefliger@hpe.com&gt;</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + * This program is free software; you can redistribute it and/or modify it</span>
<span class="quote">&gt;&gt; + * under the terms of the GNU General Public License version 2 as published by</span>
<span class="quote">&gt;&gt; + * the Free Software Foundation.</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#ifndef _LINUX_XPFO_H</span>
<span class="quote">&gt;&gt; +#define _LINUX_XPFO_H</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#ifdef CONFIG_XPFO</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +/*</span>
<span class="quote">&gt;&gt; + * XPFO page flags:</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + * PG_XPFO_user_fp denotes that the page is allocated to user space. This flag</span>
<span class="quote">&gt;&gt; + * is used in the fast path, where the page is marked accordingly but *not*</span>
<span class="quote">&gt;&gt; + * unmapped from the kernel. In most cases, the kernel will need access to the</span>
<span class="quote">&gt;&gt; + * page immediately after its acquisition so an unnecessary mapping operation</span>
<span class="quote">&gt;&gt; + * is avoided.</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + * PG_XPFO_user denotes that the page is destined for user space. This flag is</span>
<span class="quote">&gt;&gt; + * used in the slow path, where the page needs to be mapped/unmapped when the</span>
<span class="quote">&gt;&gt; + * kernel wants to access it. If a page is deallocated and this flag is set,</span>
<span class="quote">&gt;&gt; + * the page is cleared and mapped back into the kernel.</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + * PG_XPFO_kernel denotes a page that is destined to kernel space. This is used</span>
<span class="quote">&gt;&gt; + * for identifying pages that are first assigned to kernel space and then freed</span>
<span class="quote">&gt;&gt; + * and mapped to user space. In such cases, an expensive TLB shootdown is</span>
<span class="quote">&gt;&gt; + * necessary. Pages allocated to user space, freed, and subsequently allocated</span>
<span class="quote">&gt;&gt; + * to user space again, require only local TLB invalidation.</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + * PG_XPFO_zap indicates that the page has been zapped. This flag is used to</span>
<span class="quote">&gt;&gt; + * avoid zapping pages multiple times. Whenever a page is freed and was</span>
<span class="quote">&gt;&gt; + * previously mapped to user space, it needs to be zapped before mapped back</span>
<span class="quote">&gt;&gt; + * in to the kernel.</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &#39;zap&#39; doesn&#39;t really indicate what is actually happening with the page. Can you</span>
<span class="quote">&gt; be a bit more descriptive about what this actually does?</span>

It means that the page has been cleared at the time it was released back to the
free pool. To prevent multiple expensive cleaning operations. But this might go
away because of the ongoing work of sanitizing pages that you mentioned.
<span class="quote">

&gt;&gt; +</span>
<span class="quote">&gt;&gt; +enum xpfo_pageflags {</span>
<span class="quote">&gt;&gt; +    PG_XPFO_user_fp,</span>
<span class="quote">&gt;&gt; +    PG_XPFO_user,</span>
<span class="quote">&gt;&gt; +    PG_XPFO_kernel,</span>
<span class="quote">&gt;&gt; +    PG_XPFO_zap,</span>
<span class="quote">&gt;&gt; +};</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +struct xpfo_info {</span>
<span class="quote">&gt;&gt; +    unsigned long flags;    /* Flags for tracking the page&#39;s XPFO state */</span>
<span class="quote">&gt;&gt; +    atomic_t mapcount;    /* Counter for balancing page map/unmap</span>
<span class="quote">&gt;&gt; +                 * requests. Only the first map request maps</span>
<span class="quote">&gt;&gt; +                 * the page back to kernel space. Likewise,</span>
<span class="quote">&gt;&gt; +                 * only the last unmap request unmaps the page.</span>
<span class="quote">&gt;&gt; +                 */</span>
<span class="quote">&gt;&gt; +    spinlock_t lock;    /* Lock to serialize concurrent map/unmap</span>
<span class="quote">&gt;&gt; +                 * requests.</span>
<span class="quote">&gt;&gt; +                 */</span>
<span class="quote">&gt;&gt; +};</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Can you change this to use the page_ext implementation? See what</span>
<span class="quote">&gt; mm/page_owner.c does. This might lessen the impact of the extra</span>
<span class="quote">&gt; page metadata. This metadata still feels like a copy of what</span>
<span class="quote">&gt; mm/highmem.c is trying to do though.</span>

I&#39;ll look into that, thanks for the pointer.
<span class="quote">

&gt;&gt; +</span>
<span class="quote">&gt;&gt; +extern void xpfo_clear_zap(struct page *page, int order);</span>
<span class="quote">&gt;&gt; +extern int xpfo_test_and_clear_zap(struct page *page);</span>
<span class="quote">&gt;&gt; +extern int xpfo_test_kernel(struct page *page);</span>
<span class="quote">&gt;&gt; +extern int xpfo_test_user(struct page *page);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +extern void xpfo_kmap(void *kaddr, struct page *page);</span>
<span class="quote">&gt;&gt; +extern void xpfo_kunmap(void *kaddr, struct page *page);</span>
<span class="quote">&gt;&gt; +extern void xpfo_alloc_page(struct page *page, int order, gfp_t gfp);</span>
<span class="quote">&gt;&gt; +extern void xpfo_free_page(struct page *page, int order);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#else /* ifdef CONFIG_XPFO */</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +static inline void xpfo_clear_zap(struct page *page, int order) { }</span>
<span class="quote">&gt;&gt; +static inline int xpfo_test_and_clear_zap(struct page *page) { return 0; }</span>
<span class="quote">&gt;&gt; +static inline int xpfo_test_kernel(struct page *page) { return 0; }</span>
<span class="quote">&gt;&gt; +static inline int xpfo_test_user(struct page *page) { return 0; }</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +static inline void xpfo_kmap(void *kaddr, struct page *page) { }</span>
<span class="quote">&gt;&gt; +static inline void xpfo_kunmap(void *kaddr, struct page *page) { }</span>
<span class="quote">&gt;&gt; +static inline void xpfo_alloc_page(struct page *page, int order, gfp_t gfp) { }</span>
<span class="quote">&gt;&gt; +static inline void xpfo_free_page(struct page *page, int order) { }</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#endif /* ifdef CONFIG_XPFO */</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#endif /* ifndef _LINUX_XPFO_H */</span>
<span class="quote">&gt;&gt; diff --git a/lib/swiotlb.c b/lib/swiotlb.c</span>
<span class="quote">&gt;&gt; index 76f29ec..cf57ee9 100644</span>
<span class="quote">&gt;&gt; --- a/lib/swiotlb.c</span>
<span class="quote">&gt;&gt; +++ b/lib/swiotlb.c</span>
<span class="quote">&gt;&gt; @@ -390,8 +390,9 @@ static void swiotlb_bounce(phys_addr_t orig_addr,</span>
<span class="quote">&gt;&gt; phys_addr_t tlb_addr,</span>
<span class="quote">&gt;&gt;   {</span>
<span class="quote">&gt;&gt;       unsigned long pfn = PFN_DOWN(orig_addr);</span>
<span class="quote">&gt;&gt;       unsigned char *vaddr = phys_to_virt(tlb_addr);</span>
<span class="quote">&gt;&gt; +    struct page *page = pfn_to_page(pfn);</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; -    if (PageHighMem(pfn_to_page(pfn))) {</span>
<span class="quote">&gt;&gt; +    if (PageHighMem(page) || xpfo_test_user(page)) {</span>
<span class="quote">&gt;&gt;           /* The buffer does not have a mapping.  Map it in and copy */</span>
<span class="quote">&gt;&gt;           unsigned int offset = orig_addr &amp; ~PAGE_MASK;</span>
<span class="quote">&gt;&gt;           char *buffer;</span>
<span class="quote">&gt;&gt; diff --git a/mm/page_alloc.c b/mm/page_alloc.c</span>
<span class="quote">&gt;&gt; index 838ca8bb..47b42a3 100644</span>
<span class="quote">&gt;&gt; --- a/mm/page_alloc.c</span>
<span class="quote">&gt;&gt; +++ b/mm/page_alloc.c</span>
<span class="quote">&gt;&gt; @@ -1003,6 +1003,7 @@ static bool free_pages_prepare(struct page *page,</span>
<span class="quote">&gt;&gt; unsigned int order)</span>
<span class="quote">&gt;&gt;       }</span>
<span class="quote">&gt;&gt;       arch_free_page(page, order);</span>
<span class="quote">&gt;&gt;       kernel_map_pages(page, 1 &lt;&lt; order, 0);</span>
<span class="quote">&gt;&gt; +    xpfo_free_page(page, order);</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;       return true;</span>
<span class="quote">&gt;&gt;   }</span>
<span class="quote">&gt;&gt; @@ -1398,10 +1399,13 @@ static int prep_new_page(struct page *page, unsigned</span>
<span class="quote">&gt;&gt; int order, gfp_t gfp_flags,</span>
<span class="quote">&gt;&gt;       arch_alloc_page(page, order);</span>
<span class="quote">&gt;&gt;       kernel_map_pages(page, 1 &lt;&lt; order, 1);</span>
<span class="quote">&gt;&gt;       kasan_alloc_pages(page, order);</span>
<span class="quote">&gt;&gt; +    xpfo_alloc_page(page, order, gfp_flags);</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;       if (gfp_flags &amp; __GFP_ZERO)</span>
<span class="quote">&gt;&gt;           for (i = 0; i &lt; (1 &lt;&lt; order); i++)</span>
<span class="quote">&gt;&gt;               clear_highpage(page + i);</span>
<span class="quote">&gt;&gt; +    else</span>
<span class="quote">&gt;&gt; +        xpfo_clear_zap(page, order);</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;       if (order &amp;&amp; (gfp_flags &amp; __GFP_COMP))</span>
<span class="quote">&gt;&gt;           prep_compound_page(page, order);</span>
<span class="quote">&gt;&gt; @@ -2072,10 +2076,11 @@ void free_hot_cold_page(struct page *page, bool cold)</span>
<span class="quote">&gt;&gt;       }</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;       pcp = &amp;this_cpu_ptr(zone-&gt;pageset)-&gt;pcp;</span>
<span class="quote">&gt;&gt; -    if (!cold)</span>
<span class="quote">&gt;&gt; +    if (!cold &amp;&amp; !xpfo_test_kernel(page))</span>
<span class="quote">&gt;&gt;           list_add(&amp;page-&gt;lru, &amp;pcp-&gt;lists[migratetype]);</span>
<span class="quote">&gt;&gt;       else</span>
<span class="quote">&gt;&gt;           list_add_tail(&amp;page-&gt;lru, &amp;pcp-&gt;lists[migratetype]);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; What&#39;s the advantage of this?</span>

Allocating a page to userspace that was previously allocated to kernel space
requires an expensive TLB shootdown. The above will put previously
kernel-allocated pages in the cold page cache to postpone their allocation as
long as possible to minimize TLB shootdowns.
<span class="quote">

&gt;&gt;       pcp-&gt;count++;</span>
<span class="quote">&gt;&gt;       if (pcp-&gt;count &gt;= pcp-&gt;high) {</span>
<span class="quote">&gt;&gt;           unsigned long batch = READ_ONCE(pcp-&gt;batch);</span>
<span class="quote">&gt;&gt;</span>

Thanks for the review and comments! It&#39;s highly appreciated.

...Juerg
<span class="quote">

&gt; Thanks,</span>
<span class="quote">&gt; Laura</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=153531">Juerg Haefliger</a> - March 21, 2016, 8:44 a.m.</div>
<pre class="content">
Hi Balbir,

Apologies for the slow reply.


On 03/01/2016 03:10 AM, Balbir Singh wrote:
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; On 27/02/16 01:21, Juerg Haefliger wrote:</span>
<span class="quote">&gt;&gt; This patch adds support for XPFO which protects against &#39;ret2dir&#39; kernel</span>
<span class="quote">&gt;&gt; attacks. The basic idea is to enforce exclusive ownership of page frames</span>
<span class="quote">&gt;&gt; by either the kernel or userland, unless explicitly requested by the</span>
<span class="quote">&gt;&gt; kernel. Whenever a page destined for userland is allocated, it is</span>
<span class="quote">&gt;&gt; unmapped from physmap. When such a page is reclaimed from userland, it is</span>
<span class="quote">&gt;&gt; mapped back to physmap.</span>
<span class="quote">&gt; physmap == xen physmap? Please clarify</span>

No, it&#39;s not XEN related. I might have the terminology wrong. Physmap is what
the original authors used for describing &lt;quote&gt; a large, contiguous virtual
memory region inside kernel address space that contains a direct mapping of part
or all (depending on the architecture) physical memory. &lt;/quote&gt;
<span class="quote">

&gt;&gt; Mapping/unmapping from physmap is accomplished by modifying the PTE</span>
<span class="quote">&gt;&gt; permission bits to allow/disallow access to the page.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Additional fields are added to the page struct for XPFO housekeeping.</span>
<span class="quote">&gt;&gt; Specifically a flags field to distinguish user vs. kernel pages, a</span>
<span class="quote">&gt;&gt; reference counter to track physmap map/unmap operations and a lock to</span>
<span class="quote">&gt;&gt; protect the XPFO fields.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Known issues/limitations:</span>
<span class="quote">&gt;&gt;   - Only supported on x86-64.</span>
<span class="quote">&gt; Is it due to lack of porting or a design limitation?</span>

Lack of porting. Support for other architectures will come later.
<span class="quote">

&gt;&gt;   - Only supports 4k pages.</span>
<span class="quote">&gt;&gt;   - Adds additional data to the page struct.</span>
<span class="quote">&gt;&gt;   - There are most likely some additional and legitimate uses cases where</span>
<span class="quote">&gt;&gt;     the kernel needs to access userspace. Those need to be identified and</span>
<span class="quote">&gt;&gt;     made XPFO-aware.</span>
<span class="quote">&gt; Why not build an audit mode for it?</span>

Can you elaborate what you mean by this?
<span class="quote">

&gt;&gt;   - There&#39;s a performance impact if XPFO is turned on. Per the paper</span>
<span class="quote">&gt;&gt;     referenced below it&#39;s in the 1-3% ballpark. More performance testing</span>
<span class="quote">&gt;&gt;     wouldn&#39;t hurt. What tests to run though?</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Reference paper by the original patch authors:</span>
<span class="quote">&gt;&gt;   http://www.cs.columbia.edu/~vpk/papers/ret2dir.sec14.pdf</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Suggested-by: Vasileios P. Kemerlis &lt;vpk@cs.brown.edu&gt;</span>
<span class="quote">&gt;&gt; Signed-off-by: Juerg Haefliger &lt;juerg.haefliger@hpe.com&gt;</span>
<span class="quote">&gt; This patch needs to be broken down into smaller patches - a series</span>

Agreed.
<span class="quote">

&gt;&gt; ---</span>
<span class="quote">&gt;&gt;  arch/x86/Kconfig         |   2 +-</span>
<span class="quote">&gt;&gt;  arch/x86/Kconfig.debug   |  17 +++++</span>
<span class="quote">&gt;&gt;  arch/x86/mm/Makefile     |   2 +</span>
<span class="quote">&gt;&gt;  arch/x86/mm/init.c       |   3 +-</span>
<span class="quote">&gt;&gt;  arch/x86/mm/xpfo.c       | 176 +++++++++++++++++++++++++++++++++++++++++++++++</span>
<span class="quote">&gt;&gt;  block/blk-map.c          |   7 +-</span>
<span class="quote">&gt;&gt;  include/linux/highmem.h  |  23 +++++--</span>
<span class="quote">&gt;&gt;  include/linux/mm_types.h |   4 ++</span>
<span class="quote">&gt;&gt;  include/linux/xpfo.h     |  88 ++++++++++++++++++++++++</span>
<span class="quote">&gt;&gt;  lib/swiotlb.c            |   3 +-</span>
<span class="quote">&gt;&gt;  mm/page_alloc.c          |   7 +-</span>
<span class="quote">&gt;&gt;  11 files changed, 323 insertions(+), 9 deletions(-)</span>
<span class="quote">&gt;&gt;  create mode 100644 arch/x86/mm/xpfo.c</span>
<span class="quote">&gt;&gt;  create mode 100644 include/linux/xpfo.h</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig</span>
<span class="quote">&gt;&gt; index c46662f..9d32b4a 100644</span>
<span class="quote">&gt;&gt; --- a/arch/x86/Kconfig</span>
<span class="quote">&gt;&gt; +++ b/arch/x86/Kconfig</span>
<span class="quote">&gt;&gt; @@ -1343,7 +1343,7 @@ config ARCH_DMA_ADDR_T_64BIT</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  config X86_DIRECT_GBPAGES</span>
<span class="quote">&gt;&gt;  	def_bool y</span>
<span class="quote">&gt;&gt; -	depends on X86_64 &amp;&amp; !DEBUG_PAGEALLOC &amp;&amp; !KMEMCHECK</span>
<span class="quote">&gt;&gt; +	depends on X86_64 &amp;&amp; !DEBUG_PAGEALLOC &amp;&amp; !KMEMCHECK &amp;&amp; !XPFO</span>
<span class="quote">&gt;&gt;  	---help---</span>
<span class="quote">&gt;&gt;  	  Certain kernel features effectively disable kernel</span>
<span class="quote">&gt;&gt;  	  linear 1 GB mappings (even if the CPU otherwise</span>
<span class="quote">&gt;&gt; diff --git a/arch/x86/Kconfig.debug b/arch/x86/Kconfig.debug</span>
<span class="quote">&gt;&gt; index 9b18ed9..1331da5 100644</span>
<span class="quote">&gt;&gt; --- a/arch/x86/Kconfig.debug</span>
<span class="quote">&gt;&gt; +++ b/arch/x86/Kconfig.debug</span>
<span class="quote">&gt;&gt; @@ -5,6 +5,23 @@ config TRACE_IRQFLAGS_SUPPORT</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  source &quot;lib/Kconfig.debug&quot;</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; +config XPFO</span>
<span class="quote">&gt;&gt; +	bool &quot;Enable eXclusive Page Frame Ownership (XPFO)&quot;</span>
<span class="quote">&gt;&gt; +	default n</span>
<span class="quote">&gt;&gt; +	depends on DEBUG_KERNEL</span>
<span class="quote">&gt;&gt; +	depends on X86_64</span>
<span class="quote">&gt;&gt; +	select DEBUG_TLBFLUSH</span>
<span class="quote">&gt;&gt; +	---help---</span>
<span class="quote">&gt;&gt; +	  This option offers protection against &#39;ret2dir&#39; (kernel) attacks.</span>
<span class="quote">&gt;&gt; +	  When enabled, every time a page frame is allocated to user space, it</span>
<span class="quote">&gt;&gt; +	  is unmapped from the direct mapped RAM region in kernel space</span>
<span class="quote">&gt;&gt; +	  (physmap). Similarly, whenever page frames are freed/reclaimed, they</span>
<span class="quote">&gt;&gt; +	  are mapped back to physmap. Special care is taken to minimize the</span>
<span class="quote">&gt;&gt; +	  impact on performance by reducing TLB shootdowns and unnecessary page</span>
<span class="quote">&gt;&gt; +	  zero fills.</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	  If in doubt, say &quot;N&quot;.</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt;  config X86_VERBOSE_BOOTUP</span>
<span class="quote">&gt;&gt;  	bool &quot;Enable verbose x86 bootup info messages&quot;</span>
<span class="quote">&gt;&gt;  	default y</span>
<span class="quote">&gt;&gt; diff --git a/arch/x86/mm/Makefile b/arch/x86/mm/Makefile</span>
<span class="quote">&gt;&gt; index f9d38a4..8bf52b6 100644</span>
<span class="quote">&gt;&gt; --- a/arch/x86/mm/Makefile</span>
<span class="quote">&gt;&gt; +++ b/arch/x86/mm/Makefile</span>
<span class="quote">&gt;&gt; @@ -34,3 +34,5 @@ obj-$(CONFIG_ACPI_NUMA)		+= srat.o</span>
<span class="quote">&gt;&gt;  obj-$(CONFIG_NUMA_EMU)		+= numa_emulation.o</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  obj-$(CONFIG_X86_INTEL_MPX)	+= mpx.o</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +obj-$(CONFIG_XPFO)		+= xpfo.o</span>
<span class="quote">&gt;&gt; diff --git a/arch/x86/mm/init.c b/arch/x86/mm/init.c</span>
<span class="quote">&gt;&gt; index 493f541..27fc8a6 100644</span>
<span class="quote">&gt;&gt; --- a/arch/x86/mm/init.c</span>
<span class="quote">&gt;&gt; +++ b/arch/x86/mm/init.c</span>
<span class="quote">&gt;&gt; @@ -150,7 +150,8 @@ static int page_size_mask;</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  static void __init probe_page_size_mask(void)</span>
<span class="quote">&gt;&gt;  {</span>
<span class="quote">&gt;&gt; -#if !defined(CONFIG_DEBUG_PAGEALLOC) &amp;&amp; !defined(CONFIG_KMEMCHECK)</span>
<span class="quote">&gt;&gt; +#if !defined(CONFIG_DEBUG_PAGEALLOC) &amp;&amp; !defined(CONFIG_KMEMCHECK) &amp;&amp; \</span>
<span class="quote">&gt;&gt; +	!defined(CONFIG_XPFO)</span>
<span class="quote">&gt;&gt;  	/*</span>
<span class="quote">&gt;&gt;  	 * For CONFIG_DEBUG_PAGEALLOC, identity mapping will use small pages.</span>
<span class="quote">&gt;&gt;  	 * This will simplify cpa(), which otherwise needs to support splitting</span>
<span class="quote">&gt;&gt; diff --git a/arch/x86/mm/xpfo.c b/arch/x86/mm/xpfo.c</span>
<span class="quote">&gt;&gt; new file mode 100644</span>
<span class="quote">&gt;&gt; index 0000000..6bc24d3</span>
<span class="quote">&gt;&gt; --- /dev/null</span>
<span class="quote">&gt;&gt; +++ b/arch/x86/mm/xpfo.c</span>
<span class="quote">&gt;&gt; @@ -0,0 +1,176 @@</span>
<span class="quote">&gt;&gt; +/*</span>
<span class="quote">&gt;&gt; + * Copyright (C) 2016 Brown University. All rights reserved.</span>
<span class="quote">&gt;&gt; + * Copyright (C) 2016 Hewlett Packard Enterprise Development, L.P.</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + * Authors:</span>
<span class="quote">&gt;&gt; + *   Vasileios P. Kemerlis &lt;vpk@cs.brown.edu&gt;</span>
<span class="quote">&gt;&gt; + *   Juerg Haefliger &lt;juerg.haefliger@hpe.com&gt;</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + * This program is free software; you can redistribute it and/or modify it</span>
<span class="quote">&gt;&gt; + * under the terms of the GNU General Public License version 2 as published by</span>
<span class="quote">&gt;&gt; + * the Free Software Foundation.</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#include &lt;linux/mm.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;linux/module.h&gt;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#include &lt;asm/pgtable.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;asm/tlbflush.h&gt;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#define TEST_XPFO_FLAG(flag, page) \</span>
<span class="quote">&gt;&gt; +	test_bit(PG_XPFO_##flag, &amp;(page)-&gt;xpfo.flags)</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#define SET_XPFO_FLAG(flag, page)			\</span>
<span class="quote">&gt;&gt; +	__set_bit(PG_XPFO_##flag, &amp;(page)-&gt;xpfo.flags)</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#define CLEAR_XPFO_FLAG(flag, page)			\</span>
<span class="quote">&gt;&gt; +	__clear_bit(PG_XPFO_##flag, &amp;(page)-&gt;xpfo.flags)</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#define TEST_AND_CLEAR_XPFO_FLAG(flag, page)			\</span>
<span class="quote">&gt;&gt; +	__test_and_clear_bit(PG_XPFO_##flag, &amp;(page)-&gt;xpfo.flags)</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +/*</span>
<span class="quote">&gt;&gt; + * Update a single kernel page table entry</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;&gt; +static inline void set_kpte(struct page *page, unsigned long kaddr,</span>
<span class="quote">&gt;&gt; +			    pgprot_t prot) {</span>
<span class="quote">&gt;&gt; +	unsigned int level;</span>
<span class="quote">&gt;&gt; +	pte_t *kpte = lookup_address(kaddr, &amp;level);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	/* We only support 4k pages for now */</span>
<span class="quote">&gt;&gt; +	BUG_ON(!kpte || level != PG_LEVEL_4K);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	set_pte_atomic(kpte, pfn_pte(page_to_pfn(page), canon_pgprot(prot)));</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +inline void xpfo_clear_zap(struct page *page, int order)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	int i;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	for (i = 0; i &lt; (1 &lt;&lt; order); i++)</span>
<span class="quote">&gt;&gt; +		CLEAR_XPFO_FLAG(zap, page + i);</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +inline int xpfo_test_and_clear_zap(struct page *page)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	return TEST_AND_CLEAR_XPFO_FLAG(zap, page);</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +inline int xpfo_test_kernel(struct page *page)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	return TEST_XPFO_FLAG(kernel, page);</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +inline int xpfo_test_user(struct page *page)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	return TEST_XPFO_FLAG(user, page);</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +void xpfo_alloc_page(struct page *page, int order, gfp_t gfp)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	int i, tlb_shoot = 0;</span>
<span class="quote">&gt;&gt; +	unsigned long kaddr;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	for (i = 0; i &lt; (1 &lt;&lt; order); i++)  {</span>
<span class="quote">&gt;&gt; +		WARN_ON(TEST_XPFO_FLAG(user_fp, page + i) ||</span>
<span class="quote">&gt;&gt; +			TEST_XPFO_FLAG(user, page + i));</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +		if (gfp &amp; GFP_HIGHUSER) {</span>
<span class="quote">&gt; Why GFP_HIGHUSER?</span>

The check is wrong. It should be ((gfp &amp; GFP_HIGHUSER) == GFP_HIGHUSER).

Thanks
...Juerg
<span class="quote">

&gt;&gt; +			/* Initialize the xpfo lock and map counter */</span>
<span class="quote">&gt;&gt; +			spin_lock_init(&amp;(page + i)-&gt;xpfo.lock);</span>
<span class="quote">&gt;&gt; +			atomic_set(&amp;(page + i)-&gt;xpfo.mapcount, 0);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +			/* Mark it as a user page */</span>
<span class="quote">&gt;&gt; +			SET_XPFO_FLAG(user_fp, page + i);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +			/*</span>
<span class="quote">&gt;&gt; +			 * Shoot the TLB if the page was previously allocated</span>
<span class="quote">&gt;&gt; +			 * to kernel space</span>
<span class="quote">&gt;&gt; +			 */</span>
<span class="quote">&gt;&gt; +			if (TEST_AND_CLEAR_XPFO_FLAG(kernel, page + i))</span>
<span class="quote">&gt;&gt; +				tlb_shoot = 1;</span>
<span class="quote">&gt;&gt; +		} else {</span>
<span class="quote">&gt;&gt; +			/* Mark it as a kernel page */</span>
<span class="quote">&gt;&gt; +			SET_XPFO_FLAG(kernel, page + i);</span>
<span class="quote">&gt;&gt; +		}</span>
<span class="quote">&gt;&gt; +	}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	if (tlb_shoot) {</span>
<span class="quote">&gt;&gt; +		kaddr = (unsigned long)page_address(page);</span>
<span class="quote">&gt;&gt; +		flush_tlb_kernel_range(kaddr, kaddr + (1 &lt;&lt; order) *</span>
<span class="quote">&gt;&gt; +				       PAGE_SIZE);</span>
<span class="quote">&gt;&gt; +	}</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +void xpfo_free_page(struct page *page, int order)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	int i;</span>
<span class="quote">&gt;&gt; +	unsigned long kaddr;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	for (i = 0; i &lt; (1 &lt;&lt; order); i++) {</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +		/* The page frame was previously allocated to user space */</span>
<span class="quote">&gt;&gt; +		if (TEST_AND_CLEAR_XPFO_FLAG(user, page + i)) {</span>
<span class="quote">&gt;&gt; +			kaddr = (unsigned long)page_address(page + i);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +			/* Clear the page and mark it accordingly */</span>
<span class="quote">&gt;&gt; +			clear_page((void *)kaddr);</span>
<span class="quote">&gt;&gt; +			SET_XPFO_FLAG(zap, page + i);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +			/* Map it back to kernel space */</span>
<span class="quote">&gt;&gt; +			set_kpte(page + i,  kaddr, __pgprot(__PAGE_KERNEL));</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +			/* No TLB update */</span>
<span class="quote">&gt;&gt; +		}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +		/* Clear the xpfo fast-path flag */</span>
<span class="quote">&gt;&gt; +		CLEAR_XPFO_FLAG(user_fp, page + i);</span>
<span class="quote">&gt;&gt; +	}</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +void xpfo_kmap(void *kaddr, struct page *page)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	unsigned long flags;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	/* The page is allocated to kernel space, so nothing to do */</span>
<span class="quote">&gt;&gt; +	if (TEST_XPFO_FLAG(kernel, page))</span>
<span class="quote">&gt;&gt; +		return;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	spin_lock_irqsave(&amp;page-&gt;xpfo.lock, flags);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	/*</span>
<span class="quote">&gt;&gt; +	 * The page was previously allocated to user space, so map it back</span>
<span class="quote">&gt;&gt; +	 * into the kernel. No TLB update required.</span>
<span class="quote">&gt;&gt; +	 */</span>
<span class="quote">&gt;&gt; +	if ((atomic_inc_return(&amp;page-&gt;xpfo.mapcount) == 1) &amp;&amp;</span>
<span class="quote">&gt;&gt; +	    TEST_XPFO_FLAG(user, page))</span>
<span class="quote">&gt;&gt; +		set_kpte(page, (unsigned long)kaddr, __pgprot(__PAGE_KERNEL));</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	spin_unlock_irqrestore(&amp;page-&gt;xpfo.lock, flags);</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +EXPORT_SYMBOL(xpfo_kmap);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +void xpfo_kunmap(void *kaddr, struct page *page)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	unsigned long flags;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	/* The page is allocated to kernel space, so nothing to do */</span>
<span class="quote">&gt;&gt; +	if (TEST_XPFO_FLAG(kernel, page))</span>
<span class="quote">&gt;&gt; +		return;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	spin_lock_irqsave(&amp;page-&gt;xpfo.lock, flags);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	/*</span>
<span class="quote">&gt;&gt; +	 * The page frame is to be allocated back to user space. So unmap it</span>
<span class="quote">&gt;&gt; +	 * from the kernel, update the TLB and mark it as a user page.</span>
<span class="quote">&gt;&gt; +	 */</span>
<span class="quote">&gt;&gt; +	if ((atomic_dec_return(&amp;page-&gt;xpfo.mapcount) == 0) &amp;&amp;</span>
<span class="quote">&gt;&gt; +	    (TEST_XPFO_FLAG(user_fp, page) || TEST_XPFO_FLAG(user, page))) {</span>
<span class="quote">&gt;&gt; +		set_kpte(page, (unsigned long)kaddr, __pgprot(0));</span>
<span class="quote">&gt;&gt; +		__flush_tlb_one((unsigned long)kaddr);</span>
<span class="quote">&gt;&gt; +		SET_XPFO_FLAG(user, page);</span>
<span class="quote">&gt;&gt; +	}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	spin_unlock_irqrestore(&amp;page-&gt;xpfo.lock, flags);</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +EXPORT_SYMBOL(xpfo_kunmap);</span>
<span class="quote">&gt;&gt; diff --git a/block/blk-map.c b/block/blk-map.c</span>
<span class="quote">&gt;&gt; index f565e11..b7b8302 100644</span>
<span class="quote">&gt;&gt; --- a/block/blk-map.c</span>
<span class="quote">&gt;&gt; +++ b/block/blk-map.c</span>
<span class="quote">&gt;&gt; @@ -107,7 +107,12 @@ int blk_rq_map_user_iov(struct request_queue *q, struct request *rq,</span>
<span class="quote">&gt;&gt;  		prv.iov_len = iov.iov_len;</span>
<span class="quote">&gt;&gt;  	}</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; -	if (unaligned || (q-&gt;dma_pad_mask &amp; iter-&gt;count) || map_data)</span>
<span class="quote">&gt;&gt; +	/*</span>
<span class="quote">&gt;&gt; +	 * juergh: Temporary hack to force the use of a bounce buffer if XPFO</span>
<span class="quote">&gt;&gt; +	 * is enabled. Results in an XPFO page fault otherwise.</span>
<span class="quote">&gt;&gt; +	 */</span>
<span class="quote">&gt; This does look like it might add a bunch of overhead</span>
<span class="quote">&gt;&gt; +	if (unaligned || (q-&gt;dma_pad_mask &amp; iter-&gt;count) || map_data ||</span>
<span class="quote">&gt;&gt; +	    IS_ENABLED(CONFIG_XPFO))</span>
<span class="quote">&gt;&gt;  		bio = bio_copy_user_iov(q, map_data, iter, gfp_mask);</span>
<span class="quote">&gt;&gt;  	else</span>
<span class="quote">&gt;&gt;  		bio = bio_map_user_iov(q, iter, gfp_mask);</span>
<span class="quote">&gt;&gt; diff --git a/include/linux/highmem.h b/include/linux/highmem.h</span>
<span class="quote">&gt;&gt; index bb3f329..0ca9130 100644</span>
<span class="quote">&gt;&gt; --- a/include/linux/highmem.h</span>
<span class="quote">&gt;&gt; +++ b/include/linux/highmem.h</span>
<span class="quote">&gt;&gt; @@ -55,24 +55,37 @@ static inline struct page *kmap_to_page(void *addr)</span>
<span class="quote">&gt;&gt;  #ifndef ARCH_HAS_KMAP</span>
<span class="quote">&gt;&gt;  static inline void *kmap(struct page *page)</span>
<span class="quote">&gt;&gt;  {</span>
<span class="quote">&gt;&gt; +	void *kaddr;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt;  	might_sleep();</span>
<span class="quote">&gt;&gt; -	return page_address(page);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	kaddr = page_address(page);</span>
<span class="quote">&gt;&gt; +	xpfo_kmap(kaddr, page);</span>
<span class="quote">&gt;&gt; +	return kaddr;</span>
<span class="quote">&gt;&gt;  }</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  static inline void kunmap(struct page *page)</span>
<span class="quote">&gt;&gt;  {</span>
<span class="quote">&gt;&gt; +	xpfo_kunmap(page_address(page), page);</span>
<span class="quote">&gt;&gt;  }</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  static inline void *kmap_atomic(struct page *page)</span>
<span class="quote">&gt;&gt;  {</span>
<span class="quote">&gt;&gt; +	void *kaddr;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt;  	preempt_disable();</span>
<span class="quote">&gt;&gt;  	pagefault_disable();</span>
<span class="quote">&gt;&gt; -	return page_address(page);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	kaddr = page_address(page);</span>
<span class="quote">&gt;&gt; +	xpfo_kmap(kaddr, page);</span>
<span class="quote">&gt;&gt; +	return kaddr;</span>
<span class="quote">&gt;&gt;  }</span>
<span class="quote">&gt;&gt;  #define kmap_atomic_prot(page, prot)	kmap_atomic(page)</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  static inline void __kunmap_atomic(void *addr)</span>
<span class="quote">&gt;&gt;  {</span>
<span class="quote">&gt;&gt; +	xpfo_kunmap(addr, virt_to_page(addr));</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt;  	pagefault_enable();</span>
<span class="quote">&gt;&gt;  	preempt_enable();</span>
<span class="quote">&gt;&gt;  }</span>
<span class="quote">&gt;&gt; @@ -133,7 +146,8 @@ do {                                                            \</span>
<span class="quote">&gt;&gt;  static inline void clear_user_highpage(struct page *page, unsigned long vaddr)</span>
<span class="quote">&gt;&gt;  {</span>
<span class="quote">&gt;&gt;  	void *addr = kmap_atomic(page);</span>
<span class="quote">&gt;&gt; -	clear_user_page(addr, vaddr, page);</span>
<span class="quote">&gt;&gt; +	if (!xpfo_test_and_clear_zap(page))</span>
<span class="quote">&gt;&gt; +		clear_user_page(addr, vaddr, page);</span>
<span class="quote">&gt;&gt;  	kunmap_atomic(addr);</span>
<span class="quote">&gt;&gt;  }</span>
<span class="quote">&gt;&gt;  #endif</span>
<span class="quote">&gt;&gt; @@ -186,7 +200,8 @@ alloc_zeroed_user_highpage_movable(struct vm_area_struct *vma,</span>
<span class="quote">&gt;&gt;  static inline void clear_highpage(struct page *page)</span>
<span class="quote">&gt;&gt;  {</span>
<span class="quote">&gt;&gt;  	void *kaddr = kmap_atomic(page);</span>
<span class="quote">&gt;&gt; -	clear_page(kaddr);</span>
<span class="quote">&gt;&gt; +	if (!xpfo_test_and_clear_zap(page))</span>
<span class="quote">&gt;&gt; +		clear_page(kaddr);</span>
<span class="quote">&gt;&gt;  	kunmap_atomic(kaddr);</span>
<span class="quote">&gt;&gt;  }</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h</span>
<span class="quote">&gt;&gt; index 624b78b..71c95aa 100644</span>
<span class="quote">&gt;&gt; --- a/include/linux/mm_types.h</span>
<span class="quote">&gt;&gt; +++ b/include/linux/mm_types.h</span>
<span class="quote">&gt;&gt; @@ -12,6 +12,7 @@</span>
<span class="quote">&gt;&gt;  #include &lt;linux/cpumask.h&gt;</span>
<span class="quote">&gt;&gt;  #include &lt;linux/uprobes.h&gt;</span>
<span class="quote">&gt;&gt;  #include &lt;linux/page-flags-layout.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;linux/xpfo.h&gt;</span>
<span class="quote">&gt;&gt;  #include &lt;asm/page.h&gt;</span>
<span class="quote">&gt;&gt;  #include &lt;asm/mmu.h&gt;</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; @@ -215,6 +216,9 @@ struct page {</span>
<span class="quote">&gt;&gt;  #ifdef LAST_CPUPID_NOT_IN_PAGE_FLAGS</span>
<span class="quote">&gt;&gt;  	int _last_cpupid;</span>
<span class="quote">&gt;&gt;  #endif</span>
<span class="quote">&gt;&gt; +#ifdef CONFIG_XPFO</span>
<span class="quote">&gt;&gt; +	struct xpfo_info xpfo;</span>
<span class="quote">&gt;&gt; +#endif</span>
<span class="quote">&gt;&gt;  }</span>
<span class="quote">&gt;&gt;  /*</span>
<span class="quote">&gt;&gt;   * The struct page can be forced to be double word aligned so that atomic ops</span>
<span class="quote">&gt;&gt; diff --git a/include/linux/xpfo.h b/include/linux/xpfo.h</span>
<span class="quote">&gt;&gt; new file mode 100644</span>
<span class="quote">&gt;&gt; index 0000000..c4f0871</span>
<span class="quote">&gt;&gt; --- /dev/null</span>
<span class="quote">&gt;&gt; +++ b/include/linux/xpfo.h</span>
<span class="quote">&gt;&gt; @@ -0,0 +1,88 @@</span>
<span class="quote">&gt;&gt; +/*</span>
<span class="quote">&gt;&gt; + * Copyright (C) 2016 Brown University. All rights reserved.</span>
<span class="quote">&gt;&gt; + * Copyright (C) 2016 Hewlett Packard Enterprise Development, L.P.</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + * Authors:</span>
<span class="quote">&gt;&gt; + *   Vasileios P. Kemerlis &lt;vpk@cs.brown.edu&gt;</span>
<span class="quote">&gt;&gt; + *   Juerg Haefliger &lt;juerg.haefliger@hpe.com&gt;</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + * This program is free software; you can redistribute it and/or modify it</span>
<span class="quote">&gt;&gt; + * under the terms of the GNU General Public License version 2 as published by</span>
<span class="quote">&gt;&gt; + * the Free Software Foundation.</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#ifndef _LINUX_XPFO_H</span>
<span class="quote">&gt;&gt; +#define _LINUX_XPFO_H</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#ifdef CONFIG_XPFO</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +/*</span>
<span class="quote">&gt;&gt; + * XPFO page flags:</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + * PG_XPFO_user_fp denotes that the page is allocated to user space. This flag</span>
<span class="quote">&gt;&gt; + * is used in the fast path, where the page is marked accordingly but *not*</span>
<span class="quote">&gt;&gt; + * unmapped from the kernel. In most cases, the kernel will need access to the</span>
<span class="quote">&gt;&gt; + * page immediately after its acquisition so an unnecessary mapping operation</span>
<span class="quote">&gt;&gt; + * is avoided.</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + * PG_XPFO_user denotes that the page is destined for user space. This flag is</span>
<span class="quote">&gt;&gt; + * used in the slow path, where the page needs to be mapped/unmapped when the</span>
<span class="quote">&gt;&gt; + * kernel wants to access it. If a page is deallocated and this flag is set,</span>
<span class="quote">&gt;&gt; + * the page is cleared and mapped back into the kernel.</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + * PG_XPFO_kernel denotes a page that is destined to kernel space. This is used</span>
<span class="quote">&gt;&gt; + * for identifying pages that are first assigned to kernel space and then freed</span>
<span class="quote">&gt;&gt; + * and mapped to user space. In such cases, an expensive TLB shootdown is</span>
<span class="quote">&gt;&gt; + * necessary. Pages allocated to user space, freed, and subsequently allocated</span>
<span class="quote">&gt;&gt; + * to user space again, require only local TLB invalidation.</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + * PG_XPFO_zap indicates that the page has been zapped. This flag is used to</span>
<span class="quote">&gt;&gt; + * avoid zapping pages multiple times. Whenever a page is freed and was</span>
<span class="quote">&gt;&gt; + * previously mapped to user space, it needs to be zapped before mapped back</span>
<span class="quote">&gt;&gt; + * in to the kernel.</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +enum xpfo_pageflags {</span>
<span class="quote">&gt;&gt; +	PG_XPFO_user_fp,</span>
<span class="quote">&gt;&gt; +	PG_XPFO_user,</span>
<span class="quote">&gt;&gt; +	PG_XPFO_kernel,</span>
<span class="quote">&gt;&gt; +	PG_XPFO_zap,</span>
<span class="quote">&gt;&gt; +};</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +struct xpfo_info {</span>
<span class="quote">&gt;&gt; +	unsigned long flags;	/* Flags for tracking the page&#39;s XPFO state */</span>
<span class="quote">&gt;&gt; +	atomic_t mapcount;	/* Counter for balancing page map/unmap</span>
<span class="quote">&gt;&gt; +				 * requests. Only the first map request maps</span>
<span class="quote">&gt;&gt; +				 * the page back to kernel space. Likewise,</span>
<span class="quote">&gt;&gt; +				 * only the last unmap request unmaps the page.</span>
<span class="quote">&gt;&gt; +				 */</span>
<span class="quote">&gt;&gt; +	spinlock_t lock;	/* Lock to serialize concurrent map/unmap</span>
<span class="quote">&gt;&gt; +				 * requests.</span>
<span class="quote">&gt;&gt; +				 */</span>
<span class="quote">&gt;&gt; +};</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +extern void xpfo_clear_zap(struct page *page, int order);</span>
<span class="quote">&gt;&gt; +extern int xpfo_test_and_clear_zap(struct page *page);</span>
<span class="quote">&gt;&gt; +extern int xpfo_test_kernel(struct page *page);</span>
<span class="quote">&gt;&gt; +extern int xpfo_test_user(struct page *page);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +extern void xpfo_kmap(void *kaddr, struct page *page);</span>
<span class="quote">&gt;&gt; +extern void xpfo_kunmap(void *kaddr, struct page *page);</span>
<span class="quote">&gt;&gt; +extern void xpfo_alloc_page(struct page *page, int order, gfp_t gfp);</span>
<span class="quote">&gt;&gt; +extern void xpfo_free_page(struct page *page, int order);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#else /* ifdef CONFIG_XPFO */</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +static inline void xpfo_clear_zap(struct page *page, int order) { }</span>
<span class="quote">&gt;&gt; +static inline int xpfo_test_and_clear_zap(struct page *page) { return 0; }</span>
<span class="quote">&gt;&gt; +static inline int xpfo_test_kernel(struct page *page) { return 0; }</span>
<span class="quote">&gt;&gt; +static inline int xpfo_test_user(struct page *page) { return 0; }</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +static inline void xpfo_kmap(void *kaddr, struct page *page) { }</span>
<span class="quote">&gt;&gt; +static inline void xpfo_kunmap(void *kaddr, struct page *page) { }</span>
<span class="quote">&gt;&gt; +static inline void xpfo_alloc_page(struct page *page, int order, gfp_t gfp) { }</span>
<span class="quote">&gt;&gt; +static inline void xpfo_free_page(struct page *page, int order) { }</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#endif /* ifdef CONFIG_XPFO */</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#endif /* ifndef _LINUX_XPFO_H */</span>
<span class="quote">&gt;&gt; diff --git a/lib/swiotlb.c b/lib/swiotlb.c</span>
<span class="quote">&gt;&gt; index 76f29ec..cf57ee9 100644</span>
<span class="quote">&gt;&gt; --- a/lib/swiotlb.c</span>
<span class="quote">&gt;&gt; +++ b/lib/swiotlb.c</span>
<span class="quote">&gt;&gt; @@ -390,8 +390,9 @@ static void swiotlb_bounce(phys_addr_t orig_addr, phys_addr_t tlb_addr,</span>
<span class="quote">&gt;&gt;  {</span>
<span class="quote">&gt;&gt;  	unsigned long pfn = PFN_DOWN(orig_addr);</span>
<span class="quote">&gt;&gt;  	unsigned char *vaddr = phys_to_virt(tlb_addr);</span>
<span class="quote">&gt;&gt; +	struct page *page = pfn_to_page(pfn);</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; -	if (PageHighMem(pfn_to_page(pfn))) {</span>
<span class="quote">&gt;&gt; +	if (PageHighMem(page) || xpfo_test_user(page)) {</span>
<span class="quote">&gt;&gt;  		/* The buffer does not have a mapping.  Map it in and copy */</span>
<span class="quote">&gt;&gt;  		unsigned int offset = orig_addr &amp; ~PAGE_MASK;</span>
<span class="quote">&gt;&gt;  		char *buffer;</span>
<span class="quote">&gt;&gt; diff --git a/mm/page_alloc.c b/mm/page_alloc.c</span>
<span class="quote">&gt;&gt; index 838ca8bb..47b42a3 100644</span>
<span class="quote">&gt;&gt; --- a/mm/page_alloc.c</span>
<span class="quote">&gt;&gt; +++ b/mm/page_alloc.c</span>
<span class="quote">&gt;&gt; @@ -1003,6 +1003,7 @@ static bool free_pages_prepare(struct page *page, unsigned int order)</span>
<span class="quote">&gt;&gt;  	}</span>
<span class="quote">&gt;&gt;  	arch_free_page(page, order);</span>
<span class="quote">&gt;&gt;  	kernel_map_pages(page, 1 &lt;&lt; order, 0);</span>
<span class="quote">&gt;&gt; +	xpfo_free_page(page, order);</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  	return true;</span>
<span class="quote">&gt;&gt;  }</span>
<span class="quote">&gt;&gt; @@ -1398,10 +1399,13 @@ static int prep_new_page(struct page *page, unsigned int order, gfp_t gfp_flags,</span>
<span class="quote">&gt;&gt;  	arch_alloc_page(page, order);</span>
<span class="quote">&gt;&gt;  	kernel_map_pages(page, 1 &lt;&lt; order, 1);</span>
<span class="quote">&gt;&gt;  	kasan_alloc_pages(page, order);</span>
<span class="quote">&gt;&gt; +	xpfo_alloc_page(page, order, gfp_flags);</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  	if (gfp_flags &amp; __GFP_ZERO)</span>
<span class="quote">&gt;&gt;  		for (i = 0; i &lt; (1 &lt;&lt; order); i++)</span>
<span class="quote">&gt;&gt;  			clear_highpage(page + i);</span>
<span class="quote">&gt;&gt; +	else</span>
<span class="quote">&gt;&gt; +		xpfo_clear_zap(page, order);</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  	if (order &amp;&amp; (gfp_flags &amp; __GFP_COMP))</span>
<span class="quote">&gt;&gt;  		prep_compound_page(page, order);</span>
<span class="quote">&gt;&gt; @@ -2072,10 +2076,11 @@ void free_hot_cold_page(struct page *page, bool cold)</span>
<span class="quote">&gt;&gt;  	}</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  	pcp = &amp;this_cpu_ptr(zone-&gt;pageset)-&gt;pcp;</span>
<span class="quote">&gt;&gt; -	if (!cold)</span>
<span class="quote">&gt;&gt; +	if (!cold &amp;&amp; !xpfo_test_kernel(page))</span>
<span class="quote">&gt;&gt;  		list_add(&amp;page-&gt;lru, &amp;pcp-&gt;lists[migratetype]);</span>
<span class="quote">&gt;&gt;  	else</span>
<span class="quote">&gt;&gt;  		list_add_tail(&amp;page-&gt;lru, &amp;pcp-&gt;lists[migratetype]);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt;  	pcp-&gt;count++;</span>
<span class="quote">&gt;&gt;  	if (pcp-&gt;count &gt;= pcp-&gt;high) {</span>
<span class="quote">&gt;&gt;  		unsigned long batch = READ_ONCE(pcp-&gt;batch);</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=130411">Laura Abbott</a> - March 28, 2016, 7:29 p.m.</div>
<pre class="content">
On 03/21/2016 01:37 AM, Juerg Haefliger wrote:
...
<span class="quote">&gt;&gt;&gt; +void xpfo_free_page(struct page *page, int order)</span>
<span class="quote">&gt;&gt;&gt; +{</span>
<span class="quote">&gt;&gt;&gt; +    int i;</span>
<span class="quote">&gt;&gt;&gt; +    unsigned long kaddr;</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; +    for (i = 0; i &lt; (1 &lt;&lt; order); i++) {</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; +        /* The page frame was previously allocated to user space */</span>
<span class="quote">&gt;&gt;&gt; +        if (TEST_AND_CLEAR_XPFO_FLAG(user, page + i)) {</span>
<span class="quote">&gt;&gt;&gt; +            kaddr = (unsigned long)page_address(page + i);</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; +            /* Clear the page and mark it accordingly */</span>
<span class="quote">&gt;&gt;&gt; +            clear_page((void *)kaddr);</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Clearing the page isn&#39;t related to XPFO. There&#39;s other work ongoing to</span>
<span class="quote">&gt;&gt; do clearing of the page on free.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; It&#39;s not strictly related to XPFO but adds another layer of security. Do you</span>
<span class="quote">&gt; happen to have a pointer to the ongoing work that you mentioned?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;</span>

The work was merged for the 4.6 merge window
https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=8823b1dbc05fab1a8bec275eeae4709257c2661d

This is a separate option to clear the page.

...
<span class="quote">
&gt;&gt;&gt; @@ -2072,10 +2076,11 @@ void free_hot_cold_page(struct page *page, bool cold)</span>
<span class="quote">&gt;&gt;&gt;        }</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;        pcp = &amp;this_cpu_ptr(zone-&gt;pageset)-&gt;pcp;</span>
<span class="quote">&gt;&gt;&gt; -    if (!cold)</span>
<span class="quote">&gt;&gt;&gt; +    if (!cold &amp;&amp; !xpfo_test_kernel(page))</span>
<span class="quote">&gt;&gt;&gt;            list_add(&amp;page-&gt;lru, &amp;pcp-&gt;lists[migratetype]);</span>
<span class="quote">&gt;&gt;&gt;        else</span>
<span class="quote">&gt;&gt;&gt;            list_add_tail(&amp;page-&gt;lru, &amp;pcp-&gt;lists[migratetype]);</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; What&#39;s the advantage of this?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Allocating a page to userspace that was previously allocated to kernel space</span>
<span class="quote">&gt; requires an expensive TLB shootdown. The above will put previously</span>
<span class="quote">&gt; kernel-allocated pages in the cold page cache to postpone their allocation as</span>
<span class="quote">&gt; long as possible to minimize TLB shootdowns.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;</span>

That makes sense. You probably want to make this a separate commmit with
this explanation as the commit text.
<span class="quote">

&gt;&gt;&gt;        pcp-&gt;count++;</span>
<span class="quote">&gt;&gt;&gt;        if (pcp-&gt;count &gt;= pcp-&gt;high) {</span>
<span class="quote">&gt;&gt;&gt;            unsigned long batch = READ_ONCE(pcp-&gt;batch);</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Thanks for the review and comments! It&#39;s highly appreciated.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; ...Juerg</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; Thanks,</span>
<span class="quote">&gt;&gt; Laura</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=4745">Balbir Singh</a> - April 1, 2016, 12:21 a.m.</div>
<pre class="content">
On Mon, Mar 21, 2016 at 7:44 PM, Juerg Haefliger
&lt;juerg.haefliger@hpe.com&gt; wrote:
<span class="quote">&gt; Hi Balbir,</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Apologies for the slow reply.</span>
<span class="quote">&gt;</span>
No problem, I lost this in my inbox as well due to the reply latency.
<span class="quote">&gt;</span>
<span class="quote">&gt; On 03/01/2016 03:10 AM, Balbir Singh wrote:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; On 27/02/16 01:21, Juerg Haefliger wrote:</span>
<span class="quote">&gt;&gt;&gt; This patch adds support for XPFO which protects against &#39;ret2dir&#39; kernel</span>
<span class="quote">&gt;&gt;&gt; attacks. The basic idea is to enforce exclusive ownership of page frames</span>
<span class="quote">&gt;&gt;&gt; by either the kernel or userland, unless explicitly requested by the</span>
<span class="quote">&gt;&gt;&gt; kernel. Whenever a page destined for userland is allocated, it is</span>
<span class="quote">&gt;&gt;&gt; unmapped from physmap. When such a page is reclaimed from userland, it is</span>
<span class="quote">&gt;&gt;&gt; mapped back to physmap.</span>
<span class="quote">&gt;&gt; physmap == xen physmap? Please clarify</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; No, it&#39;s not XEN related. I might have the terminology wrong. Physmap is what</span>
<span class="quote">&gt; the original authors used for describing &lt;quote&gt; a large, contiguous virtual</span>
<span class="quote">&gt; memory region inside kernel address space that contains a direct mapping of part</span>
<span class="quote">&gt; or all (depending on the architecture) physical memory. &lt;/quote&gt;</span>
<span class="quote">&gt;</span>
Thanks for clarifying
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt;&gt; Mapping/unmapping from physmap is accomplished by modifying the PTE</span>
<span class="quote">&gt;&gt;&gt; permission bits to allow/disallow access to the page.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Additional fields are added to the page struct for XPFO housekeeping.</span>
<span class="quote">&gt;&gt;&gt; Specifically a flags field to distinguish user vs. kernel pages, a</span>
<span class="quote">&gt;&gt;&gt; reference counter to track physmap map/unmap operations and a lock to</span>
<span class="quote">&gt;&gt;&gt; protect the XPFO fields.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Known issues/limitations:</span>
<span class="quote">&gt;&gt;&gt;   - Only supported on x86-64.</span>
<span class="quote">&gt;&gt; Is it due to lack of porting or a design limitation?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Lack of porting. Support for other architectures will come later.</span>
<span class="quote">&gt;</span>
OK
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt;&gt;   - Only supports 4k pages.</span>
<span class="quote">&gt;&gt;&gt;   - Adds additional data to the page struct.</span>
<span class="quote">&gt;&gt;&gt;   - There are most likely some additional and legitimate uses cases where</span>
<span class="quote">&gt;&gt;&gt;     the kernel needs to access userspace. Those need to be identified and</span>
<span class="quote">&gt;&gt;&gt;     made XPFO-aware.</span>
<span class="quote">&gt;&gt; Why not build an audit mode for it?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Can you elaborate what you mean by this?</span>
<span class="quote">&gt;</span>
What I meant is when the kernel needs to access userspace and XPFO is
not aware of it
and is going to block it, write to a log/trace buffer so that it can
be audited for correctness
<span class="quote">
&gt;</span>
<span class="quote">&gt;&gt;&gt;   - There&#39;s a performance impact if XPFO is turned on. Per the paper</span>
<span class="quote">&gt;&gt;&gt;     referenced below it&#39;s in the 1-3% ballpark. More performance testing</span>
<span class="quote">&gt;&gt;&gt;     wouldn&#39;t hurt. What tests to run though?</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Reference paper by the original patch authors:</span>
<span class="quote">&gt;&gt;&gt;   http://www.cs.columbia.edu/~vpk/papers/ret2dir.sec14.pdf</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Suggested-by: Vasileios P. Kemerlis &lt;vpk@cs.brown.edu&gt;</span>
<span class="quote">&gt;&gt;&gt; Signed-off-by: Juerg Haefliger &lt;juerg.haefliger@hpe.com&gt;</span>
<span class="quote">&gt;&gt; This patch needs to be broken down into smaller patches - a series</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Agreed.</span>
<span class="quote">&gt;</span>

I think it will be good to describe what is XPFO aware

1. How are device mmap&#39;d shared between kernel/user covered?
2. How is copy_from/to_user covered?
3. How is vdso covered?
4. More...


Balbir Singh.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=153531">Juerg Haefliger</a> - Sept. 2, 2016, 11:39 a.m.</div>
<pre class="content">
Changes from:
  v1 -&gt; v2:
    - Moved the code from arch/x86/mm/ to mm/ since it&#39;s (mostly)
      arch-agnostic.
    - Moved the config to the generic layer and added ARCH_SUPPORTS_XPFO
      for x86.
    - Use page_ext for the additional per-page data.
    - Removed the clearing of pages. This can be accomplished by using
      PAGE_POISONING.
    - Split up the patch into multiple patches.
    - Fixed additional issues identified by reviewers.

This patch series adds support for XPFO which protects against &#39;ret2dir&#39;
kernel attacks. The basic idea is to enforce exclusive ownership of page
frames by either the kernel or userspace, unless explicitly requested by
the kernel. Whenever a page destined for userspace is allocated, it is
unmapped from physmap (the kernel&#39;s page table). When such a page is
reclaimed from userspace, it is mapped back to physmap.

Additional fields in the page_ext struct are used for XPFO housekeeping.
Specifically two flags to distinguish user vs. kernel pages and to tag
unmapped pages and a reference counter to balance kmap/kunmap operations
and a lock to serialize access to the XPFO fields.

Known issues/limitations:
  - Only supports x86-64 (for now)
  - Only supports 4k pages (for now)
  - There are most likely some legitimate uses cases where the kernel needs
    to access userspace which need to be made XPFO-aware
  - Performance penalty

Reference paper by the original patch authors:
  http://www.cs.columbia.edu/~vpk/papers/ret2dir.sec14.pdf

Juerg Haefliger (3):
  Add support for eXclusive Page Frame Ownership (XPFO)
  xpfo: Only put previous userspace pages into the hot cache
  block: Always use a bounce buffer when XPFO is enabled

 arch/x86/Kconfig         |   3 +-
 arch/x86/mm/init.c       |   2 +-
 block/blk-map.c          |   2 +-
 include/linux/highmem.h  |  15 +++-
 include/linux/page_ext.h |   7 ++
 include/linux/xpfo.h     |  41 +++++++++
 lib/swiotlb.c            |   3 +-
 mm/Makefile              |   1 +
 mm/page_alloc.c          |  10 ++-
 mm/page_ext.c            |   4 +
 mm/xpfo.c                | 213 +++++++++++++++++++++++++++++++++++++++++++++++
 security/Kconfig         |  20 +++++
 12 files changed, 314 insertions(+), 7 deletions(-)
 create mode 100644 include/linux/xpfo.h
 create mode 100644 mm/xpfo.c
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=153531">Juerg Haefliger</a> - Sept. 14, 2016, 7:18 a.m.</div>
<pre class="content">
Changes from:
  v1 -&gt; v2:
    - Moved the code from arch/x86/mm/ to mm/ since it&#39;s (mostly)
      arch-agnostic.
    - Moved the config to the generic layer and added ARCH_SUPPORTS_XPFO
      for x86.
    - Use page_ext for the additional per-page data.
    - Removed the clearing of pages. This can be accomplished by using
      PAGE_POISONING.
    - Split up the patch into multiple patches.
    - Fixed additional issues identified by reviewers.

This patch series adds support for XPFO which protects against &#39;ret2dir&#39;
kernel attacks. The basic idea is to enforce exclusive ownership of page
frames by either the kernel or userspace, unless explicitly requested by
the kernel. Whenever a page destined for userspace is allocated, it is
unmapped from physmap (the kernel&#39;s page table). When such a page is
reclaimed from userspace, it is mapped back to physmap.

Additional fields in the page_ext struct are used for XPFO housekeeping.
Specifically two flags to distinguish user vs. kernel pages and to tag
unmapped pages and a reference counter to balance kmap/kunmap operations
and a lock to serialize access to the XPFO fields.

Known issues/limitations:
  - Only supports x86-64 (for now)
  - Only supports 4k pages (for now)
  - There are most likely some legitimate uses cases where the kernel needs
    to access userspace which need to be made XPFO-aware
  - Performance penalty

Reference paper by the original patch authors:
  http://www.cs.columbia.edu/~vpk/papers/ret2dir.sec14.pdf

Juerg Haefliger (3):
  Add support for eXclusive Page Frame Ownership (XPFO)
  xpfo: Only put previous userspace pages into the hot cache
  block: Always use a bounce buffer when XPFO is enabled

 arch/x86/Kconfig         |   3 +-
 arch/x86/mm/init.c       |   2 +-
 block/blk-map.c          |   2 +-
 include/linux/highmem.h  |  15 +++-
 include/linux/page_ext.h |   7 ++
 include/linux/xpfo.h     |  41 +++++++++
 lib/swiotlb.c            |   3 +-
 mm/Makefile              |   1 +
 mm/page_alloc.c          |  10 ++-
 mm/page_ext.c            |   4 +
 mm/xpfo.c                | 213 +++++++++++++++++++++++++++++++++++++++++++++++
 security/Kconfig         |  20 +++++
 12 files changed, 314 insertions(+), 7 deletions(-)
 create mode 100644 include/linux/xpfo.h
 create mode 100644 mm/xpfo.c
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=153531">Juerg Haefliger</a> - Sept. 14, 2016, 7:23 a.m.</div>
<pre class="content">
Resending to include the kernel-hardening list. Sorry, I wasn&#39;t subscribed with the correct email
address when I sent this the first time.

...Juerg

On 09/14/2016 09:18 AM, Juerg Haefliger wrote:
<span class="quote">&gt; Changes from:</span>
<span class="quote">&gt;   v1 -&gt; v2:</span>
<span class="quote">&gt;     - Moved the code from arch/x86/mm/ to mm/ since it&#39;s (mostly)</span>
<span class="quote">&gt;       arch-agnostic.</span>
<span class="quote">&gt;     - Moved the config to the generic layer and added ARCH_SUPPORTS_XPFO</span>
<span class="quote">&gt;       for x86.</span>
<span class="quote">&gt;     - Use page_ext for the additional per-page data.</span>
<span class="quote">&gt;     - Removed the clearing of pages. This can be accomplished by using</span>
<span class="quote">&gt;       PAGE_POISONING.</span>
<span class="quote">&gt;     - Split up the patch into multiple patches.</span>
<span class="quote">&gt;     - Fixed additional issues identified by reviewers.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This patch series adds support for XPFO which protects against &#39;ret2dir&#39;</span>
<span class="quote">&gt; kernel attacks. The basic idea is to enforce exclusive ownership of page</span>
<span class="quote">&gt; frames by either the kernel or userspace, unless explicitly requested by</span>
<span class="quote">&gt; the kernel. Whenever a page destined for userspace is allocated, it is</span>
<span class="quote">&gt; unmapped from physmap (the kernel&#39;s page table). When such a page is</span>
<span class="quote">&gt; reclaimed from userspace, it is mapped back to physmap.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Additional fields in the page_ext struct are used for XPFO housekeeping.</span>
<span class="quote">&gt; Specifically two flags to distinguish user vs. kernel pages and to tag</span>
<span class="quote">&gt; unmapped pages and a reference counter to balance kmap/kunmap operations</span>
<span class="quote">&gt; and a lock to serialize access to the XPFO fields.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Known issues/limitations:</span>
<span class="quote">&gt;   - Only supports x86-64 (for now)</span>
<span class="quote">&gt;   - Only supports 4k pages (for now)</span>
<span class="quote">&gt;   - There are most likely some legitimate uses cases where the kernel needs</span>
<span class="quote">&gt;     to access userspace which need to be made XPFO-aware</span>
<span class="quote">&gt;   - Performance penalty</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Reference paper by the original patch authors:</span>
<span class="quote">&gt;   http://www.cs.columbia.edu/~vpk/papers/ret2dir.sec14.pdf</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Juerg Haefliger (3):</span>
<span class="quote">&gt;   Add support for eXclusive Page Frame Ownership (XPFO)</span>
<span class="quote">&gt;   xpfo: Only put previous userspace pages into the hot cache</span>
<span class="quote">&gt;   block: Always use a bounce buffer when XPFO is enabled</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  arch/x86/Kconfig         |   3 +-</span>
<span class="quote">&gt;  arch/x86/mm/init.c       |   2 +-</span>
<span class="quote">&gt;  block/blk-map.c          |   2 +-</span>
<span class="quote">&gt;  include/linux/highmem.h  |  15 +++-</span>
<span class="quote">&gt;  include/linux/page_ext.h |   7 ++</span>
<span class="quote">&gt;  include/linux/xpfo.h     |  41 +++++++++</span>
<span class="quote">&gt;  lib/swiotlb.c            |   3 +-</span>
<span class="quote">&gt;  mm/Makefile              |   1 +</span>
<span class="quote">&gt;  mm/page_alloc.c          |  10 ++-</span>
<span class="quote">&gt;  mm/page_ext.c            |   4 +</span>
<span class="quote">&gt;  mm/xpfo.c                | 213 +++++++++++++++++++++++++++++++++++++++++++++++</span>
<span class="quote">&gt;  security/Kconfig         |  20 +++++</span>
<span class="quote">&gt;  12 files changed, 314 insertions(+), 7 deletions(-)</span>
<span class="quote">&gt;  create mode 100644 include/linux/xpfo.h</span>
<span class="quote">&gt;  create mode 100644 mm/xpfo.c</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=30282">Mark Rutland</a> - Sept. 14, 2016, 9:36 a.m.</div>
<pre class="content">
Hi,

On Wed, Sep 14, 2016 at 09:18:58AM +0200, Juerg Haefliger wrote:
<span class="quote">
&gt; This patch series adds support for XPFO which protects against &#39;ret2dir&#39;</span>
<span class="quote">&gt; kernel attacks. The basic idea is to enforce exclusive ownership of page</span>
<span class="quote">&gt; frames by either the kernel or userspace, unless explicitly requested by</span>
<span class="quote">&gt; the kernel. Whenever a page destined for userspace is allocated, it is</span>
<span class="quote">&gt; unmapped from physmap (the kernel&#39;s page table). When such a page is</span>
<span class="quote">&gt; reclaimed from userspace, it is mapped back to physmap.</span>
<span class="quote">
&gt; Known issues/limitations:</span>
<span class="quote">&gt;   - Only supports x86-64 (for now)</span>
<span class="quote">&gt;   - Only supports 4k pages (for now)</span>
<span class="quote">&gt;   - There are most likely some legitimate uses cases where the kernel needs</span>
<span class="quote">&gt;     to access userspace which need to be made XPFO-aware</span>
<span class="quote">&gt;   - Performance penalty</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Reference paper by the original patch authors:</span>
<span class="quote">&gt;   http://www.cs.columbia.edu/~vpk/papers/ret2dir.sec14.pdf</span>

Just to check, doesn&#39;t DEBUG_RODATA ensure that the linear mapping is
non-executable on x86_64 (as it does for arm64)?

For both arm64 and x86_64, DEBUG_RODATA is mandatory (or soon to be so).
Assuming that implies a lack of execute permission for x86_64, that
should provide a similar level of protection against erroneously
branching to addresses in the linear map, without the complexity and
overhead of mapping/unmapping pages.

So to me it looks like this approach may only be useful for
architectures without page-granular execute permission controls.

Is this also intended to protect against erroneous *data* accesses to
the linear map?

Am I missing something?

Thanks,
Mark.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=30282">Mark Rutland</a> - Sept. 14, 2016, 9:49 a.m.</div>
<pre class="content">
On Wed, Sep 14, 2016 at 10:36:34AM +0100, Mark Rutland wrote:
<span class="quote">&gt; On Wed, Sep 14, 2016 at 09:18:58AM +0200, Juerg Haefliger wrote:</span>
<span class="quote">&gt; &gt; This patch series adds support for XPFO which protects against &#39;ret2dir&#39;</span>
<span class="quote">&gt; &gt; kernel attacks. The basic idea is to enforce exclusive ownership of page</span>
<span class="quote">&gt; &gt; frames by either the kernel or userspace, unless explicitly requested by</span>
<span class="quote">&gt; &gt; the kernel. Whenever a page destined for userspace is allocated, it is</span>
<span class="quote">&gt; &gt; unmapped from physmap (the kernel&#39;s page table). When such a page is</span>
<span class="quote">&gt; &gt; reclaimed from userspace, it is mapped back to physmap.</span>
<span class="quote">
&gt; &gt; Reference paper by the original patch authors:</span>
<span class="quote">&gt; &gt;   http://www.cs.columbia.edu/~vpk/papers/ret2dir.sec14.pdf</span>
<span class="quote">
&gt; For both arm64 and x86_64, DEBUG_RODATA is mandatory (or soon to be so).</span>
<span class="quote">&gt; Assuming that implies a lack of execute permission for x86_64, that</span>
<span class="quote">&gt; should provide a similar level of protection against erroneously</span>
<span class="quote">&gt; branching to addresses in the linear map, without the complexity and</span>
<span class="quote">&gt; overhead of mapping/unmapping pages.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So to me it looks like this approach may only be useful for</span>
<span class="quote">&gt; architectures without page-granular execute permission controls.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Is this also intended to protect against erroneous *data* accesses to</span>
<span class="quote">&gt; the linear map?</span>

Now that I read the paper more carefully, I can see that this is the
case, and this does catch issues which DEBUG_RODATA cannot.

Apologies for the noise.

Thanks,
Mark.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=153531">Juerg Haefliger</a> - Nov. 4, 2016, 2:45 p.m.</div>
<pre class="content">
Changes from:
  v2 -&gt; v3:
    - Removed &#39;depends on DEBUG_KERNEL&#39; and &#39;select DEBUG_TLBFLUSH&#39;.
      These are left-overs from the original patch and are not required.
    - Make libata XPFO-aware, i.e., properly handle pages that were
      unmapped by XPFO. This takes care of the temporary hack in v2 that
      forced the use of a bounce buffer in block/blk-map.c.
  v1 -&gt; v2:
    - Moved the code from arch/x86/mm/ to mm/ since it&#39;s (mostly)
      arch-agnostic.
    - Moved the config to the generic layer and added ARCH_SUPPORTS_XPFO
      for x86.
    - Use page_ext for the additional per-page data.
    - Removed the clearing of pages. This can be accomplished by using
      PAGE_POISONING.
    - Split up the patch into multiple patches.
    - Fixed additional issues identified by reviewers.

This patch series adds support for XPFO which protects against &#39;ret2dir&#39;
kernel attacks. The basic idea is to enforce exclusive ownership of page
frames by either the kernel or userspace, unless explicitly requested by
the kernel. Whenever a page destined for userspace is allocated, it is
unmapped from physmap (removed from the kernel&#39;s page table). When such a
page is reclaimed from userspace, it is mapped back to physmap.

Additional fields in the page_ext struct are used for XPFO housekeeping.
Specifically two flags to distinguish user vs. kernel pages and to tag
unmapped pages and a reference counter to balance kmap/kunmap operations
and a lock to serialize access to the XPFO fields.

Known issues/limitations:
  - Only supports x86-64 (for now)
  - Only supports 4k pages (for now)
  - There are most likely some legitimate uses cases where the kernel needs
    to access userspace which need to be made XPFO-aware
  - Performance penalty

Reference paper by the original patch authors:
  http://www.cs.columbia.edu/~vpk/papers/ret2dir.sec14.pdf

Juerg Haefliger (2):
  Add support for eXclusive Page Frame Ownership (XPFO)
  xpfo: Only put previous userspace pages into the hot cache

 arch/x86/Kconfig         |   3 +-
 arch/x86/mm/init.c       |   2 +-
 drivers/ata/libata-sff.c |   4 +-
 include/linux/highmem.h  |  15 +++-
 include/linux/page_ext.h |   7 ++
 include/linux/xpfo.h     |  41 +++++++++
 lib/swiotlb.c            |   3 +-
 mm/Makefile              |   1 +
 mm/page_alloc.c          |  10 ++-
 mm/page_ext.c            |   4 +
 mm/xpfo.c                | 214 +++++++++++++++++++++++++++++++++++++++++++++++
 security/Kconfig         |  19 +++++
 12 files changed, 315 insertions(+), 8 deletions(-)
 create mode 100644 include/linux/xpfo.h
 create mode 100644 mm/xpfo.c
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig</span>
<span class="p_header">index c46662f..9d32b4a 100644</span>
<span class="p_header">--- a/arch/x86/Kconfig</span>
<span class="p_header">+++ b/arch/x86/Kconfig</span>
<span class="p_chunk">@@ -1343,7 +1343,7 @@</span> <span class="p_context"> config ARCH_DMA_ADDR_T_64BIT</span>
 
 config X86_DIRECT_GBPAGES
 	def_bool y
<span class="p_del">-	depends on X86_64 &amp;&amp; !DEBUG_PAGEALLOC &amp;&amp; !KMEMCHECK</span>
<span class="p_add">+	depends on X86_64 &amp;&amp; !DEBUG_PAGEALLOC &amp;&amp; !KMEMCHECK &amp;&amp; !XPFO</span>
 	---help---
 	  Certain kernel features effectively disable kernel
 	  linear 1 GB mappings (even if the CPU otherwise
<span class="p_header">diff --git a/arch/x86/Kconfig.debug b/arch/x86/Kconfig.debug</span>
<span class="p_header">index 9b18ed9..1331da5 100644</span>
<span class="p_header">--- a/arch/x86/Kconfig.debug</span>
<span class="p_header">+++ b/arch/x86/Kconfig.debug</span>
<span class="p_chunk">@@ -5,6 +5,23 @@</span> <span class="p_context"> config TRACE_IRQFLAGS_SUPPORT</span>
 
 source &quot;lib/Kconfig.debug&quot;
 
<span class="p_add">+config XPFO</span>
<span class="p_add">+	bool &quot;Enable eXclusive Page Frame Ownership (XPFO)&quot;</span>
<span class="p_add">+	default n</span>
<span class="p_add">+	depends on DEBUG_KERNEL</span>
<span class="p_add">+	depends on X86_64</span>
<span class="p_add">+	select DEBUG_TLBFLUSH</span>
<span class="p_add">+	---help---</span>
<span class="p_add">+	  This option offers protection against &#39;ret2dir&#39; (kernel) attacks.</span>
<span class="p_add">+	  When enabled, every time a page frame is allocated to user space, it</span>
<span class="p_add">+	  is unmapped from the direct mapped RAM region in kernel space</span>
<span class="p_add">+	  (physmap). Similarly, whenever page frames are freed/reclaimed, they</span>
<span class="p_add">+	  are mapped back to physmap. Special care is taken to minimize the</span>
<span class="p_add">+	  impact on performance by reducing TLB shootdowns and unnecessary page</span>
<span class="p_add">+	  zero fills.</span>
<span class="p_add">+</span>
<span class="p_add">+	  If in doubt, say &quot;N&quot;.</span>
<span class="p_add">+</span>
 config X86_VERBOSE_BOOTUP
 	bool &quot;Enable verbose x86 bootup info messages&quot;
 	default y
<span class="p_header">diff --git a/arch/x86/mm/Makefile b/arch/x86/mm/Makefile</span>
<span class="p_header">index f9d38a4..8bf52b6 100644</span>
<span class="p_header">--- a/arch/x86/mm/Makefile</span>
<span class="p_header">+++ b/arch/x86/mm/Makefile</span>
<span class="p_chunk">@@ -34,3 +34,5 @@</span> <span class="p_context"> obj-$(CONFIG_ACPI_NUMA)		+= srat.o</span>
 obj-$(CONFIG_NUMA_EMU)		+= numa_emulation.o
 
 obj-$(CONFIG_X86_INTEL_MPX)	+= mpx.o
<span class="p_add">+</span>
<span class="p_add">+obj-$(CONFIG_XPFO)		+= xpfo.o</span>
<span class="p_header">diff --git a/arch/x86/mm/init.c b/arch/x86/mm/init.c</span>
<span class="p_header">index 493f541..27fc8a6 100644</span>
<span class="p_header">--- a/arch/x86/mm/init.c</span>
<span class="p_header">+++ b/arch/x86/mm/init.c</span>
<span class="p_chunk">@@ -150,7 +150,8 @@</span> <span class="p_context"> static int page_size_mask;</span>
 
 static void __init probe_page_size_mask(void)
 {
<span class="p_del">-#if !defined(CONFIG_DEBUG_PAGEALLOC) &amp;&amp; !defined(CONFIG_KMEMCHECK)</span>
<span class="p_add">+#if !defined(CONFIG_DEBUG_PAGEALLOC) &amp;&amp; !defined(CONFIG_KMEMCHECK) &amp;&amp; \</span>
<span class="p_add">+	!defined(CONFIG_XPFO)</span>
 	/*
 	 * For CONFIG_DEBUG_PAGEALLOC, identity mapping will use small pages.
 	 * This will simplify cpa(), which otherwise needs to support splitting
<span class="p_header">diff --git a/arch/x86/mm/xpfo.c b/arch/x86/mm/xpfo.c</span>
new file mode 100644
<span class="p_header">index 0000000..6bc24d3</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/x86/mm/xpfo.c</span>
<span class="p_chunk">@@ -0,0 +1,176 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2016 Brown University. All rights reserved.</span>
<span class="p_add">+ * Copyright (C) 2016 Hewlett Packard Enterprise Development, L.P.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Authors:</span>
<span class="p_add">+ *   Vasileios P. Kemerlis &lt;vpk@cs.brown.edu&gt;</span>
<span class="p_add">+ *   Juerg Haefliger &lt;juerg.haefliger@hpe.com&gt;</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify it</span>
<span class="p_add">+ * under the terms of the GNU General Public License version 2 as published by</span>
<span class="p_add">+ * the Free Software Foundation.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/mm.h&gt;</span>
<span class="p_add">+#include &lt;linux/module.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/pgtable.h&gt;</span>
<span class="p_add">+#include &lt;asm/tlbflush.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#define TEST_XPFO_FLAG(flag, page) \</span>
<span class="p_add">+	test_bit(PG_XPFO_##flag, &amp;(page)-&gt;xpfo.flags)</span>
<span class="p_add">+</span>
<span class="p_add">+#define SET_XPFO_FLAG(flag, page)			\</span>
<span class="p_add">+	__set_bit(PG_XPFO_##flag, &amp;(page)-&gt;xpfo.flags)</span>
<span class="p_add">+</span>
<span class="p_add">+#define CLEAR_XPFO_FLAG(flag, page)			\</span>
<span class="p_add">+	__clear_bit(PG_XPFO_##flag, &amp;(page)-&gt;xpfo.flags)</span>
<span class="p_add">+</span>
<span class="p_add">+#define TEST_AND_CLEAR_XPFO_FLAG(flag, page)			\</span>
<span class="p_add">+	__test_and_clear_bit(PG_XPFO_##flag, &amp;(page)-&gt;xpfo.flags)</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Update a single kernel page table entry</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void set_kpte(struct page *page, unsigned long kaddr,</span>
<span class="p_add">+			    pgprot_t prot) {</span>
<span class="p_add">+	unsigned int level;</span>
<span class="p_add">+	pte_t *kpte = lookup_address(kaddr, &amp;level);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* We only support 4k pages for now */</span>
<span class="p_add">+	BUG_ON(!kpte || level != PG_LEVEL_4K);</span>
<span class="p_add">+</span>
<span class="p_add">+	set_pte_atomic(kpte, pfn_pte(page_to_pfn(page), canon_pgprot(prot)));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+inline void xpfo_clear_zap(struct page *page, int order)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i &lt; (1 &lt;&lt; order); i++)</span>
<span class="p_add">+		CLEAR_XPFO_FLAG(zap, page + i);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+inline int xpfo_test_and_clear_zap(struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return TEST_AND_CLEAR_XPFO_FLAG(zap, page);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+inline int xpfo_test_kernel(struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return TEST_XPFO_FLAG(kernel, page);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+inline int xpfo_test_user(struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return TEST_XPFO_FLAG(user, page);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void xpfo_alloc_page(struct page *page, int order, gfp_t gfp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int i, tlb_shoot = 0;</span>
<span class="p_add">+	unsigned long kaddr;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i &lt; (1 &lt;&lt; order); i++)  {</span>
<span class="p_add">+		WARN_ON(TEST_XPFO_FLAG(user_fp, page + i) ||</span>
<span class="p_add">+			TEST_XPFO_FLAG(user, page + i));</span>
<span class="p_add">+</span>
<span class="p_add">+		if (gfp &amp; GFP_HIGHUSER) {</span>
<span class="p_add">+			/* Initialize the xpfo lock and map counter */</span>
<span class="p_add">+			spin_lock_init(&amp;(page + i)-&gt;xpfo.lock);</span>
<span class="p_add">+			atomic_set(&amp;(page + i)-&gt;xpfo.mapcount, 0);</span>
<span class="p_add">+</span>
<span class="p_add">+			/* Mark it as a user page */</span>
<span class="p_add">+			SET_XPFO_FLAG(user_fp, page + i);</span>
<span class="p_add">+</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * Shoot the TLB if the page was previously allocated</span>
<span class="p_add">+			 * to kernel space</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			if (TEST_AND_CLEAR_XPFO_FLAG(kernel, page + i))</span>
<span class="p_add">+				tlb_shoot = 1;</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			/* Mark it as a kernel page */</span>
<span class="p_add">+			SET_XPFO_FLAG(kernel, page + i);</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (tlb_shoot) {</span>
<span class="p_add">+		kaddr = (unsigned long)page_address(page);</span>
<span class="p_add">+		flush_tlb_kernel_range(kaddr, kaddr + (1 &lt;&lt; order) *</span>
<span class="p_add">+				       PAGE_SIZE);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void xpfo_free_page(struct page *page, int order)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+	unsigned long kaddr;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i &lt; (1 &lt;&lt; order); i++) {</span>
<span class="p_add">+</span>
<span class="p_add">+		/* The page frame was previously allocated to user space */</span>
<span class="p_add">+		if (TEST_AND_CLEAR_XPFO_FLAG(user, page + i)) {</span>
<span class="p_add">+			kaddr = (unsigned long)page_address(page + i);</span>
<span class="p_add">+</span>
<span class="p_add">+			/* Clear the page and mark it accordingly */</span>
<span class="p_add">+			clear_page((void *)kaddr);</span>
<span class="p_add">+			SET_XPFO_FLAG(zap, page + i);</span>
<span class="p_add">+</span>
<span class="p_add">+			/* Map it back to kernel space */</span>
<span class="p_add">+			set_kpte(page + i,  kaddr, __pgprot(__PAGE_KERNEL));</span>
<span class="p_add">+</span>
<span class="p_add">+			/* No TLB update */</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		/* Clear the xpfo fast-path flag */</span>
<span class="p_add">+		CLEAR_XPFO_FLAG(user_fp, page + i);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void xpfo_kmap(void *kaddr, struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* The page is allocated to kernel space, so nothing to do */</span>
<span class="p_add">+	if (TEST_XPFO_FLAG(kernel, page))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	spin_lock_irqsave(&amp;page-&gt;xpfo.lock, flags);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The page was previously allocated to user space, so map it back</span>
<span class="p_add">+	 * into the kernel. No TLB update required.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if ((atomic_inc_return(&amp;page-&gt;xpfo.mapcount) == 1) &amp;&amp;</span>
<span class="p_add">+	    TEST_XPFO_FLAG(user, page))</span>
<span class="p_add">+		set_kpte(page, (unsigned long)kaddr, __pgprot(__PAGE_KERNEL));</span>
<span class="p_add">+</span>
<span class="p_add">+	spin_unlock_irqrestore(&amp;page-&gt;xpfo.lock, flags);</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(xpfo_kmap);</span>
<span class="p_add">+</span>
<span class="p_add">+void xpfo_kunmap(void *kaddr, struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* The page is allocated to kernel space, so nothing to do */</span>
<span class="p_add">+	if (TEST_XPFO_FLAG(kernel, page))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	spin_lock_irqsave(&amp;page-&gt;xpfo.lock, flags);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The page frame is to be allocated back to user space. So unmap it</span>
<span class="p_add">+	 * from the kernel, update the TLB and mark it as a user page.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if ((atomic_dec_return(&amp;page-&gt;xpfo.mapcount) == 0) &amp;&amp;</span>
<span class="p_add">+	    (TEST_XPFO_FLAG(user_fp, page) || TEST_XPFO_FLAG(user, page))) {</span>
<span class="p_add">+		set_kpte(page, (unsigned long)kaddr, __pgprot(0));</span>
<span class="p_add">+		__flush_tlb_one((unsigned long)kaddr);</span>
<span class="p_add">+		SET_XPFO_FLAG(user, page);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	spin_unlock_irqrestore(&amp;page-&gt;xpfo.lock, flags);</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(xpfo_kunmap);</span>
<span class="p_header">diff --git a/block/blk-map.c b/block/blk-map.c</span>
<span class="p_header">index f565e11..b7b8302 100644</span>
<span class="p_header">--- a/block/blk-map.c</span>
<span class="p_header">+++ b/block/blk-map.c</span>
<span class="p_chunk">@@ -107,7 +107,12 @@</span> <span class="p_context"> int blk_rq_map_user_iov(struct request_queue *q, struct request *rq,</span>
 		prv.iov_len = iov.iov_len;
 	}
 
<span class="p_del">-	if (unaligned || (q-&gt;dma_pad_mask &amp; iter-&gt;count) || map_data)</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * juergh: Temporary hack to force the use of a bounce buffer if XPFO</span>
<span class="p_add">+	 * is enabled. Results in an XPFO page fault otherwise.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (unaligned || (q-&gt;dma_pad_mask &amp; iter-&gt;count) || map_data ||</span>
<span class="p_add">+	    IS_ENABLED(CONFIG_XPFO))</span>
 		bio = bio_copy_user_iov(q, map_data, iter, gfp_mask);
 	else
 		bio = bio_map_user_iov(q, iter, gfp_mask);
<span class="p_header">diff --git a/include/linux/highmem.h b/include/linux/highmem.h</span>
<span class="p_header">index bb3f329..0ca9130 100644</span>
<span class="p_header">--- a/include/linux/highmem.h</span>
<span class="p_header">+++ b/include/linux/highmem.h</span>
<span class="p_chunk">@@ -55,24 +55,37 @@</span> <span class="p_context"> static inline struct page *kmap_to_page(void *addr)</span>
 #ifndef ARCH_HAS_KMAP
 static inline void *kmap(struct page *page)
 {
<span class="p_add">+	void *kaddr;</span>
<span class="p_add">+</span>
 	might_sleep();
<span class="p_del">-	return page_address(page);</span>
<span class="p_add">+</span>
<span class="p_add">+	kaddr = page_address(page);</span>
<span class="p_add">+	xpfo_kmap(kaddr, page);</span>
<span class="p_add">+	return kaddr;</span>
 }
 
 static inline void kunmap(struct page *page)
 {
<span class="p_add">+	xpfo_kunmap(page_address(page), page);</span>
 }
 
 static inline void *kmap_atomic(struct page *page)
 {
<span class="p_add">+	void *kaddr;</span>
<span class="p_add">+</span>
 	preempt_disable();
 	pagefault_disable();
<span class="p_del">-	return page_address(page);</span>
<span class="p_add">+</span>
<span class="p_add">+	kaddr = page_address(page);</span>
<span class="p_add">+	xpfo_kmap(kaddr, page);</span>
<span class="p_add">+	return kaddr;</span>
 }
 #define kmap_atomic_prot(page, prot)	kmap_atomic(page)
 
 static inline void __kunmap_atomic(void *addr)
 {
<span class="p_add">+	xpfo_kunmap(addr, virt_to_page(addr));</span>
<span class="p_add">+</span>
 	pagefault_enable();
 	preempt_enable();
 }
<span class="p_chunk">@@ -133,7 +146,8 @@</span> <span class="p_context"> do {                                                            \</span>
 static inline void clear_user_highpage(struct page *page, unsigned long vaddr)
 {
 	void *addr = kmap_atomic(page);
<span class="p_del">-	clear_user_page(addr, vaddr, page);</span>
<span class="p_add">+	if (!xpfo_test_and_clear_zap(page))</span>
<span class="p_add">+		clear_user_page(addr, vaddr, page);</span>
 	kunmap_atomic(addr);
 }
 #endif
<span class="p_chunk">@@ -186,7 +200,8 @@</span> <span class="p_context"> alloc_zeroed_user_highpage_movable(struct vm_area_struct *vma,</span>
 static inline void clear_highpage(struct page *page)
 {
 	void *kaddr = kmap_atomic(page);
<span class="p_del">-	clear_page(kaddr);</span>
<span class="p_add">+	if (!xpfo_test_and_clear_zap(page))</span>
<span class="p_add">+		clear_page(kaddr);</span>
 	kunmap_atomic(kaddr);
 }
 
<span class="p_header">diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h</span>
<span class="p_header">index 624b78b..71c95aa 100644</span>
<span class="p_header">--- a/include/linux/mm_types.h</span>
<span class="p_header">+++ b/include/linux/mm_types.h</span>
<span class="p_chunk">@@ -12,6 +12,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/cpumask.h&gt;
 #include &lt;linux/uprobes.h&gt;
 #include &lt;linux/page-flags-layout.h&gt;
<span class="p_add">+#include &lt;linux/xpfo.h&gt;</span>
 #include &lt;asm/page.h&gt;
 #include &lt;asm/mmu.h&gt;
 
<span class="p_chunk">@@ -215,6 +216,9 @@</span> <span class="p_context"> struct page {</span>
 #ifdef LAST_CPUPID_NOT_IN_PAGE_FLAGS
 	int _last_cpupid;
 #endif
<span class="p_add">+#ifdef CONFIG_XPFO</span>
<span class="p_add">+	struct xpfo_info xpfo;</span>
<span class="p_add">+#endif</span>
 }
 /*
  * The struct page can be forced to be double word aligned so that atomic ops
<span class="p_header">diff --git a/include/linux/xpfo.h b/include/linux/xpfo.h</span>
new file mode 100644
<span class="p_header">index 0000000..c4f0871</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/include/linux/xpfo.h</span>
<span class="p_chunk">@@ -0,0 +1,88 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2016 Brown University. All rights reserved.</span>
<span class="p_add">+ * Copyright (C) 2016 Hewlett Packard Enterprise Development, L.P.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Authors:</span>
<span class="p_add">+ *   Vasileios P. Kemerlis &lt;vpk@cs.brown.edu&gt;</span>
<span class="p_add">+ *   Juerg Haefliger &lt;juerg.haefliger@hpe.com&gt;</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify it</span>
<span class="p_add">+ * under the terms of the GNU General Public License version 2 as published by</span>
<span class="p_add">+ * the Free Software Foundation.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _LINUX_XPFO_H</span>
<span class="p_add">+#define _LINUX_XPFO_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_XPFO</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * XPFO page flags:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * PG_XPFO_user_fp denotes that the page is allocated to user space. This flag</span>
<span class="p_add">+ * is used in the fast path, where the page is marked accordingly but *not*</span>
<span class="p_add">+ * unmapped from the kernel. In most cases, the kernel will need access to the</span>
<span class="p_add">+ * page immediately after its acquisition so an unnecessary mapping operation</span>
<span class="p_add">+ * is avoided.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * PG_XPFO_user denotes that the page is destined for user space. This flag is</span>
<span class="p_add">+ * used in the slow path, where the page needs to be mapped/unmapped when the</span>
<span class="p_add">+ * kernel wants to access it. If a page is deallocated and this flag is set,</span>
<span class="p_add">+ * the page is cleared and mapped back into the kernel.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * PG_XPFO_kernel denotes a page that is destined to kernel space. This is used</span>
<span class="p_add">+ * for identifying pages that are first assigned to kernel space and then freed</span>
<span class="p_add">+ * and mapped to user space. In such cases, an expensive TLB shootdown is</span>
<span class="p_add">+ * necessary. Pages allocated to user space, freed, and subsequently allocated</span>
<span class="p_add">+ * to user space again, require only local TLB invalidation.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * PG_XPFO_zap indicates that the page has been zapped. This flag is used to</span>
<span class="p_add">+ * avoid zapping pages multiple times. Whenever a page is freed and was</span>
<span class="p_add">+ * previously mapped to user space, it needs to be zapped before mapped back</span>
<span class="p_add">+ * in to the kernel.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+enum xpfo_pageflags {</span>
<span class="p_add">+	PG_XPFO_user_fp,</span>
<span class="p_add">+	PG_XPFO_user,</span>
<span class="p_add">+	PG_XPFO_kernel,</span>
<span class="p_add">+	PG_XPFO_zap,</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+struct xpfo_info {</span>
<span class="p_add">+	unsigned long flags;	/* Flags for tracking the page&#39;s XPFO state */</span>
<span class="p_add">+	atomic_t mapcount;	/* Counter for balancing page map/unmap</span>
<span class="p_add">+				 * requests. Only the first map request maps</span>
<span class="p_add">+				 * the page back to kernel space. Likewise,</span>
<span class="p_add">+				 * only the last unmap request unmaps the page.</span>
<span class="p_add">+				 */</span>
<span class="p_add">+	spinlock_t lock;	/* Lock to serialize concurrent map/unmap</span>
<span class="p_add">+				 * requests.</span>
<span class="p_add">+				 */</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+extern void xpfo_clear_zap(struct page *page, int order);</span>
<span class="p_add">+extern int xpfo_test_and_clear_zap(struct page *page);</span>
<span class="p_add">+extern int xpfo_test_kernel(struct page *page);</span>
<span class="p_add">+extern int xpfo_test_user(struct page *page);</span>
<span class="p_add">+</span>
<span class="p_add">+extern void xpfo_kmap(void *kaddr, struct page *page);</span>
<span class="p_add">+extern void xpfo_kunmap(void *kaddr, struct page *page);</span>
<span class="p_add">+extern void xpfo_alloc_page(struct page *page, int order, gfp_t gfp);</span>
<span class="p_add">+extern void xpfo_free_page(struct page *page, int order);</span>
<span class="p_add">+</span>
<span class="p_add">+#else /* ifdef CONFIG_XPFO */</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void xpfo_clear_zap(struct page *page, int order) { }</span>
<span class="p_add">+static inline int xpfo_test_and_clear_zap(struct page *page) { return 0; }</span>
<span class="p_add">+static inline int xpfo_test_kernel(struct page *page) { return 0; }</span>
<span class="p_add">+static inline int xpfo_test_user(struct page *page) { return 0; }</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void xpfo_kmap(void *kaddr, struct page *page) { }</span>
<span class="p_add">+static inline void xpfo_kunmap(void *kaddr, struct page *page) { }</span>
<span class="p_add">+static inline void xpfo_alloc_page(struct page *page, int order, gfp_t gfp) { }</span>
<span class="p_add">+static inline void xpfo_free_page(struct page *page, int order) { }</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* ifdef CONFIG_XPFO */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* ifndef _LINUX_XPFO_H */</span>
<span class="p_header">diff --git a/lib/swiotlb.c b/lib/swiotlb.c</span>
<span class="p_header">index 76f29ec..cf57ee9 100644</span>
<span class="p_header">--- a/lib/swiotlb.c</span>
<span class="p_header">+++ b/lib/swiotlb.c</span>
<span class="p_chunk">@@ -390,8 +390,9 @@</span> <span class="p_context"> static void swiotlb_bounce(phys_addr_t orig_addr, phys_addr_t tlb_addr,</span>
 {
 	unsigned long pfn = PFN_DOWN(orig_addr);
 	unsigned char *vaddr = phys_to_virt(tlb_addr);
<span class="p_add">+	struct page *page = pfn_to_page(pfn);</span>
 
<span class="p_del">-	if (PageHighMem(pfn_to_page(pfn))) {</span>
<span class="p_add">+	if (PageHighMem(page) || xpfo_test_user(page)) {</span>
 		/* The buffer does not have a mapping.  Map it in and copy */
 		unsigned int offset = orig_addr &amp; ~PAGE_MASK;
 		char *buffer;
<span class="p_header">diff --git a/mm/page_alloc.c b/mm/page_alloc.c</span>
<span class="p_header">index 838ca8bb..47b42a3 100644</span>
<span class="p_header">--- a/mm/page_alloc.c</span>
<span class="p_header">+++ b/mm/page_alloc.c</span>
<span class="p_chunk">@@ -1003,6 +1003,7 @@</span> <span class="p_context"> static bool free_pages_prepare(struct page *page, unsigned int order)</span>
 	}
 	arch_free_page(page, order);
 	kernel_map_pages(page, 1 &lt;&lt; order, 0);
<span class="p_add">+	xpfo_free_page(page, order);</span>
 
 	return true;
 }
<span class="p_chunk">@@ -1398,10 +1399,13 @@</span> <span class="p_context"> static int prep_new_page(struct page *page, unsigned int order, gfp_t gfp_flags,</span>
 	arch_alloc_page(page, order);
 	kernel_map_pages(page, 1 &lt;&lt; order, 1);
 	kasan_alloc_pages(page, order);
<span class="p_add">+	xpfo_alloc_page(page, order, gfp_flags);</span>
 
 	if (gfp_flags &amp; __GFP_ZERO)
 		for (i = 0; i &lt; (1 &lt;&lt; order); i++)
 			clear_highpage(page + i);
<span class="p_add">+	else</span>
<span class="p_add">+		xpfo_clear_zap(page, order);</span>
 
 	if (order &amp;&amp; (gfp_flags &amp; __GFP_COMP))
 		prep_compound_page(page, order);
<span class="p_chunk">@@ -2072,10 +2076,11 @@</span> <span class="p_context"> void free_hot_cold_page(struct page *page, bool cold)</span>
 	}
 
 	pcp = &amp;this_cpu_ptr(zone-&gt;pageset)-&gt;pcp;
<span class="p_del">-	if (!cold)</span>
<span class="p_add">+	if (!cold &amp;&amp; !xpfo_test_kernel(page))</span>
 		list_add(&amp;page-&gt;lru, &amp;pcp-&gt;lists[migratetype]);
 	else
 		list_add_tail(&amp;page-&gt;lru, &amp;pcp-&gt;lists[migratetype]);
<span class="p_add">+</span>
 	pcp-&gt;count++;
 	if (pcp-&gt;count &gt;= pcp-&gt;high) {
 		unsigned long batch = READ_ONCE(pcp-&gt;batch);

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



