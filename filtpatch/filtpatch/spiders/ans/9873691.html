
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v2,4/4] mm: fix KSM data corruption - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v2,4/4] mm: fix KSM data corruption</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=36811">Minchan Kim</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Aug. 1, 2017, 5:56 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1501566977-20293-5-git-send-email-minchan@kernel.org&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9873691/mbox/"
   >mbox</a>
|
   <a href="/patch/9873691/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9873691/">/patch/9873691/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	DF3FD60361 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  1 Aug 2017 05:56:41 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id CCBA028563
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  1 Aug 2017 05:56:41 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id C192F2856D; Tue,  1 Aug 2017 05:56:41 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 266EC28563
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  1 Aug 2017 05:56:41 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1751779AbdHAF40 (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 1 Aug 2017 01:56:26 -0400
Received: from LGEAMRELO12.lge.com ([156.147.23.52]:59030 &quot;EHLO
	lgeamrelo12.lge.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1751201AbdHAF4V (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 1 Aug 2017 01:56:21 -0400
Received: from unknown (HELO lgeamrelo02.lge.com) (156.147.1.126)
	by 156.147.23.52 with ESMTP; 1 Aug 2017 14:56:20 +0900
X-Original-SENDERIP: 156.147.1.126
X-Original-MAILFROM: minchan@kernel.org
Received: from unknown (HELO localhost.localdomain) (10.177.220.163)
	by 156.147.1.126 with ESMTP; 1 Aug 2017 14:56:19 +0900
X-Original-SENDERIP: 10.177.220.163
X-Original-MAILFROM: minchan@kernel.org
From: Minchan Kim &lt;minchan@kernel.org&gt;
To: Andrew Morton &lt;akpm@linux-foundation.org&gt;
Cc: linux-kernel@vger.kernel.org, linux-mm@kvack.org,
	kernel-team &lt;kernel-team@lge.com&gt;, Minchan Kim &lt;minchan@kernel.org&gt;,
	Nadav Amit &lt;nadav.amit@gmail.com&gt;,
	Mel Gorman &lt;mgorman@techsingularity.net&gt;,
	Hugh Dickins &lt;hughd@google.com&gt;, Andrea Arcangeli &lt;aarcange@redhat.com&gt;
Subject: [PATCH v2 4/4] mm: fix KSM data corruption
Date: Tue,  1 Aug 2017 14:56:17 +0900
Message-Id: &lt;1501566977-20293-5-git-send-email-minchan@kernel.org&gt;
X-Mailer: git-send-email 2.7.4
In-Reply-To: &lt;1501566977-20293-1-git-send-email-minchan@kernel.org&gt;
References: &lt;1501566977-20293-1-git-send-email-minchan@kernel.org&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36811">Minchan Kim</a> - Aug. 1, 2017, 5:56 a.m.</div>
<pre class="content">
Nadav reported KSM can corrupt the user data by the TLB batching race[1].
That means data user written can be lost.

Quote from Nadav Amit
&quot;
For this race we need 4 CPUs:

CPU0: Caches a writable and dirty PTE entry, and uses the stale value for
write later.

CPU1: Runs madvise_free on the range that includes the PTE. It would clear
the dirty-bit. It batches TLB flushes.

CPU2: Writes 4 to /proc/PID/clear_refs , clearing the PTEs soft-dirty. We
care about the fact that it clears the PTE write-bit, and of course, batches
TLB flushes.

CPU3: Runs KSM. Our purpose is to pass the following test in
write_protect_page():

	if (pte_write(*pvmw.pte) || pte_dirty(*pvmw.pte) ||
	    (pte_protnone(*pvmw.pte) &amp;&amp; pte_savedwrite(*pvmw.pte)))

Since it will avoid TLB flush. And we want to do it while the PTE is stale.
Later, and before replacing the page, we would be able to change the page.

Note that all the operations the CPU1-3 perform canhappen in parallel since
they only acquire mmap_sem for read.

We start with two identical pages. Everything below regards the same
page/PTE.

CPU0		CPU1		CPU2		CPU3
----		----		----		----
Write the same
value on page

[cache PTE as
 dirty in TLB]

		MADV_FREE
		pte_mkclean()

				4 &gt; clear_refs
				pte_wrprotect()

						write_protect_page()
						[ success, no flush ]

						pages_indentical()
						[ ok ]

Write to page
different value

[Ok, using stale
 PTE]

						replace_page()

Later, CPU1, CPU2 and CPU3 would flush the TLB, but that is too late. CPU0
already wrote on the page, but KSM ignored this write, and it got lost.
&quot;

In above scenario, MADV_FREE is fixed by changing TLB batching API
including [set|clear]_tlb_flush_pending. Remained thing is soft-dirty part.

This patch changes soft-dirty uses TLB batching API instead of flush_tlb_mm
and KSM checks pending TLB flush by using mm_tlb_flush_pending so that
it will flush TLB to avoid data lost if there are other parallel threads
pending TLB flush.

[1] http://lkml.kernel.org/r/BD3A0EBE-ECF4-41D4-87FA-C755EA9AB6BD@gmail.com

Note:
I failed to reproduce this problem through Nadav&#39;s test program which
need to tune timing in my system speed so didn&#39;t confirm it work.
Nadav, Could you test this patch on your test machine?

Thanks!

Cc: Nadav Amit &lt;nadav.amit@gmail.com&gt;
Cc: Mel Gorman &lt;mgorman@techsingularity.net&gt;
Cc: Hugh Dickins &lt;hughd@google.com&gt;
Cc: Andrea Arcangeli &lt;aarcange@redhat.com&gt;
<span class="signed-off-by">Signed-off-by: Minchan Kim &lt;minchan@kernel.org&gt;</span>
---
 fs/proc/task_mmu.c | 4 +++-
 mm/ksm.c           | 3 ++-
 2 files changed, 5 insertions(+), 2 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=55071">Nadav Amit</a> - Aug. 1, 2017, 7:21 p.m.</div>
<pre class="content">
Minchan Kim &lt;minchan@kernel.org&gt; wrote:
<span class="quote">
&gt; Nadav reported KSM can corrupt the user data by the TLB batching race[1].</span>
<span class="quote">&gt; That means data user written can be lost.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Quote from Nadav Amit</span>
<span class="quote">&gt; &quot;</span>
<span class="quote">&gt; For this race we need 4 CPUs:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; CPU0: Caches a writable and dirty PTE entry, and uses the stale value for</span>
<span class="quote">&gt; write later.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; CPU1: Runs madvise_free on the range that includes the PTE. It would clear</span>
<span class="quote">&gt; the dirty-bit. It batches TLB flushes.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; CPU2: Writes 4 to /proc/PID/clear_refs , clearing the PTEs soft-dirty. We</span>
<span class="quote">&gt; care about the fact that it clears the PTE write-bit, and of course, batches</span>
<span class="quote">&gt; TLB flushes.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; CPU3: Runs KSM. Our purpose is to pass the following test in</span>
<span class="quote">&gt; write_protect_page():</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	if (pte_write(*pvmw.pte) || pte_dirty(*pvmw.pte) ||</span>
<span class="quote">&gt; 	    (pte_protnone(*pvmw.pte) &amp;&amp; pte_savedwrite(*pvmw.pte)))</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Since it will avoid TLB flush. And we want to do it while the PTE is stale.</span>
<span class="quote">&gt; Later, and before replacing the page, we would be able to change the page.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Note that all the operations the CPU1-3 perform canhappen in parallel since</span>
<span class="quote">&gt; they only acquire mmap_sem for read.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; We start with two identical pages. Everything below regards the same</span>
<span class="quote">&gt; page/PTE.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; CPU0		CPU1		CPU2		CPU3</span>
<span class="quote">&gt; ----		----		----		----</span>
<span class="quote">&gt; Write the same</span>
<span class="quote">&gt; value on page</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; [cache PTE as</span>
<span class="quote">&gt; dirty in TLB]</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 		MADV_FREE</span>
<span class="quote">&gt; 		pte_mkclean()</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 				4 &gt; clear_refs</span>
<span class="quote">&gt; 				pte_wrprotect()</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 						write_protect_page()</span>
<span class="quote">&gt; 						[ success, no flush ]</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 						pages_indentical()</span>
<span class="quote">&gt; 						[ ok ]</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Write to page</span>
<span class="quote">&gt; different value</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; [Ok, using stale</span>
<span class="quote">&gt; PTE]</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 						replace_page()</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Later, CPU1, CPU2 and CPU3 would flush the TLB, but that is too late. CPU0</span>
<span class="quote">&gt; already wrote on the page, but KSM ignored this write, and it got lost.</span>
<span class="quote">&gt; &quot;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; In above scenario, MADV_FREE is fixed by changing TLB batching API</span>
<span class="quote">&gt; including [set|clear]_tlb_flush_pending. Remained thing is soft-dirty part.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This patch changes soft-dirty uses TLB batching API instead of flush_tlb_mm</span>
<span class="quote">&gt; and KSM checks pending TLB flush by using mm_tlb_flush_pending so that</span>
<span class="quote">&gt; it will flush TLB to avoid data lost if there are other parallel threads</span>
<span class="quote">&gt; pending TLB flush.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; [1] http://lkml.kernel.org/r/BD3A0EBE-ECF4-41D4-87FA-C755EA9AB6BD@gmail.com</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Note:</span>
<span class="quote">&gt; I failed to reproduce this problem through Nadav&#39;s test program which</span>
<span class="quote">&gt; need to tune timing in my system speed so didn&#39;t confirm it work.</span>
<span class="quote">&gt; Nadav, Could you test this patch on your test machine?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Thanks!</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Cc: Nadav Amit &lt;nadav.amit@gmail.com&gt;</span>
<span class="quote">&gt; Cc: Mel Gorman &lt;mgorman@techsingularity.net&gt;</span>
<span class="quote">&gt; Cc: Hugh Dickins &lt;hughd@google.com&gt;</span>
<span class="quote">&gt; Cc: Andrea Arcangeli &lt;aarcange@redhat.com&gt;</span>
<span class="quote">&gt; Signed-off-by: Minchan Kim &lt;minchan@kernel.org&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt; fs/proc/task_mmu.c | 4 +++-</span>
<span class="quote">&gt; mm/ksm.c           | 3 ++-</span>
<span class="quote">&gt; 2 files changed, 5 insertions(+), 2 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/fs/proc/task_mmu.c b/fs/proc/task_mmu.c</span>
<span class="quote">&gt; index 9782dedeead7..58ef3a6abbc0 100644</span>
<span class="quote">&gt; --- a/fs/proc/task_mmu.c</span>
<span class="quote">&gt; +++ b/fs/proc/task_mmu.c</span>
<span class="quote">&gt; @@ -1018,6 +1018,7 @@ static ssize_t clear_refs_write(struct file *file, const char __user *buf,</span>
<span class="quote">&gt; 	enum clear_refs_types type;</span>
<span class="quote">&gt; 	int itype;</span>
<span class="quote">&gt; 	int rv;</span>
<span class="quote">&gt; +	struct mmu_gather tlb;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	memset(buffer, 0, sizeof(buffer));</span>
<span class="quote">&gt; 	if (count &gt; sizeof(buffer) - 1)</span>
<span class="quote">&gt; @@ -1062,6 +1063,7 @@ static ssize_t clear_refs_write(struct file *file, const char __user *buf,</span>
<span class="quote">&gt; 		}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 		down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt; +		tlb_gather_mmu(&amp;tlb, mm, 0, -1);</span>
<span class="quote">&gt; 		if (type == CLEAR_REFS_SOFT_DIRTY) {</span>
<span class="quote">&gt; 			for (vma = mm-&gt;mmap; vma; vma = vma-&gt;vm_next) {</span>
<span class="quote">&gt; 				if (!(vma-&gt;vm_flags &amp; VM_SOFTDIRTY))</span>
<span class="quote">&gt; @@ -1083,7 +1085,7 @@ static ssize_t clear_refs_write(struct file *file, const char __user *buf,</span>
<span class="quote">&gt; 		walk_page_range(0, mm-&gt;highest_vm_end, &amp;clear_refs_walk);</span>
<span class="quote">&gt; 		if (type == CLEAR_REFS_SOFT_DIRTY)</span>
<span class="quote">&gt; 			mmu_notifier_invalidate_range_end(mm, 0, -1);</span>
<span class="quote">&gt; -		flush_tlb_mm(mm);</span>
<span class="quote">&gt; +		tlb_finish_mmu(&amp;tlb, 0, -1);</span>
<span class="quote">&gt; 		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt; out_mm:</span>
<span class="quote">&gt; 		mmput(mm);</span>
<span class="quote">&gt; diff --git a/mm/ksm.c b/mm/ksm.c</span>
<span class="quote">&gt; index 0c927e36a639..15dd7415f7b3 100644</span>
<span class="quote">&gt; --- a/mm/ksm.c</span>
<span class="quote">&gt; +++ b/mm/ksm.c</span>
<span class="quote">&gt; @@ -1038,7 +1038,8 @@ static int write_protect_page(struct vm_area_struct *vma, struct page *page,</span>
<span class="quote">&gt; 		goto out_unlock;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	if (pte_write(*pvmw.pte) || pte_dirty(*pvmw.pte) ||</span>
<span class="quote">&gt; -	    (pte_protnone(*pvmw.pte) &amp;&amp; pte_savedwrite(*pvmw.pte))) {</span>
<span class="quote">&gt; +	    (pte_protnone(*pvmw.pte) &amp;&amp; pte_savedwrite(*pvmw.pte)) ||</span>
<span class="quote">&gt; +						mm_tlb_flush_pending(mm)) {</span>
<span class="quote">&gt; 		pte_t entry;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 		swapped = PageSwapCache(page);</span>
<span class="quote">&gt; -- </span>
<span class="quote">&gt; 2.7.4</span>

I tested the patch-set, and my PoC does not fail anymore.

Thanks,
Nadav
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=965">Andrea Arcangeli</a> - Aug. 1, 2017, 7:33 p.m.</div>
<pre class="content">
Hello,

On Tue, Aug 01, 2017 at 02:56:17PM +0900, Minchan Kim wrote:
<span class="quote">&gt; CPU0		CPU1		CPU2		CPU3</span>
<span class="quote">&gt; ----		----		----		----</span>
<span class="quote">&gt; Write the same</span>
<span class="quote">&gt; value on page</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; [cache PTE as</span>
<span class="quote">&gt;  dirty in TLB]</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 		MADV_FREE</span>
<span class="quote">&gt; 		pte_mkclean()</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 				4 &gt; clear_refs</span>
<span class="quote">&gt; 				pte_wrprotect()</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 						write_protect_page()</span>
<span class="quote">&gt; 						[ success, no flush ]</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 						pages_indentical()</span>
<span class="quote">&gt; 						[ ok ]</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Write to page</span>
<span class="quote">&gt; different value</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; [Ok, using stale</span>
<span class="quote">&gt;  PTE]</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 						replace_page()</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Later, CPU1, CPU2 and CPU3 would flush the TLB, but that is too late. CPU0</span>
<span class="quote">&gt; already wrote on the page, but KSM ignored this write, and it got lost.</span>
<span class="quote">&gt; &quot;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; In above scenario, MADV_FREE is fixed by changing TLB batching API</span>
<span class="quote">&gt; including [set|clear]_tlb_flush_pending. Remained thing is soft-dirty part.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This patch changes soft-dirty uses TLB batching API instead of flush_tlb_mm</span>
<span class="quote">&gt; and KSM checks pending TLB flush by using mm_tlb_flush_pending so that</span>
<span class="quote">&gt; it will flush TLB to avoid data lost if there are other parallel threads</span>
<span class="quote">&gt; pending TLB flush.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; [1] http://lkml.kernel.org/r/BD3A0EBE-ECF4-41D4-87FA-C755EA9AB6BD@gmail.com</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Note:</span>
<span class="quote">&gt; I failed to reproduce this problem through Nadav&#39;s test program which</span>
<span class="quote">&gt; need to tune timing in my system speed so didn&#39;t confirm it work.</span>
<span class="quote">&gt; Nadav, Could you test this patch on your test machine?</span>
<span class="reviewed-by">
Reviewed-by: Andrea Arcangeli &lt;aarcange@redhat.com&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36811">Minchan Kim</a> - Aug. 2, 2017, 12:58 a.m.</div>
<pre class="content">
On Tue, Aug 01, 2017 at 12:21:41PM -0700, Nadav Amit wrote:
<span class="quote">&gt; Minchan Kim &lt;minchan@kernel.org&gt; wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; Nadav reported KSM can corrupt the user data by the TLB batching race[1].</span>
<span class="quote">&gt; &gt; That means data user written can be lost.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Quote from Nadav Amit</span>
<span class="quote">&gt; &gt; &quot;</span>
<span class="quote">&gt; &gt; For this race we need 4 CPUs:</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; CPU0: Caches a writable and dirty PTE entry, and uses the stale value for</span>
<span class="quote">&gt; &gt; write later.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; CPU1: Runs madvise_free on the range that includes the PTE. It would clear</span>
<span class="quote">&gt; &gt; the dirty-bit. It batches TLB flushes.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; CPU2: Writes 4 to /proc/PID/clear_refs , clearing the PTEs soft-dirty. We</span>
<span class="quote">&gt; &gt; care about the fact that it clears the PTE write-bit, and of course, batches</span>
<span class="quote">&gt; &gt; TLB flushes.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; CPU3: Runs KSM. Our purpose is to pass the following test in</span>
<span class="quote">&gt; &gt; write_protect_page():</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; 	if (pte_write(*pvmw.pte) || pte_dirty(*pvmw.pte) ||</span>
<span class="quote">&gt; &gt; 	    (pte_protnone(*pvmw.pte) &amp;&amp; pte_savedwrite(*pvmw.pte)))</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Since it will avoid TLB flush. And we want to do it while the PTE is stale.</span>
<span class="quote">&gt; &gt; Later, and before replacing the page, we would be able to change the page.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Note that all the operations the CPU1-3 perform canhappen in parallel since</span>
<span class="quote">&gt; &gt; they only acquire mmap_sem for read.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; We start with two identical pages. Everything below regards the same</span>
<span class="quote">&gt; &gt; page/PTE.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; CPU0		CPU1		CPU2		CPU3</span>
<span class="quote">&gt; &gt; ----		----		----		----</span>
<span class="quote">&gt; &gt; Write the same</span>
<span class="quote">&gt; &gt; value on page</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; [cache PTE as</span>
<span class="quote">&gt; &gt; dirty in TLB]</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; 		MADV_FREE</span>
<span class="quote">&gt; &gt; 		pte_mkclean()</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; 				4 &gt; clear_refs</span>
<span class="quote">&gt; &gt; 				pte_wrprotect()</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; 						write_protect_page()</span>
<span class="quote">&gt; &gt; 						[ success, no flush ]</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; 						pages_indentical()</span>
<span class="quote">&gt; &gt; 						[ ok ]</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Write to page</span>
<span class="quote">&gt; &gt; different value</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; [Ok, using stale</span>
<span class="quote">&gt; &gt; PTE]</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; 						replace_page()</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Later, CPU1, CPU2 and CPU3 would flush the TLB, but that is too late. CPU0</span>
<span class="quote">&gt; &gt; already wrote on the page, but KSM ignored this write, and it got lost.</span>
<span class="quote">&gt; &gt; &quot;</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; In above scenario, MADV_FREE is fixed by changing TLB batching API</span>
<span class="quote">&gt; &gt; including [set|clear]_tlb_flush_pending. Remained thing is soft-dirty part.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; This patch changes soft-dirty uses TLB batching API instead of flush_tlb_mm</span>
<span class="quote">&gt; &gt; and KSM checks pending TLB flush by using mm_tlb_flush_pending so that</span>
<span class="quote">&gt; &gt; it will flush TLB to avoid data lost if there are other parallel threads</span>
<span class="quote">&gt; &gt; pending TLB flush.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; [1] http://lkml.kernel.org/r/BD3A0EBE-ECF4-41D4-87FA-C755EA9AB6BD@gmail.com</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Note:</span>
<span class="quote">&gt; &gt; I failed to reproduce this problem through Nadav&#39;s test program which</span>
<span class="quote">&gt; &gt; need to tune timing in my system speed so didn&#39;t confirm it work.</span>
<span class="quote">&gt; &gt; Nadav, Could you test this patch on your test machine?</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Thanks!</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Cc: Nadav Amit &lt;nadav.amit@gmail.com&gt;</span>
<span class="quote">&gt; &gt; Cc: Mel Gorman &lt;mgorman@techsingularity.net&gt;</span>
<span class="quote">&gt; &gt; Cc: Hugh Dickins &lt;hughd@google.com&gt;</span>
<span class="quote">&gt; &gt; Cc: Andrea Arcangeli &lt;aarcange@redhat.com&gt;</span>
<span class="quote">&gt; &gt; Signed-off-by: Minchan Kim &lt;minchan@kernel.org&gt;</span>
<span class="quote">&gt; &gt; ---</span>
<span class="quote">&gt; &gt; fs/proc/task_mmu.c | 4 +++-</span>
<span class="quote">&gt; &gt; mm/ksm.c           | 3 ++-</span>
<span class="quote">&gt; &gt; 2 files changed, 5 insertions(+), 2 deletions(-)</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; diff --git a/fs/proc/task_mmu.c b/fs/proc/task_mmu.c</span>
<span class="quote">&gt; &gt; index 9782dedeead7..58ef3a6abbc0 100644</span>
<span class="quote">&gt; &gt; --- a/fs/proc/task_mmu.c</span>
<span class="quote">&gt; &gt; +++ b/fs/proc/task_mmu.c</span>
<span class="quote">&gt; &gt; @@ -1018,6 +1018,7 @@ static ssize_t clear_refs_write(struct file *file, const char __user *buf,</span>
<span class="quote">&gt; &gt; 	enum clear_refs_types type;</span>
<span class="quote">&gt; &gt; 	int itype;</span>
<span class="quote">&gt; &gt; 	int rv;</span>
<span class="quote">&gt; &gt; +	struct mmu_gather tlb;</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; 	memset(buffer, 0, sizeof(buffer));</span>
<span class="quote">&gt; &gt; 	if (count &gt; sizeof(buffer) - 1)</span>
<span class="quote">&gt; &gt; @@ -1062,6 +1063,7 @@ static ssize_t clear_refs_write(struct file *file, const char __user *buf,</span>
<span class="quote">&gt; &gt; 		}</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; 		down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt; &gt; +		tlb_gather_mmu(&amp;tlb, mm, 0, -1);</span>
<span class="quote">&gt; &gt; 		if (type == CLEAR_REFS_SOFT_DIRTY) {</span>
<span class="quote">&gt; &gt; 			for (vma = mm-&gt;mmap; vma; vma = vma-&gt;vm_next) {</span>
<span class="quote">&gt; &gt; 				if (!(vma-&gt;vm_flags &amp; VM_SOFTDIRTY))</span>
<span class="quote">&gt; &gt; @@ -1083,7 +1085,7 @@ static ssize_t clear_refs_write(struct file *file, const char __user *buf,</span>
<span class="quote">&gt; &gt; 		walk_page_range(0, mm-&gt;highest_vm_end, &amp;clear_refs_walk);</span>
<span class="quote">&gt; &gt; 		if (type == CLEAR_REFS_SOFT_DIRTY)</span>
<span class="quote">&gt; &gt; 			mmu_notifier_invalidate_range_end(mm, 0, -1);</span>
<span class="quote">&gt; &gt; -		flush_tlb_mm(mm);</span>
<span class="quote">&gt; &gt; +		tlb_finish_mmu(&amp;tlb, 0, -1);</span>
<span class="quote">&gt; &gt; 		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt; &gt; out_mm:</span>
<span class="quote">&gt; &gt; 		mmput(mm);</span>
<span class="quote">&gt; &gt; diff --git a/mm/ksm.c b/mm/ksm.c</span>
<span class="quote">&gt; &gt; index 0c927e36a639..15dd7415f7b3 100644</span>
<span class="quote">&gt; &gt; --- a/mm/ksm.c</span>
<span class="quote">&gt; &gt; +++ b/mm/ksm.c</span>
<span class="quote">&gt; &gt; @@ -1038,7 +1038,8 @@ static int write_protect_page(struct vm_area_struct *vma, struct page *page,</span>
<span class="quote">&gt; &gt; 		goto out_unlock;</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; 	if (pte_write(*pvmw.pte) || pte_dirty(*pvmw.pte) ||</span>
<span class="quote">&gt; &gt; -	    (pte_protnone(*pvmw.pte) &amp;&amp; pte_savedwrite(*pvmw.pte))) {</span>
<span class="quote">&gt; &gt; +	    (pte_protnone(*pvmw.pte) &amp;&amp; pte_savedwrite(*pvmw.pte)) ||</span>
<span class="quote">&gt; &gt; +						mm_tlb_flush_pending(mm)) {</span>
<span class="quote">&gt; &gt; 		pte_t entry;</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; 		swapped = PageSwapCache(page);</span>
<span class="quote">&gt; &gt; -- </span>
<span class="quote">&gt; &gt; 2.7.4</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I tested the patch-set, and my PoC does not fail anymore.</span>

Thanks for the testing with great reproduing application, Nadav!
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36811">Minchan Kim</a> - Aug. 2, 2017, 12:58 a.m.</div>
<pre class="content">
Hi Andrea,

On Tue, Aug 01, 2017 at 09:33:41PM +0200, Andrea Arcangeli wrote:
<span class="quote">&gt; Hello,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; On Tue, Aug 01, 2017 at 02:56:17PM +0900, Minchan Kim wrote:</span>
<span class="quote">&gt; &gt; CPU0		CPU1		CPU2		CPU3</span>
<span class="quote">&gt; &gt; ----		----		----		----</span>
<span class="quote">&gt; &gt; Write the same</span>
<span class="quote">&gt; &gt; value on page</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; [cache PTE as</span>
<span class="quote">&gt; &gt;  dirty in TLB]</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; 		MADV_FREE</span>
<span class="quote">&gt; &gt; 		pte_mkclean()</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; 				4 &gt; clear_refs</span>
<span class="quote">&gt; &gt; 				pte_wrprotect()</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; 						write_protect_page()</span>
<span class="quote">&gt; &gt; 						[ success, no flush ]</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; 						pages_indentical()</span>
<span class="quote">&gt; &gt; 						[ ok ]</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Write to page</span>
<span class="quote">&gt; &gt; different value</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; [Ok, using stale</span>
<span class="quote">&gt; &gt;  PTE]</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; 						replace_page()</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Later, CPU1, CPU2 and CPU3 would flush the TLB, but that is too late. CPU0</span>
<span class="quote">&gt; &gt; already wrote on the page, but KSM ignored this write, and it got lost.</span>
<span class="quote">&gt; &gt; &quot;</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; In above scenario, MADV_FREE is fixed by changing TLB batching API</span>
<span class="quote">&gt; &gt; including [set|clear]_tlb_flush_pending. Remained thing is soft-dirty part.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; This patch changes soft-dirty uses TLB batching API instead of flush_tlb_mm</span>
<span class="quote">&gt; &gt; and KSM checks pending TLB flush by using mm_tlb_flush_pending so that</span>
<span class="quote">&gt; &gt; it will flush TLB to avoid data lost if there are other parallel threads</span>
<span class="quote">&gt; &gt; pending TLB flush.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; [1] http://lkml.kernel.org/r/BD3A0EBE-ECF4-41D4-87FA-C755EA9AB6BD@gmail.com</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Note:</span>
<span class="quote">&gt; &gt; I failed to reproduce this problem through Nadav&#39;s test program which</span>
<span class="quote">&gt; &gt; need to tune timing in my system speed so didn&#39;t confirm it work.</span>
<span class="quote">&gt; &gt; Nadav, Could you test this patch on your test machine?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Reviewed-by: Andrea Arcangeli &lt;aarcange@redhat.com&gt;</span>

Thanks for the review!
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/fs/proc/task_mmu.c b/fs/proc/task_mmu.c</span>
<span class="p_header">index 9782dedeead7..58ef3a6abbc0 100644</span>
<span class="p_header">--- a/fs/proc/task_mmu.c</span>
<span class="p_header">+++ b/fs/proc/task_mmu.c</span>
<span class="p_chunk">@@ -1018,6 +1018,7 @@</span> <span class="p_context"> static ssize_t clear_refs_write(struct file *file, const char __user *buf,</span>
 	enum clear_refs_types type;
 	int itype;
 	int rv;
<span class="p_add">+	struct mmu_gather tlb;</span>
 
 	memset(buffer, 0, sizeof(buffer));
 	if (count &gt; sizeof(buffer) - 1)
<span class="p_chunk">@@ -1062,6 +1063,7 @@</span> <span class="p_context"> static ssize_t clear_refs_write(struct file *file, const char __user *buf,</span>
 		}
 
 		down_read(&amp;mm-&gt;mmap_sem);
<span class="p_add">+		tlb_gather_mmu(&amp;tlb, mm, 0, -1);</span>
 		if (type == CLEAR_REFS_SOFT_DIRTY) {
 			for (vma = mm-&gt;mmap; vma; vma = vma-&gt;vm_next) {
 				if (!(vma-&gt;vm_flags &amp; VM_SOFTDIRTY))
<span class="p_chunk">@@ -1083,7 +1085,7 @@</span> <span class="p_context"> static ssize_t clear_refs_write(struct file *file, const char __user *buf,</span>
 		walk_page_range(0, mm-&gt;highest_vm_end, &amp;clear_refs_walk);
 		if (type == CLEAR_REFS_SOFT_DIRTY)
 			mmu_notifier_invalidate_range_end(mm, 0, -1);
<span class="p_del">-		flush_tlb_mm(mm);</span>
<span class="p_add">+		tlb_finish_mmu(&amp;tlb, 0, -1);</span>
 		up_read(&amp;mm-&gt;mmap_sem);
 out_mm:
 		mmput(mm);
<span class="p_header">diff --git a/mm/ksm.c b/mm/ksm.c</span>
<span class="p_header">index 0c927e36a639..15dd7415f7b3 100644</span>
<span class="p_header">--- a/mm/ksm.c</span>
<span class="p_header">+++ b/mm/ksm.c</span>
<span class="p_chunk">@@ -1038,7 +1038,8 @@</span> <span class="p_context"> static int write_protect_page(struct vm_area_struct *vma, struct page *page,</span>
 		goto out_unlock;
 
 	if (pte_write(*pvmw.pte) || pte_dirty(*pvmw.pte) ||
<span class="p_del">-	    (pte_protnone(*pvmw.pte) &amp;&amp; pte_savedwrite(*pvmw.pte))) {</span>
<span class="p_add">+	    (pte_protnone(*pvmw.pte) &amp;&amp; pte_savedwrite(*pvmw.pte)) ||</span>
<span class="p_add">+						mm_tlb_flush_pending(mm)) {</span>
 		pte_t entry;
 
 		swapped = PageSwapCache(page);

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



