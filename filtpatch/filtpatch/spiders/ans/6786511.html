
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[3/3] arm64, mm: Use IPIs for TLB invalidation. - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [3/3] arm64, mm: Use IPIs for TLB invalidation.</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=352">Catalin Marinas</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>July 14, 2015, 11:13 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20150714111342.GD13555@e104818-lin.cambridge.arm.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/6786511/mbox/"
   >mbox</a>
|
   <a href="/patch/6786511/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/6786511/">/patch/6786511/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork2.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork2.web.kernel.org (Postfix) with ESMTP id ED4DAC05AC
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 14 Jul 2015 11:13:59 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 0424D206F5
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 14 Jul 2015 11:13:59 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id B149A206F1
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 14 Jul 2015 11:13:57 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1753413AbbGNLNs (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 14 Jul 2015 07:13:48 -0400
Received: from foss.arm.com ([217.140.101.70]:33795 &quot;EHLO foss.arm.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1751782AbbGNLNq (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 14 Jul 2015 07:13:46 -0400
Received: from usa-sjc-imap-foss1.foss.arm.com (unknown [10.72.51.249])
	by usa-sjc-mx-foss1.foss.arm.com (Postfix) with ESMTP id 53A6175;
	Tue, 14 Jul 2015 04:14:07 -0700 (PDT)
Received: from e104818-lin.cambridge.arm.com
	(usa-sjc-imap-foss1.foss.arm.com [10.72.51.249])
	by usa-sjc-imap-foss1.foss.arm.com (Postfix) with ESMTPSA id
	DCFBD3F317; Tue, 14 Jul 2015 04:13:44 -0700 (PDT)
Date: Tue, 14 Jul 2015 12:13:42 +0100
From: Catalin Marinas &lt;catalin.marinas@arm.com&gt;
To: David Daney &lt;ddaney@caviumnetworks.com&gt;
Cc: Will Deacon &lt;will.deacon@arm.com&gt;, David Daney &lt;david.daney@cavium.com&gt;,
	&quot;linux-kernel@vger.kernel.org&quot; &lt;linux-kernel@vger.kernel.org&gt;,
	Robert Richter &lt;rrichter@cavium.com&gt;,
	David Daney &lt;ddaney.cavm@gmail.com&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	&quot;linux-arm-kernel@lists.infradead.org&quot; 
	&lt;linux-arm-kernel@lists.infradead.org&gt;
Subject: Re: [PATCH 3/3] arm64, mm: Use IPIs for TLB invalidation.
Message-ID: &lt;20150714111342.GD13555@e104818-lin.cambridge.arm.com&gt;
References: &lt;1436646323-10527-1-git-send-email-ddaney.cavm@gmail.com&gt;
	&lt;1436646323-10527-4-git-send-email-ddaney.cavm@gmail.com&gt;
	&lt;20150713181755.GP2632@arm.com&gt;
	&lt;55A40A50.8080902@caviumnetworks.com&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Disposition: inline
In-Reply-To: &lt;55A40A50.8080902@caviumnetworks.com&gt;
User-Agent: Mutt/1.5.23 (2014-03-12)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-8.3 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=352">Catalin Marinas</a> - July 14, 2015, 11:13 a.m.</div>
<pre class="content">
On Mon, Jul 13, 2015 at 11:58:24AM -0700, David Daney wrote:
<span class="quote">&gt; On 07/13/2015 11:17 AM, Will Deacon wrote:</span>
<span class="quote">&gt; &gt;On Sat, Jul 11, 2015 at 09:25:23PM +0100, David Daney wrote:</span>
<span class="quote">&gt; &gt;&gt;From: David Daney &lt;david.daney@cavium.com&gt;</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;Most broadcast TLB invalidations are unnecessary.  So when</span>
<span class="quote">&gt; &gt;&gt;invalidating for a given mm/vma target the only the needed CPUs via</span>
<span class="quote">&gt; &gt;&gt;and IPI.</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;For global TLB invalidations, also use IPI.</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;Tested on Cavium ThunderX.</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;This change reduces &#39;time make -j48&#39; on kernel from 139s to 116s (83%</span>
<span class="quote">&gt; &gt;&gt;as long).</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;Any idea *why* you&#39;re seeing such an improvement? Some older kernels had</span>
<span class="quote">&gt; &gt;a bug where we&#39;d try to flush a negative (i.e. huge) range by page, so it</span>
<span class="quote">&gt; &gt;would be nice to rule that out. I assume these measurements are using</span>
<span class="quote">&gt; &gt;mainline?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I have an untested multi-part theory:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 1) Most of the invalidations in the kernel build will be for a mm that was</span>
<span class="quote">&gt; only used on a single CPU (the current CPU), so IPIs are for the most part</span>
<span class="quote">&gt; not needed.  We win by not having to synchronize across all CPUs waiting for</span>
<span class="quote">&gt; the DSB to complete.  I think most of it occurs at process exit.  Q: why do</span>
<span class="quote">&gt; anything at process exit?  The use of ASIDs should make TLB invalidations at</span>
<span class="quote">&gt; process death unnecessary.</span>

I think for the process exit, something like below may work (but it
needs proper review and a lot of testing to make sure I haven&#39;t missed
anything; note that it&#39;s only valid for the current ASID allocation
algorithm on arm64 which does not allow ASID reusing until roll-over):

------------8&lt;---------------------------
------------8&lt;---------------------------

AFAICT, we have three main cases for full mm TLBI (and another when the
VA range is is too large):

1. fork - dup_mmap() needs to flush the parent after changing the pages
   to read-only for CoW. Here we can&#39;t really do anything
2. sys_exit - exit_mmap() clearing the page tables, the above TLBI
   deferring would help
3. sys_execve - by the time we call exit_mmap(old_mm), we already
   activated the new mm via exec_mmap(), so deferring TLBI should work

BTW, if we do the TLBI deferring to the ASID roll-over event, your
flush_context() patch to use local TLBI would no longer work. It is
called from __new_context() when allocating a new ASID, so it needs to
be broadcast to all the CPUs.
<span class="quote">
&gt; 2) By simplifying the VA range invalidations to just a single ASID based</span>
<span class="quote">&gt; invalidation, we are issuing many fewer TLBI broadcasts.  The overhead of</span>
<span class="quote">&gt; refilling the local TLB with still needed mappings may be lower than the</span>
<span class="quote">&gt; overhead of all those TLBI operations.</span>

That the munmap case usually. In our tests, we haven&#39;t seen large
ranges, mostly 1-2 4KB pages (especially with kernbench when median file
size fits in 4KB). Maybe the new batching code for x86 could help ARM as
well if we implement it. We would still issue TLBIs but it allows us to
issue a single DSB at the end.

Once we manage to optimise the current implementation, maybe it would
still be faster on a large machine (48 cores) with IPIs but it is highly
dependent on the type of workload (single-threaded tasks would benefit).
Also note that under KVM the cost of the IPI is much higher.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=7096">Will Deacon</a> - July 14, 2015, 11:40 a.m.</div>
<pre class="content">
On Tue, Jul 14, 2015 at 12:13:42PM +0100, Catalin Marinas wrote:
<span class="quote">&gt; BTW, if we do the TLBI deferring to the ASID roll-over event, your</span>
<span class="quote">&gt; flush_context() patch to use local TLBI would no longer work. It is</span>
<span class="quote">&gt; called from __new_context() when allocating a new ASID, so it needs to</span>
<span class="quote">&gt; be broadcast to all the CPUs.</span>

What we can do instead is:

 - Keep track of the CPUs on which an mm has been active
 - Do a local TLBI if only the current CPU is in the list
 - Move to the same ASID allocation algorithm as arch/arm/
 - Change the ASID re-use policy so that we only mark an ASID as free
   if we succeeded in performing a local TLBI, postponing anything else
   until rollover

That should handle the fork() + exec() case nicely, I reckon. I tried
something similar in the past for arch/arm/, but it didn&#39;t make a difference
on any of the platforms I have access to (where TLBI traffic was cheap).

It would *really* help if I had some Thunder-X hardware...
<span class="quote">
&gt; That the munmap case usually. In our tests, we haven&#39;t seen large</span>
<span class="quote">&gt; ranges, mostly 1-2 4KB pages (especially with kernbench when median file</span>
<span class="quote">&gt; size fits in 4KB). Maybe the new batching code for x86 could help ARM as</span>
<span class="quote">&gt; well if we implement it. We would still issue TLBIs but it allows us to</span>
<span class="quote">&gt; issue a single DSB at the end.</span>

Again, I looked at this in the past but it turns out that the DSB ISHST
needed to publish PTEs tends to sync TLBIs on most cores (even though
it&#39;s not an architectural requirement), so postponing the full DSB to
the end didn&#39;t help on existing microarchitectures.

Finally, it might be worth dusting off the leaf-only TLBI stuff you
looked at in the past. It doesn&#39;t reduce the message traffic, but I can&#39;t
see it making things worse.

Will
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=352">Catalin Marinas</a> - July 14, 2015, 1:09 p.m.</div>
<pre class="content">
On Tue, Jul 14, 2015 at 12:40:30PM +0100, Will Deacon wrote:
<span class="quote">&gt; On Tue, Jul 14, 2015 at 12:13:42PM +0100, Catalin Marinas wrote:</span>
<span class="quote">&gt; &gt; BTW, if we do the TLBI deferring to the ASID roll-over event, your</span>
<span class="quote">&gt; &gt; flush_context() patch to use local TLBI would no longer work. It is</span>
<span class="quote">&gt; &gt; called from __new_context() when allocating a new ASID, so it needs to</span>
<span class="quote">&gt; &gt; be broadcast to all the CPUs.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; What we can do instead is:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  - Keep track of the CPUs on which an mm has been active</span>

We already do this in switch_mm().
<span class="quote">
&gt;  - Do a local TLBI if only the current CPU is in the list</span>

This would be beneficial independent of the following two items. I think
it&#39;s worth doing.
<span class="quote">
&gt;  - Move to the same ASID allocation algorithm as arch/arm/</span>

This is useful to avoid the IPI on roll-over. With 16-bit ASIDs, I don&#39;t
think this is too urgent but, well, the benchmarks may say otherwise.
<span class="quote">
&gt;  - Change the ASID re-use policy so that we only mark an ASID as free</span>
<span class="quote">&gt;    if we succeeded in performing a local TLBI, postponing anything else</span>
<span class="quote">&gt;    until rollover</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; That should handle the fork() + exec() case nicely, I reckon. I tried</span>
<span class="quote">&gt; something similar in the past for arch/arm/, but it didn&#39;t make a difference</span>
<span class="quote">&gt; on any of the platforms I have access to (where TLBI traffic was cheap).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It would *really* help if I had some Thunder-X hardware...</span>

I agree. With only 8 CPUs, we don&#39;t notice any difference with the above
optimisations.
<span class="quote">
&gt; &gt; That the munmap case usually. In our tests, we haven&#39;t seen large</span>
<span class="quote">&gt; &gt; ranges, mostly 1-2 4KB pages (especially with kernbench when median file</span>
<span class="quote">&gt; &gt; size fits in 4KB). Maybe the new batching code for x86 could help ARM as</span>
<span class="quote">&gt; &gt; well if we implement it. We would still issue TLBIs but it allows us to</span>
<span class="quote">&gt; &gt; issue a single DSB at the end.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Again, I looked at this in the past but it turns out that the DSB ISHST</span>
<span class="quote">&gt; needed to publish PTEs tends to sync TLBIs on most cores (even though</span>
<span class="quote">&gt; it&#39;s not an architectural requirement), so postponing the full DSB to</span>
<span class="quote">&gt; the end didn&#39;t help on existing microarchitectures.</span>

We could postpone all the TLBI, including the first DSB ISHST. But I
need to look in detail at the recent TLBI batching patches for x86, they
do it to reduce IPIs but we could similarly use them to reduce the total
sync time after broadcast (i.e. DSB for pte, lots of TLBIs, DSB for TLBI
sync).
<span class="quote">
&gt; Finally, it might be worth dusting off the leaf-only TLBI stuff you</span>
<span class="quote">&gt; looked at in the past. It doesn&#39;t reduce the message traffic, but I can&#39;t</span>
<span class="quote">&gt; see it making things worse.</span>

I didn&#39;t see a difference but I&#39;ll post them to the list.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/arm64/include/asm/tlb.h b/arch/arm64/include/asm/tlb.h</span>
<span class="p_header">index 3a0242c7eb8d..0176cda688cb 100644</span>
<span class="p_header">--- a/arch/arm64/include/asm/tlb.h</span>
<span class="p_header">+++ b/arch/arm64/include/asm/tlb.h</span>
<span class="p_chunk">@@ -38,7 +38,8 @@</span> <span class="p_context"> static inline void __tlb_remove_table(void *_table)</span>
 static inline void tlb_flush(struct mmu_gather *tlb)
 {
 	if (tlb-&gt;fullmm) {
<span class="p_del">-		flush_tlb_mm(tlb-&gt;mm);</span>
<span class="p_add">+		/* Deferred until ASID roll-over */</span>
<span class="p_add">+		WARN_ON(atomic_read(&amp;tlb-&gt;mm-&gt;mm_users));</span>
 	} else {
 		struct vm_area_struct vma = { .vm_mm = tlb-&gt;mm, };
 		flush_tlb_range(&amp;vma, tlb-&gt;start, tlb-&gt;end);
<span class="p_header">diff --git a/arch/arm64/include/asm/tlbflush.h b/arch/arm64/include/asm/tlbflush.h</span>
<span class="p_header">index 934815d45eda..2e595933864a 100644</span>
<span class="p_header">--- a/arch/arm64/include/asm/tlbflush.h</span>
<span class="p_header">+++ b/arch/arm64/include/asm/tlbflush.h</span>
<span class="p_chunk">@@ -150,6 +150,13 @@</span> <span class="p_context"> static inline void __flush_tlb_pgtable(struct mm_struct *mm,</span>
 {
 	unsigned long addr = uaddr &gt;&gt; 12 | ((unsigned long)ASID(mm) &lt;&lt; 48);
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Check for concurrent users of this mm. If there are no users with</span>
<span class="p_add">+	 * user space, we do not have any (speculative) page table walkers.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (!atomic_read(&amp;mm-&gt;mm_users))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
 	dsb(ishst);
 	asm(&quot;tlbi	vae1is, %0&quot; : : &quot;r&quot; (addr));
 	dsb(ish);

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



