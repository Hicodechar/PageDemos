
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[07/23] x86, kaiser: unmap kernel from userspace page tables (core patch) - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [07/23] x86, kaiser: unmap kernel from userspace page tables (core patch)</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=64071">Dave Hansen</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Oct. 31, 2017, 10:31 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20171031223159.64173B6F@viggo.jf.intel.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10035465/mbox/"
   >mbox</a>
|
   <a href="/patch/10035465/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10035465/">/patch/10035465/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	19D1B60327 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 31 Oct 2017 22:32:14 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 0B13728B21
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 31 Oct 2017 22:32:14 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id F3EBD28B25; Tue, 31 Oct 2017 22:32:13 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id DC78828B24
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 31 Oct 2017 22:32:11 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S932953AbdJaWcI (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 31 Oct 2017 18:32:08 -0400
Received: from mga09.intel.com ([134.134.136.24]:59218 &quot;EHLO mga09.intel.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S932885AbdJaWcB (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 31 Oct 2017 18:32:01 -0400
Received: from fmsmga001.fm.intel.com ([10.253.24.23])
	by orsmga102.jf.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384;
	31 Oct 2017 15:32:00 -0700
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i=&quot;5.44,326,1505804400&quot;; d=&quot;scan&#39;208&quot;;a=&quot;1212573884&quot;
Received: from viggo.jf.intel.com (HELO localhost.localdomain)
	([10.54.39.20])
	by fmsmga001.fm.intel.com with ESMTP; 31 Oct 2017 15:31:59 -0700
Subject: [PATCH 07/23] x86,
	kaiser: unmap kernel from userspace page tables (core patch)
To: linux-kernel@vger.kernel.org
Cc: linux-mm@kvack.org, dave.hansen@linux.intel.com,
	moritz.lipp@iaik.tugraz.at, daniel.gruss@iaik.tugraz.at,
	michael.schwarz@iaik.tugraz.at, luto@kernel.org,
	torvalds@linux-foundation.org, keescook@google.com,
	hughd@google.com, x86@kernel.org
From: Dave Hansen &lt;dave.hansen@linux.intel.com&gt;
Date: Tue, 31 Oct 2017 15:31:59 -0700
References: &lt;20171031223146.6B47C861@viggo.jf.intel.com&gt;
In-Reply-To: &lt;20171031223146.6B47C861@viggo.jf.intel.com&gt;
Message-Id: &lt;20171031223159.64173B6F@viggo.jf.intel.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=64071">Dave Hansen</a> - Oct. 31, 2017, 10:31 p.m.</div>
<pre class="content">
These patches are based on work from a team at Graz University of
Technology: https://github.com/IAIK/KAISER .  This work would not have
been possible without their work as a starting point.

KAISER is a countermeasure against side channel attacks against kernel
virtual memory.  It leaves the existing page tables largely alone and
refers to them as the &quot;kernel page tables.  It adds a &quot;shadow&quot; pgd for
every process which is intended for use when we run userspace.  The
shadow pgd maps all the same user memory as the &quot;kernel&quot; copy, but
only maps a minimal set of kernel memory.

Whenever we enter the kernel (syscalls, interrupts, exceptions), the
pgd is switched to the &quot;kernel&quot; copy.  When the system switches back
to user mode, the shadow pgd is used.

The minimalistic kernel page tables try to map only what is needed to
enter/exit the kernel such as the entry/exit functions themselves and
the interrupt descriptors (IDT).

Changes from original KAISER patch:
 * Gobs of coding style cleanups
 * The original patch tried to allocate an order-2 page, then
   8k-align the result.  That&#39;s silly since order-2 is already
   guaranteed to be 16k-aligned.  Removed that gunk and just
   allocate an order-1 page.
 * Handle (or at least detect and warn on) allocation failures
 * Use _KERNPG_TABLE, not _PAGE_TABLE when creating mappings for
   the kernel in the shadow (user) page tables.
 * BUG_ON() for !pte_none() case was totally insane: it checked
   the physical address of the &#39;struct page&#39; against the physical
   address of the page being mapped.
 * Added 5-level page table support
 * Never free kaiser page tables.  We don&#39;t have the locking to
   keep them from getting used while we free them.
 * Use a totally different scheme in the entry code.  The
   original code just fell apart in horrific ways in debug faults,
   NMIs, or when iret faults.  Big thanks to Andy Lutomirski for
   reducing the number of places we had to patch.  He made the
   code a ton simpler.

Note: The original KAISER authors signed-off on their patch.  Some of
their code has been broken out into other patches in this series, but
their SoB was only retained here.
<span class="signed-off-by">
Signed-off-by: Moritz Lipp &lt;moritz.lipp@iaik.tugraz.at&gt;</span>
<span class="signed-off-by">Signed-off-by: Daniel Gruss &lt;daniel.gruss@iaik.tugraz.at&gt;</span>
<span class="signed-off-by">Signed-off-by: Michael Schwarz &lt;michael.schwarz@iaik.tugraz.at&gt;</span>
<span class="signed-off-by">Signed-off-by: Dave Hansen &lt;dave.hansen@linux.intel.com&gt;</span>
Cc: Moritz Lipp &lt;moritz.lipp@iaik.tugraz.at&gt;
Cc: Daniel Gruss &lt;daniel.gruss@iaik.tugraz.at&gt;
Cc: Michael Schwarz &lt;michael.schwarz@iaik.tugraz.at&gt;
Cc: Andy Lutomirski &lt;luto@kernel.org&gt;
Cc: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;
Cc: Kees Cook &lt;keescook@google.com&gt;
Cc: Hugh Dickins &lt;hughd@google.com&gt;
Cc: x86@kernel.org
---

 b/Documentation/x86/kaiser.txt      |  128 ++++++++++++
 b/arch/x86/entry/calling.h          |   32 ++-
 b/arch/x86/include/asm/kaiser.h     |   59 +++++
 b/arch/x86/include/asm/pgtable.h    |    6 
 b/arch/x86/include/asm/pgtable_64.h |   93 ++++++++
 b/arch/x86/kernel/espfix_64.c       |   17 +
 b/arch/x86/kernel/head_64.S         |   14 +
 b/arch/x86/mm/Makefile              |    1 
 b/arch/x86/mm/kaiser.c              |  380 ++++++++++++++++++++++++++++++++++++
 b/arch/x86/mm/pageattr.c            |    2 
 b/arch/x86/mm/pgtable.c             |   16 +
 b/include/linux/kaiser.h            |   34 +++
 b/init/main.c                       |    2 
 b/kernel/fork.c                     |    6 
 14 files changed, 781 insertions(+), 9 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff -puN arch/x86/entry/calling.h~kaiser-base arch/x86/entry/calling.h</span>
<span class="p_header">--- a/arch/x86/entry/calling.h~kaiser-base	2017-10-31 15:03:51.817182716 -0700</span>
<span class="p_header">+++ b/arch/x86/entry/calling.h	2017-10-31 15:03:51.842183897 -0700</span>
<span class="p_chunk">@@ -1,6 +1,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/jump_label.h&gt;
 #include &lt;asm/unwind_hints.h&gt;
 #include &lt;asm/cpufeatures.h&gt;
<span class="p_add">+#include &lt;asm/page_types.h&gt;</span>
 
 /*
 
<span class="p_chunk">@@ -218,10 +219,19 @@</span> <span class="p_context"> For 32-bit we have the following convent</span>
 #endif
 .endm
 
<span class="p_add">+#ifdef CONFIG_KAISER</span>
<span class="p_add">+</span>
<span class="p_add">+/* KAISER PGDs are 8k.  We flip bit 12 to switch between the two halves: */</span>
<span class="p_add">+#define KAISER_SWITCH_MASK (1&lt;&lt;PAGE_SHIFT)</span>
<span class="p_add">+</span>
 .macro ADJUST_KERNEL_CR3 reg:req
<span class="p_add">+	/* Clear &quot;KAISER bit&quot;, point CR3 at kernel pagetables: */</span>
<span class="p_add">+	andq	$(~KAISER_SWITCH_MASK), \reg</span>
 .endm
 
 .macro ADJUST_USER_CR3 reg:req
<span class="p_add">+	/* Move CR3 up a page to the user page tables: */</span>
<span class="p_add">+	orq	$(KAISER_SWITCH_MASK), \reg</span>
 .endm
 
 .macro SWITCH_TO_KERNEL_CR3 scratch_reg:req
<span class="p_chunk">@@ -240,10 +250,10 @@</span> <span class="p_context"> For 32-bit we have the following convent</span>
 	movq	%cr3, %r\scratch_reg
 	movq	%r\scratch_reg, \save_reg
 	/*
<span class="p_del">-	 * Just stick a random bit in here that never gets set.  Fixed</span>
<span class="p_add">+	 * Is the switch bit zero?  This means the address is</span>
 	 * up in real KAISER patches in a moment.
 	 */
<span class="p_del">-	bt	$63, %r\scratch_reg</span>
<span class="p_add">+	testq	$(KAISER_SWITCH_MASK), %r\scratch_reg</span>
 	jz	.Ldone_\@
 
 	ADJUST_KERNEL_CR3 %r\scratch_reg
<span class="p_chunk">@@ -253,10 +263,26 @@</span> <span class="p_context"> For 32-bit we have the following convent</span>
 .endm
 
 .macro RESTORE_CR3 save_reg:req
<span class="p_del">-	/* optimize this */</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We could avoid the CR3 write if not changing its value,</span>
<span class="p_add">+	 * but that requires a CR3 read *and* a scratch register.</span>
<span class="p_add">+	 */</span>
 	movq	\save_reg, %cr3
 .endm
 
<span class="p_add">+#else /* CONFIG_KAISER=n: */</span>
<span class="p_add">+</span>
<span class="p_add">+.macro SWITCH_TO_KERNEL_CR3 scratch_reg:req</span>
<span class="p_add">+.endm</span>
<span class="p_add">+.macro SWITCH_TO_USER_CR3 scratch_reg:req</span>
<span class="p_add">+.endm</span>
<span class="p_add">+.macro SAVE_AND_SWITCH_TO_KERNEL_CR3 scratch_reg:req save_reg:req</span>
<span class="p_add">+.endm</span>
<span class="p_add">+.macro RESTORE_CR3 save_reg:req</span>
<span class="p_add">+.endm</span>
<span class="p_add">+</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 #endif /* CONFIG_X86_64 */
 
 /*
<span class="p_header">diff -puN /dev/null arch/x86/include/asm/kaiser.h</span>
<span class="p_header">--- /dev/null	2017-05-17 09:46:39.241182829 -0700</span>
<span class="p_header">+++ b/arch/x86/include/asm/kaiser.h	2017-10-31 15:03:51.843183945 -0700</span>
<span class="p_chunk">@@ -0,0 +1,59 @@</span> <span class="p_context"></span>
<span class="p_add">+#ifndef _ASM_X86_KAISER_H</span>
<span class="p_add">+#define _ASM_X86_KAISER_H</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright(c) 2017 Intel Corporation. All rights reserved.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of version 2 of the GNU General Public License as</span>
<span class="p_add">+ * published by the Free Software Foundation.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is distributed in the hope that it will be useful, but</span>
<span class="p_add">+ * WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU</span>
<span class="p_add">+ * General Public License for more details.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Based on work published here: https://github.com/IAIK/KAISER</span>
<span class="p_add">+ * Modified by Dave Hansen &lt;dave.hansen@intel.com to actually work.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#ifndef __ASSEMBLY__</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_KAISER</span>
<span class="p_add">+/**</span>
<span class="p_add">+ *  kaiser_add_mapping - map a kernel range into the user page tables</span>
<span class="p_add">+ *  @addr: the start address of the range</span>
<span class="p_add">+ *  @size: the size of the range</span>
<span class="p_add">+ *  @flags: The mapping flags of the pages</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *  Use this on all data and code that need to be mapped into both</span>
<span class="p_add">+ *  copies of the page tables.  This includes the code that switches</span>
<span class="p_add">+ *  to/from userspace and all of the hardware structures that are</span>
<span class="p_add">+ *  virtually-addressed and needed in userspace like the interrupt</span>
<span class="p_add">+ *  table.</span>
<span class="p_add">+ */</span>
<span class="p_add">+extern int kaiser_add_mapping(unsigned long addr, unsigned long size,</span>
<span class="p_add">+			      unsigned long flags);</span>
<span class="p_add">+</span>
<span class="p_add">+extern int kaiser_map_stack(struct task_struct *tsk);</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ *  kaiser_remove_mapping - remove a kernel mapping from the userpage tables</span>
<span class="p_add">+ *  @addr: the start address of the range</span>
<span class="p_add">+ *  @size: the size of the range</span>
<span class="p_add">+ */</span>
<span class="p_add">+extern void kaiser_remove_mapping(unsigned long start, unsigned long size);</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ *  kaiser_init - Initialize the shadow mapping</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *  Most parts of the shadow mapping can be mapped upon boot</span>
<span class="p_add">+ *  time.  Only per-process things like the thread stacks</span>
<span class="p_add">+ *  or a new LDT have to be mapped at runtime.  These boot-</span>
<span class="p_add">+ *  time mappings are permanent and never unmapped.</span>
<span class="p_add">+ */</span>
<span class="p_add">+extern void kaiser_init(void);</span>
<span class="p_add">+</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* __ASSEMBLY__ */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_X86_KAISER_H */</span>
<span class="p_header">diff -puN arch/x86/include/asm/pgtable_64.h~kaiser-base arch/x86/include/asm/pgtable_64.h</span>
<span class="p_header">--- a/arch/x86/include/asm/pgtable_64.h~kaiser-base	2017-10-31 15:03:51.819182810 -0700</span>
<span class="p_header">+++ b/arch/x86/include/asm/pgtable_64.h	2017-10-31 15:03:51.843183945 -0700</span>
<span class="p_chunk">@@ -130,9 +130,88 @@</span> <span class="p_context"> static inline pud_t native_pudp_get_and_</span>
 #endif
 }
 
<span class="p_add">+#ifdef CONFIG_KAISER</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * All top-level KAISER page tables are order-1 pages (8k-aligned</span>
<span class="p_add">+ * and 8k in size).  The kernel one is at the beginning 4k and</span>
<span class="p_add">+ * the user (shadow) one is in the last 4k.  To switch between</span>
<span class="p_add">+ * them, you just need to flip the 12th bit in their addresses.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define KAISER_PGTABLE_SWITCH_BIT	PAGE_SHIFT</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * This generates better code than the inline assembly in</span>
<span class="p_add">+ * __set_bit().</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void *ptr_set_bit(void *ptr, int bit)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long __ptr = (unsigned long)ptr;</span>
<span class="p_add">+	__ptr |= (1&lt;&lt;bit);</span>
<span class="p_add">+	return (void *)__ptr;</span>
<span class="p_add">+}</span>
<span class="p_add">+static inline void *ptr_clear_bit(void *ptr, int bit)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long __ptr = (unsigned long)ptr;</span>
<span class="p_add">+	__ptr &amp;= ~(1&lt;&lt;bit);</span>
<span class="p_add">+	return (void *)__ptr;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline pgd_t *native_get_shadow_pgd(pgd_t *pgdp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return ptr_set_bit(pgdp, KAISER_PGTABLE_SWITCH_BIT);</span>
<span class="p_add">+}</span>
<span class="p_add">+static inline pgd_t *native_get_normal_pgd(pgd_t *pgdp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return ptr_clear_bit(pgdp, KAISER_PGTABLE_SWITCH_BIT);</span>
<span class="p_add">+}</span>
<span class="p_add">+static inline p4d_t *native_get_shadow_p4d(p4d_t *p4dp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return ptr_set_bit(p4dp, KAISER_PGTABLE_SWITCH_BIT);</span>
<span class="p_add">+}</span>
<span class="p_add">+static inline p4d_t *native_get_normal_p4d(p4d_t *p4dp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return ptr_clear_bit(p4dp, KAISER_PGTABLE_SWITCH_BIT);</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif /* CONFIG_KAISER */</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Page table pages are page-aligned.  The lower half of the top</span>
<span class="p_add">+ * level is used for userspace and the top half for the kernel.</span>
<span class="p_add">+ * This returns true for user pages that need to get copied into</span>
<span class="p_add">+ * both the user and kernel copies of the page tables, and false</span>
<span class="p_add">+ * for kernel pages that should only be in the kernel copy.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline bool is_userspace_pgd(void *__ptr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long ptr = (unsigned long)__ptr;</span>
<span class="p_add">+</span>
<span class="p_add">+	return ((ptr % PAGE_SIZE) &lt; (PAGE_SIZE / 2));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static inline void native_set_p4d(p4d_t *p4dp, p4d_t p4d)
 {
<span class="p_add">+#if defined(CONFIG_KAISER) &amp;&amp; !defined(CONFIG_X86_5LEVEL)</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * set_pgd() does not get called when we are running</span>
<span class="p_add">+	 * CONFIG_X86_5LEVEL=y.  So, just hack around it.  We</span>
<span class="p_add">+	 * know here that we have a p4d but that it is really at</span>
<span class="p_add">+	 * the top level of the page tables; it is really just a</span>
<span class="p_add">+	 * pgd.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	/* Do we need to also populate the shadow p4d? */</span>
<span class="p_add">+	if (is_userspace_pgd(p4dp))</span>
<span class="p_add">+		native_get_shadow_p4d(p4dp)-&gt;pgd = p4d.pgd;</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Even if the entry is *mapping* userspace, ensure</span>
<span class="p_add">+	 * that userspace can not use it.  This way, if we</span>
<span class="p_add">+	 * get out to userspace with the wrong CR3 value,</span>
<span class="p_add">+	 * userspace will crash instead of running.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (!p4d.pgd.pgd)</span>
<span class="p_add">+		p4dp-&gt;pgd.pgd = p4d.pgd.pgd | _PAGE_NX;</span>
<span class="p_add">+#else /* CONFIG_KAISER */</span>
 	*p4dp = p4d;
<span class="p_add">+#endif</span>
 }
 
 static inline void native_p4d_clear(p4d_t *p4d)
<span class="p_chunk">@@ -146,7 +225,21 @@</span> <span class="p_context"> static inline void native_p4d_clear(p4d_</span>
 
 static inline void native_set_pgd(pgd_t *pgdp, pgd_t pgd)
 {
<span class="p_add">+#ifdef CONFIG_KAISER</span>
<span class="p_add">+	/* Do we need to also populate the shadow pgd? */</span>
<span class="p_add">+	if (is_userspace_pgd(pgdp))</span>
<span class="p_add">+		native_get_shadow_pgd(pgdp)-&gt;pgd = pgd.pgd;</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Even if the entry is mapping userspace, ensure</span>
<span class="p_add">+	 * that it is unusable for userspace.  This way,</span>
<span class="p_add">+	 * if we get out to userspace with the wrong CR3</span>
<span class="p_add">+	 * value, userspace will crash instead of running.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (!pgd_none(pgd))</span>
<span class="p_add">+		pgdp-&gt;pgd = pgd.pgd | _PAGE_NX;</span>
<span class="p_add">+#else /* CONFIG_KAISER */</span>
 	*pgdp = pgd;
<span class="p_add">+#endif</span>
 }
 
 static inline void native_pgd_clear(pgd_t *pgd)
<span class="p_header">diff -puN arch/x86/include/asm/pgtable.h~kaiser-base arch/x86/include/asm/pgtable.h</span>
<span class="p_header">--- a/arch/x86/include/asm/pgtable.h~kaiser-base	2017-10-31 15:03:51.821182905 -0700</span>
<span class="p_header">+++ b/arch/x86/include/asm/pgtable.h	2017-10-31 15:03:51.844183992 -0700</span>
<span class="p_chunk">@@ -1105,6 +1105,12 @@</span> <span class="p_context"> static inline void pmdp_set_wrprotect(st</span>
 static inline void clone_pgd_range(pgd_t *dst, pgd_t *src, int count)
 {
        memcpy(dst, src, count * sizeof(pgd_t));
<span class="p_add">+#ifdef CONFIG_KAISER</span>
<span class="p_add">+	/* Clone the shadow pgd part as well */</span>
<span class="p_add">+	memcpy(native_get_shadow_pgd(dst),</span>
<span class="p_add">+	       native_get_shadow_pgd(src),</span>
<span class="p_add">+	       count * sizeof(pgd_t));</span>
<span class="p_add">+#endif</span>
 }
 
 #define PTE_SHIFT ilog2(PTRS_PER_PTE)
<span class="p_header">diff -puN arch/x86/kernel/espfix_64.c~kaiser-base arch/x86/kernel/espfix_64.c</span>
<span class="p_header">--- a/arch/x86/kernel/espfix_64.c~kaiser-base	2017-10-31 15:03:51.823182999 -0700</span>
<span class="p_header">+++ b/arch/x86/kernel/espfix_64.c	2017-10-31 15:03:51.844183992 -0700</span>
<span class="p_chunk">@@ -41,6 +41,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/pgalloc.h&gt;
 #include &lt;asm/setup.h&gt;
 #include &lt;asm/espfix.h&gt;
<span class="p_add">+#include &lt;asm/kaiser.h&gt;</span>
 
 /*
  * Note: we only need 6*8 = 48 bytes for the espfix stack, but round
<span class="p_chunk">@@ -128,6 +129,22 @@</span> <span class="p_context"> void __init init_espfix_bsp(void)</span>
 	pgd = &amp;init_top_pgt[pgd_index(ESPFIX_BASE_ADDR)];
 	p4d = p4d_alloc(&amp;init_mm, pgd, ESPFIX_BASE_ADDR);
 	p4d_populate(&amp;init_mm, p4d, espfix_pud_page);
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Just copy the top-level PGD that is mapping the espfix</span>
<span class="p_add">+	 * area to ensure it is mapped into the shadow user page</span>
<span class="p_add">+	 * tables.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * For 5-level paging, we should have already populated</span>
<span class="p_add">+	 * the espfix pgd when kaiser_init() pre-populated all</span>
<span class="p_add">+	 * the pgd entries.  The above p4d_alloc() would never do</span>
<span class="p_add">+	 * anything and the p4d_populate() would be done to a p4d</span>
<span class="p_add">+	 * already mapped in the userspace pgd.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+#ifdef CONFIG_KAISER</span>
<span class="p_add">+	if (CONFIG_PGTABLE_LEVELS &lt;= 4)</span>
<span class="p_add">+		set_pgd(native_get_shadow_pgd(pgd),</span>
<span class="p_add">+			__pgd(_KERNPG_TABLE | (p4d_pfn(*p4d) &lt;&lt; PAGE_SHIFT)));</span>
<span class="p_add">+#endif</span>
 
 	/* Randomize the locations */
 	init_espfix_random();
<span class="p_header">diff -puN arch/x86/kernel/head_64.S~kaiser-base arch/x86/kernel/head_64.S</span>
<span class="p_header">--- a/arch/x86/kernel/head_64.S~kaiser-base	2017-10-31 15:03:51.826183141 -0700</span>
<span class="p_header">+++ b/arch/x86/kernel/head_64.S	2017-10-31 15:03:51.844183992 -0700</span>
<span class="p_chunk">@@ -339,6 +339,14 @@</span> <span class="p_context"> GLOBAL(early_recursion_flag)</span>
 	.balign	PAGE_SIZE; \
 GLOBAL(name)
 
<span class="p_add">+#ifdef CONFIG_KAISER</span>
<span class="p_add">+#define NEXT_PGD_PAGE(name) \</span>
<span class="p_add">+	.balign 2 * PAGE_SIZE; \</span>
<span class="p_add">+GLOBAL(name)</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define NEXT_PGD_PAGE(name) NEXT_PAGE(name)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 /* Automate the creation of 1 to 1 mapping pmd entries */
 #define PMDS(START, PERM, COUNT)			\
 	i = 0 ;						\
<span class="p_chunk">@@ -348,7 +356,7 @@</span> <span class="p_context"> GLOBAL(name)</span>
 	.endr
 
 	__INITDATA
<span class="p_del">-NEXT_PAGE(early_top_pgt)</span>
<span class="p_add">+NEXT_PGD_PAGE(early_top_pgt)</span>
 	.fill	511,8,0
 #ifdef CONFIG_X86_5LEVEL
 	.quad	level4_kernel_pgt - __START_KERNEL_map + _PAGE_TABLE_NOENC
<span class="p_chunk">@@ -362,10 +370,10 @@</span> <span class="p_context"> NEXT_PAGE(early_dynamic_pgts)</span>
 	.data
 
 #ifndef CONFIG_XEN
<span class="p_del">-NEXT_PAGE(init_top_pgt)</span>
<span class="p_add">+NEXT_PGD_PAGE(init_top_pgt)</span>
 	.fill	512,8,0
 #else
<span class="p_del">-NEXT_PAGE(init_top_pgt)</span>
<span class="p_add">+NEXT_PGD_PAGE(init_top_pgt)</span>
 	.quad   level3_ident_pgt - __START_KERNEL_map + _KERNPG_TABLE_NOENC
 	.org    init_top_pgt + PGD_PAGE_OFFSET*8, 0
 	.quad   level3_ident_pgt - __START_KERNEL_map + _KERNPG_TABLE_NOENC
<span class="p_header">diff -puN /dev/null arch/x86/mm/kaiser.c</span>
<span class="p_header">--- /dev/null	2017-05-17 09:46:39.241182829 -0700</span>
<span class="p_header">+++ b/arch/x86/mm/kaiser.c	2017-10-31 15:03:51.845184039 -0700</span>
<span class="p_chunk">@@ -0,0 +1,380 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright(c) 2017 Intel Corporation. All rights reserved.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of version 2 of the GNU General Public License as</span>
<span class="p_add">+ * published by the Free Software Foundation.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is distributed in the hope that it will be useful, but</span>
<span class="p_add">+ * WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU</span>
<span class="p_add">+ * General Public License for more details.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Based on work published here: https://github.com/IAIK/KAISER</span>
<span class="p_add">+ * Modified by Dave Hansen &lt;dave.hansen@intel.com to actually work.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#include &lt;linux/kernel.h&gt;</span>
<span class="p_add">+#include &lt;linux/errno.h&gt;</span>
<span class="p_add">+#include &lt;linux/string.h&gt;</span>
<span class="p_add">+#include &lt;linux/types.h&gt;</span>
<span class="p_add">+#include &lt;linux/bug.h&gt;</span>
<span class="p_add">+#include &lt;linux/init.h&gt;</span>
<span class="p_add">+#include &lt;linux/spinlock.h&gt;</span>
<span class="p_add">+#include &lt;linux/mm.h&gt;</span>
<span class="p_add">+#include &lt;linux/uaccess.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/kaiser.h&gt;</span>
<span class="p_add">+#include &lt;asm/pgtable.h&gt;</span>
<span class="p_add">+#include &lt;asm/pgalloc.h&gt;</span>
<span class="p_add">+#include &lt;asm/tlbflush.h&gt;</span>
<span class="p_add">+#include &lt;asm/desc.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * At runtime, the only things we map are some things for CPU</span>
<span class="p_add">+ * hotplug, and stacks for new processes.  No two CPUs will ever</span>
<span class="p_add">+ * be populating the same addresses, so we only need to ensure</span>
<span class="p_add">+ * that we protect between two CPUs trying to allocate and</span>
<span class="p_add">+ * populate the same page table page.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Only take this lock when doing a set_p[4um]d(), but it is not</span>
<span class="p_add">+ * needed for doing a set_pte().  We assume that only the *owner*</span>
<span class="p_add">+ * of a given allocation will be doing this for _their_</span>
<span class="p_add">+ * allocation.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This ensures that once a system has been running for a while</span>
<span class="p_add">+ * and there have been stacks all over and these page tables</span>
<span class="p_add">+ * are fully populated, there will be no further acquisitions of</span>
<span class="p_add">+ * this lock.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static DEFINE_SPINLOCK(shadow_table_allocation_lock);</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Returns -1 on error.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline unsigned long get_pa_from_mapping(unsigned long vaddr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pgd_t *pgd;</span>
<span class="p_add">+	p4d_t *p4d;</span>
<span class="p_add">+	pud_t *pud;</span>
<span class="p_add">+	pmd_t *pmd;</span>
<span class="p_add">+	pte_t *pte;</span>
<span class="p_add">+</span>
<span class="p_add">+	pgd = pgd_offset_k(vaddr);</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We made all the kernel PGDs present in kaiser_init().</span>
<span class="p_add">+	 * We expect them to stay that way.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (pgd_none(*pgd)) {</span>
<span class="p_add">+		WARN_ON_ONCE(1);</span>
<span class="p_add">+		return -1;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * PGDs are either 512GB or 128TB on all x86_64</span>
<span class="p_add">+	 * configurations.  We don&#39;t handle these.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (pgd_large(*pgd)) {</span>
<span class="p_add">+		WARN_ON_ONCE(1);</span>
<span class="p_add">+		return -1;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	p4d = p4d_offset(pgd, vaddr);</span>
<span class="p_add">+	if (p4d_none(*p4d)) {</span>
<span class="p_add">+		WARN_ON_ONCE(1);</span>
<span class="p_add">+		return -1;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	pud = pud_offset(p4d, vaddr);</span>
<span class="p_add">+	if (pud_none(*pud)) {</span>
<span class="p_add">+		WARN_ON_ONCE(1);</span>
<span class="p_add">+		return -1;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (pud_large(*pud))</span>
<span class="p_add">+		return (pud_pfn(*pud) &lt;&lt; PAGE_SHIFT) | (vaddr &amp; ~PUD_PAGE_MASK);</span>
<span class="p_add">+</span>
<span class="p_add">+	pmd = pmd_offset(pud, vaddr);</span>
<span class="p_add">+	if (pmd_none(*pmd)) {</span>
<span class="p_add">+		WARN_ON_ONCE(1);</span>
<span class="p_add">+		return -1;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (pmd_large(*pmd))</span>
<span class="p_add">+		return (pmd_pfn(*pmd) &lt;&lt; PAGE_SHIFT) | (vaddr &amp; ~PMD_PAGE_MASK);</span>
<span class="p_add">+</span>
<span class="p_add">+	pte = pte_offset_kernel(pmd, vaddr);</span>
<span class="p_add">+	if (pte_none(*pte)) {</span>
<span class="p_add">+		WARN_ON_ONCE(1);</span>
<span class="p_add">+		return -1;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return (pte_pfn(*pte) &lt;&lt; PAGE_SHIFT) | (vaddr &amp; ~PAGE_MASK);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * This is a relatively normal page table walk, except that it</span>
<span class="p_add">+ * also tries to allocate page tables pages along the way.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Returns a pointer to a PTE on success, or NULL on failure.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define KAISER_WALK_ATOMIC  0x1</span>
<span class="p_add">+static pte_t *kaiser_pagetable_walk(unsigned long address, unsigned long flags)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pmd_t *pmd;</span>
<span class="p_add">+	pud_t *pud;</span>
<span class="p_add">+	p4d_t *p4d;</span>
<span class="p_add">+	pgd_t *pgd = native_get_shadow_pgd(pgd_offset_k(address));</span>
<span class="p_add">+	gfp_t gfp = (GFP_KERNEL | __GFP_NOTRACK | __GFP_ZERO);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (flags &amp; KAISER_WALK_ATOMIC) {</span>
<span class="p_add">+		gfp &amp;= ~GFP_KERNEL;</span>
<span class="p_add">+		gfp |= __GFP_HIGH | __GFP_ATOMIC;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (pgd_none(*pgd)) {</span>
<span class="p_add">+		WARN_ONCE(1, &quot;All shadow pgds should have been populated&quot;);</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	BUILD_BUG_ON(pgd_large(*pgd) != 0);</span>
<span class="p_add">+</span>
<span class="p_add">+	p4d = p4d_offset(pgd, address);</span>
<span class="p_add">+	BUILD_BUG_ON(p4d_large(*p4d) != 0);</span>
<span class="p_add">+	if (p4d_none(*p4d)) {</span>
<span class="p_add">+		unsigned long new_pud_page = __get_free_page(gfp);</span>
<span class="p_add">+		if (!new_pud_page)</span>
<span class="p_add">+			return NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+		spin_lock(&amp;shadow_table_allocation_lock);</span>
<span class="p_add">+		if (p4d_none(*p4d))</span>
<span class="p_add">+			set_p4d(p4d, __p4d(_KERNPG_TABLE | __pa(new_pud_page)));</span>
<span class="p_add">+		else</span>
<span class="p_add">+			free_page(new_pud_page);</span>
<span class="p_add">+		spin_unlock(&amp;shadow_table_allocation_lock);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	pud = pud_offset(p4d, address);</span>
<span class="p_add">+	/* The shadow page tables do not use large mappings: */</span>
<span class="p_add">+	if (pud_large(*pud)) {</span>
<span class="p_add">+		WARN_ON(1);</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	if (pud_none(*pud)) {</span>
<span class="p_add">+		unsigned long new_pmd_page = __get_free_page(gfp);</span>
<span class="p_add">+		if (!new_pmd_page)</span>
<span class="p_add">+			return NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+		spin_lock(&amp;shadow_table_allocation_lock);</span>
<span class="p_add">+		if (pud_none(*pud))</span>
<span class="p_add">+			set_pud(pud, __pud(_KERNPG_TABLE | __pa(new_pmd_page)));</span>
<span class="p_add">+		else</span>
<span class="p_add">+			free_page(new_pmd_page);</span>
<span class="p_add">+		spin_unlock(&amp;shadow_table_allocation_lock);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	pmd = pmd_offset(pud, address);</span>
<span class="p_add">+	/* The shadow page tables do not use large mappings: */</span>
<span class="p_add">+	if (pmd_large(*pmd)) {</span>
<span class="p_add">+		WARN_ON(1);</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	if (pmd_none(*pmd)) {</span>
<span class="p_add">+		unsigned long new_pte_page = __get_free_page(gfp);</span>
<span class="p_add">+		if (!new_pte_page)</span>
<span class="p_add">+			return NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+		spin_lock(&amp;shadow_table_allocation_lock);</span>
<span class="p_add">+		if (pmd_none(*pmd))</span>
<span class="p_add">+			set_pmd(pmd, __pmd(_KERNPG_TABLE  | __pa(new_pte_page)));</span>
<span class="p_add">+		else</span>
<span class="p_add">+			free_page(new_pte_page);</span>
<span class="p_add">+		spin_unlock(&amp;shadow_table_allocation_lock);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return pte_offset_kernel(pmd, address);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Given a kernel address, @__start_addr, copy that mapping into</span>
<span class="p_add">+ * the user (shadow) page tables.  This may need to allocate page</span>
<span class="p_add">+ * table pages.</span>
<span class="p_add">+ */</span>
<span class="p_add">+int kaiser_add_user_map(const void *__start_addr, unsigned long size,</span>
<span class="p_add">+			unsigned long flags)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pte_t *pte;</span>
<span class="p_add">+	unsigned long start_addr = (unsigned long)__start_addr;</span>
<span class="p_add">+	unsigned long address = start_addr &amp; PAGE_MASK;</span>
<span class="p_add">+	unsigned long end_addr = PAGE_ALIGN(start_addr + size);</span>
<span class="p_add">+	unsigned long target_address;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (; address &lt; end_addr; address += PAGE_SIZE) {</span>
<span class="p_add">+		target_address = get_pa_from_mapping(address);</span>
<span class="p_add">+		if (target_address == -1)</span>
<span class="p_add">+			return -EIO;</span>
<span class="p_add">+</span>
<span class="p_add">+		pte = kaiser_pagetable_walk(address, false);</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Errors come from either -ENOMEM for a page</span>
<span class="p_add">+		 * table page, or something screwy that did a</span>
<span class="p_add">+		 * WARN_ON().  Just return -ENOMEM.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (!pte)</span>
<span class="p_add">+			return -ENOMEM;</span>
<span class="p_add">+		if (pte_none(*pte)) {</span>
<span class="p_add">+			set_pte(pte, __pte(flags | target_address));</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			pte_t tmp;</span>
<span class="p_add">+			set_pte(&amp;tmp, __pte(flags | target_address));</span>
<span class="p_add">+			WARN_ON_ONCE(!pte_same(*pte, tmp));</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * The stack mapping is called in generic code and can&#39;t use</span>
<span class="p_add">+ * __PAGE_KERNEL</span>
<span class="p_add">+ */</span>
<span class="p_add">+int kaiser_map_stack(struct task_struct *tsk)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return kaiser_add_mapping((unsigned long)tsk-&gt;stack, THREAD_SIZE,</span>
<span class="p_add">+				  __PAGE_KERNEL);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+int kaiser_add_user_map_ptrs(const void *__start_addr,</span>
<span class="p_add">+			     const void *__end_addr,</span>
<span class="p_add">+			     unsigned long flags)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return kaiser_add_user_map(__start_addr,</span>
<span class="p_add">+				   __end_addr - __start_addr,</span>
<span class="p_add">+				   flags);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Ensure that the top level of the (shadow) page tables are</span>
<span class="p_add">+ * entirely populated.  This ensures that all processes that get</span>
<span class="p_add">+ * forked have the same entries.  This way, we do not have to</span>
<span class="p_add">+ * ever go set up new entries in older processes.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Note: we never free these, so there are no updates to them</span>
<span class="p_add">+ * after this.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static void __init kaiser_init_all_pgds(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pgd_t *pgd;</span>
<span class="p_add">+	int i = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	pgd = native_get_shadow_pgd(pgd_offset_k(0UL));</span>
<span class="p_add">+	for (i = PTRS_PER_PGD / 2; i &lt; PTRS_PER_PGD; i++) {</span>
<span class="p_add">+		unsigned long addr = PAGE_OFFSET + i * PGDIR_SIZE;</span>
<span class="p_add">+#if CONFIG_PGTABLE_LEVELS &gt; 4</span>
<span class="p_add">+		p4d_t *p4d = p4d_alloc_one(&amp;init_mm, addr);</span>
<span class="p_add">+		if (!p4d) {</span>
<span class="p_add">+			WARN_ON(1);</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		set_pgd(pgd + i, __pgd(_KERNPG_TABLE | __pa(p4d)));</span>
<span class="p_add">+#else /* CONFIG_PGTABLE_LEVELS &lt;= 4 */</span>
<span class="p_add">+		pud_t *pud = pud_alloc_one(&amp;init_mm, addr);</span>
<span class="p_add">+		if (!pud) {</span>
<span class="p_add">+			WARN_ON(1);</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		set_pgd(pgd + i, __pgd(_KERNPG_TABLE | __pa(pud)));</span>
<span class="p_add">+#endif /* CONFIG_PGTABLE_LEVELS */</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * The page table allocations in here can theoretically fail, but</span>
<span class="p_add">+ * we can not do much about it in early boot.  Do the checking</span>
<span class="p_add">+ * and warning in a macro to make it more readable.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define kaiser_add_user_map_early(start, size, flags) do {	\</span>
<span class="p_add">+	int __ret = kaiser_add_user_map(start, size, flags);	\</span>
<span class="p_add">+	WARN_ON(__ret);						\</span>
<span class="p_add">+} while (0)</span>
<span class="p_add">+</span>
<span class="p_add">+#define kaiser_add_user_map_ptrs_early(start, end, flags) do {		\</span>
<span class="p_add">+	int __ret = kaiser_add_user_map_ptrs(start, end, flags);	\</span>
<span class="p_add">+	WARN_ON(__ret);							\</span>
<span class="p_add">+} while (0)</span>
<span class="p_add">+</span>
<span class="p_add">+extern char __per_cpu_user_mapped_start[], __per_cpu_user_mapped_end[];</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * If anything in here fails, we will likely die on one of the</span>
<span class="p_add">+ * first kernel-&gt;user transitions and init will die.  But, we</span>
<span class="p_add">+ * will have most of the kernel up by then and should be able to</span>
<span class="p_add">+ * get a clean warning out of it.  If we BUG_ON() here, we run</span>
<span class="p_add">+ * the risk of being before we have good console output.</span>
<span class="p_add">+ */</span>
<span class="p_add">+void __init kaiser_init(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int cpu;</span>
<span class="p_add">+</span>
<span class="p_add">+	kaiser_init_all_pgds();</span>
<span class="p_add">+</span>
<span class="p_add">+	for_each_possible_cpu(cpu) {</span>
<span class="p_add">+		void *percpu_vaddr = __per_cpu_user_mapped_start +</span>
<span class="p_add">+				     per_cpu_offset(cpu);</span>
<span class="p_add">+		unsigned long percpu_sz = __per_cpu_user_mapped_end -</span>
<span class="p_add">+					  __per_cpu_user_mapped_start;</span>
<span class="p_add">+		kaiser_add_user_map_early(percpu_vaddr, percpu_sz,</span>
<span class="p_add">+					  __PAGE_KERNEL);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	kaiser_add_user_map_ptrs_early(__entry_text_start, __entry_text_end,</span>
<span class="p_add">+				       __PAGE_KERNEL_RX);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* the fixed map address of the idt_table */</span>
<span class="p_add">+	kaiser_add_user_map_early((void *)idt_descr.address,</span>
<span class="p_add">+				  sizeof(gate_desc) * NR_VECTORS,</span>
<span class="p_add">+				  __PAGE_KERNEL_RO);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+int kaiser_add_mapping(unsigned long addr, unsigned long size,</span>
<span class="p_add">+		       unsigned long flags)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return kaiser_add_user_map((const void *)addr, size, flags);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void kaiser_remove_mapping(unsigned long start, unsigned long size)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long addr;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* The shadow page tables always use small pages: */</span>
<span class="p_add">+	for (addr = start; addr &lt; start + size; addr += PAGE_SIZE) {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Do an &quot;atomic&quot; walk in case this got called from an atomic</span>
<span class="p_add">+		 * context.  This should not do any allocations because we</span>
<span class="p_add">+		 * should only be walking things that are known to be mapped.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		pte_t *pte = kaiser_pagetable_walk(addr, KAISER_WALK_ATOMIC);</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * We are removing a mapping that shoud</span>
<span class="p_add">+		 * exist.  WARN if it was not there:</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (!pte) {</span>
<span class="p_add">+			WARN_ON_ONCE(1);</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		pte_clear(&amp;init_mm, addr, pte);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * This ensures that the TLB entries used to map this data are</span>
<span class="p_add">+	 * no longer usable on *this* CPU.  We theoretically want to</span>
<span class="p_add">+	 * flush the entries on all CPUs here, but that&#39;s too</span>
<span class="p_add">+	 * expensive right now: this is called to unmap process</span>
<span class="p_add">+	 * stacks in the exit() path path.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * This can change if we get to the point where this is not</span>
<span class="p_add">+	 * in a remotely hot path, like only called via write_ldt().</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * Note: we could probably also just invalidate the individual</span>
<span class="p_add">+	 * addresses to take care of *this* PCID and then do a</span>
<span class="p_add">+	 * tlb_flush_shared_nonglobals() to ensure that all other</span>
<span class="p_add">+	 * PCIDs get flushed before being used again.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	__native_flush_tlb_global();</span>
<span class="p_add">+}</span>
<span class="p_header">diff -puN arch/x86/mm/Makefile~kaiser-base arch/x86/mm/Makefile</span>
<span class="p_header">--- a/arch/x86/mm/Makefile~kaiser-base	2017-10-31 15:03:51.828183236 -0700</span>
<span class="p_header">+++ b/arch/x86/mm/Makefile	2017-10-31 15:03:51.845184039 -0700</span>
<span class="p_chunk">@@ -45,6 +45,7 @@</span> <span class="p_context"> obj-$(CONFIG_NUMA_EMU)		+= numa_emulatio</span>
 obj-$(CONFIG_X86_INTEL_MPX)	+= mpx.o
 obj-$(CONFIG_X86_INTEL_MEMORY_PROTECTION_KEYS) += pkeys.o
 obj-$(CONFIG_RANDOMIZE_MEMORY) += kaslr.o
<span class="p_add">+obj-$(CONFIG_KAISER)		+= kaiser.o</span>
 
 obj-$(CONFIG_AMD_MEM_ENCRYPT)	+= mem_encrypt.o
 obj-$(CONFIG_AMD_MEM_ENCRYPT)	+= mem_encrypt_boot.o
<span class="p_header">diff -puN arch/x86/mm/pageattr.c~kaiser-base arch/x86/mm/pageattr.c</span>
<span class="p_header">--- a/arch/x86/mm/pageattr.c~kaiser-base	2017-10-31 15:03:51.830183330 -0700</span>
<span class="p_header">+++ b/arch/x86/mm/pageattr.c	2017-10-31 15:03:51.847184134 -0700</span>
<span class="p_chunk">@@ -859,7 +859,7 @@</span> <span class="p_context"> static void unmap_pmd_range(pud_t *pud,</span>
 			pud_clear(pud);
 }
 
<span class="p_del">-static void unmap_pud_range(p4d_t *p4d, unsigned long start, unsigned long end)</span>
<span class="p_add">+void unmap_pud_range(p4d_t *p4d, unsigned long start, unsigned long end)</span>
 {
 	pud_t *pud = pud_offset(p4d, start);
 
<span class="p_header">diff -puN arch/x86/mm/pgtable.c~kaiser-base arch/x86/mm/pgtable.c</span>
<span class="p_header">--- a/arch/x86/mm/pgtable.c~kaiser-base	2017-10-31 15:03:51.833183472 -0700</span>
<span class="p_header">+++ b/arch/x86/mm/pgtable.c	2017-10-31 15:03:51.847184134 -0700</span>
<span class="p_chunk">@@ -354,14 +354,26 @@</span> <span class="p_context"> static inline void _pgd_free(pgd_t *pgd)</span>
 		kmem_cache_free(pgd_cache, pgd);
 }
 #else
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_KAISER</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Instead of one pgd, we aquire two pgds.  Being order-1, it is</span>
<span class="p_add">+ * both 8k in size and 8k-aligned.  That lets us just flip bit 12</span>
<span class="p_add">+ * in a pointer to swap between the two 4k halves.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define PGD_ALLOCATION_ORDER 1</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define PGD_ALLOCATION_ORDER 0</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 static inline pgd_t *_pgd_alloc(void)
 {
<span class="p_del">-	return (pgd_t *)__get_free_page(PGALLOC_GFP);</span>
<span class="p_add">+	return (pgd_t *)__get_free_pages(PGALLOC_GFP, PGD_ALLOCATION_ORDER);</span>
 }
 
 static inline void _pgd_free(pgd_t *pgd)
 {
<span class="p_del">-	free_page((unsigned long)pgd);</span>
<span class="p_add">+	free_pages((unsigned long)pgd, PGD_ALLOCATION_ORDER);</span>
 }
 #endif /* CONFIG_X86_PAE */
 
<span class="p_header">diff -puN /dev/null Documentation/x86/kaiser.txt</span>
<span class="p_header">--- /dev/null	2017-05-17 09:46:39.241182829 -0700</span>
<span class="p_header">+++ b/Documentation/x86/kaiser.txt	2017-10-31 15:03:51.848184181 -0700</span>
<span class="p_chunk">@@ -0,0 +1,128 @@</span> <span class="p_context"></span>
<span class="p_add">+KAISER is a countermeasure against attacks on kernel address</span>
<span class="p_add">+information.  There are at least three existing, published,</span>
<span class="p_add">+approaches using the shared user/kernel mapping and hardware features</span>
<span class="p_add">+to defeat KASLR.  One approach referenced in the paper locates the</span>
<span class="p_add">+kernel by observing differences in page fault timing between</span>
<span class="p_add">+present-but-inaccessable kernel pages and non-present pages.</span>
<span class="p_add">+</span>
<span class="p_add">+When we enter the kernel via syscalls, interrupts or exceptions,</span>
<span class="p_add">+page tables are switched to the full &quot;kernel&quot; copy.  When the</span>
<span class="p_add">+system switches back to user mode, the user/shadow copy is used.</span>
<span class="p_add">+</span>
<span class="p_add">+The minimalistic kernel portion of the user page tables try to</span>
<span class="p_add">+map only what is needed to enter/exit the kernel such as the</span>
<span class="p_add">+entry/exit functions themselves and the interrupt descriptor</span>
<span class="p_add">+table (IDT).</span>
<span class="p_add">+</span>
<span class="p_add">+This helps ensure that side-channel attacks that leverage the</span>
<span class="p_add">+paging structures do not function when KAISER is enabled.  It</span>
<span class="p_add">+can be enabled by setting CONFIG_KAISER=y</span>
<span class="p_add">+</span>
<span class="p_add">+Protection against side-channel attacks is important.  But,</span>
<span class="p_add">+this protection comes at a cost:</span>
<span class="p_add">+</span>
<span class="p_add">+1. Increased Memory Use</span>
<span class="p_add">+  a. Each process now needs an order-1 PGD instead of order-0.</span>
<span class="p_add">+     (Consumes 4k per process).</span>
<span class="p_add">+  b. The pre-allocated second-level (p4d or pud) kernel page</span>
<span class="p_add">+     table pages cost ~1MB of additional memory at boot.  This</span>
<span class="p_add">+     is not totally wasted because some of these pages would</span>
<span class="p_add">+     have been needed eventually for normal kernel page tables</span>
<span class="p_add">+     and things in the vmalloc() area like vmemmap[].</span>
<span class="p_add">+  c. Statically-allocated structures and entry/exit text must</span>
<span class="p_add">+     be padded out to 4k (or 8k for PGDs) so they can be mapped</span>
<span class="p_add">+     into the user page tables.  This bloats the kernel image</span>
<span class="p_add">+     by ~20-30k.</span>
<span class="p_add">+  d. The shadow page tables eventually grow to map all of used</span>
<span class="p_add">+     vmalloc() space.  They can have roughly the same memory</span>
<span class="p_add">+     consumption as the vmalloc() page tables.</span>
<span class="p_add">+</span>
<span class="p_add">+2. Runtime Cost</span>
<span class="p_add">+  a. CR3 manipulation to switch between the page table copies</span>
<span class="p_add">+     must be done at interrupt, syscall, and exception entry</span>
<span class="p_add">+     and exit (it can be skipped when the kernel is interrupted,</span>
<span class="p_add">+     though.)  Moves to CR3 are on the order of a hundred</span>
<span class="p_add">+     cycles, and we need one at entry and another at exit.</span>
<span class="p_add">+  b. Task stacks must be mapped/unmapped.  We need to walk</span>
<span class="p_add">+     and modify the shadow page tables at fork() and exit().</span>
<span class="p_add">+  c. Global pages are disabled.  This feature of the MMU</span>
<span class="p_add">+     allows different processes to share TLB entries mapping</span>
<span class="p_add">+     the kernel.  Losing the feature means potentially more</span>
<span class="p_add">+     TLB misses after a context switch.</span>
<span class="p_add">+  d. Process Context IDentifiers (PCID) is a CPU feature that</span>
<span class="p_add">+     allows us to skip flushing the entire TLB when we switch</span>
<span class="p_add">+     the page tables.  This makes switching the page tables</span>
<span class="p_add">+     (at context switch, or kernel entry/exit) cheaper.  But,</span>
<span class="p_add">+     on systems with PCID support, the context switch code</span>
<span class="p_add">+     must flush both the user and kernel entries out of the</span>
<span class="p_add">+     TLB, with an INVPCID in addition to the CR3 write.  This</span>
<span class="p_add">+     INVPCID is generally slower than a CR3 write, but still</span>
<span class="p_add">+     on the order of a hundred cycles.</span>
<span class="p_add">+  e. The shadow page tables must be populated for each new</span>
<span class="p_add">+     process.  Even without KAISER, since we share all of the</span>
<span class="p_add">+     kernel mappings in all processes, we can do all this</span>
<span class="p_add">+     population for kernel addresses at the top level of the</span>
<span class="p_add">+     page tables (the PGD level).  But, with KAISER, we now</span>
<span class="p_add">+     have *two* kernel mappings: one in the kernel page tables</span>
<span class="p_add">+     that maps everything and one in the user/shadow page</span>
<span class="p_add">+     tables mapping the &quot;minimal&quot; kernel.  At fork(), we</span>
<span class="p_add">+     copy the portion of the shadow PGD that maps the minimal</span>
<span class="p_add">+     kernel structures in addition to the normal kernel one.</span>
<span class="p_add">+  f. In addition to the fork()-time copying, we must also</span>
<span class="p_add">+     update the shadow PGD any time a set_pgd() is done on a</span>
<span class="p_add">+     PGD used to map userspace.  This ensures that the kernel</span>
<span class="p_add">+     and user/shadow copies always map the same userspace</span>
<span class="p_add">+     memory.</span>
<span class="p_add">+  g. On systems without PCID support, each CR3 write flushes</span>
<span class="p_add">+     the entire TLB.  That means that each syscall, interrupt</span>
<span class="p_add">+     or exception flushes the TLB.</span>
<span class="p_add">+</span>
<span class="p_add">+Possible Future Work:</span>
<span class="p_add">+1. We can be more careful about not actually writing to CR3</span>
<span class="p_add">+   unless we actually switch it.</span>
<span class="p_add">+2. Try to have dedicated entry/exit kernel stacks so we do</span>
<span class="p_add">+   not have to map/unmap the task/thread stacks.</span>
<span class="p_add">+3. Compress the user/shadow-mapped data to be mapped together</span>
<span class="p_add">+   underneath a single PGD entry.</span>
<span class="p_add">+4. Re-enable global pages, but use them for mappings in the</span>
<span class="p_add">+   user/shadow page tables.  This would allow the kernel to</span>
<span class="p_add">+   take advantage of TLB entries that were established from</span>
<span class="p_add">+   the user page tables.  This might speed up the entry/exit</span>
<span class="p_add">+   code or userspace since it will not have to reload all of</span>
<span class="p_add">+   its TLB entries.  However, its upside is limited by PCID</span>
<span class="p_add">+   being used.</span>
<span class="p_add">+5. Allow KAISER to enabled/disabled at runtime so folks can</span>
<span class="p_add">+   run a single kernel image.</span>
<span class="p_add">+</span>
<span class="p_add">+Debugging:</span>
<span class="p_add">+</span>
<span class="p_add">+Bugs in KAISER cause a few different signatures of crashes</span>
<span class="p_add">+that are worth noting here.</span>
<span class="p_add">+</span>
<span class="p_add">+ * Crashes in early boot, especially around CPU bringup.  Bugs</span>
<span class="p_add">+   in the trampoline code or mappings cause these.</span>
<span class="p_add">+ * Crashes at the first interrupt.  Caused by bugs in entry_64.S,</span>
<span class="p_add">+   like screwing up a page table switch.  Also caused by</span>
<span class="p_add">+   incorrectly mapping the IRQ handler entry code.</span>
<span class="p_add">+ * Crashes at the first NMI.  The NMI code is separate from main</span>
<span class="p_add">+   interrupt handlers and can have bugs that do not affect</span>
<span class="p_add">+   normal interrupts.  Also caused by incorrectly mapping NMI</span>
<span class="p_add">+   code.  NMIs that interrupt the entry code must be very</span>
<span class="p_add">+   careful and can be the cause of crashes that show up when</span>
<span class="p_add">+   running perf.</span>
<span class="p_add">+ * Kernel crashes at the first exit to userspace.  entry_64.S</span>
<span class="p_add">+   bugs, or failing to map some of the exit code.</span>
<span class="p_add">+ * Crashes at first interrupt that interrupts userspace. The paths</span>
<span class="p_add">+   in entry_64.S that return to userspace are sometimes separate</span>
<span class="p_add">+   from the ones that return to the kernel.</span>
<span class="p_add">+ * Double faults: overflowing the kernel stack because of page</span>
<span class="p_add">+   faults upon page faults.  Caused by touching non-kaiser-mapped</span>
<span class="p_add">+   data in the entry code, or forgetting to switch to kernel</span>
<span class="p_add">+   CR3 before calling into C functions which are not kaiser-mapped.</span>
<span class="p_add">+ * Failures of the selftests/x86 code.  Usually a bug in one of the</span>
<span class="p_add">+   more obscure corners of entry_64.S</span>
<span class="p_add">+ * Userspace segfaults early in boot, sometimes manifesting</span>
<span class="p_add">+   as mount(8) failing to mount the rootfs.  These have</span>
<span class="p_add">+   tended to be TLB invalidation issues.  Usually invalidating</span>
<span class="p_add">+   the wrong PCID, or otherwise missing an invalidation.</span>
<span class="p_add">+</span>
<span class="p_header">diff -puN /dev/null include/linux/kaiser.h</span>
<span class="p_header">--- /dev/null	2017-05-17 09:46:39.241182829 -0700</span>
<span class="p_header">+++ b/include/linux/kaiser.h	2017-10-31 15:03:51.848184181 -0700</span>
<span class="p_chunk">@@ -0,0 +1,34 @@</span> <span class="p_context"></span>
<span class="p_add">+#ifndef _INCLUDE_KAISER_H</span>
<span class="p_add">+#define _INCLUDE_KAISER_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_KAISER</span>
<span class="p_add">+#include &lt;asm/kaiser.h&gt;</span>
<span class="p_add">+#else</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * These stubs are used whenever CONFIG_KAISER is off, which</span>
<span class="p_add">+ * includes architectures that support KAISER, but have it</span>
<span class="p_add">+ * disabled.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int kaiser_map_stack(struct task_struct *tsk)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void kaiser_init(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void kaiser_remove_mapping(unsigned long start, unsigned long size)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int kaiser_add_mapping(unsigned long addr, unsigned long size,</span>
<span class="p_add">+				     unsigned long flags)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* !CONFIG_KAISER */</span>
<span class="p_add">+#endif /* _INCLUDE_KAISER_H */</span>
<span class="p_header">diff -puN init/main.c~kaiser-base init/main.c</span>
<span class="p_header">--- a/init/main.c~kaiser-base	2017-10-31 15:03:51.836183614 -0700</span>
<span class="p_header">+++ b/init/main.c	2017-10-31 15:03:51.848184181 -0700</span>
<span class="p_chunk">@@ -75,6 +75,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/slab.h&gt;
 #include &lt;linux/perf_event.h&gt;
 #include &lt;linux/ptrace.h&gt;
<span class="p_add">+#include &lt;linux/kaiser.h&gt;</span>
 #include &lt;linux/blkdev.h&gt;
 #include &lt;linux/elevator.h&gt;
 #include &lt;linux/sched_clock.h&gt;
<span class="p_chunk">@@ -504,6 +505,7 @@</span> <span class="p_context"> static void __init mm_init(void)</span>
 	pgtable_init();
 	vmalloc_init();
 	ioremap_huge_init();
<span class="p_add">+	kaiser_init();</span>
 }
 
 asmlinkage __visible void __init start_kernel(void)
<span class="p_header">diff -puN kernel/fork.c~kaiser-base kernel/fork.c</span>
<span class="p_header">--- a/kernel/fork.c~kaiser-base	2017-10-31 15:03:51.838183708 -0700</span>
<span class="p_header">+++ b/kernel/fork.c	2017-10-31 15:03:51.849184228 -0700</span>
<span class="p_chunk">@@ -70,6 +70,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/tsacct_kern.h&gt;
 #include &lt;linux/cn_proc.h&gt;
 #include &lt;linux/freezer.h&gt;
<span class="p_add">+#include &lt;linux/kaiser.h&gt;</span>
 #include &lt;linux/delayacct.h&gt;
 #include &lt;linux/taskstats_kern.h&gt;
 #include &lt;linux/random.h&gt;
<span class="p_chunk">@@ -247,6 +248,8 @@</span> <span class="p_context"> static unsigned long *alloc_thread_stack</span>
 
 static inline void free_thread_stack(struct task_struct *tsk)
 {
<span class="p_add">+	kaiser_remove_mapping((unsigned long)tsk-&gt;stack, THREAD_SIZE);</span>
<span class="p_add">+</span>
 #ifdef CONFIG_VMAP_STACK
 	if (task_stack_vm_area(tsk)) {
 		int i;
<span class="p_chunk">@@ -536,6 +539,9 @@</span> <span class="p_context"> static struct task_struct *dup_task_stru</span>
 	 * functions again.
 	 */
 	tsk-&gt;stack = stack;
<span class="p_add">+	err = kaiser_map_stack(tsk);</span>
<span class="p_add">+	if (err)</span>
<span class="p_add">+		goto free_stack;</span>
 #ifdef CONFIG_VMAP_STACK
 	tsk-&gt;stack_vm_area = stack_vm_area;
 #endif

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



