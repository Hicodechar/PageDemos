
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[RFC,1/2] x86/ibpb: Skip IBPB when we switch back to same user process - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [RFC,1/2] x86/ibpb: Skip IBPB when we switch back to same user process</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=7886">tim</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Jan. 25, 2018, 12:36 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;c5cc429013c2248ddebe0a8480b172ae70b29733.1516840211.git.tim.c.chen@linux.intel.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10183447/mbox/"
   >mbox</a>
|
   <a href="/patch/10183447/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10183447/">/patch/10183447/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	88A7F6037F for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 25 Jan 2018 00:57:19 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 6BDA1289A7
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 25 Jan 2018 00:57:19 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 5BE34289C7; Thu, 25 Jan 2018 00:57:19 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id C203C289A7
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 25 Jan 2018 00:57:18 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S932969AbeAYA5P (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Wed, 24 Jan 2018 19:57:15 -0500
Received: from mga09.intel.com ([134.134.136.24]:55857 &quot;EHLO mga09.intel.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S932250AbeAYA5O (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Wed, 24 Jan 2018 19:57:14 -0500
X-Amp-Result: SKIPPED(no attachment in message)
X-Amp-File-Uploaded: False
Received: from orsmga003.jf.intel.com ([10.7.209.27])
	by orsmga102.jf.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384;
	24 Jan 2018 16:57:14 -0800
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i=&quot;5.46,409,1511856000&quot;; d=&quot;scan&#39;208&quot;;a=&quot;22116101&quot;
Received: from skl-02.jf.intel.com ([10.54.74.43])
	by orsmga003.jf.intel.com with ESMTP; 24 Jan 2018 16:57:14 -0800
From: Tim Chen &lt;tim.c.chen@linux.intel.com&gt;
To: linux-kernel@vger.kernel.org
Cc: Tim Chen &lt;tim.c.chen@linux.intel.com&gt;,
	KarimAllah Ahmed &lt;karahmed@amazon.de&gt;, Andi Kleen &lt;ak@linux.intel.com&gt;,
	Andrea Arcangeli &lt;aarcange@redhat.com&gt;,
	Andy Lutomirski &lt;luto@kernel.org&gt;,
	Arjan van de Ven &lt;arjan@linux.intel.com&gt;,
	Ashok Raj &lt;ashok.raj@intel.com&gt;, Asit Mallick &lt;asit.k.mallick@intel.com&gt;,
	Borislav Petkov &lt;bp@suse.de&gt;, Dan Williams &lt;dan.j.williams@intel.com&gt;,
	Dave Hansen &lt;dave.hansen@intel.com&gt;, David Woodhouse &lt;dwmw@amazon.co.uk&gt;,
	Greg Kroah-Hartman &lt;gregkh@linuxfoundation.org&gt;,
	&quot;H . Peter Anvin&quot; &lt;hpa@zytor.com&gt;, Ingo Molnar &lt;mingo@redhat.com&gt;,
	Janakarajan Natarajan &lt;Janakarajan.Natarajan@amd.com&gt;,
	Joerg Roedel &lt;joro@8bytes.org&gt;, Jun Nakajima &lt;jun.nakajima@intel.com&gt;,
	Laura Abbott &lt;labbott@redhat.com&gt;,
	Linus Torvalds &lt;torvalds@linux-foundation.org&gt;,
	Masami Hiramatsu &lt;mhiramat@kernel.org&gt;,
	Paolo Bonzini &lt;pbonzini@redhat.com&gt;,
	Peter Zijlstra &lt;peterz@infradead.org&gt;, rkrcmar@redhat.com,
	Thomas Gleixner &lt;tglx@linutronix.de&gt;,
	Tom Lendacky &lt;thomas.lendacky@amd.com&gt;, x86@kernel.org
Subject: [RFC PATCH 1/2] x86/ibpb: Skip IBPB when we switch back to same
	user process
Date: Wed, 24 Jan 2018 16:36:41 -0800
Message-Id: &lt;c5cc429013c2248ddebe0a8480b172ae70b29733.1516840211.git.tim.c.chen@linux.intel.com&gt;
X-Mailer: git-send-email 2.9.4
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=7886">tim</a> - Jan. 25, 2018, 12:36 a.m.</div>
<pre class="content">
These two patches provide optimization to skip IBPB for this
commonly encountered scenario:
We could switch to a kernel idle thread and then back to the original
process such as:
process A -&gt; idle -&gt; process A

In such scenario, we do not have to do IBPB here even though the process
is non-dumpable, as we are switching back to the same process after
an hiatus.

The cost is to have an extra pointer to track the last mm we were using before
switching to the init_mm used by idle.  But avoiding the extra IBPB
is probably worth the extra memory for such a common scenario.
<span class="signed-off-by">
Signed-off-by: Tim Chen &lt;tim.c.chen@linux.intel.com&gt;</span>
---
 arch/x86/include/asm/tlbflush.h |  2 ++
 arch/x86/mm/tlb.c               | 13 +++++++++----
 2 files changed, 11 insertions(+), 4 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - Jan. 25, 2018, 8:58 a.m.</div>
<pre class="content">
On Wed, Jan 24, 2018 at 04:36:41PM -0800, Tim Chen wrote:
<span class="quote">&gt; These two patches provide optimization to skip IBPB for this</span>
<span class="quote">&gt; commonly encountered scenario:</span>
<span class="quote">&gt; We could switch to a kernel idle thread and then back to the original</span>
<span class="quote">&gt; process such as:</span>
<span class="quote">&gt; process A -&gt; idle -&gt; process A</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; In such scenario, we do not have to do IBPB here even though the process</span>
<span class="quote">&gt; is non-dumpable, as we are switching back to the same process after</span>
<span class="quote">&gt; an hiatus.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The cost is to have an extra pointer to track the last mm we were using before</span>
<span class="quote">&gt; switching to the init_mm used by idle.  But avoiding the extra IBPB</span>
<span class="quote">&gt; is probably worth the extra memory for such a common scenario.</span>

So we already track active_mm for kernel threads. I can&#39;t immediately
see where this fails for idle and your changelog doesn&#39;t say.
<span class="quote">
&gt; @@ -229,15 +230,17 @@ void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,</span>
<span class="quote">&gt;                   * As an optimization flush indirect branches only when</span>
<span class="quote">&gt;                   * switching into processes that disable dumping.</span>
<span class="quote">&gt;                   *</span>
<span class="quote">&gt; -                 * This will not flush branches when switching into kernel</span>
<span class="quote">&gt; -                 * threads, but it would flush them when switching to the</span>
<span class="quote">&gt; -                 * idle thread and back.</span>
<span class="quote">&gt; +		 * This will not flush branches when switching into kernel</span>
<span class="quote">&gt; +		 * threads. It will also not flush if we switch to idle</span>
<span class="quote">&gt; +		 * thread and back to the same process. It will flush if we</span>
<span class="quote">&gt; +		 * switch to a different non-dumpable process.</span>

Whitespace damage.
<span class="quote">
&gt;                   *</span>
<span class="quote">&gt;                   * It might be useful to have a one-off cache here</span>
<span class="quote">&gt;                   * to also not flush the idle case, but we would need some</span>
<span class="quote">&gt;                   * kind of stable sequence number to remember the previous mm.</span>
<span class="quote">&gt;  		 */</span>
<span class="quote">&gt; -		if (tsk &amp;&amp; tsk-&gt;mm &amp;&amp; get_dumpable(tsk-&gt;mm) != SUID_DUMP_USER)</span>
<span class="quote">&gt; +		if (tsk &amp;&amp; tsk-&gt;mm &amp;&amp; (tsk-&gt;mm != last)</span>
<span class="quote">&gt; +			&amp;&amp; get_dumpable(tsk-&gt;mm) != SUID_DUMP_USER)</span>

Broken coding style, operators go at the end of the previous line.
<span class="quote">
&gt;  			indirect_branch_prediction_barrier();</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		if (IS_ENABLED(CONFIG_VMAP_STACK)) {</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - Jan. 25, 2018, 9:22 a.m.</div>
<pre class="content">
On Thu, Jan 25, 2018 at 09:58:20AM +0100, Peter Zijlstra wrote:
<span class="quote">&gt; On Wed, Jan 24, 2018 at 04:36:41PM -0800, Tim Chen wrote:</span>
<span class="quote">&gt; &gt; These two patches provide optimization to skip IBPB for this</span>
<span class="quote">&gt; &gt; commonly encountered scenario:</span>
<span class="quote">&gt; &gt; We could switch to a kernel idle thread and then back to the original</span>
<span class="quote">&gt; &gt; process such as:</span>
<span class="quote">&gt; &gt; process A -&gt; idle -&gt; process A</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; In such scenario, we do not have to do IBPB here even though the process</span>
<span class="quote">&gt; &gt; is non-dumpable, as we are switching back to the same process after</span>
<span class="quote">&gt; &gt; an hiatus.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; The cost is to have an extra pointer to track the last mm we were using before</span>
<span class="quote">&gt; &gt; switching to the init_mm used by idle.  But avoiding the extra IBPB</span>
<span class="quote">&gt; &gt; is probably worth the extra memory for such a common scenario.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So we already track active_mm for kernel threads. I can&#39;t immediately</span>
<span class="quote">&gt; see where this fails for idle and your changelog doesn&#39;t say.</span>

idle_task_exit() explicitly switches back to init_mm when we take the
CPU offline, this very much suggests the active_mm thing works for idle
too.

This means that &#39;A -&gt; idle -&gt; A&#39; should never pass through switch_mm to
begin with.

Please clarify how you think it does.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=467">Arjan van de Ven</a> - Jan. 25, 2018, 1:21 p.m.</div>
<pre class="content">
<span class="quote">&gt; </span>
<span class="quote">&gt; This means that &#39;A -&gt; idle -&gt; A&#39; should never pass through switch_mm to</span>
<span class="quote">&gt; begin with.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Please clarify how you think it does.</span>
<span class="quote">&gt; </span>

the idle code does leave_mm() to avoid having to IPI CPUs in deep sleep states
for a tlb flush.

(trust me, that you really want, sequentially IPI&#39;s a pile of cores in a deep sleep
state to just flush a tlb that&#39;s empty, the performance of that is horrific)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - Jan. 25, 2018, 1:50 p.m.</div>
<pre class="content">
On Thu, Jan 25, 2018 at 05:21:30AM -0800, Arjan van de Ven wrote:
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; This means that &#39;A -&gt; idle -&gt; A&#39; should never pass through switch_mm to</span>
<span class="quote">&gt; &gt; begin with.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Please clarify how you think it does.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; the idle code does leave_mm() to avoid having to IPI CPUs in deep sleep states</span>
<span class="quote">&gt; for a tlb flush.</span>

The intel_idle code does, not the idle code. This is squirreled away in
some driver :/
<span class="quote">
&gt; (trust me, that you really want, sequentially IPI&#39;s a pile of cores in a deep sleep</span>
<span class="quote">&gt; state to just flush a tlb that&#39;s empty, the performance of that is horrific)</span>

Hurmph. I&#39;d rather fix that some other way than leave_mm(), this is
piling special on special.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=467">Arjan van de Ven</a> - Jan. 25, 2018, 2:07 p.m.</div>
<pre class="content">
On 1/25/2018 5:50 AM, Peter Zijlstra wrote:
<span class="quote">&gt; On Thu, Jan 25, 2018 at 05:21:30AM -0800, Arjan van de Ven wrote:</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; This means that &#39;A -&gt; idle -&gt; A&#39; should never pass through switch_mm to</span>
<span class="quote">&gt;&gt;&gt; begin with.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Please clarify how you think it does.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; the idle code does leave_mm() to avoid having to IPI CPUs in deep sleep states</span>
<span class="quote">&gt;&gt; for a tlb flush.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The intel_idle code does, not the idle code. This is squirreled away in</span>
<span class="quote">&gt; some driver :/</span>

afaik (but haven&#39;t looked in a while) acpi drivers did too
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; (trust me, that you really want, sequentially IPI&#39;s a pile of cores in a deep sleep</span>
<span class="quote">&gt;&gt; state to just flush a tlb that&#39;s empty, the performance of that is horrific)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Hurmph. I&#39;d rather fix that some other way than leave_mm(), this is</span>
<span class="quote">&gt; piling special on special.</span>
<span class="quote">&gt; </span>
the problem was tricky. but of course if something better is possible lets figure this out

problem is that an IPI to an idle cpu is both power inefficient and will take time,
exit of a deep C state can be, say 50 to 100 usec range of time (it varies by many things, but
for abstractly thinking about the problem one should generally round up to nice round numbers)

if you have say 64 cores that had the mm at some point, but 63 are in idle, the 64th
really does not want to IPI each of those 63 serially (technically this is does not need
to be serial but IPI code is tricky, some things end up serializing this a bit)
to get the 100 usec hit 63 times. Actually, even if it&#39;s not serialized, even ONE hit of 100 usec
is unpleasant.

so a CPU that goes idle wants to &quot;unsubscribe&quot; itself from those IPIs as general objective.

but not getting flush IPIs is only safe if the TLBs in the CPU have nothing that such IPI would
want to flush, so the TLB needs to be empty of those things.

the only way to do THAT is to switch to an mm that is safe; a leave_mm() does this, but I&#39;m sure other
options exist.

note: While a CPU that is in a deeper C state will itself flush the TLB, you don&#39;t know if you will actually
enter that deep at the time of making OS decisions (if an interrupt comes in the cycle before mwait, mwait
becomes a nop for example). In addition, once you wake up, you don&#39;t want the CPU to go start filling
the TLBs with invalid data so you can&#39;t really just set a bit and flush after leaving idle
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/include/asm/tlbflush.h b/arch/x86/include/asm/tlbflush.h</span>
<span class="p_header">index 3effd3c..c5e325f 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/tlbflush.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/tlbflush.h</span>
<span class="p_chunk">@@ -174,6 +174,8 @@</span> <span class="p_context"> struct tlb_state {</span>
 	struct mm_struct *loaded_mm;
 	u16 loaded_mm_asid;
 	u16 next_asid;
<span class="p_add">+	/* last user mm */</span>
<span class="p_add">+	struct mm_struct *last_usr_mm;</span>
 
 	/*
 	 * We can be in one of several states:
<span class="p_header">diff --git a/arch/x86/mm/tlb.c b/arch/x86/mm/tlb.c</span>
<span class="p_header">index d5a35fe..86ed07f 100644</span>
<span class="p_header">--- a/arch/x86/mm/tlb.c</span>
<span class="p_header">+++ b/arch/x86/mm/tlb.c</span>
<span class="p_chunk">@@ -220,6 +220,7 @@</span> <span class="p_context"> void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,</span>
 	} else {
 		u16 new_asid;
 		bool need_flush;
<span class="p_add">+		struct mm_struct *last = this_cpu_read(cpu_tlbstate.last_usr_mm);</span>
 
 		/*
 		 * Avoid user/user BTB poisoning by flushing the branch predictor
<span class="p_chunk">@@ -229,15 +230,17 @@</span> <span class="p_context"> void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,</span>
                  * As an optimization flush indirect branches only when
                  * switching into processes that disable dumping.
                  *
<span class="p_del">-                 * This will not flush branches when switching into kernel</span>
<span class="p_del">-                 * threads, but it would flush them when switching to the</span>
<span class="p_del">-                 * idle thread and back.</span>
<span class="p_add">+		 * This will not flush branches when switching into kernel</span>
<span class="p_add">+		 * threads. It will also not flush if we switch to idle</span>
<span class="p_add">+		 * thread and back to the same process. It will flush if we</span>
<span class="p_add">+		 * switch to a different non-dumpable process.</span>
                  *
                  * It might be useful to have a one-off cache here
                  * to also not flush the idle case, but we would need some
                  * kind of stable sequence number to remember the previous mm.
 		 */
<span class="p_del">-		if (tsk &amp;&amp; tsk-&gt;mm &amp;&amp; get_dumpable(tsk-&gt;mm) != SUID_DUMP_USER)</span>
<span class="p_add">+		if (tsk &amp;&amp; tsk-&gt;mm &amp;&amp; (tsk-&gt;mm != last)</span>
<span class="p_add">+			&amp;&amp; get_dumpable(tsk-&gt;mm) != SUID_DUMP_USER)</span>
 			indirect_branch_prediction_barrier();
 
 		if (IS_ENABLED(CONFIG_VMAP_STACK)) {
<span class="p_chunk">@@ -288,6 +291,8 @@</span> <span class="p_context"> void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,</span>
 			trace_tlb_flush_rcuidle(TLB_FLUSH_ON_TASK_SWITCH, 0);
 		}
 
<span class="p_add">+		if (next != &amp;init_mm &amp;&amp; next != last)</span>
<span class="p_add">+			this_cpu_write(cpu_tlbstate.last_usr_mm, next);</span>
 		this_cpu_write(cpu_tlbstate.loaded_mm, next);
 		this_cpu_write(cpu_tlbstate.loaded_mm_asid, new_asid);
 	}

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



