
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[RFC,v3,1/2] Add support for eXclusive Page Frame Ownership (XPFO) - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [RFC,v3,1/2] Add support for eXclusive Page Frame Ownership (XPFO)</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=153531">Juerg Haefliger</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Nov. 4, 2016, 2:45 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20161104144534.14790-2-juerg.haefliger@hpe.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9412695/mbox/"
   >mbox</a>
|
   <a href="/patch/9412695/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9412695/">/patch/9412695/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	6EE6E60722 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri,  4 Nov 2016 14:46:03 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 613A72B1A5
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri,  4 Nov 2016 14:46:03 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 5617C2B1A8; Fri,  4 Nov 2016 14:46:03 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 0A5612B1A7
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri,  4 Nov 2016 14:46:02 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S935125AbcKDOp5 (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Fri, 4 Nov 2016 10:45:57 -0400
Received: from g2t1383g.austin.hpe.com ([15.233.16.89]:53068 &quot;EHLO
	g2t1383g.austin.hpe.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S934888AbcKDOpy (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Fri, 4 Nov 2016 10:45:54 -0400
Received: from g9t5008.houston.hpe.com (g9t5008.houston.hpe.com
	[15.241.48.72])
	(using TLSv1.2 with cipher AECDH-AES256-SHA (256/256 bits))
	(No client certificate requested)
	by g2t1383g.austin.hpe.com (Postfix) with ESMTPS id 42500FAF;
	Fri,  4 Nov 2016 14:45:53 +0000 (UTC)
Received: from smtp1.hpe.com (unknown [16.29.185.92])
	(using TLSv1.2 with cipher ECDHE-RSA-AES128-GCM-SHA256 (128/128
	bits)) (No client certificate requested)
	by g9t5008.houston.hpe.com (Postfix) with ESMTPS id C323968;
	Fri,  4 Nov 2016 14:45:49 +0000 (UTC)
From: Juerg Haefliger &lt;juerg.haefliger@hpe.com&gt;
To: linux-kernel@vger.kernel.org, linux-mm@kvack.org,
	kernel-hardening@lists.openwall.com, linux-x86_64@vger.kernel.org
Cc: vpk@cs.columbia.edu, juerg.haefliger@hpe.com
Subject: [RFC PATCH v3 1/2] Add support for eXclusive Page Frame Ownership
	(XPFO)
Date: Fri,  4 Nov 2016 15:45:33 +0100
Message-Id: &lt;20161104144534.14790-2-juerg.haefliger@hpe.com&gt;
X-Mailer: git-send-email 2.10.1
In-Reply-To: &lt;20161104144534.14790-1-juerg.haefliger@hpe.com&gt;
References: &lt;20160914071901.8127-1-juerg.haefliger@hpe.com&gt;
	&lt;20161104144534.14790-1-juerg.haefliger@hpe.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=153531">Juerg Haefliger</a> - Nov. 4, 2016, 2:45 p.m.</div>
<pre class="content">
This patch adds support for XPFO which protects against &#39;ret2dir&#39; kernel
attacks. The basic idea is to enforce exclusive ownership of page frames
by either the kernel or userspace, unless explicitly requested by the
kernel. Whenever a page destined for userspace is allocated, it is
unmapped from physmap (the kernel&#39;s page table). When such a page is
reclaimed from userspace, it is mapped back to physmap.

Additional fields in the page_ext struct are used for XPFO housekeeping.
Specifically two flags to distinguish user vs. kernel pages and to tag
unmapped pages and a reference counter to balance kmap/kunmap operations
and a lock to serialize access to the XPFO fields.

Known issues/limitations:
  - Only supports x86-64 (for now)
  - Only supports 4k pages (for now)
  - There are most likely some legitimate uses cases where the kernel needs
    to access userspace which need to be made XPFO-aware
  - Performance penalty

Reference paper by the original patch authors:
  http://www.cs.columbia.edu/~vpk/papers/ret2dir.sec14.pdf

Suggested-by: Vasileios P. Kemerlis &lt;vpk@cs.columbia.edu&gt;
<span class="signed-off-by">Signed-off-by: Juerg Haefliger &lt;juerg.haefliger@hpe.com&gt;</span>
---
 arch/x86/Kconfig         |   3 +-
 arch/x86/mm/init.c       |   2 +-
 drivers/ata/libata-sff.c |   4 +-
 include/linux/highmem.h  |  15 +++-
 include/linux/page_ext.h |   7 ++
 include/linux/xpfo.h     |  39 +++++++++
 lib/swiotlb.c            |   3 +-
 mm/Makefile              |   1 +
 mm/page_alloc.c          |   2 +
 mm/page_ext.c            |   4 +
 mm/xpfo.c                | 206 +++++++++++++++++++++++++++++++++++++++++++++++
 security/Kconfig         |  19 +++++
 12 files changed, 298 insertions(+), 7 deletions(-)
 create mode 100644 include/linux/xpfo.h
 create mode 100644 mm/xpfo.c
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=101">Christoph Hellwig</a> - Nov. 4, 2016, 2:50 p.m.</div>
<pre class="content">
The libata parts here really need to be split out and the proper list
and maintainer need to be Cc&#39;ed.
<span class="quote">
&gt; diff --git a/drivers/ata/libata-sff.c b/drivers/ata/libata-sff.c</span>
<span class="quote">&gt; index 051b6158d1b7..58af734be25d 100644</span>
<span class="quote">&gt; --- a/drivers/ata/libata-sff.c</span>
<span class="quote">&gt; +++ b/drivers/ata/libata-sff.c</span>
<span class="quote">&gt; @@ -715,7 +715,7 @@ static void ata_pio_sector(struct ata_queued_cmd *qc)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	DPRINTK(&quot;data %s\n&quot;, qc-&gt;tf.flags &amp; ATA_TFLAG_WRITE ? &quot;write&quot; : &quot;read&quot;);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (PageHighMem(page)) {</span>
<span class="quote">&gt; +	if (PageHighMem(page) || xpfo_page_is_unmapped(page)) {</span>
<span class="quote">&gt;  		unsigned long flags;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		/* FIXME: use a bounce buffer */</span>
<span class="quote">&gt; @@ -860,7 +860,7 @@ static int __atapi_pio_bytes(struct ata_queued_cmd *qc, unsigned int bytes)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	DPRINTK(&quot;data %s\n&quot;, qc-&gt;tf.flags &amp; ATA_TFLAG_WRITE ? &quot;write&quot; : &quot;read&quot;);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (PageHighMem(page)) {</span>
<span class="quote">&gt; +	if (PageHighMem(page) || xpfo_page_is_unmapped(page)) {</span>
<span class="quote">&gt;  		unsigned long flags;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		/* FIXME: use bounce buffer */</span>
<span class="quote">&gt; diff --git a/include/linux/highmem.h b/include/linux/highmem.h</span>

This is just piling one nasty hack on top of another.  libata should
just use the highmem case unconditionally, as it is the correct thing
to do for all cases.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=171131">ZhaoJunmin Zhao(Junmin)</a> - Nov. 10, 2016, 5:53 a.m.</div>
<pre class="content">
<span class="quote">&gt; This patch adds support for XPFO which protects against &#39;ret2dir&#39; kernel</span>
<span class="quote">&gt; attacks. The basic idea is to enforce exclusive ownership of page frames</span>
<span class="quote">&gt; by either the kernel or userspace, unless explicitly requested by the</span>
<span class="quote">&gt; kernel. Whenever a page destined for userspace is allocated, it is</span>
<span class="quote">&gt; unmapped from physmap (the kernel&#39;s page table). When such a page is</span>
<span class="quote">&gt; reclaimed from userspace, it is mapped back to physmap.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Additional fields in the page_ext struct are used for XPFO housekeeping.</span>
<span class="quote">&gt; Specifically two flags to distinguish user vs. kernel pages and to tag</span>
<span class="quote">&gt; unmapped pages and a reference counter to balance kmap/kunmap operations</span>
<span class="quote">&gt; and a lock to serialize access to the XPFO fields.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Known issues/limitations:</span>
<span class="quote">&gt;    - Only supports x86-64 (for now)</span>
<span class="quote">&gt;    - Only supports 4k pages (for now)</span>
<span class="quote">&gt;    - There are most likely some legitimate uses cases where the kernel needs</span>
<span class="quote">&gt;      to access userspace which need to be made XPFO-aware</span>
<span class="quote">&gt;    - Performance penalty</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Reference paper by the original patch authors:</span>
<span class="quote">&gt;    http://www.cs.columbia.edu/~vpk/papers/ret2dir.sec14.pdf</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Suggested-by: Vasileios P. Kemerlis &lt;vpk@cs.columbia.edu&gt;</span>
<span class="quote">&gt; Signed-off-by: Juerg Haefliger &lt;juerg.haefliger@hpe.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;   arch/x86/Kconfig         |   3 +-</span>
<span class="quote">&gt;   arch/x86/mm/init.c       |   2 +-</span>
<span class="quote">&gt;   drivers/ata/libata-sff.c |   4 +-</span>
<span class="quote">&gt;   include/linux/highmem.h  |  15 +++-</span>
<span class="quote">&gt;   include/linux/page_ext.h |   7 ++</span>
<span class="quote">&gt;   include/linux/xpfo.h     |  39 +++++++++</span>
<span class="quote">&gt;   lib/swiotlb.c            |   3 +-</span>
<span class="quote">&gt;   mm/Makefile              |   1 +</span>
<span class="quote">&gt;   mm/page_alloc.c          |   2 +</span>
<span class="quote">&gt;   mm/page_ext.c            |   4 +</span>
<span class="quote">&gt;   mm/xpfo.c                | 206 +++++++++++++++++++++++++++++++++++++++++++++++</span>
<span class="quote">&gt;   security/Kconfig         |  19 +++++</span>
<span class="quote">&gt;   12 files changed, 298 insertions(+), 7 deletions(-)</span>
<span class="quote">&gt;   create mode 100644 include/linux/xpfo.h</span>
<span class="quote">&gt;   create mode 100644 mm/xpfo.c</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig</span>
<span class="quote">&gt; index bada636d1065..38b334f8fde5 100644</span>
<span class="quote">&gt; --- a/arch/x86/Kconfig</span>
<span class="quote">&gt; +++ b/arch/x86/Kconfig</span>
<span class="quote">&gt; @@ -165,6 +165,7 @@ config X86</span>
<span class="quote">&gt;   	select HAVE_STACK_VALIDATION		if X86_64</span>
<span class="quote">&gt;   	select ARCH_USES_HIGH_VMA_FLAGS		if X86_INTEL_MEMORY_PROTECTION_KEYS</span>
<span class="quote">&gt;   	select ARCH_HAS_PKEYS			if X86_INTEL_MEMORY_PROTECTION_KEYS</span>
<span class="quote">&gt; +	select ARCH_SUPPORTS_XPFO		if X86_64</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;   config INSTRUCTION_DECODER</span>
<span class="quote">&gt;   	def_bool y</span>
<span class="quote">&gt; @@ -1361,7 +1362,7 @@ config ARCH_DMA_ADDR_T_64BIT</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;   config X86_DIRECT_GBPAGES</span>
<span class="quote">&gt;   	def_bool y</span>
<span class="quote">&gt; -	depends on X86_64 &amp;&amp; !DEBUG_PAGEALLOC &amp;&amp; !KMEMCHECK</span>
<span class="quote">&gt; +	depends on X86_64 &amp;&amp; !DEBUG_PAGEALLOC &amp;&amp; !KMEMCHECK &amp;&amp; !XPFO</span>
<span class="quote">&gt;   	---help---</span>
<span class="quote">&gt;   	  Certain kernel features effectively disable kernel</span>
<span class="quote">&gt;   	  linear 1 GB mappings (even if the CPU otherwise</span>
<span class="quote">&gt; diff --git a/arch/x86/mm/init.c b/arch/x86/mm/init.c</span>
<span class="quote">&gt; index 22af912d66d2..a6fafbae02bb 100644</span>
<span class="quote">&gt; --- a/arch/x86/mm/init.c</span>
<span class="quote">&gt; +++ b/arch/x86/mm/init.c</span>
<span class="quote">&gt; @@ -161,7 +161,7 @@ static int page_size_mask;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;   static void __init probe_page_size_mask(void)</span>
<span class="quote">&gt;   {</span>
<span class="quote">&gt; -#if !defined(CONFIG_KMEMCHECK)</span>
<span class="quote">&gt; +#if !defined(CONFIG_KMEMCHECK) &amp;&amp; !defined(CONFIG_XPFO)</span>
<span class="quote">&gt;   	/*</span>
<span class="quote">&gt;   	 * For CONFIG_KMEMCHECK or pagealloc debugging, identity mapping will</span>
<span class="quote">&gt;   	 * use small pages.</span>
<span class="quote">&gt; diff --git a/drivers/ata/libata-sff.c b/drivers/ata/libata-sff.c</span>
<span class="quote">&gt; index 051b6158d1b7..58af734be25d 100644</span>
<span class="quote">&gt; --- a/drivers/ata/libata-sff.c</span>
<span class="quote">&gt; +++ b/drivers/ata/libata-sff.c</span>
<span class="quote">&gt; @@ -715,7 +715,7 @@ static void ata_pio_sector(struct ata_queued_cmd *qc)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;   	DPRINTK(&quot;data %s\n&quot;, qc-&gt;tf.flags &amp; ATA_TFLAG_WRITE ? &quot;write&quot; : &quot;read&quot;);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -	if (PageHighMem(page)) {</span>
<span class="quote">&gt; +	if (PageHighMem(page) || xpfo_page_is_unmapped(page)) {</span>
<span class="quote">&gt;   		unsigned long flags;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;   		/* FIXME: use a bounce buffer */</span>
<span class="quote">&gt; @@ -860,7 +860,7 @@ static int __atapi_pio_bytes(struct ata_queued_cmd *qc, unsigned int bytes)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;   	DPRINTK(&quot;data %s\n&quot;, qc-&gt;tf.flags &amp; ATA_TFLAG_WRITE ? &quot;write&quot; : &quot;read&quot;);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -	if (PageHighMem(page)) {</span>
<span class="quote">&gt; +	if (PageHighMem(page) || xpfo_page_is_unmapped(page)) {</span>
<span class="quote">&gt;   		unsigned long flags;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;   		/* FIXME: use bounce buffer */</span>
<span class="quote">&gt; diff --git a/include/linux/highmem.h b/include/linux/highmem.h</span>
<span class="quote">&gt; index bb3f3297062a..7a17c166532f 100644</span>
<span class="quote">&gt; --- a/include/linux/highmem.h</span>
<span class="quote">&gt; +++ b/include/linux/highmem.h</span>
<span class="quote">&gt; @@ -7,6 +7,7 @@</span>
<span class="quote">&gt;   #include &lt;linux/mm.h&gt;</span>
<span class="quote">&gt;   #include &lt;linux/uaccess.h&gt;</span>
<span class="quote">&gt;   #include &lt;linux/hardirq.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/xpfo.h&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;   #include &lt;asm/cacheflush.h&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; @@ -55,24 +56,34 @@ static inline struct page *kmap_to_page(void *addr)</span>
<span class="quote">&gt;   #ifndef ARCH_HAS_KMAP</span>
<span class="quote">&gt;   static inline void *kmap(struct page *page)</span>
<span class="quote">&gt;   {</span>
<span class="quote">&gt; +	void *kaddr;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;   	might_sleep();</span>
<span class="quote">&gt; -	return page_address(page);</span>
<span class="quote">&gt; +	kaddr = page_address(page);</span>
<span class="quote">&gt; +	xpfo_kmap(kaddr, page);</span>
<span class="quote">&gt; +	return kaddr;</span>
<span class="quote">&gt;   }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;   static inline void kunmap(struct page *page)</span>
<span class="quote">&gt;   {</span>
<span class="quote">&gt; +	xpfo_kunmap(page_address(page), page);</span>
<span class="quote">&gt;   }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;   static inline void *kmap_atomic(struct page *page)</span>
<span class="quote">&gt;   {</span>
<span class="quote">&gt; +	void *kaddr;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;   	preempt_disable();</span>
<span class="quote">&gt;   	pagefault_disable();</span>
<span class="quote">&gt; -	return page_address(page);</span>
<span class="quote">&gt; +	kaddr = page_address(page);</span>
<span class="quote">&gt; +	xpfo_kmap(kaddr, page);</span>
<span class="quote">&gt; +	return kaddr;</span>
<span class="quote">&gt;   }</span>
<span class="quote">&gt;   #define kmap_atomic_prot(page, prot)	kmap_atomic(page)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;   static inline void __kunmap_atomic(void *addr)</span>
<span class="quote">&gt;   {</span>
<span class="quote">&gt; +	xpfo_kunmap(addr, virt_to_page(addr));</span>
<span class="quote">&gt;   	pagefault_enable();</span>
<span class="quote">&gt;   	preempt_enable();</span>
<span class="quote">&gt;   }</span>
<span class="quote">&gt; diff --git a/include/linux/page_ext.h b/include/linux/page_ext.h</span>
<span class="quote">&gt; index 9298c393ddaa..0e451a42e5a3 100644</span>
<span class="quote">&gt; --- a/include/linux/page_ext.h</span>
<span class="quote">&gt; +++ b/include/linux/page_ext.h</span>
<span class="quote">&gt; @@ -29,6 +29,8 @@ enum page_ext_flags {</span>
<span class="quote">&gt;   	PAGE_EXT_DEBUG_POISON,		/* Page is poisoned */</span>
<span class="quote">&gt;   	PAGE_EXT_DEBUG_GUARD,</span>
<span class="quote">&gt;   	PAGE_EXT_OWNER,</span>
<span class="quote">&gt; +	PAGE_EXT_XPFO_KERNEL,		/* Page is a kernel page */</span>
<span class="quote">&gt; +	PAGE_EXT_XPFO_UNMAPPED,		/* Page is unmapped */</span>
<span class="quote">&gt;   #if defined(CONFIG_IDLE_PAGE_TRACKING) &amp;&amp; !defined(CONFIG_64BIT)</span>
<span class="quote">&gt;   	PAGE_EXT_YOUNG,</span>
<span class="quote">&gt;   	PAGE_EXT_IDLE,</span>
<span class="quote">&gt; @@ -44,6 +46,11 @@ enum page_ext_flags {</span>
<span class="quote">&gt;    */</span>
<span class="quote">&gt;   struct page_ext {</span>
<span class="quote">&gt;   	unsigned long flags;</span>
<span class="quote">&gt; +#ifdef CONFIG_XPFO</span>
<span class="quote">&gt; +	int inited;		/* Map counter and lock initialized */</span>
<span class="quote">&gt; +	atomic_t mapcount;	/* Counter for balancing map/unmap requests */</span>
<span class="quote">&gt; +	spinlock_t maplock;	/* Lock to serialize map/unmap requests */</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt;   };</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;   extern void pgdat_page_ext_init(struct pglist_data *pgdat);</span>
<span class="quote">&gt; diff --git a/include/linux/xpfo.h b/include/linux/xpfo.h</span>
<span class="quote">&gt; new file mode 100644</span>
<span class="quote">&gt; index 000000000000..77187578ca33</span>
<span class="quote">&gt; --- /dev/null</span>
<span class="quote">&gt; +++ b/include/linux/xpfo.h</span>
<span class="quote">&gt; @@ -0,0 +1,39 @@</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * Copyright (C) 2016 Hewlett Packard Enterprise Development, L.P.</span>
<span class="quote">&gt; + * Copyright (C) 2016 Brown University. All rights reserved.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Authors:</span>
<span class="quote">&gt; + *   Juerg Haefliger &lt;juerg.haefliger@hpe.com&gt;</span>
<span class="quote">&gt; + *   Vasileios P. Kemerlis &lt;vpk@cs.brown.edu&gt;</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * This program is free software; you can redistribute it and/or modify it</span>
<span class="quote">&gt; + * under the terms of the GNU General Public License version 2 as published by</span>
<span class="quote">&gt; + * the Free Software Foundation.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#ifndef _LINUX_XPFO_H</span>
<span class="quote">&gt; +#define _LINUX_XPFO_H</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#ifdef CONFIG_XPFO</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +extern struct page_ext_operations page_xpfo_ops;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +extern void xpfo_kmap(void *kaddr, struct page *page);</span>
<span class="quote">&gt; +extern void xpfo_kunmap(void *kaddr, struct page *page);</span>
<span class="quote">&gt; +extern void xpfo_alloc_page(struct page *page, int order, gfp_t gfp);</span>
<span class="quote">&gt; +extern void xpfo_free_page(struct page *page, int order);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +extern bool xpfo_page_is_unmapped(struct page *page);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#else /* !CONFIG_XPFO */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline void xpfo_kmap(void *kaddr, struct page *page) { }</span>
<span class="quote">&gt; +static inline void xpfo_kunmap(void *kaddr, struct page *page) { }</span>
<span class="quote">&gt; +static inline void xpfo_alloc_page(struct page *page, int order, gfp_t gfp) { }</span>
<span class="quote">&gt; +static inline void xpfo_free_page(struct page *page, int order) { }</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline bool xpfo_page_is_unmapped(struct page *page) { return false; }</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#endif /* CONFIG_XPFO */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#endif /* _LINUX_XPFO_H */</span>
<span class="quote">&gt; diff --git a/lib/swiotlb.c b/lib/swiotlb.c</span>
<span class="quote">&gt; index 22e13a0e19d7..455eff44604e 100644</span>
<span class="quote">&gt; --- a/lib/swiotlb.c</span>
<span class="quote">&gt; +++ b/lib/swiotlb.c</span>
<span class="quote">&gt; @@ -390,8 +390,9 @@ static void swiotlb_bounce(phys_addr_t orig_addr, phys_addr_t tlb_addr,</span>
<span class="quote">&gt;   {</span>
<span class="quote">&gt;   	unsigned long pfn = PFN_DOWN(orig_addr);</span>
<span class="quote">&gt;   	unsigned char *vaddr = phys_to_virt(tlb_addr);</span>
<span class="quote">&gt; +	struct page *page = pfn_to_page(pfn);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -	if (PageHighMem(pfn_to_page(pfn))) {</span>
<span class="quote">&gt; +	if (PageHighMem(page) || xpfo_page_is_unmapped(page)) {</span>
<span class="quote">&gt;   		/* The buffer does not have a mapping.  Map it in and copy */</span>
<span class="quote">&gt;   		unsigned int offset = orig_addr &amp; ~PAGE_MASK;</span>
<span class="quote">&gt;   		char *buffer;</span>
<span class="quote">&gt; diff --git a/mm/Makefile b/mm/Makefile</span>
<span class="quote">&gt; index 295bd7a9f76b..175680f516aa 100644</span>
<span class="quote">&gt; --- a/mm/Makefile</span>
<span class="quote">&gt; +++ b/mm/Makefile</span>
<span class="quote">&gt; @@ -100,3 +100,4 @@ obj-$(CONFIG_IDLE_PAGE_TRACKING) += page_idle.o</span>
<span class="quote">&gt;   obj-$(CONFIG_FRAME_VECTOR) += frame_vector.o</span>
<span class="quote">&gt;   obj-$(CONFIG_DEBUG_PAGE_REF) += debug_page_ref.o</span>
<span class="quote">&gt;   obj-$(CONFIG_HARDENED_USERCOPY) += usercopy.o</span>
<span class="quote">&gt; +obj-$(CONFIG_XPFO) += xpfo.o</span>
<span class="quote">&gt; diff --git a/mm/page_alloc.c b/mm/page_alloc.c</span>
<span class="quote">&gt; index 8fd42aa7c4bd..100e80e008e2 100644</span>
<span class="quote">&gt; --- a/mm/page_alloc.c</span>
<span class="quote">&gt; +++ b/mm/page_alloc.c</span>
<span class="quote">&gt; @@ -1045,6 +1045,7 @@ static __always_inline bool free_pages_prepare(struct page *page,</span>
<span class="quote">&gt;   	kernel_poison_pages(page, 1 &lt;&lt; order, 0);</span>
<span class="quote">&gt;   	kernel_map_pages(page, 1 &lt;&lt; order, 0);</span>
<span class="quote">&gt;   	kasan_free_pages(page, order);</span>
<span class="quote">&gt; +	xpfo_free_page(page, order);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;   	return true;</span>
<span class="quote">&gt;   }</span>
<span class="quote">&gt; @@ -1745,6 +1746,7 @@ inline void post_alloc_hook(struct page *page, unsigned int order,</span>
<span class="quote">&gt;   	kernel_map_pages(page, 1 &lt;&lt; order, 1);</span>
<span class="quote">&gt;   	kernel_poison_pages(page, 1 &lt;&lt; order, 1);</span>
<span class="quote">&gt;   	kasan_alloc_pages(page, order);</span>
<span class="quote">&gt; +	xpfo_alloc_page(page, order, gfp_flags);</span>
<span class="quote">&gt;   	set_page_owner(page, order, gfp_flags);</span>
<span class="quote">&gt;   }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; diff --git a/mm/page_ext.c b/mm/page_ext.c</span>
<span class="quote">&gt; index 121dcffc4ec1..ba6dbcacc2db 100644</span>
<span class="quote">&gt; --- a/mm/page_ext.c</span>
<span class="quote">&gt; +++ b/mm/page_ext.c</span>
<span class="quote">&gt; @@ -7,6 +7,7 @@</span>
<span class="quote">&gt;   #include &lt;linux/kmemleak.h&gt;</span>
<span class="quote">&gt;   #include &lt;linux/page_owner.h&gt;</span>
<span class="quote">&gt;   #include &lt;linux/page_idle.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/xpfo.h&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;   /*</span>
<span class="quote">&gt;    * struct page extension</span>
<span class="quote">&gt; @@ -68,6 +69,9 @@ static struct page_ext_operations *page_ext_ops[] = {</span>
<span class="quote">&gt;   #if defined(CONFIG_IDLE_PAGE_TRACKING) &amp;&amp; !defined(CONFIG_64BIT)</span>
<span class="quote">&gt;   	&amp;page_idle_ops,</span>
<span class="quote">&gt;   #endif</span>
<span class="quote">&gt; +#ifdef CONFIG_XPFO</span>
<span class="quote">&gt; +	&amp;page_xpfo_ops,</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt;   };</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;   static unsigned long total_usage;</span>
<span class="quote">&gt; diff --git a/mm/xpfo.c b/mm/xpfo.c</span>
<span class="quote">&gt; new file mode 100644</span>
<span class="quote">&gt; index 000000000000..8e3a6a694b6a</span>
<span class="quote">&gt; --- /dev/null</span>
<span class="quote">&gt; +++ b/mm/xpfo.c</span>
<span class="quote">&gt; @@ -0,0 +1,206 @@</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * Copyright (C) 2016 Hewlett Packard Enterprise Development, L.P.</span>
<span class="quote">&gt; + * Copyright (C) 2016 Brown University. All rights reserved.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Authors:</span>
<span class="quote">&gt; + *   Juerg Haefliger &lt;juerg.haefliger@hpe.com&gt;</span>
<span class="quote">&gt; + *   Vasileios P. Kemerlis &lt;vpk@cs.brown.edu&gt;</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * This program is free software; you can redistribute it and/or modify it</span>
<span class="quote">&gt; + * under the terms of the GNU General Public License version 2 as published by</span>
<span class="quote">&gt; + * the Free Software Foundation.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#include &lt;linux/mm.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/module.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/page_ext.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/xpfo.h&gt;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#include &lt;asm/tlbflush.h&gt;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +DEFINE_STATIC_KEY_FALSE(xpfo_inited);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static bool need_xpfo(void)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return true;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static void init_xpfo(void)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	printk(KERN_INFO &quot;XPFO enabled\n&quot;);</span>
<span class="quote">&gt; +	static_branch_enable(&amp;xpfo_inited);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +struct page_ext_operations page_xpfo_ops = {</span>
<span class="quote">&gt; +	.need = need_xpfo,</span>
<span class="quote">&gt; +	.init = init_xpfo,</span>
<span class="quote">&gt; +};</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * Update a single kernel page table entry</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +static inline void set_kpte(struct page *page, unsigned long kaddr,</span>
<span class="quote">&gt; +			    pgprot_t prot) {</span>
<span class="quote">&gt; +	unsigned int level;</span>
<span class="quote">&gt; +	pte_t *kpte = lookup_address(kaddr, &amp;level);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/* We only support 4k pages for now */</span>
<span class="quote">&gt; +	BUG_ON(!kpte || level != PG_LEVEL_4K);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	set_pte_atomic(kpte, pfn_pte(page_to_pfn(page), canon_pgprot(prot)));</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +void xpfo_alloc_page(struct page *page, int order, gfp_t gfp)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	int i, flush_tlb = 0;</span>
<span class="quote">&gt; +	struct page_ext *page_ext;</span>
<span class="quote">&gt; +	unsigned long kaddr;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (!static_branch_unlikely(&amp;xpfo_inited))</span>
<span class="quote">&gt; +		return;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	for (i = 0; i &lt; (1 &lt;&lt; order); i++)  {</span>
<span class="quote">&gt; +		page_ext = lookup_page_ext(page + i);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		BUG_ON(test_bit(PAGE_EXT_XPFO_UNMAPPED, &amp;page_ext-&gt;flags));</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		/* Initialize the map lock and map counter */</span>
<span class="quote">&gt; +		if (!page_ext-&gt;inited) {</span>
<span class="quote">&gt; +			spin_lock_init(&amp;page_ext-&gt;maplock);</span>
<span class="quote">&gt; +			atomic_set(&amp;page_ext-&gt;mapcount, 0);</span>
<span class="quote">&gt; +			page_ext-&gt;inited = 1;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +		BUG_ON(atomic_read(&amp;page_ext-&gt;mapcount));</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		if ((gfp &amp; GFP_HIGHUSER) == GFP_HIGHUSER) {</span>
<span class="quote">&gt; +			/*</span>
<span class="quote">&gt; +			 * Flush the TLB if the page was previously allocated</span>
<span class="quote">&gt; +			 * to the kernel.</span>
<span class="quote">&gt; +			 */</span>
<span class="quote">&gt; +			if (test_and_clear_bit(PAGE_EXT_XPFO_KERNEL,</span>
<span class="quote">&gt; +					       &amp;page_ext-&gt;flags))</span>
<span class="quote">&gt; +				flush_tlb = 1;</span>
<span class="quote">&gt; +		} else {</span>
<span class="quote">&gt; +			/* Tag the page as a kernel page */</span>
<span class="quote">&gt; +			set_bit(PAGE_EXT_XPFO_KERNEL, &amp;page_ext-&gt;flags);</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (flush_tlb) {</span>
<span class="quote">&gt; +		kaddr = (unsigned long)page_address(page);</span>
<span class="quote">&gt; +		flush_tlb_kernel_range(kaddr, kaddr + (1 &lt;&lt; order) *</span>
<span class="quote">&gt; +				       PAGE_SIZE);</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +void xpfo_free_page(struct page *page, int order)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	int i;</span>
<span class="quote">&gt; +	struct page_ext *page_ext;</span>
<span class="quote">&gt; +	unsigned long kaddr;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (!static_branch_unlikely(&amp;xpfo_inited))</span>
<span class="quote">&gt; +		return;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	for (i = 0; i &lt; (1 &lt;&lt; order); i++) {</span>
<span class="quote">&gt; +		page_ext = lookup_page_ext(page + i);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		if (!page_ext-&gt;inited) {</span>
<span class="quote">&gt; +			/*</span>
<span class="quote">&gt; +			 * The page was allocated before page_ext was</span>
<span class="quote">&gt; +			 * initialized, so it is a kernel page and it needs to</span>
<span class="quote">&gt; +			 * be tagged accordingly.</span>
<span class="quote">&gt; +			 */</span>
<span class="quote">&gt; +			set_bit(PAGE_EXT_XPFO_KERNEL, &amp;page_ext-&gt;flags);</span>
<span class="quote">&gt; +			continue;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		/*</span>
<span class="quote">&gt; +		 * Map the page back into the kernel if it was previously</span>
<span class="quote">&gt; +		 * allocated to user space.</span>
<span class="quote">&gt; +		 */</span>
<span class="quote">&gt; +		if (test_and_clear_bit(PAGE_EXT_XPFO_UNMAPPED,</span>
<span class="quote">&gt; +				       &amp;page_ext-&gt;flags)) {</span>
<span class="quote">&gt; +			kaddr = (unsigned long)page_address(page + i);</span>
<span class="quote">&gt; +			set_kpte(page + i,  kaddr, __pgprot(__PAGE_KERNEL));</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +void xpfo_kmap(void *kaddr, struct page *page)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct page_ext *page_ext;</span>
<span class="quote">&gt; +	unsigned long flags;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (!static_branch_unlikely(&amp;xpfo_inited))</span>
<span class="quote">&gt; +		return;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	page_ext = lookup_page_ext(page);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * The page was allocated before page_ext was initialized (which means</span>
<span class="quote">&gt; +	 * it&#39;s a kernel page) or it&#39;s allocated to the kernel, so nothing to</span>
<span class="quote">&gt; +	 * do.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	if (!page_ext-&gt;inited ||</span>
<span class="quote">&gt; +	    test_bit(PAGE_EXT_XPFO_KERNEL, &amp;page_ext-&gt;flags))</span>
<span class="quote">&gt; +		return;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	spin_lock_irqsave(&amp;page_ext-&gt;maplock, flags);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * The page was previously allocated to user space, so map it back</span>
<span class="quote">&gt; +	 * into the kernel. No TLB flush required.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	if ((atomic_inc_return(&amp;page_ext-&gt;mapcount) == 1) &amp;&amp;</span>
<span class="quote">&gt; +	    test_and_clear_bit(PAGE_EXT_XPFO_UNMAPPED, &amp;page_ext-&gt;flags))</span>
<span class="quote">&gt; +		set_kpte(page, (unsigned long)kaddr, __pgprot(__PAGE_KERNEL));</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	spin_unlock_irqrestore(&amp;page_ext-&gt;maplock, flags);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +EXPORT_SYMBOL(xpfo_kmap);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +void xpfo_kunmap(void *kaddr, struct page *page)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct page_ext *page_ext;</span>
<span class="quote">&gt; +	unsigned long flags;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (!static_branch_unlikely(&amp;xpfo_inited))</span>
<span class="quote">&gt; +		return;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	page_ext = lookup_page_ext(page);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * The page was allocated before page_ext was initialized (which means</span>
<span class="quote">&gt; +	 * it&#39;s a kernel page) or it&#39;s allocated to the kernel, so nothing to</span>
<span class="quote">&gt; +	 * do.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	if (!page_ext-&gt;inited ||</span>
<span class="quote">&gt; +	    test_bit(PAGE_EXT_XPFO_KERNEL, &amp;page_ext-&gt;flags))</span>
<span class="quote">&gt; +		return;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	spin_lock_irqsave(&amp;page_ext-&gt;maplock, flags);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * The page is to be allocated back to user space, so unmap it from the</span>
<span class="quote">&gt; +	 * kernel, flush the TLB and tag it as a user page.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	if (atomic_dec_return(&amp;page_ext-&gt;mapcount) == 0) {</span>
<span class="quote">&gt; +		BUG_ON(test_bit(PAGE_EXT_XPFO_UNMAPPED, &amp;page_ext-&gt;flags));</span>
<span class="quote">&gt; +		set_bit(PAGE_EXT_XPFO_UNMAPPED, &amp;page_ext-&gt;flags);</span>
<span class="quote">&gt; +		set_kpte(page, (unsigned long)kaddr, __pgprot(0));</span>
<span class="quote">&gt; +		__flush_tlb_one((unsigned long)kaddr);</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	spin_unlock_irqrestore(&amp;page_ext-&gt;maplock, flags);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +EXPORT_SYMBOL(xpfo_kunmap);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +inline bool xpfo_page_is_unmapped(struct page *page)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	if (!static_branch_unlikely(&amp;xpfo_inited))</span>
<span class="quote">&gt; +		return false;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return test_bit(PAGE_EXT_XPFO_UNMAPPED, &amp;lookup_page_ext(page)-&gt;flags);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +EXPORT_SYMBOL(xpfo_page_is_unmapped);</span>
<span class="quote">&gt; diff --git a/security/Kconfig b/security/Kconfig</span>
<span class="quote">&gt; index 118f4549404e..4502e15c8419 100644</span>
<span class="quote">&gt; --- a/security/Kconfig</span>
<span class="quote">&gt; +++ b/security/Kconfig</span>
<span class="quote">&gt; @@ -6,6 +6,25 @@ menu &quot;Security options&quot;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;   source security/keys/Kconfig</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; +config ARCH_SUPPORTS_XPFO</span>
<span class="quote">&gt; +	bool</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +config XPFO</span>
<span class="quote">&gt; +	bool &quot;Enable eXclusive Page Frame Ownership (XPFO)&quot;</span>
<span class="quote">&gt; +	default n</span>
<span class="quote">&gt; +	depends on ARCH_SUPPORTS_XPFO</span>
<span class="quote">&gt; +	select PAGE_EXTENSION</span>
<span class="quote">&gt; +	help</span>
<span class="quote">&gt; +	  This option offers protection against &#39;ret2dir&#39; kernel attacks.</span>
<span class="quote">&gt; +	  When enabled, every time a page frame is allocated to user space, it</span>
<span class="quote">&gt; +	  is unmapped from the direct mapped RAM region in kernel space</span>
<span class="quote">&gt; +	  (physmap). Similarly, when a page frame is freed/reclaimed, it is</span>
<span class="quote">&gt; +	  mapped back to physmap.</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	  There is a slight performance impact when this option is enabled.</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	  If in doubt, say &quot;N&quot;.</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;   config SECURITY_DMESG_RESTRICT</span>
<span class="quote">&gt;   	bool &quot;Restrict unprivileged access to the kernel syslog&quot;</span>
<span class="quote">&gt;   	default n</span>
<span class="quote">&gt;</span>

When a physical page is assigned to a process in user space, it should 
be unmaped from kernel physmap.  From the code, I can see the patch only 
handle the page in high memory zone. if the kernel use the high memory 
zone, it will call the kmap. So I would like to know if the physical 
page is coming from normal zone,how to handle it.

Thanks
Zhaojunmin
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=38541">Kees Cook</a> - Nov. 10, 2016, 7:11 p.m.</div>
<pre class="content">
On Fri, Nov 4, 2016 at 7:45 AM, Juerg Haefliger &lt;juerg.haefliger@hpe.com&gt; wrote:
<span class="quote">&gt; This patch adds support for XPFO which protects against &#39;ret2dir&#39; kernel</span>
<span class="quote">&gt; attacks. The basic idea is to enforce exclusive ownership of page frames</span>
<span class="quote">&gt; by either the kernel or userspace, unless explicitly requested by the</span>
<span class="quote">&gt; kernel. Whenever a page destined for userspace is allocated, it is</span>
<span class="quote">&gt; unmapped from physmap (the kernel&#39;s page table). When such a page is</span>
<span class="quote">&gt; reclaimed from userspace, it is mapped back to physmap.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Additional fields in the page_ext struct are used for XPFO housekeeping.</span>
<span class="quote">&gt; Specifically two flags to distinguish user vs. kernel pages and to tag</span>
<span class="quote">&gt; unmapped pages and a reference counter to balance kmap/kunmap operations</span>
<span class="quote">&gt; and a lock to serialize access to the XPFO fields.</span>

Thanks for keeping on this! I&#39;d really like to see it land and then
get more architectures to support it.
<span class="quote">
&gt; Known issues/limitations:</span>
<span class="quote">&gt;   - Only supports x86-64 (for now)</span>
<span class="quote">&gt;   - Only supports 4k pages (for now)</span>
<span class="quote">&gt;   - There are most likely some legitimate uses cases where the kernel needs</span>
<span class="quote">&gt;     to access userspace which need to be made XPFO-aware</span>
<span class="quote">&gt;   - Performance penalty</span>

In the Kconfig you say &quot;slight&quot;, but I&#39;m curious what kinds of
benchmarks you&#39;ve done and if there&#39;s a more specific cost we can
declare, just to give people more of an idea what the hit looks like?
(What workloads would trigger a lot of XPFO unmapping, for example?)

Thanks!

-Kees
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=38541">Kees Cook</a> - Nov. 10, 2016, 7:24 p.m.</div>
<pre class="content">
On Fri, Nov 4, 2016 at 7:45 AM, Juerg Haefliger &lt;juerg.haefliger@hpe.com&gt; wrote:
<span class="quote">&gt; This patch adds support for XPFO which protects against &#39;ret2dir&#39; kernel</span>
<span class="quote">&gt; attacks. The basic idea is to enforce exclusive ownership of page frames</span>
<span class="quote">&gt; by either the kernel or userspace, unless explicitly requested by the</span>
<span class="quote">&gt; kernel. Whenever a page destined for userspace is allocated, it is</span>
<span class="quote">&gt; unmapped from physmap (the kernel&#39;s page table). When such a page is</span>
<span class="quote">&gt; reclaimed from userspace, it is mapped back to physmap.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Additional fields in the page_ext struct are used for XPFO housekeeping.</span>
<span class="quote">&gt; Specifically two flags to distinguish user vs. kernel pages and to tag</span>
<span class="quote">&gt; unmapped pages and a reference counter to balance kmap/kunmap operations</span>
<span class="quote">&gt; and a lock to serialize access to the XPFO fields.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Known issues/limitations:</span>
<span class="quote">&gt;   - Only supports x86-64 (for now)</span>
<span class="quote">&gt;   - Only supports 4k pages (for now)</span>
<span class="quote">&gt;   - There are most likely some legitimate uses cases where the kernel needs</span>
<span class="quote">&gt;     to access userspace which need to be made XPFO-aware</span>
<span class="quote">&gt;   - Performance penalty</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Reference paper by the original patch authors:</span>
<span class="quote">&gt;   http://www.cs.columbia.edu/~vpk/papers/ret2dir.sec14.pdf</span>

Would it be possible to create an lkdtm test that can exercise this protection?
<span class="quote">
&gt; Suggested-by: Vasileios P. Kemerlis &lt;vpk@cs.columbia.edu&gt;</span>
<span class="quote">&gt; Signed-off-by: Juerg Haefliger &lt;juerg.haefliger@hpe.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  arch/x86/Kconfig         |   3 +-</span>
<span class="quote">&gt;  arch/x86/mm/init.c       |   2 +-</span>
<span class="quote">&gt;  drivers/ata/libata-sff.c |   4 +-</span>
<span class="quote">&gt;  include/linux/highmem.h  |  15 +++-</span>
<span class="quote">&gt;  include/linux/page_ext.h |   7 ++</span>
<span class="quote">&gt;  include/linux/xpfo.h     |  39 +++++++++</span>
<span class="quote">&gt;  lib/swiotlb.c            |   3 +-</span>
<span class="quote">&gt;  mm/Makefile              |   1 +</span>
<span class="quote">&gt;  mm/page_alloc.c          |   2 +</span>
<span class="quote">&gt;  mm/page_ext.c            |   4 +</span>
<span class="quote">&gt;  mm/xpfo.c                | 206 +++++++++++++++++++++++++++++++++++++++++++++++</span>
<span class="quote">&gt;  security/Kconfig         |  19 +++++</span>
<span class="quote">&gt;  12 files changed, 298 insertions(+), 7 deletions(-)</span>
<span class="quote">&gt;  create mode 100644 include/linux/xpfo.h</span>
<span class="quote">&gt;  create mode 100644 mm/xpfo.c</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig</span>
<span class="quote">&gt; index bada636d1065..38b334f8fde5 100644</span>
<span class="quote">&gt; --- a/arch/x86/Kconfig</span>
<span class="quote">&gt; +++ b/arch/x86/Kconfig</span>
<span class="quote">&gt; @@ -165,6 +165,7 @@ config X86</span>
<span class="quote">&gt;         select HAVE_STACK_VALIDATION            if X86_64</span>
<span class="quote">&gt;         select ARCH_USES_HIGH_VMA_FLAGS         if X86_INTEL_MEMORY_PROTECTION_KEYS</span>
<span class="quote">&gt;         select ARCH_HAS_PKEYS                   if X86_INTEL_MEMORY_PROTECTION_KEYS</span>
<span class="quote">&gt; +       select ARCH_SUPPORTS_XPFO               if X86_64</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  config INSTRUCTION_DECODER</span>
<span class="quote">&gt;         def_bool y</span>
<span class="quote">&gt; @@ -1361,7 +1362,7 @@ config ARCH_DMA_ADDR_T_64BIT</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  config X86_DIRECT_GBPAGES</span>
<span class="quote">&gt;         def_bool y</span>
<span class="quote">&gt; -       depends on X86_64 &amp;&amp; !DEBUG_PAGEALLOC &amp;&amp; !KMEMCHECK</span>
<span class="quote">&gt; +       depends on X86_64 &amp;&amp; !DEBUG_PAGEALLOC &amp;&amp; !KMEMCHECK &amp;&amp; !XPFO</span>
<span class="quote">&gt;         ---help---</span>
<span class="quote">&gt;           Certain kernel features effectively disable kernel</span>
<span class="quote">&gt;           linear 1 GB mappings (even if the CPU otherwise</span>
<span class="quote">&gt; diff --git a/arch/x86/mm/init.c b/arch/x86/mm/init.c</span>
<span class="quote">&gt; index 22af912d66d2..a6fafbae02bb 100644</span>
<span class="quote">&gt; --- a/arch/x86/mm/init.c</span>
<span class="quote">&gt; +++ b/arch/x86/mm/init.c</span>
<span class="quote">&gt; @@ -161,7 +161,7 @@ static int page_size_mask;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  static void __init probe_page_size_mask(void)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -#if !defined(CONFIG_KMEMCHECK)</span>
<span class="quote">&gt; +#if !defined(CONFIG_KMEMCHECK) &amp;&amp; !defined(CONFIG_XPFO)</span>
<span class="quote">&gt;         /*</span>
<span class="quote">&gt;          * For CONFIG_KMEMCHECK or pagealloc debugging, identity mapping will</span>
<span class="quote">&gt;          * use small pages.</span>
<span class="quote">&gt; diff --git a/drivers/ata/libata-sff.c b/drivers/ata/libata-sff.c</span>
<span class="quote">&gt; index 051b6158d1b7..58af734be25d 100644</span>
<span class="quote">&gt; --- a/drivers/ata/libata-sff.c</span>
<span class="quote">&gt; +++ b/drivers/ata/libata-sff.c</span>
<span class="quote">&gt; @@ -715,7 +715,7 @@ static void ata_pio_sector(struct ata_queued_cmd *qc)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         DPRINTK(&quot;data %s\n&quot;, qc-&gt;tf.flags &amp; ATA_TFLAG_WRITE ? &quot;write&quot; : &quot;read&quot;);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -       if (PageHighMem(page)) {</span>
<span class="quote">&gt; +       if (PageHighMem(page) || xpfo_page_is_unmapped(page)) {</span>
<span class="quote">&gt;                 unsigned long flags;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;                 /* FIXME: use a bounce buffer */</span>
<span class="quote">&gt; @@ -860,7 +860,7 @@ static int __atapi_pio_bytes(struct ata_queued_cmd *qc, unsigned int bytes)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         DPRINTK(&quot;data %s\n&quot;, qc-&gt;tf.flags &amp; ATA_TFLAG_WRITE ? &quot;write&quot; : &quot;read&quot;);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -       if (PageHighMem(page)) {</span>
<span class="quote">&gt; +       if (PageHighMem(page) || xpfo_page_is_unmapped(page)) {</span>
<span class="quote">&gt;                 unsigned long flags;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;                 /* FIXME: use bounce buffer */</span>
<span class="quote">&gt; diff --git a/include/linux/highmem.h b/include/linux/highmem.h</span>
<span class="quote">&gt; index bb3f3297062a..7a17c166532f 100644</span>
<span class="quote">&gt; --- a/include/linux/highmem.h</span>
<span class="quote">&gt; +++ b/include/linux/highmem.h</span>
<span class="quote">&gt; @@ -7,6 +7,7 @@</span>
<span class="quote">&gt;  #include &lt;linux/mm.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/uaccess.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/hardirq.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/xpfo.h&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  #include &lt;asm/cacheflush.h&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; @@ -55,24 +56,34 @@ static inline struct page *kmap_to_page(void *addr)</span>
<span class="quote">&gt;  #ifndef ARCH_HAS_KMAP</span>
<span class="quote">&gt;  static inline void *kmap(struct page *page)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; +       void *kaddr;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;         might_sleep();</span>
<span class="quote">&gt; -       return page_address(page);</span>
<span class="quote">&gt; +       kaddr = page_address(page);</span>
<span class="quote">&gt; +       xpfo_kmap(kaddr, page);</span>
<span class="quote">&gt; +       return kaddr;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  static inline void kunmap(struct page *page)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; +       xpfo_kunmap(page_address(page), page);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  static inline void *kmap_atomic(struct page *page)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; +       void *kaddr;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;         preempt_disable();</span>
<span class="quote">&gt;         pagefault_disable();</span>
<span class="quote">&gt; -       return page_address(page);</span>
<span class="quote">&gt; +       kaddr = page_address(page);</span>
<span class="quote">&gt; +       xpfo_kmap(kaddr, page);</span>
<span class="quote">&gt; +       return kaddr;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  #define kmap_atomic_prot(page, prot)   kmap_atomic(page)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  static inline void __kunmap_atomic(void *addr)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; +       xpfo_kunmap(addr, virt_to_page(addr));</span>
<span class="quote">&gt;         pagefault_enable();</span>
<span class="quote">&gt;         preempt_enable();</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; diff --git a/include/linux/page_ext.h b/include/linux/page_ext.h</span>
<span class="quote">&gt; index 9298c393ddaa..0e451a42e5a3 100644</span>
<span class="quote">&gt; --- a/include/linux/page_ext.h</span>
<span class="quote">&gt; +++ b/include/linux/page_ext.h</span>
<span class="quote">&gt; @@ -29,6 +29,8 @@ enum page_ext_flags {</span>
<span class="quote">&gt;         PAGE_EXT_DEBUG_POISON,          /* Page is poisoned */</span>
<span class="quote">&gt;         PAGE_EXT_DEBUG_GUARD,</span>
<span class="quote">&gt;         PAGE_EXT_OWNER,</span>
<span class="quote">&gt; +       PAGE_EXT_XPFO_KERNEL,           /* Page is a kernel page */</span>
<span class="quote">&gt; +       PAGE_EXT_XPFO_UNMAPPED,         /* Page is unmapped */</span>
<span class="quote">&gt;  #if defined(CONFIG_IDLE_PAGE_TRACKING) &amp;&amp; !defined(CONFIG_64BIT)</span>
<span class="quote">&gt;         PAGE_EXT_YOUNG,</span>
<span class="quote">&gt;         PAGE_EXT_IDLE,</span>
<span class="quote">&gt; @@ -44,6 +46,11 @@ enum page_ext_flags {</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  struct page_ext {</span>
<span class="quote">&gt;         unsigned long flags;</span>
<span class="quote">&gt; +#ifdef CONFIG_XPFO</span>
<span class="quote">&gt; +       int inited;             /* Map counter and lock initialized */</span>
<span class="quote">&gt; +       atomic_t mapcount;      /* Counter for balancing map/unmap requests */</span>
<span class="quote">&gt; +       spinlock_t maplock;     /* Lock to serialize map/unmap requests */</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt;  };</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  extern void pgdat_page_ext_init(struct pglist_data *pgdat);</span>
<span class="quote">&gt; diff --git a/include/linux/xpfo.h b/include/linux/xpfo.h</span>
<span class="quote">&gt; new file mode 100644</span>
<span class="quote">&gt; index 000000000000..77187578ca33</span>
<span class="quote">&gt; --- /dev/null</span>
<span class="quote">&gt; +++ b/include/linux/xpfo.h</span>
<span class="quote">&gt; @@ -0,0 +1,39 @@</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * Copyright (C) 2016 Hewlett Packard Enterprise Development, L.P.</span>
<span class="quote">&gt; + * Copyright (C) 2016 Brown University. All rights reserved.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Authors:</span>
<span class="quote">&gt; + *   Juerg Haefliger &lt;juerg.haefliger@hpe.com&gt;</span>
<span class="quote">&gt; + *   Vasileios P. Kemerlis &lt;vpk@cs.brown.edu&gt;</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * This program is free software; you can redistribute it and/or modify it</span>
<span class="quote">&gt; + * under the terms of the GNU General Public License version 2 as published by</span>
<span class="quote">&gt; + * the Free Software Foundation.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#ifndef _LINUX_XPFO_H</span>
<span class="quote">&gt; +#define _LINUX_XPFO_H</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#ifdef CONFIG_XPFO</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +extern struct page_ext_operations page_xpfo_ops;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +extern void xpfo_kmap(void *kaddr, struct page *page);</span>
<span class="quote">&gt; +extern void xpfo_kunmap(void *kaddr, struct page *page);</span>
<span class="quote">&gt; +extern void xpfo_alloc_page(struct page *page, int order, gfp_t gfp);</span>
<span class="quote">&gt; +extern void xpfo_free_page(struct page *page, int order);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +extern bool xpfo_page_is_unmapped(struct page *page);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#else /* !CONFIG_XPFO */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline void xpfo_kmap(void *kaddr, struct page *page) { }</span>
<span class="quote">&gt; +static inline void xpfo_kunmap(void *kaddr, struct page *page) { }</span>
<span class="quote">&gt; +static inline void xpfo_alloc_page(struct page *page, int order, gfp_t gfp) { }</span>
<span class="quote">&gt; +static inline void xpfo_free_page(struct page *page, int order) { }</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline bool xpfo_page_is_unmapped(struct page *page) { return false; }</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#endif /* CONFIG_XPFO */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#endif /* _LINUX_XPFO_H */</span>
<span class="quote">&gt; diff --git a/lib/swiotlb.c b/lib/swiotlb.c</span>
<span class="quote">&gt; index 22e13a0e19d7..455eff44604e 100644</span>
<span class="quote">&gt; --- a/lib/swiotlb.c</span>
<span class="quote">&gt; +++ b/lib/swiotlb.c</span>
<span class="quote">&gt; @@ -390,8 +390,9 @@ static void swiotlb_bounce(phys_addr_t orig_addr, phys_addr_t tlb_addr,</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;         unsigned long pfn = PFN_DOWN(orig_addr);</span>
<span class="quote">&gt;         unsigned char *vaddr = phys_to_virt(tlb_addr);</span>
<span class="quote">&gt; +       struct page *page = pfn_to_page(pfn);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -       if (PageHighMem(pfn_to_page(pfn))) {</span>
<span class="quote">&gt; +       if (PageHighMem(page) || xpfo_page_is_unmapped(page)) {</span>
<span class="quote">&gt;                 /* The buffer does not have a mapping.  Map it in and copy */</span>
<span class="quote">&gt;                 unsigned int offset = orig_addr &amp; ~PAGE_MASK;</span>
<span class="quote">&gt;                 char *buffer;</span>
<span class="quote">&gt; diff --git a/mm/Makefile b/mm/Makefile</span>
<span class="quote">&gt; index 295bd7a9f76b..175680f516aa 100644</span>
<span class="quote">&gt; --- a/mm/Makefile</span>
<span class="quote">&gt; +++ b/mm/Makefile</span>
<span class="quote">&gt; @@ -100,3 +100,4 @@ obj-$(CONFIG_IDLE_PAGE_TRACKING) += page_idle.o</span>
<span class="quote">&gt;  obj-$(CONFIG_FRAME_VECTOR) += frame_vector.o</span>
<span class="quote">&gt;  obj-$(CONFIG_DEBUG_PAGE_REF) += debug_page_ref.o</span>
<span class="quote">&gt;  obj-$(CONFIG_HARDENED_USERCOPY) += usercopy.o</span>
<span class="quote">&gt; +obj-$(CONFIG_XPFO) += xpfo.o</span>
<span class="quote">&gt; diff --git a/mm/page_alloc.c b/mm/page_alloc.c</span>
<span class="quote">&gt; index 8fd42aa7c4bd..100e80e008e2 100644</span>
<span class="quote">&gt; --- a/mm/page_alloc.c</span>
<span class="quote">&gt; +++ b/mm/page_alloc.c</span>
<span class="quote">&gt; @@ -1045,6 +1045,7 @@ static __always_inline bool free_pages_prepare(struct page *page,</span>
<span class="quote">&gt;         kernel_poison_pages(page, 1 &lt;&lt; order, 0);</span>
<span class="quote">&gt;         kernel_map_pages(page, 1 &lt;&lt; order, 0);</span>
<span class="quote">&gt;         kasan_free_pages(page, order);</span>
<span class="quote">&gt; +       xpfo_free_page(page, order);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         return true;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; @@ -1745,6 +1746,7 @@ inline void post_alloc_hook(struct page *page, unsigned int order,</span>
<span class="quote">&gt;         kernel_map_pages(page, 1 &lt;&lt; order, 1);</span>
<span class="quote">&gt;         kernel_poison_pages(page, 1 &lt;&lt; order, 1);</span>
<span class="quote">&gt;         kasan_alloc_pages(page, order);</span>
<span class="quote">&gt; +       xpfo_alloc_page(page, order, gfp_flags);</span>
<span class="quote">&gt;         set_page_owner(page, order, gfp_flags);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; diff --git a/mm/page_ext.c b/mm/page_ext.c</span>
<span class="quote">&gt; index 121dcffc4ec1..ba6dbcacc2db 100644</span>
<span class="quote">&gt; --- a/mm/page_ext.c</span>
<span class="quote">&gt; +++ b/mm/page_ext.c</span>
<span class="quote">&gt; @@ -7,6 +7,7 @@</span>
<span class="quote">&gt;  #include &lt;linux/kmemleak.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/page_owner.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/page_idle.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/xpfo.h&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt;   * struct page extension</span>
<span class="quote">&gt; @@ -68,6 +69,9 @@ static struct page_ext_operations *page_ext_ops[] = {</span>
<span class="quote">&gt;  #if defined(CONFIG_IDLE_PAGE_TRACKING) &amp;&amp; !defined(CONFIG_64BIT)</span>
<span class="quote">&gt;         &amp;page_idle_ops,</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt; +#ifdef CONFIG_XPFO</span>
<span class="quote">&gt; +       &amp;page_xpfo_ops,</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt;  };</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  static unsigned long total_usage;</span>
<span class="quote">&gt; diff --git a/mm/xpfo.c b/mm/xpfo.c</span>
<span class="quote">&gt; new file mode 100644</span>
<span class="quote">&gt; index 000000000000..8e3a6a694b6a</span>
<span class="quote">&gt; --- /dev/null</span>
<span class="quote">&gt; +++ b/mm/xpfo.c</span>
<span class="quote">&gt; @@ -0,0 +1,206 @@</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * Copyright (C) 2016 Hewlett Packard Enterprise Development, L.P.</span>
<span class="quote">&gt; + * Copyright (C) 2016 Brown University. All rights reserved.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Authors:</span>
<span class="quote">&gt; + *   Juerg Haefliger &lt;juerg.haefliger@hpe.com&gt;</span>
<span class="quote">&gt; + *   Vasileios P. Kemerlis &lt;vpk@cs.brown.edu&gt;</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * This program is free software; you can redistribute it and/or modify it</span>
<span class="quote">&gt; + * under the terms of the GNU General Public License version 2 as published by</span>
<span class="quote">&gt; + * the Free Software Foundation.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#include &lt;linux/mm.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/module.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/page_ext.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/xpfo.h&gt;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#include &lt;asm/tlbflush.h&gt;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +DEFINE_STATIC_KEY_FALSE(xpfo_inited);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static bool need_xpfo(void)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       return true;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static void init_xpfo(void)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       printk(KERN_INFO &quot;XPFO enabled\n&quot;);</span>
<span class="quote">&gt; +       static_branch_enable(&amp;xpfo_inited);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +struct page_ext_operations page_xpfo_ops = {</span>
<span class="quote">&gt; +       .need = need_xpfo,</span>
<span class="quote">&gt; +       .init = init_xpfo,</span>
<span class="quote">&gt; +};</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * Update a single kernel page table entry</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +static inline void set_kpte(struct page *page, unsigned long kaddr,</span>
<span class="quote">&gt; +                           pgprot_t prot) {</span>
<span class="quote">&gt; +       unsigned int level;</span>
<span class="quote">&gt; +       pte_t *kpte = lookup_address(kaddr, &amp;level);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       /* We only support 4k pages for now */</span>
<span class="quote">&gt; +       BUG_ON(!kpte || level != PG_LEVEL_4K);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       set_pte_atomic(kpte, pfn_pte(page_to_pfn(page), canon_pgprot(prot)));</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +void xpfo_alloc_page(struct page *page, int order, gfp_t gfp)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       int i, flush_tlb = 0;</span>
<span class="quote">&gt; +       struct page_ext *page_ext;</span>
<span class="quote">&gt; +       unsigned long kaddr;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       if (!static_branch_unlikely(&amp;xpfo_inited))</span>
<span class="quote">&gt; +               return;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       for (i = 0; i &lt; (1 &lt;&lt; order); i++)  {</span>
<span class="quote">&gt; +               page_ext = lookup_page_ext(page + i);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +               BUG_ON(test_bit(PAGE_EXT_XPFO_UNMAPPED, &amp;page_ext-&gt;flags));</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +               /* Initialize the map lock and map counter */</span>
<span class="quote">&gt; +               if (!page_ext-&gt;inited) {</span>
<span class="quote">&gt; +                       spin_lock_init(&amp;page_ext-&gt;maplock);</span>
<span class="quote">&gt; +                       atomic_set(&amp;page_ext-&gt;mapcount, 0);</span>
<span class="quote">&gt; +                       page_ext-&gt;inited = 1;</span>
<span class="quote">&gt; +               }</span>
<span class="quote">&gt; +               BUG_ON(atomic_read(&amp;page_ext-&gt;mapcount));</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +               if ((gfp &amp; GFP_HIGHUSER) == GFP_HIGHUSER) {</span>
<span class="quote">&gt; +                       /*</span>
<span class="quote">&gt; +                        * Flush the TLB if the page was previously allocated</span>
<span class="quote">&gt; +                        * to the kernel.</span>
<span class="quote">&gt; +                        */</span>
<span class="quote">&gt; +                       if (test_and_clear_bit(PAGE_EXT_XPFO_KERNEL,</span>
<span class="quote">&gt; +                                              &amp;page_ext-&gt;flags))</span>
<span class="quote">&gt; +                               flush_tlb = 1;</span>
<span class="quote">&gt; +               } else {</span>
<span class="quote">&gt; +                       /* Tag the page as a kernel page */</span>
<span class="quote">&gt; +                       set_bit(PAGE_EXT_XPFO_KERNEL, &amp;page_ext-&gt;flags);</span>
<span class="quote">&gt; +               }</span>
<span class="quote">&gt; +       }</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       if (flush_tlb) {</span>
<span class="quote">&gt; +               kaddr = (unsigned long)page_address(page);</span>
<span class="quote">&gt; +               flush_tlb_kernel_range(kaddr, kaddr + (1 &lt;&lt; order) *</span>
<span class="quote">&gt; +                                      PAGE_SIZE);</span>
<span class="quote">&gt; +       }</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +void xpfo_free_page(struct page *page, int order)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       int i;</span>
<span class="quote">&gt; +       struct page_ext *page_ext;</span>
<span class="quote">&gt; +       unsigned long kaddr;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       if (!static_branch_unlikely(&amp;xpfo_inited))</span>
<span class="quote">&gt; +               return;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       for (i = 0; i &lt; (1 &lt;&lt; order); i++) {</span>
<span class="quote">&gt; +               page_ext = lookup_page_ext(page + i);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +               if (!page_ext-&gt;inited) {</span>
<span class="quote">&gt; +                       /*</span>
<span class="quote">&gt; +                        * The page was allocated before page_ext was</span>
<span class="quote">&gt; +                        * initialized, so it is a kernel page and it needs to</span>
<span class="quote">&gt; +                        * be tagged accordingly.</span>
<span class="quote">&gt; +                        */</span>
<span class="quote">&gt; +                       set_bit(PAGE_EXT_XPFO_KERNEL, &amp;page_ext-&gt;flags);</span>
<span class="quote">&gt; +                       continue;</span>
<span class="quote">&gt; +               }</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +               /*</span>
<span class="quote">&gt; +                * Map the page back into the kernel if it was previously</span>
<span class="quote">&gt; +                * allocated to user space.</span>
<span class="quote">&gt; +                */</span>
<span class="quote">&gt; +               if (test_and_clear_bit(PAGE_EXT_XPFO_UNMAPPED,</span>
<span class="quote">&gt; +                                      &amp;page_ext-&gt;flags)) {</span>
<span class="quote">&gt; +                       kaddr = (unsigned long)page_address(page + i);</span>
<span class="quote">&gt; +                       set_kpte(page + i,  kaddr, __pgprot(__PAGE_KERNEL));</span>
<span class="quote">&gt; +               }</span>
<span class="quote">&gt; +       }</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +void xpfo_kmap(void *kaddr, struct page *page)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       struct page_ext *page_ext;</span>
<span class="quote">&gt; +       unsigned long flags;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       if (!static_branch_unlikely(&amp;xpfo_inited))</span>
<span class="quote">&gt; +               return;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       page_ext = lookup_page_ext(page);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       /*</span>
<span class="quote">&gt; +        * The page was allocated before page_ext was initialized (which means</span>
<span class="quote">&gt; +        * it&#39;s a kernel page) or it&#39;s allocated to the kernel, so nothing to</span>
<span class="quote">&gt; +        * do.</span>
<span class="quote">&gt; +        */</span>
<span class="quote">&gt; +       if (!page_ext-&gt;inited ||</span>
<span class="quote">&gt; +           test_bit(PAGE_EXT_XPFO_KERNEL, &amp;page_ext-&gt;flags))</span>
<span class="quote">&gt; +               return;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       spin_lock_irqsave(&amp;page_ext-&gt;maplock, flags);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       /*</span>
<span class="quote">&gt; +        * The page was previously allocated to user space, so map it back</span>
<span class="quote">&gt; +        * into the kernel. No TLB flush required.</span>
<span class="quote">&gt; +        */</span>
<span class="quote">&gt; +       if ((atomic_inc_return(&amp;page_ext-&gt;mapcount) == 1) &amp;&amp;</span>
<span class="quote">&gt; +           test_and_clear_bit(PAGE_EXT_XPFO_UNMAPPED, &amp;page_ext-&gt;flags))</span>
<span class="quote">&gt; +               set_kpte(page, (unsigned long)kaddr, __pgprot(__PAGE_KERNEL));</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       spin_unlock_irqrestore(&amp;page_ext-&gt;maplock, flags);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +EXPORT_SYMBOL(xpfo_kmap);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +void xpfo_kunmap(void *kaddr, struct page *page)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       struct page_ext *page_ext;</span>
<span class="quote">&gt; +       unsigned long flags;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       if (!static_branch_unlikely(&amp;xpfo_inited))</span>
<span class="quote">&gt; +               return;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       page_ext = lookup_page_ext(page);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       /*</span>
<span class="quote">&gt; +        * The page was allocated before page_ext was initialized (which means</span>
<span class="quote">&gt; +        * it&#39;s a kernel page) or it&#39;s allocated to the kernel, so nothing to</span>
<span class="quote">&gt; +        * do.</span>
<span class="quote">&gt; +        */</span>
<span class="quote">&gt; +       if (!page_ext-&gt;inited ||</span>
<span class="quote">&gt; +           test_bit(PAGE_EXT_XPFO_KERNEL, &amp;page_ext-&gt;flags))</span>
<span class="quote">&gt; +               return;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       spin_lock_irqsave(&amp;page_ext-&gt;maplock, flags);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       /*</span>
<span class="quote">&gt; +        * The page is to be allocated back to user space, so unmap it from the</span>
<span class="quote">&gt; +        * kernel, flush the TLB and tag it as a user page.</span>
<span class="quote">&gt; +        */</span>
<span class="quote">&gt; +       if (atomic_dec_return(&amp;page_ext-&gt;mapcount) == 0) {</span>
<span class="quote">&gt; +               BUG_ON(test_bit(PAGE_EXT_XPFO_UNMAPPED, &amp;page_ext-&gt;flags));</span>
<span class="quote">&gt; +               set_bit(PAGE_EXT_XPFO_UNMAPPED, &amp;page_ext-&gt;flags);</span>
<span class="quote">&gt; +               set_kpte(page, (unsigned long)kaddr, __pgprot(0));</span>
<span class="quote">&gt; +               __flush_tlb_one((unsigned long)kaddr);</span>
<span class="quote">&gt; +       }</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       spin_unlock_irqrestore(&amp;page_ext-&gt;maplock, flags);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +EXPORT_SYMBOL(xpfo_kunmap);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +inline bool xpfo_page_is_unmapped(struct page *page)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       if (!static_branch_unlikely(&amp;xpfo_inited))</span>
<span class="quote">&gt; +               return false;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       return test_bit(PAGE_EXT_XPFO_UNMAPPED, &amp;lookup_page_ext(page)-&gt;flags);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +EXPORT_SYMBOL(xpfo_page_is_unmapped);</span>
<span class="quote">&gt; diff --git a/security/Kconfig b/security/Kconfig</span>
<span class="quote">&gt; index 118f4549404e..4502e15c8419 100644</span>
<span class="quote">&gt; --- a/security/Kconfig</span>
<span class="quote">&gt; +++ b/security/Kconfig</span>
<span class="quote">&gt; @@ -6,6 +6,25 @@ menu &quot;Security options&quot;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  source security/keys/Kconfig</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; +config ARCH_SUPPORTS_XPFO</span>
<span class="quote">&gt; +       bool</span>

Can you include a &quot;help&quot; section here to describe what requirements an
architecture needs to support XPFO? See HAVE_ARCH_SECCOMP_FILTER and
HAVE_ARCH_VMAP_STACK or some examples.
<span class="quote">
&gt; +config XPFO</span>
<span class="quote">&gt; +       bool &quot;Enable eXclusive Page Frame Ownership (XPFO)&quot;</span>
<span class="quote">&gt; +       default n</span>
<span class="quote">&gt; +       depends on ARCH_SUPPORTS_XPFO</span>
<span class="quote">&gt; +       select PAGE_EXTENSION</span>
<span class="quote">&gt; +       help</span>
<span class="quote">&gt; +         This option offers protection against &#39;ret2dir&#39; kernel attacks.</span>
<span class="quote">&gt; +         When enabled, every time a page frame is allocated to user space, it</span>
<span class="quote">&gt; +         is unmapped from the direct mapped RAM region in kernel space</span>
<span class="quote">&gt; +         (physmap). Similarly, when a page frame is freed/reclaimed, it is</span>
<span class="quote">&gt; +         mapped back to physmap.</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +         There is a slight performance impact when this option is enabled.</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +         If in doubt, say &quot;N&quot;.</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  config SECURITY_DMESG_RESTRICT</span>
<span class="quote">&gt;         bool &quot;Restrict unprivileged access to the kernel syslog&quot;</span>
<span class="quote">&gt;         default n</span>
<span class="quote">&gt; --</span>
<span class="quote">&gt; 2.10.1</span>
<span class="quote">&gt;</span>

I&#39;ve added these patches to my kspp tree on kernel.org, so it should
get some 0-day testing now...

Thanks!

-Kees
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=153531">Juerg Haefliger</a> - Nov. 15, 2016, 11:15 a.m.</div>
<pre class="content">
Sorry for the late reply, I just found your email in my cluttered inbox.

On 11/10/2016 08:11 PM, Kees Cook wrote:
<span class="quote">&gt; On Fri, Nov 4, 2016 at 7:45 AM, Juerg Haefliger &lt;juerg.haefliger@hpe.com&gt; wrote:</span>
<span class="quote">&gt;&gt; This patch adds support for XPFO which protects against &#39;ret2dir&#39; kernel</span>
<span class="quote">&gt;&gt; attacks. The basic idea is to enforce exclusive ownership of page frames</span>
<span class="quote">&gt;&gt; by either the kernel or userspace, unless explicitly requested by the</span>
<span class="quote">&gt;&gt; kernel. Whenever a page destined for userspace is allocated, it is</span>
<span class="quote">&gt;&gt; unmapped from physmap (the kernel&#39;s page table). When such a page is</span>
<span class="quote">&gt;&gt; reclaimed from userspace, it is mapped back to physmap.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Additional fields in the page_ext struct are used for XPFO housekeeping.</span>
<span class="quote">&gt;&gt; Specifically two flags to distinguish user vs. kernel pages and to tag</span>
<span class="quote">&gt;&gt; unmapped pages and a reference counter to balance kmap/kunmap operations</span>
<span class="quote">&gt;&gt; and a lock to serialize access to the XPFO fields.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Thanks for keeping on this! I&#39;d really like to see it land and then</span>
<span class="quote">&gt; get more architectures to support it.</span>

Good to hear :-)
<span class="quote">

&gt;&gt; Known issues/limitations:</span>
<span class="quote">&gt;&gt;   - Only supports x86-64 (for now)</span>
<span class="quote">&gt;&gt;   - Only supports 4k pages (for now)</span>
<span class="quote">&gt;&gt;   - There are most likely some legitimate uses cases where the kernel needs</span>
<span class="quote">&gt;&gt;     to access userspace which need to be made XPFO-aware</span>
<span class="quote">&gt;&gt;   - Performance penalty</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; In the Kconfig you say &quot;slight&quot;, but I&#39;m curious what kinds of</span>
<span class="quote">&gt; benchmarks you&#39;ve done and if there&#39;s a more specific cost we can</span>
<span class="quote">&gt; declare, just to give people more of an idea what the hit looks like?</span>
<span class="quote">&gt; (What workloads would trigger a lot of XPFO unmapping, for example?)</span>

That &#39;slight&#39; wording is based on the performance numbers published in the referenced paper.

So far I&#39;ve only run kernel compilation tests. For that workload, the big performance hit comes from
disabling &gt;4k page sizes (around 10%). Adding XPFO on top causes &#39;only&#39; another 0.5% performance
penalty. I&#39;m currently looking into adding support for larger page sizes to see what the real impact
is and then generate some more relevant numbers.

...Juerg
<span class="quote">

&gt; Thanks!</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -Kees</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=153531">Juerg Haefliger</a> - Nov. 15, 2016, 11:18 a.m.</div>
<pre class="content">
On 11/10/2016 08:24 PM, Kees Cook wrote:
<span class="quote">&gt; On Fri, Nov 4, 2016 at 7:45 AM, Juerg Haefliger &lt;juerg.haefliger@hpe.com&gt; wrote:</span>
<span class="quote">&gt;&gt; This patch adds support for XPFO which protects against &#39;ret2dir&#39; kernel</span>
<span class="quote">&gt;&gt; attacks. The basic idea is to enforce exclusive ownership of page frames</span>
<span class="quote">&gt;&gt; by either the kernel or userspace, unless explicitly requested by the</span>
<span class="quote">&gt;&gt; kernel. Whenever a page destined for userspace is allocated, it is</span>
<span class="quote">&gt;&gt; unmapped from physmap (the kernel&#39;s page table). When such a page is</span>
<span class="quote">&gt;&gt; reclaimed from userspace, it is mapped back to physmap.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Additional fields in the page_ext struct are used for XPFO housekeeping.</span>
<span class="quote">&gt;&gt; Specifically two flags to distinguish user vs. kernel pages and to tag</span>
<span class="quote">&gt;&gt; unmapped pages and a reference counter to balance kmap/kunmap operations</span>
<span class="quote">&gt;&gt; and a lock to serialize access to the XPFO fields.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Known issues/limitations:</span>
<span class="quote">&gt;&gt;   - Only supports x86-64 (for now)</span>
<span class="quote">&gt;&gt;   - Only supports 4k pages (for now)</span>
<span class="quote">&gt;&gt;   - There are most likely some legitimate uses cases where the kernel needs</span>
<span class="quote">&gt;&gt;     to access userspace which need to be made XPFO-aware</span>
<span class="quote">&gt;&gt;   - Performance penalty</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Reference paper by the original patch authors:</span>
<span class="quote">&gt;&gt;   http://www.cs.columbia.edu/~vpk/papers/ret2dir.sec14.pdf</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Would it be possible to create an lkdtm test that can exercise this protection?</span>

I&#39;ll look into it.
<span class="quote">

&gt;&gt; diff --git a/security/Kconfig b/security/Kconfig</span>
<span class="quote">&gt;&gt; index 118f4549404e..4502e15c8419 100644</span>
<span class="quote">&gt;&gt; --- a/security/Kconfig</span>
<span class="quote">&gt;&gt; +++ b/security/Kconfig</span>
<span class="quote">&gt;&gt; @@ -6,6 +6,25 @@ menu &quot;Security options&quot;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;  source security/keys/Kconfig</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; +config ARCH_SUPPORTS_XPFO</span>
<span class="quote">&gt;&gt; +       bool</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Can you include a &quot;help&quot; section here to describe what requirements an</span>
<span class="quote">&gt; architecture needs to support XPFO? See HAVE_ARCH_SECCOMP_FILTER and</span>
<span class="quote">&gt; HAVE_ARCH_VMAP_STACK or some examples.</span>

Will do.
<span class="quote">

&gt;&gt; +config XPFO</span>
<span class="quote">&gt;&gt; +       bool &quot;Enable eXclusive Page Frame Ownership (XPFO)&quot;</span>
<span class="quote">&gt;&gt; +       default n</span>
<span class="quote">&gt;&gt; +       depends on ARCH_SUPPORTS_XPFO</span>
<span class="quote">&gt;&gt; +       select PAGE_EXTENSION</span>
<span class="quote">&gt;&gt; +       help</span>
<span class="quote">&gt;&gt; +         This option offers protection against &#39;ret2dir&#39; kernel attacks.</span>
<span class="quote">&gt;&gt; +         When enabled, every time a page frame is allocated to user space, it</span>
<span class="quote">&gt;&gt; +         is unmapped from the direct mapped RAM region in kernel space</span>
<span class="quote">&gt;&gt; +         (physmap). Similarly, when a page frame is freed/reclaimed, it is</span>
<span class="quote">&gt;&gt; +         mapped back to physmap.</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +         There is a slight performance impact when this option is enabled.</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +         If in doubt, say &quot;N&quot;.</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt;  config SECURITY_DMESG_RESTRICT</span>
<span class="quote">&gt;&gt;         bool &quot;Restrict unprivileged access to the kernel syslog&quot;</span>
<span class="quote">&gt;&gt;         default n</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I&#39;ve added these patches to my kspp tree on kernel.org, so it should</span>
<span class="quote">&gt; get some 0-day testing now...</span>

Very good. Thanks!
<span class="quote">

&gt; Thanks!</span>

Appreciate the feedback.

...Juerg
<span class="quote">

&gt; -Kees</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=75021">Takahiro Akashi</a> - Nov. 24, 2016, 10:56 a.m.</div>
<pre class="content">
Hi,

I&#39;m trying to give it a spin on arm64, but ...

On Fri, Nov 04, 2016 at 03:45:33PM +0100, Juerg Haefliger wrote:
<span class="quote">&gt; This patch adds support for XPFO which protects against &#39;ret2dir&#39; kernel</span>
<span class="quote">&gt; attacks. The basic idea is to enforce exclusive ownership of page frames</span>
<span class="quote">&gt; by either the kernel or userspace, unless explicitly requested by the</span>
<span class="quote">&gt; kernel. Whenever a page destined for userspace is allocated, it is</span>
<span class="quote">&gt; unmapped from physmap (the kernel&#39;s page table). When such a page is</span>
<span class="quote">&gt; reclaimed from userspace, it is mapped back to physmap.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Additional fields in the page_ext struct are used for XPFO housekeeping.</span>
<span class="quote">&gt; Specifically two flags to distinguish user vs. kernel pages and to tag</span>
<span class="quote">&gt; unmapped pages and a reference counter to balance kmap/kunmap operations</span>
<span class="quote">&gt; and a lock to serialize access to the XPFO fields.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Known issues/limitations:</span>
<span class="quote">&gt;   - Only supports x86-64 (for now)</span>
<span class="quote">&gt;   - Only supports 4k pages (for now)</span>
<span class="quote">&gt;   - There are most likely some legitimate uses cases where the kernel needs</span>
<span class="quote">&gt;     to access userspace which need to be made XPFO-aware</span>
<span class="quote">&gt;   - Performance penalty</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Reference paper by the original patch authors:</span>
<span class="quote">&gt;   http://www.cs.columbia.edu/~vpk/papers/ret2dir.sec14.pdf</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Suggested-by: Vasileios P. Kemerlis &lt;vpk@cs.columbia.edu&gt;</span>
<span class="quote">&gt; Signed-off-by: Juerg Haefliger &lt;juerg.haefliger@hpe.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  arch/x86/Kconfig         |   3 +-</span>
<span class="quote">&gt;  arch/x86/mm/init.c       |   2 +-</span>
<span class="quote">&gt;  drivers/ata/libata-sff.c |   4 +-</span>
<span class="quote">&gt;  include/linux/highmem.h  |  15 +++-</span>
<span class="quote">&gt;  include/linux/page_ext.h |   7 ++</span>
<span class="quote">&gt;  include/linux/xpfo.h     |  39 +++++++++</span>
<span class="quote">&gt;  lib/swiotlb.c            |   3 +-</span>
<span class="quote">&gt;  mm/Makefile              |   1 +</span>
<span class="quote">&gt;  mm/page_alloc.c          |   2 +</span>
<span class="quote">&gt;  mm/page_ext.c            |   4 +</span>
<span class="quote">&gt;  mm/xpfo.c                | 206 +++++++++++++++++++++++++++++++++++++++++++++++</span>
<span class="quote">&gt;  security/Kconfig         |  19 +++++</span>
<span class="quote">&gt;  12 files changed, 298 insertions(+), 7 deletions(-)</span>
<span class="quote">&gt;  create mode 100644 include/linux/xpfo.h</span>
<span class="quote">&gt;  create mode 100644 mm/xpfo.c</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig</span>
<span class="quote">&gt; index bada636d1065..38b334f8fde5 100644</span>
<span class="quote">&gt; --- a/arch/x86/Kconfig</span>
<span class="quote">&gt; +++ b/arch/x86/Kconfig</span>
<span class="quote">&gt; @@ -165,6 +165,7 @@ config X86</span>
<span class="quote">&gt;  	select HAVE_STACK_VALIDATION		if X86_64</span>
<span class="quote">&gt;  	select ARCH_USES_HIGH_VMA_FLAGS		if X86_INTEL_MEMORY_PROTECTION_KEYS</span>
<span class="quote">&gt;  	select ARCH_HAS_PKEYS			if X86_INTEL_MEMORY_PROTECTION_KEYS</span>
<span class="quote">&gt; +	select ARCH_SUPPORTS_XPFO		if X86_64</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  config INSTRUCTION_DECODER</span>
<span class="quote">&gt;  	def_bool y</span>
<span class="quote">&gt; @@ -1361,7 +1362,7 @@ config ARCH_DMA_ADDR_T_64BIT</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  config X86_DIRECT_GBPAGES</span>
<span class="quote">&gt;  	def_bool y</span>
<span class="quote">&gt; -	depends on X86_64 &amp;&amp; !DEBUG_PAGEALLOC &amp;&amp; !KMEMCHECK</span>
<span class="quote">&gt; +	depends on X86_64 &amp;&amp; !DEBUG_PAGEALLOC &amp;&amp; !KMEMCHECK &amp;&amp; !XPFO</span>
<span class="quote">&gt;  	---help---</span>
<span class="quote">&gt;  	  Certain kernel features effectively disable kernel</span>
<span class="quote">&gt;  	  linear 1 GB mappings (even if the CPU otherwise</span>
<span class="quote">&gt; diff --git a/arch/x86/mm/init.c b/arch/x86/mm/init.c</span>
<span class="quote">&gt; index 22af912d66d2..a6fafbae02bb 100644</span>
<span class="quote">&gt; --- a/arch/x86/mm/init.c</span>
<span class="quote">&gt; +++ b/arch/x86/mm/init.c</span>
<span class="quote">&gt; @@ -161,7 +161,7 @@ static int page_size_mask;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static void __init probe_page_size_mask(void)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -#if !defined(CONFIG_KMEMCHECK)</span>
<span class="quote">&gt; +#if !defined(CONFIG_KMEMCHECK) &amp;&amp; !defined(CONFIG_XPFO)</span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt;  	 * For CONFIG_KMEMCHECK or pagealloc debugging, identity mapping will</span>
<span class="quote">&gt;  	 * use small pages.</span>
<span class="quote">&gt; diff --git a/drivers/ata/libata-sff.c b/drivers/ata/libata-sff.c</span>
<span class="quote">&gt; index 051b6158d1b7..58af734be25d 100644</span>
<span class="quote">&gt; --- a/drivers/ata/libata-sff.c</span>
<span class="quote">&gt; +++ b/drivers/ata/libata-sff.c</span>
<span class="quote">&gt; @@ -715,7 +715,7 @@ static void ata_pio_sector(struct ata_queued_cmd *qc)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	DPRINTK(&quot;data %s\n&quot;, qc-&gt;tf.flags &amp; ATA_TFLAG_WRITE ? &quot;write&quot; : &quot;read&quot;);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (PageHighMem(page)) {</span>
<span class="quote">&gt; +	if (PageHighMem(page) || xpfo_page_is_unmapped(page)) {</span>
<span class="quote">&gt;  		unsigned long flags;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		/* FIXME: use a bounce buffer */</span>
<span class="quote">&gt; @@ -860,7 +860,7 @@ static int __atapi_pio_bytes(struct ata_queued_cmd *qc, unsigned int bytes)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	DPRINTK(&quot;data %s\n&quot;, qc-&gt;tf.flags &amp; ATA_TFLAG_WRITE ? &quot;write&quot; : &quot;read&quot;);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (PageHighMem(page)) {</span>
<span class="quote">&gt; +	if (PageHighMem(page) || xpfo_page_is_unmapped(page)) {</span>
<span class="quote">&gt;  		unsigned long flags;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		/* FIXME: use bounce buffer */</span>
<span class="quote">&gt; diff --git a/include/linux/highmem.h b/include/linux/highmem.h</span>
<span class="quote">&gt; index bb3f3297062a..7a17c166532f 100644</span>
<span class="quote">&gt; --- a/include/linux/highmem.h</span>
<span class="quote">&gt; +++ b/include/linux/highmem.h</span>
<span class="quote">&gt; @@ -7,6 +7,7 @@</span>
<span class="quote">&gt;  #include &lt;linux/mm.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/uaccess.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/hardirq.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/xpfo.h&gt;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #include &lt;asm/cacheflush.h&gt;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -55,24 +56,34 @@ static inline struct page *kmap_to_page(void *addr)</span>
<span class="quote">&gt;  #ifndef ARCH_HAS_KMAP</span>
<span class="quote">&gt;  static inline void *kmap(struct page *page)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; +	void *kaddr;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	might_sleep();</span>
<span class="quote">&gt; -	return page_address(page);</span>
<span class="quote">&gt; +	kaddr = page_address(page);</span>
<span class="quote">&gt; +	xpfo_kmap(kaddr, page);</span>
<span class="quote">&gt; +	return kaddr;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static inline void kunmap(struct page *page)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; +	xpfo_kunmap(page_address(page), page);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static inline void *kmap_atomic(struct page *page)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; +	void *kaddr;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	preempt_disable();</span>
<span class="quote">&gt;  	pagefault_disable();</span>
<span class="quote">&gt; -	return page_address(page);</span>
<span class="quote">&gt; +	kaddr = page_address(page);</span>
<span class="quote">&gt; +	xpfo_kmap(kaddr, page);</span>
<span class="quote">&gt; +	return kaddr;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  #define kmap_atomic_prot(page, prot)	kmap_atomic(page)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static inline void __kunmap_atomic(void *addr)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; +	xpfo_kunmap(addr, virt_to_page(addr));</span>
<span class="quote">&gt;  	pagefault_enable();</span>
<span class="quote">&gt;  	preempt_enable();</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; diff --git a/include/linux/page_ext.h b/include/linux/page_ext.h</span>
<span class="quote">&gt; index 9298c393ddaa..0e451a42e5a3 100644</span>
<span class="quote">&gt; --- a/include/linux/page_ext.h</span>
<span class="quote">&gt; +++ b/include/linux/page_ext.h</span>
<span class="quote">&gt; @@ -29,6 +29,8 @@ enum page_ext_flags {</span>
<span class="quote">&gt;  	PAGE_EXT_DEBUG_POISON,		/* Page is poisoned */</span>
<span class="quote">&gt;  	PAGE_EXT_DEBUG_GUARD,</span>
<span class="quote">&gt;  	PAGE_EXT_OWNER,</span>
<span class="quote">&gt; +	PAGE_EXT_XPFO_KERNEL,		/* Page is a kernel page */</span>
<span class="quote">&gt; +	PAGE_EXT_XPFO_UNMAPPED,		/* Page is unmapped */</span>
<span class="quote">&gt;  #if defined(CONFIG_IDLE_PAGE_TRACKING) &amp;&amp; !defined(CONFIG_64BIT)</span>
<span class="quote">&gt;  	PAGE_EXT_YOUNG,</span>
<span class="quote">&gt;  	PAGE_EXT_IDLE,</span>
<span class="quote">&gt; @@ -44,6 +46,11 @@ enum page_ext_flags {</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  struct page_ext {</span>
<span class="quote">&gt;  	unsigned long flags;</span>
<span class="quote">&gt; +#ifdef CONFIG_XPFO</span>
<span class="quote">&gt; +	int inited;		/* Map counter and lock initialized */</span>
<span class="quote">&gt; +	atomic_t mapcount;	/* Counter for balancing map/unmap requests */</span>
<span class="quote">&gt; +	spinlock_t maplock;	/* Lock to serialize map/unmap requests */</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt;  };</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  extern void pgdat_page_ext_init(struct pglist_data *pgdat);</span>
<span class="quote">&gt; diff --git a/include/linux/xpfo.h b/include/linux/xpfo.h</span>
<span class="quote">&gt; new file mode 100644</span>
<span class="quote">&gt; index 000000000000..77187578ca33</span>
<span class="quote">&gt; --- /dev/null</span>
<span class="quote">&gt; +++ b/include/linux/xpfo.h</span>
<span class="quote">&gt; @@ -0,0 +1,39 @@</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * Copyright (C) 2016 Hewlett Packard Enterprise Development, L.P.</span>
<span class="quote">&gt; + * Copyright (C) 2016 Brown University. All rights reserved.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Authors:</span>
<span class="quote">&gt; + *   Juerg Haefliger &lt;juerg.haefliger@hpe.com&gt;</span>
<span class="quote">&gt; + *   Vasileios P. Kemerlis &lt;vpk@cs.brown.edu&gt;</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * This program is free software; you can redistribute it and/or modify it</span>
<span class="quote">&gt; + * under the terms of the GNU General Public License version 2 as published by</span>
<span class="quote">&gt; + * the Free Software Foundation.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#ifndef _LINUX_XPFO_H</span>
<span class="quote">&gt; +#define _LINUX_XPFO_H</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#ifdef CONFIG_XPFO</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +extern struct page_ext_operations page_xpfo_ops;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +extern void xpfo_kmap(void *kaddr, struct page *page);</span>
<span class="quote">&gt; +extern void xpfo_kunmap(void *kaddr, struct page *page);</span>
<span class="quote">&gt; +extern void xpfo_alloc_page(struct page *page, int order, gfp_t gfp);</span>
<span class="quote">&gt; +extern void xpfo_free_page(struct page *page, int order);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +extern bool xpfo_page_is_unmapped(struct page *page);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#else /* !CONFIG_XPFO */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline void xpfo_kmap(void *kaddr, struct page *page) { }</span>
<span class="quote">&gt; +static inline void xpfo_kunmap(void *kaddr, struct page *page) { }</span>
<span class="quote">&gt; +static inline void xpfo_alloc_page(struct page *page, int order, gfp_t gfp) { }</span>
<span class="quote">&gt; +static inline void xpfo_free_page(struct page *page, int order) { }</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline bool xpfo_page_is_unmapped(struct page *page) { return false; }</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#endif /* CONFIG_XPFO */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#endif /* _LINUX_XPFO_H */</span>
<span class="quote">&gt; diff --git a/lib/swiotlb.c b/lib/swiotlb.c</span>
<span class="quote">&gt; index 22e13a0e19d7..455eff44604e 100644</span>
<span class="quote">&gt; --- a/lib/swiotlb.c</span>
<span class="quote">&gt; +++ b/lib/swiotlb.c</span>
<span class="quote">&gt; @@ -390,8 +390,9 @@ static void swiotlb_bounce(phys_addr_t orig_addr, phys_addr_t tlb_addr,</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	unsigned long pfn = PFN_DOWN(orig_addr);</span>
<span class="quote">&gt;  	unsigned char *vaddr = phys_to_virt(tlb_addr);</span>
<span class="quote">&gt; +	struct page *page = pfn_to_page(pfn);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (PageHighMem(pfn_to_page(pfn))) {</span>
<span class="quote">&gt; +	if (PageHighMem(page) || xpfo_page_is_unmapped(page)) {</span>
<span class="quote">&gt;  		/* The buffer does not have a mapping.  Map it in and copy */</span>
<span class="quote">&gt;  		unsigned int offset = orig_addr &amp; ~PAGE_MASK;</span>
<span class="quote">&gt;  		char *buffer;</span>
<span class="quote">&gt; diff --git a/mm/Makefile b/mm/Makefile</span>
<span class="quote">&gt; index 295bd7a9f76b..175680f516aa 100644</span>
<span class="quote">&gt; --- a/mm/Makefile</span>
<span class="quote">&gt; +++ b/mm/Makefile</span>
<span class="quote">&gt; @@ -100,3 +100,4 @@ obj-$(CONFIG_IDLE_PAGE_TRACKING) += page_idle.o</span>
<span class="quote">&gt;  obj-$(CONFIG_FRAME_VECTOR) += frame_vector.o</span>
<span class="quote">&gt;  obj-$(CONFIG_DEBUG_PAGE_REF) += debug_page_ref.o</span>
<span class="quote">&gt;  obj-$(CONFIG_HARDENED_USERCOPY) += usercopy.o</span>
<span class="quote">&gt; +obj-$(CONFIG_XPFO) += xpfo.o</span>
<span class="quote">&gt; diff --git a/mm/page_alloc.c b/mm/page_alloc.c</span>
<span class="quote">&gt; index 8fd42aa7c4bd..100e80e008e2 100644</span>
<span class="quote">&gt; --- a/mm/page_alloc.c</span>
<span class="quote">&gt; +++ b/mm/page_alloc.c</span>
<span class="quote">&gt; @@ -1045,6 +1045,7 @@ static __always_inline bool free_pages_prepare(struct page *page,</span>
<span class="quote">&gt;  	kernel_poison_pages(page, 1 &lt;&lt; order, 0);</span>
<span class="quote">&gt;  	kernel_map_pages(page, 1 &lt;&lt; order, 0);</span>
<span class="quote">&gt;  	kasan_free_pages(page, order);</span>
<span class="quote">&gt; +	xpfo_free_page(page, order);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	return true;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; @@ -1745,6 +1746,7 @@ inline void post_alloc_hook(struct page *page, unsigned int order,</span>
<span class="quote">&gt;  	kernel_map_pages(page, 1 &lt;&lt; order, 1);</span>
<span class="quote">&gt;  	kernel_poison_pages(page, 1 &lt;&lt; order, 1);</span>
<span class="quote">&gt;  	kasan_alloc_pages(page, order);</span>
<span class="quote">&gt; +	xpfo_alloc_page(page, order, gfp_flags);</span>
<span class="quote">&gt;  	set_page_owner(page, order, gfp_flags);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/mm/page_ext.c b/mm/page_ext.c</span>
<span class="quote">&gt; index 121dcffc4ec1..ba6dbcacc2db 100644</span>
<span class="quote">&gt; --- a/mm/page_ext.c</span>
<span class="quote">&gt; +++ b/mm/page_ext.c</span>
<span class="quote">&gt; @@ -7,6 +7,7 @@</span>
<span class="quote">&gt;  #include &lt;linux/kmemleak.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/page_owner.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/page_idle.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/xpfo.h&gt;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt;   * struct page extension</span>
<span class="quote">&gt; @@ -68,6 +69,9 @@ static struct page_ext_operations *page_ext_ops[] = {</span>
<span class="quote">&gt;  #if defined(CONFIG_IDLE_PAGE_TRACKING) &amp;&amp; !defined(CONFIG_64BIT)</span>
<span class="quote">&gt;  	&amp;page_idle_ops,</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt; +#ifdef CONFIG_XPFO</span>
<span class="quote">&gt; +	&amp;page_xpfo_ops,</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt;  };</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static unsigned long total_usage;</span>
<span class="quote">&gt; diff --git a/mm/xpfo.c b/mm/xpfo.c</span>
<span class="quote">&gt; new file mode 100644</span>
<span class="quote">&gt; index 000000000000..8e3a6a694b6a</span>
<span class="quote">&gt; --- /dev/null</span>
<span class="quote">&gt; +++ b/mm/xpfo.c</span>
<span class="quote">&gt; @@ -0,0 +1,206 @@</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * Copyright (C) 2016 Hewlett Packard Enterprise Development, L.P.</span>
<span class="quote">&gt; + * Copyright (C) 2016 Brown University. All rights reserved.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Authors:</span>
<span class="quote">&gt; + *   Juerg Haefliger &lt;juerg.haefliger@hpe.com&gt;</span>
<span class="quote">&gt; + *   Vasileios P. Kemerlis &lt;vpk@cs.brown.edu&gt;</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * This program is free software; you can redistribute it and/or modify it</span>
<span class="quote">&gt; + * under the terms of the GNU General Public License version 2 as published by</span>
<span class="quote">&gt; + * the Free Software Foundation.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#include &lt;linux/mm.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/module.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/page_ext.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/xpfo.h&gt;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#include &lt;asm/tlbflush.h&gt;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +DEFINE_STATIC_KEY_FALSE(xpfo_inited);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static bool need_xpfo(void)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return true;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static void init_xpfo(void)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	printk(KERN_INFO &quot;XPFO enabled\n&quot;);</span>
<span class="quote">&gt; +	static_branch_enable(&amp;xpfo_inited);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +struct page_ext_operations page_xpfo_ops = {</span>
<span class="quote">&gt; +	.need = need_xpfo,</span>
<span class="quote">&gt; +	.init = init_xpfo,</span>
<span class="quote">&gt; +};</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * Update a single kernel page table entry</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +static inline void set_kpte(struct page *page, unsigned long kaddr,</span>
<span class="quote">&gt; +			    pgprot_t prot) {</span>
<span class="quote">&gt; +	unsigned int level;</span>
<span class="quote">&gt; +	pte_t *kpte = lookup_address(kaddr, &amp;level);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/* We only support 4k pages for now */</span>
<span class="quote">&gt; +	BUG_ON(!kpte || level != PG_LEVEL_4K);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	set_pte_atomic(kpte, pfn_pte(page_to_pfn(page), canon_pgprot(prot)));</span>
<span class="quote">&gt; +}</span>

As lookup_address() and set_pte_atomic() (and PG_LEVEL_4K), are arch-specific,
would it be better to put the whole definition into arch-specific part?
<span class="quote">
&gt; +</span>
<span class="quote">&gt; +void xpfo_alloc_page(struct page *page, int order, gfp_t gfp)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	int i, flush_tlb = 0;</span>
<span class="quote">&gt; +	struct page_ext *page_ext;</span>
<span class="quote">&gt; +	unsigned long kaddr;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (!static_branch_unlikely(&amp;xpfo_inited))</span>
<span class="quote">&gt; +		return;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	for (i = 0; i &lt; (1 &lt;&lt; order); i++)  {</span>
<span class="quote">&gt; +		page_ext = lookup_page_ext(page + i);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		BUG_ON(test_bit(PAGE_EXT_XPFO_UNMAPPED, &amp;page_ext-&gt;flags));</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		/* Initialize the map lock and map counter */</span>
<span class="quote">&gt; +		if (!page_ext-&gt;inited) {</span>
<span class="quote">&gt; +			spin_lock_init(&amp;page_ext-&gt;maplock);</span>
<span class="quote">&gt; +			atomic_set(&amp;page_ext-&gt;mapcount, 0);</span>
<span class="quote">&gt; +			page_ext-&gt;inited = 1;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +		BUG_ON(atomic_read(&amp;page_ext-&gt;mapcount));</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		if ((gfp &amp; GFP_HIGHUSER) == GFP_HIGHUSER) {</span>
<span class="quote">&gt; +			/*</span>
<span class="quote">&gt; +			 * Flush the TLB if the page was previously allocated</span>
<span class="quote">&gt; +			 * to the kernel.</span>
<span class="quote">&gt; +			 */</span>
<span class="quote">&gt; +			if (test_and_clear_bit(PAGE_EXT_XPFO_KERNEL,</span>
<span class="quote">&gt; +					       &amp;page_ext-&gt;flags))</span>
<span class="quote">&gt; +				flush_tlb = 1;</span>
<span class="quote">&gt; +		} else {</span>
<span class="quote">&gt; +			/* Tag the page as a kernel page */</span>
<span class="quote">&gt; +			set_bit(PAGE_EXT_XPFO_KERNEL, &amp;page_ext-&gt;flags);</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (flush_tlb) {</span>
<span class="quote">&gt; +		kaddr = (unsigned long)page_address(page);</span>
<span class="quote">&gt; +		flush_tlb_kernel_range(kaddr, kaddr + (1 &lt;&lt; order) *</span>
<span class="quote">&gt; +				       PAGE_SIZE);</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +void xpfo_free_page(struct page *page, int order)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	int i;</span>
<span class="quote">&gt; +	struct page_ext *page_ext;</span>
<span class="quote">&gt; +	unsigned long kaddr;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (!static_branch_unlikely(&amp;xpfo_inited))</span>
<span class="quote">&gt; +		return;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	for (i = 0; i &lt; (1 &lt;&lt; order); i++) {</span>
<span class="quote">&gt; +		page_ext = lookup_page_ext(page + i);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		if (!page_ext-&gt;inited) {</span>
<span class="quote">&gt; +			/*</span>
<span class="quote">&gt; +			 * The page was allocated before page_ext was</span>
<span class="quote">&gt; +			 * initialized, so it is a kernel page and it needs to</span>
<span class="quote">&gt; +			 * be tagged accordingly.</span>
<span class="quote">&gt; +			 */</span>
<span class="quote">&gt; +			set_bit(PAGE_EXT_XPFO_KERNEL, &amp;page_ext-&gt;flags);</span>
<span class="quote">&gt; +			continue;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		/*</span>
<span class="quote">&gt; +		 * Map the page back into the kernel if it was previously</span>
<span class="quote">&gt; +		 * allocated to user space.</span>
<span class="quote">&gt; +		 */</span>
<span class="quote">&gt; +		if (test_and_clear_bit(PAGE_EXT_XPFO_UNMAPPED,</span>
<span class="quote">&gt; +				       &amp;page_ext-&gt;flags)) {</span>
<span class="quote">&gt; +			kaddr = (unsigned long)page_address(page + i);</span>
<span class="quote">&gt; +			set_kpte(page + i,  kaddr, __pgprot(__PAGE_KERNEL));</span>

Why not PAGE_KERNEL?
<span class="quote">
&gt; +		}</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +void xpfo_kmap(void *kaddr, struct page *page)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct page_ext *page_ext;</span>
<span class="quote">&gt; +	unsigned long flags;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (!static_branch_unlikely(&amp;xpfo_inited))</span>
<span class="quote">&gt; +		return;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	page_ext = lookup_page_ext(page);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * The page was allocated before page_ext was initialized (which means</span>
<span class="quote">&gt; +	 * it&#39;s a kernel page) or it&#39;s allocated to the kernel, so nothing to</span>
<span class="quote">&gt; +	 * do.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	if (!page_ext-&gt;inited ||</span>
<span class="quote">&gt; +	    test_bit(PAGE_EXT_XPFO_KERNEL, &amp;page_ext-&gt;flags))</span>
<span class="quote">&gt; +		return;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	spin_lock_irqsave(&amp;page_ext-&gt;maplock, flags);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * The page was previously allocated to user space, so map it back</span>
<span class="quote">&gt; +	 * into the kernel. No TLB flush required.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	if ((atomic_inc_return(&amp;page_ext-&gt;mapcount) == 1) &amp;&amp;</span>
<span class="quote">&gt; +	    test_and_clear_bit(PAGE_EXT_XPFO_UNMAPPED, &amp;page_ext-&gt;flags))</span>
<span class="quote">&gt; +		set_kpte(page, (unsigned long)kaddr, __pgprot(__PAGE_KERNEL));</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	spin_unlock_irqrestore(&amp;page_ext-&gt;maplock, flags);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +EXPORT_SYMBOL(xpfo_kmap);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +void xpfo_kunmap(void *kaddr, struct page *page)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct page_ext *page_ext;</span>
<span class="quote">&gt; +	unsigned long flags;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (!static_branch_unlikely(&amp;xpfo_inited))</span>
<span class="quote">&gt; +		return;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	page_ext = lookup_page_ext(page);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * The page was allocated before page_ext was initialized (which means</span>
<span class="quote">&gt; +	 * it&#39;s a kernel page) or it&#39;s allocated to the kernel, so nothing to</span>
<span class="quote">&gt; +	 * do.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	if (!page_ext-&gt;inited ||</span>
<span class="quote">&gt; +	    test_bit(PAGE_EXT_XPFO_KERNEL, &amp;page_ext-&gt;flags))</span>
<span class="quote">&gt; +		return;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	spin_lock_irqsave(&amp;page_ext-&gt;maplock, flags);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * The page is to be allocated back to user space, so unmap it from the</span>
<span class="quote">&gt; +	 * kernel, flush the TLB and tag it as a user page.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	if (atomic_dec_return(&amp;page_ext-&gt;mapcount) == 0) {</span>
<span class="quote">&gt; +		BUG_ON(test_bit(PAGE_EXT_XPFO_UNMAPPED, &amp;page_ext-&gt;flags));</span>
<span class="quote">&gt; +		set_bit(PAGE_EXT_XPFO_UNMAPPED, &amp;page_ext-&gt;flags);</span>
<span class="quote">&gt; +		set_kpte(page, (unsigned long)kaddr, __pgprot(0));</span>
<span class="quote">&gt; +		__flush_tlb_one((unsigned long)kaddr);</span>

Again __flush_tlb_one() is x86-specific.
flush_tlb_kernel_range() instead?

Thanks,
-Takahiro AKASHI
<span class="quote">
&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	spin_unlock_irqrestore(&amp;page_ext-&gt;maplock, flags);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +EXPORT_SYMBOL(xpfo_kunmap);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +inline bool xpfo_page_is_unmapped(struct page *page)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	if (!static_branch_unlikely(&amp;xpfo_inited))</span>
<span class="quote">&gt; +		return false;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return test_bit(PAGE_EXT_XPFO_UNMAPPED, &amp;lookup_page_ext(page)-&gt;flags);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +EXPORT_SYMBOL(xpfo_page_is_unmapped);</span>
<span class="quote">&gt; diff --git a/security/Kconfig b/security/Kconfig</span>
<span class="quote">&gt; index 118f4549404e..4502e15c8419 100644</span>
<span class="quote">&gt; --- a/security/Kconfig</span>
<span class="quote">&gt; +++ b/security/Kconfig</span>
<span class="quote">&gt; @@ -6,6 +6,25 @@ menu &quot;Security options&quot;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  source security/keys/Kconfig</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +config ARCH_SUPPORTS_XPFO</span>
<span class="quote">&gt; +	bool</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +config XPFO</span>
<span class="quote">&gt; +	bool &quot;Enable eXclusive Page Frame Ownership (XPFO)&quot;</span>
<span class="quote">&gt; +	default n</span>
<span class="quote">&gt; +	depends on ARCH_SUPPORTS_XPFO</span>
<span class="quote">&gt; +	select PAGE_EXTENSION</span>
<span class="quote">&gt; +	help</span>
<span class="quote">&gt; +	  This option offers protection against &#39;ret2dir&#39; kernel attacks.</span>
<span class="quote">&gt; +	  When enabled, every time a page frame is allocated to user space, it</span>
<span class="quote">&gt; +	  is unmapped from the direct mapped RAM region in kernel space</span>
<span class="quote">&gt; +	  (physmap). Similarly, when a page frame is freed/reclaimed, it is</span>
<span class="quote">&gt; +	  mapped back to physmap.</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	  There is a slight performance impact when this option is enabled.</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	  If in doubt, say &quot;N&quot;.</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  config SECURITY_DMESG_RESTRICT</span>
<span class="quote">&gt;  	bool &quot;Restrict unprivileged access to the kernel syslog&quot;</span>
<span class="quote">&gt;  	default n</span>
<span class="quote">&gt; -- </span>
<span class="quote">&gt; 2.10.1</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=153531">Juerg Haefliger</a> - Nov. 28, 2016, 11:15 a.m.</div>
<pre class="content">
On 11/24/2016 11:56 AM, AKASHI Takahiro wrote:
<span class="quote">&gt; Hi,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I&#39;m trying to give it a spin on arm64, but ...</span>

Thanks for trying this.
<span class="quote">

&gt;&gt; +/*</span>
<span class="quote">&gt;&gt; + * Update a single kernel page table entry</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;&gt; +static inline void set_kpte(struct page *page, unsigned long kaddr,</span>
<span class="quote">&gt;&gt; +			    pgprot_t prot) {</span>
<span class="quote">&gt;&gt; +	unsigned int level;</span>
<span class="quote">&gt;&gt; +	pte_t *kpte = lookup_address(kaddr, &amp;level);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	/* We only support 4k pages for now */</span>
<span class="quote">&gt;&gt; +	BUG_ON(!kpte || level != PG_LEVEL_4K);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	set_pte_atomic(kpte, pfn_pte(page_to_pfn(page), canon_pgprot(prot)));</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; As lookup_address() and set_pte_atomic() (and PG_LEVEL_4K), are arch-specific,</span>
<span class="quote">&gt; would it be better to put the whole definition into arch-specific part?</span>

Well yes but I haven&#39;t really looked into splitting up the arch specific stuff.
<span class="quote">

&gt;&gt; +		/*</span>
<span class="quote">&gt;&gt; +		 * Map the page back into the kernel if it was previously</span>
<span class="quote">&gt;&gt; +		 * allocated to user space.</span>
<span class="quote">&gt;&gt; +		 */</span>
<span class="quote">&gt;&gt; +		if (test_and_clear_bit(PAGE_EXT_XPFO_UNMAPPED,</span>
<span class="quote">&gt;&gt; +				       &amp;page_ext-&gt;flags)) {</span>
<span class="quote">&gt;&gt; +			kaddr = (unsigned long)page_address(page + i);</span>
<span class="quote">&gt;&gt; +			set_kpte(page + i,  kaddr, __pgprot(__PAGE_KERNEL));</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Why not PAGE_KERNEL?</span>

Good catch, thanks!
<span class="quote">

&gt;&gt; +	/*</span>
<span class="quote">&gt;&gt; +	 * The page is to be allocated back to user space, so unmap it from the</span>
<span class="quote">&gt;&gt; +	 * kernel, flush the TLB and tag it as a user page.</span>
<span class="quote">&gt;&gt; +	 */</span>
<span class="quote">&gt;&gt; +	if (atomic_dec_return(&amp;page_ext-&gt;mapcount) == 0) {</span>
<span class="quote">&gt;&gt; +		BUG_ON(test_bit(PAGE_EXT_XPFO_UNMAPPED, &amp;page_ext-&gt;flags));</span>
<span class="quote">&gt;&gt; +		set_bit(PAGE_EXT_XPFO_UNMAPPED, &amp;page_ext-&gt;flags);</span>
<span class="quote">&gt;&gt; +		set_kpte(page, (unsigned long)kaddr, __pgprot(0));</span>
<span class="quote">&gt;&gt; +		__flush_tlb_one((unsigned long)kaddr);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Again __flush_tlb_one() is x86-specific.</span>
<span class="quote">&gt; flush_tlb_kernel_range() instead?</span>

I&#39;ll take a look. If you can tell me what the relevant arm64 equivalents are for the arch-specific
functions, that would help tremendously.

Thanks for the comments!

...Juerg
<span class="quote">


&gt; Thanks,</span>
<span class="quote">&gt; -Takahiro AKASHI</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=75021">Takahiro Akashi</a> - Dec. 9, 2016, 9:02 a.m.</div>
<pre class="content">
On Thu, Nov 24, 2016 at 07:56:30PM +0900, AKASHI Takahiro wrote:
<span class="quote">&gt; Hi,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I&#39;m trying to give it a spin on arm64, but ...</span>

In my experiment on hikey,
the kernel boot failed, catching a page fault around cache operations,
(a) __clean_dcache_area_pou() on 4KB-page kernel, 
(b) __inval_cache_range() on 64KB-page kernel,
(See more details for backtrace below.)

This is because, on arm64, cache operations are by VA (in particular,
of direct/linear mapping of physical memory). So I think that 
naively unmapping a page from physmap in xpfo_kunmap() won&#39;t work well
on arm64.

-Takahiro AKASHI

case (a)
--------
Unable to handle kernel paging request at virtual address ffff800000cba000
pgd = ffff80003ba8c000
*pgd=0000000000000000
task: ffff80003be38000 task.stack: ffff80003be40000
PC is at __clean_dcache_area_pou+0x20/0x38
LR is at sync_icache_aliases+0x2c/0x40
 ...
Call trace:
 ...
__clean_dcache_area_pou+0x20/0x38
__sync_icache_dcache+0x6c/0xa8
alloc_set_pte+0x33c/0x588
filemap_map_pages+0x3a8/0x3b8
handle_mm_fault+0x910/0x1080
do_page_fault+0x2b0/0x358
do_mem_abort+0x44/0xa0
el0_ia+0x18/0x1c

case (b)
--------
Unable to handle kernel paging request at virtual address ffff80002aed0000
pgd = ffff000008f40000
, *pud=000000003dfc0003
, *pmd=000000003dfa0003
, *pte=000000002aed0000
task: ffff800028711900 task.stack: ffff800029020000
PC is at __inval_cache_range+0x3c/0x60
LR is at __swiotlb_map_sg_attrs+0x6c/0x98
 ...

Call trace:
 ...
__inval_cache_range+0x3c/0x60
dw_mci_pre_dma_transfer.isra.7+0xfc/0x190
dw_mci_pre_req+0x50/0x60
mmc_start_req+0x4c/0x420
mmc_blk_issue_rw_rq+0xb0/0x9b8
mmc_blk_issue_rq+0x154/0x518
mmc_queue_thread+0xac/0x158
kthread+0xd0/0xe8
ret_from_fork+0x10/0x20
<span class="quote">

&gt; </span>
<span class="quote">&gt; On Fri, Nov 04, 2016 at 03:45:33PM +0100, Juerg Haefliger wrote:</span>
<span class="quote">&gt; &gt; This patch adds support for XPFO which protects against &#39;ret2dir&#39; kernel</span>
<span class="quote">&gt; &gt; attacks. The basic idea is to enforce exclusive ownership of page frames</span>
<span class="quote">&gt; &gt; by either the kernel or userspace, unless explicitly requested by the</span>
<span class="quote">&gt; &gt; kernel. Whenever a page destined for userspace is allocated, it is</span>
<span class="quote">&gt; &gt; unmapped from physmap (the kernel&#39;s page table). When such a page is</span>
<span class="quote">&gt; &gt; reclaimed from userspace, it is mapped back to physmap.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Additional fields in the page_ext struct are used for XPFO housekeeping.</span>
<span class="quote">&gt; &gt; Specifically two flags to distinguish user vs. kernel pages and to tag</span>
<span class="quote">&gt; &gt; unmapped pages and a reference counter to balance kmap/kunmap operations</span>
<span class="quote">&gt; &gt; and a lock to serialize access to the XPFO fields.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Known issues/limitations:</span>
<span class="quote">&gt; &gt;   - Only supports x86-64 (for now)</span>
<span class="quote">&gt; &gt;   - Only supports 4k pages (for now)</span>
<span class="quote">&gt; &gt;   - There are most likely some legitimate uses cases where the kernel needs</span>
<span class="quote">&gt; &gt;     to access userspace which need to be made XPFO-aware</span>
<span class="quote">&gt; &gt;   - Performance penalty</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Reference paper by the original patch authors:</span>
<span class="quote">&gt; &gt;   http://www.cs.columbia.edu/~vpk/papers/ret2dir.sec14.pdf</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Suggested-by: Vasileios P. Kemerlis &lt;vpk@cs.columbia.edu&gt;</span>
<span class="quote">&gt; &gt; Signed-off-by: Juerg Haefliger &lt;juerg.haefliger@hpe.com&gt;</span>
<span class="quote">&gt; &gt; ---</span>
<span class="quote">&gt; &gt;  arch/x86/Kconfig         |   3 +-</span>
<span class="quote">&gt; &gt;  arch/x86/mm/init.c       |   2 +-</span>
<span class="quote">&gt; &gt;  drivers/ata/libata-sff.c |   4 +-</span>
<span class="quote">&gt; &gt;  include/linux/highmem.h  |  15 +++-</span>
<span class="quote">&gt; &gt;  include/linux/page_ext.h |   7 ++</span>
<span class="quote">&gt; &gt;  include/linux/xpfo.h     |  39 +++++++++</span>
<span class="quote">&gt; &gt;  lib/swiotlb.c            |   3 +-</span>
<span class="quote">&gt; &gt;  mm/Makefile              |   1 +</span>
<span class="quote">&gt; &gt;  mm/page_alloc.c          |   2 +</span>
<span class="quote">&gt; &gt;  mm/page_ext.c            |   4 +</span>
<span class="quote">&gt; &gt;  mm/xpfo.c                | 206 +++++++++++++++++++++++++++++++++++++++++++++++</span>
<span class="quote">&gt; &gt;  security/Kconfig         |  19 +++++</span>
<span class="quote">&gt; &gt;  12 files changed, 298 insertions(+), 7 deletions(-)</span>
<span class="quote">&gt; &gt;  create mode 100644 include/linux/xpfo.h</span>
<span class="quote">&gt; &gt;  create mode 100644 mm/xpfo.c</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig</span>
<span class="quote">&gt; &gt; index bada636d1065..38b334f8fde5 100644</span>
<span class="quote">&gt; &gt; --- a/arch/x86/Kconfig</span>
<span class="quote">&gt; &gt; +++ b/arch/x86/Kconfig</span>
<span class="quote">&gt; &gt; @@ -165,6 +165,7 @@ config X86</span>
<span class="quote">&gt; &gt;  	select HAVE_STACK_VALIDATION		if X86_64</span>
<span class="quote">&gt; &gt;  	select ARCH_USES_HIGH_VMA_FLAGS		if X86_INTEL_MEMORY_PROTECTION_KEYS</span>
<span class="quote">&gt; &gt;  	select ARCH_HAS_PKEYS			if X86_INTEL_MEMORY_PROTECTION_KEYS</span>
<span class="quote">&gt; &gt; +	select ARCH_SUPPORTS_XPFO		if X86_64</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  config INSTRUCTION_DECODER</span>
<span class="quote">&gt; &gt;  	def_bool y</span>
<span class="quote">&gt; &gt; @@ -1361,7 +1362,7 @@ config ARCH_DMA_ADDR_T_64BIT</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  config X86_DIRECT_GBPAGES</span>
<span class="quote">&gt; &gt;  	def_bool y</span>
<span class="quote">&gt; &gt; -	depends on X86_64 &amp;&amp; !DEBUG_PAGEALLOC &amp;&amp; !KMEMCHECK</span>
<span class="quote">&gt; &gt; +	depends on X86_64 &amp;&amp; !DEBUG_PAGEALLOC &amp;&amp; !KMEMCHECK &amp;&amp; !XPFO</span>
<span class="quote">&gt; &gt;  	---help---</span>
<span class="quote">&gt; &gt;  	  Certain kernel features effectively disable kernel</span>
<span class="quote">&gt; &gt;  	  linear 1 GB mappings (even if the CPU otherwise</span>
<span class="quote">&gt; &gt; diff --git a/arch/x86/mm/init.c b/arch/x86/mm/init.c</span>
<span class="quote">&gt; &gt; index 22af912d66d2..a6fafbae02bb 100644</span>
<span class="quote">&gt; &gt; --- a/arch/x86/mm/init.c</span>
<span class="quote">&gt; &gt; +++ b/arch/x86/mm/init.c</span>
<span class="quote">&gt; &gt; @@ -161,7 +161,7 @@ static int page_size_mask;</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  static void __init probe_page_size_mask(void)</span>
<span class="quote">&gt; &gt;  {</span>
<span class="quote">&gt; &gt; -#if !defined(CONFIG_KMEMCHECK)</span>
<span class="quote">&gt; &gt; +#if !defined(CONFIG_KMEMCHECK) &amp;&amp; !defined(CONFIG_XPFO)</span>
<span class="quote">&gt; &gt;  	/*</span>
<span class="quote">&gt; &gt;  	 * For CONFIG_KMEMCHECK or pagealloc debugging, identity mapping will</span>
<span class="quote">&gt; &gt;  	 * use small pages.</span>
<span class="quote">&gt; &gt; diff --git a/drivers/ata/libata-sff.c b/drivers/ata/libata-sff.c</span>
<span class="quote">&gt; &gt; index 051b6158d1b7..58af734be25d 100644</span>
<span class="quote">&gt; &gt; --- a/drivers/ata/libata-sff.c</span>
<span class="quote">&gt; &gt; +++ b/drivers/ata/libata-sff.c</span>
<span class="quote">&gt; &gt; @@ -715,7 +715,7 @@ static void ata_pio_sector(struct ata_queued_cmd *qc)</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  	DPRINTK(&quot;data %s\n&quot;, qc-&gt;tf.flags &amp; ATA_TFLAG_WRITE ? &quot;write&quot; : &quot;read&quot;);</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; -	if (PageHighMem(page)) {</span>
<span class="quote">&gt; &gt; +	if (PageHighMem(page) || xpfo_page_is_unmapped(page)) {</span>
<span class="quote">&gt; &gt;  		unsigned long flags;</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  		/* FIXME: use a bounce buffer */</span>
<span class="quote">&gt; &gt; @@ -860,7 +860,7 @@ static int __atapi_pio_bytes(struct ata_queued_cmd *qc, unsigned int bytes)</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  	DPRINTK(&quot;data %s\n&quot;, qc-&gt;tf.flags &amp; ATA_TFLAG_WRITE ? &quot;write&quot; : &quot;read&quot;);</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; -	if (PageHighMem(page)) {</span>
<span class="quote">&gt; &gt; +	if (PageHighMem(page) || xpfo_page_is_unmapped(page)) {</span>
<span class="quote">&gt; &gt;  		unsigned long flags;</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  		/* FIXME: use bounce buffer */</span>
<span class="quote">&gt; &gt; diff --git a/include/linux/highmem.h b/include/linux/highmem.h</span>
<span class="quote">&gt; &gt; index bb3f3297062a..7a17c166532f 100644</span>
<span class="quote">&gt; &gt; --- a/include/linux/highmem.h</span>
<span class="quote">&gt; &gt; +++ b/include/linux/highmem.h</span>
<span class="quote">&gt; &gt; @@ -7,6 +7,7 @@</span>
<span class="quote">&gt; &gt;  #include &lt;linux/mm.h&gt;</span>
<span class="quote">&gt; &gt;  #include &lt;linux/uaccess.h&gt;</span>
<span class="quote">&gt; &gt;  #include &lt;linux/hardirq.h&gt;</span>
<span class="quote">&gt; &gt; +#include &lt;linux/xpfo.h&gt;</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  #include &lt;asm/cacheflush.h&gt;</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; @@ -55,24 +56,34 @@ static inline struct page *kmap_to_page(void *addr)</span>
<span class="quote">&gt; &gt;  #ifndef ARCH_HAS_KMAP</span>
<span class="quote">&gt; &gt;  static inline void *kmap(struct page *page)</span>
<span class="quote">&gt; &gt;  {</span>
<span class="quote">&gt; &gt; +	void *kaddr;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt;  	might_sleep();</span>
<span class="quote">&gt; &gt; -	return page_address(page);</span>
<span class="quote">&gt; &gt; +	kaddr = page_address(page);</span>
<span class="quote">&gt; &gt; +	xpfo_kmap(kaddr, page);</span>
<span class="quote">&gt; &gt; +	return kaddr;</span>
<span class="quote">&gt; &gt;  }</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  static inline void kunmap(struct page *page)</span>
<span class="quote">&gt; &gt;  {</span>
<span class="quote">&gt; &gt; +	xpfo_kunmap(page_address(page), page);</span>
<span class="quote">&gt; &gt;  }</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  static inline void *kmap_atomic(struct page *page)</span>
<span class="quote">&gt; &gt;  {</span>
<span class="quote">&gt; &gt; +	void *kaddr;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt;  	preempt_disable();</span>
<span class="quote">&gt; &gt;  	pagefault_disable();</span>
<span class="quote">&gt; &gt; -	return page_address(page);</span>
<span class="quote">&gt; &gt; +	kaddr = page_address(page);</span>
<span class="quote">&gt; &gt; +	xpfo_kmap(kaddr, page);</span>
<span class="quote">&gt; &gt; +	return kaddr;</span>
<span class="quote">&gt; &gt;  }</span>
<span class="quote">&gt; &gt;  #define kmap_atomic_prot(page, prot)	kmap_atomic(page)</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  static inline void __kunmap_atomic(void *addr)</span>
<span class="quote">&gt; &gt;  {</span>
<span class="quote">&gt; &gt; +	xpfo_kunmap(addr, virt_to_page(addr));</span>
<span class="quote">&gt; &gt;  	pagefault_enable();</span>
<span class="quote">&gt; &gt;  	preempt_enable();</span>
<span class="quote">&gt; &gt;  }</span>
<span class="quote">&gt; &gt; diff --git a/include/linux/page_ext.h b/include/linux/page_ext.h</span>
<span class="quote">&gt; &gt; index 9298c393ddaa..0e451a42e5a3 100644</span>
<span class="quote">&gt; &gt; --- a/include/linux/page_ext.h</span>
<span class="quote">&gt; &gt; +++ b/include/linux/page_ext.h</span>
<span class="quote">&gt; &gt; @@ -29,6 +29,8 @@ enum page_ext_flags {</span>
<span class="quote">&gt; &gt;  	PAGE_EXT_DEBUG_POISON,		/* Page is poisoned */</span>
<span class="quote">&gt; &gt;  	PAGE_EXT_DEBUG_GUARD,</span>
<span class="quote">&gt; &gt;  	PAGE_EXT_OWNER,</span>
<span class="quote">&gt; &gt; +	PAGE_EXT_XPFO_KERNEL,		/* Page is a kernel page */</span>
<span class="quote">&gt; &gt; +	PAGE_EXT_XPFO_UNMAPPED,		/* Page is unmapped */</span>
<span class="quote">&gt; &gt;  #if defined(CONFIG_IDLE_PAGE_TRACKING) &amp;&amp; !defined(CONFIG_64BIT)</span>
<span class="quote">&gt; &gt;  	PAGE_EXT_YOUNG,</span>
<span class="quote">&gt; &gt;  	PAGE_EXT_IDLE,</span>
<span class="quote">&gt; &gt; @@ -44,6 +46,11 @@ enum page_ext_flags {</span>
<span class="quote">&gt; &gt;   */</span>
<span class="quote">&gt; &gt;  struct page_ext {</span>
<span class="quote">&gt; &gt;  	unsigned long flags;</span>
<span class="quote">&gt; &gt; +#ifdef CONFIG_XPFO</span>
<span class="quote">&gt; &gt; +	int inited;		/* Map counter and lock initialized */</span>
<span class="quote">&gt; &gt; +	atomic_t mapcount;	/* Counter for balancing map/unmap requests */</span>
<span class="quote">&gt; &gt; +	spinlock_t maplock;	/* Lock to serialize map/unmap requests */</span>
<span class="quote">&gt; &gt; +#endif</span>
<span class="quote">&gt; &gt;  };</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  extern void pgdat_page_ext_init(struct pglist_data *pgdat);</span>
<span class="quote">&gt; &gt; diff --git a/include/linux/xpfo.h b/include/linux/xpfo.h</span>
<span class="quote">&gt; &gt; new file mode 100644</span>
<span class="quote">&gt; &gt; index 000000000000..77187578ca33</span>
<span class="quote">&gt; &gt; --- /dev/null</span>
<span class="quote">&gt; &gt; +++ b/include/linux/xpfo.h</span>
<span class="quote">&gt; &gt; @@ -0,0 +1,39 @@</span>
<span class="quote">&gt; &gt; +/*</span>
<span class="quote">&gt; &gt; + * Copyright (C) 2016 Hewlett Packard Enterprise Development, L.P.</span>
<span class="quote">&gt; &gt; + * Copyright (C) 2016 Brown University. All rights reserved.</span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; &gt; + * Authors:</span>
<span class="quote">&gt; &gt; + *   Juerg Haefliger &lt;juerg.haefliger@hpe.com&gt;</span>
<span class="quote">&gt; &gt; + *   Vasileios P. Kemerlis &lt;vpk@cs.brown.edu&gt;</span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; &gt; + * This program is free software; you can redistribute it and/or modify it</span>
<span class="quote">&gt; &gt; + * under the terms of the GNU General Public License version 2 as published by</span>
<span class="quote">&gt; &gt; + * the Free Software Foundation.</span>
<span class="quote">&gt; &gt; + */</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +#ifndef _LINUX_XPFO_H</span>
<span class="quote">&gt; &gt; +#define _LINUX_XPFO_H</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +#ifdef CONFIG_XPFO</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +extern struct page_ext_operations page_xpfo_ops;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +extern void xpfo_kmap(void *kaddr, struct page *page);</span>
<span class="quote">&gt; &gt; +extern void xpfo_kunmap(void *kaddr, struct page *page);</span>
<span class="quote">&gt; &gt; +extern void xpfo_alloc_page(struct page *page, int order, gfp_t gfp);</span>
<span class="quote">&gt; &gt; +extern void xpfo_free_page(struct page *page, int order);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +extern bool xpfo_page_is_unmapped(struct page *page);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +#else /* !CONFIG_XPFO */</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static inline void xpfo_kmap(void *kaddr, struct page *page) { }</span>
<span class="quote">&gt; &gt; +static inline void xpfo_kunmap(void *kaddr, struct page *page) { }</span>
<span class="quote">&gt; &gt; +static inline void xpfo_alloc_page(struct page *page, int order, gfp_t gfp) { }</span>
<span class="quote">&gt; &gt; +static inline void xpfo_free_page(struct page *page, int order) { }</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static inline bool xpfo_page_is_unmapped(struct page *page) { return false; }</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +#endif /* CONFIG_XPFO */</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +#endif /* _LINUX_XPFO_H */</span>
<span class="quote">&gt; &gt; diff --git a/lib/swiotlb.c b/lib/swiotlb.c</span>
<span class="quote">&gt; &gt; index 22e13a0e19d7..455eff44604e 100644</span>
<span class="quote">&gt; &gt; --- a/lib/swiotlb.c</span>
<span class="quote">&gt; &gt; +++ b/lib/swiotlb.c</span>
<span class="quote">&gt; &gt; @@ -390,8 +390,9 @@ static void swiotlb_bounce(phys_addr_t orig_addr, phys_addr_t tlb_addr,</span>
<span class="quote">&gt; &gt;  {</span>
<span class="quote">&gt; &gt;  	unsigned long pfn = PFN_DOWN(orig_addr);</span>
<span class="quote">&gt; &gt;  	unsigned char *vaddr = phys_to_virt(tlb_addr);</span>
<span class="quote">&gt; &gt; +	struct page *page = pfn_to_page(pfn);</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; -	if (PageHighMem(pfn_to_page(pfn))) {</span>
<span class="quote">&gt; &gt; +	if (PageHighMem(page) || xpfo_page_is_unmapped(page)) {</span>
<span class="quote">&gt; &gt;  		/* The buffer does not have a mapping.  Map it in and copy */</span>
<span class="quote">&gt; &gt;  		unsigned int offset = orig_addr &amp; ~PAGE_MASK;</span>
<span class="quote">&gt; &gt;  		char *buffer;</span>
<span class="quote">&gt; &gt; diff --git a/mm/Makefile b/mm/Makefile</span>
<span class="quote">&gt; &gt; index 295bd7a9f76b..175680f516aa 100644</span>
<span class="quote">&gt; &gt; --- a/mm/Makefile</span>
<span class="quote">&gt; &gt; +++ b/mm/Makefile</span>
<span class="quote">&gt; &gt; @@ -100,3 +100,4 @@ obj-$(CONFIG_IDLE_PAGE_TRACKING) += page_idle.o</span>
<span class="quote">&gt; &gt;  obj-$(CONFIG_FRAME_VECTOR) += frame_vector.o</span>
<span class="quote">&gt; &gt;  obj-$(CONFIG_DEBUG_PAGE_REF) += debug_page_ref.o</span>
<span class="quote">&gt; &gt;  obj-$(CONFIG_HARDENED_USERCOPY) += usercopy.o</span>
<span class="quote">&gt; &gt; +obj-$(CONFIG_XPFO) += xpfo.o</span>
<span class="quote">&gt; &gt; diff --git a/mm/page_alloc.c b/mm/page_alloc.c</span>
<span class="quote">&gt; &gt; index 8fd42aa7c4bd..100e80e008e2 100644</span>
<span class="quote">&gt; &gt; --- a/mm/page_alloc.c</span>
<span class="quote">&gt; &gt; +++ b/mm/page_alloc.c</span>
<span class="quote">&gt; &gt; @@ -1045,6 +1045,7 @@ static __always_inline bool free_pages_prepare(struct page *page,</span>
<span class="quote">&gt; &gt;  	kernel_poison_pages(page, 1 &lt;&lt; order, 0);</span>
<span class="quote">&gt; &gt;  	kernel_map_pages(page, 1 &lt;&lt; order, 0);</span>
<span class="quote">&gt; &gt;  	kasan_free_pages(page, order);</span>
<span class="quote">&gt; &gt; +	xpfo_free_page(page, order);</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  	return true;</span>
<span class="quote">&gt; &gt;  }</span>
<span class="quote">&gt; &gt; @@ -1745,6 +1746,7 @@ inline void post_alloc_hook(struct page *page, unsigned int order,</span>
<span class="quote">&gt; &gt;  	kernel_map_pages(page, 1 &lt;&lt; order, 1);</span>
<span class="quote">&gt; &gt;  	kernel_poison_pages(page, 1 &lt;&lt; order, 1);</span>
<span class="quote">&gt; &gt;  	kasan_alloc_pages(page, order);</span>
<span class="quote">&gt; &gt; +	xpfo_alloc_page(page, order, gfp_flags);</span>
<span class="quote">&gt; &gt;  	set_page_owner(page, order, gfp_flags);</span>
<span class="quote">&gt; &gt;  }</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; diff --git a/mm/page_ext.c b/mm/page_ext.c</span>
<span class="quote">&gt; &gt; index 121dcffc4ec1..ba6dbcacc2db 100644</span>
<span class="quote">&gt; &gt; --- a/mm/page_ext.c</span>
<span class="quote">&gt; &gt; +++ b/mm/page_ext.c</span>
<span class="quote">&gt; &gt; @@ -7,6 +7,7 @@</span>
<span class="quote">&gt; &gt;  #include &lt;linux/kmemleak.h&gt;</span>
<span class="quote">&gt; &gt;  #include &lt;linux/page_owner.h&gt;</span>
<span class="quote">&gt; &gt;  #include &lt;linux/page_idle.h&gt;</span>
<span class="quote">&gt; &gt; +#include &lt;linux/xpfo.h&gt;</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  /*</span>
<span class="quote">&gt; &gt;   * struct page extension</span>
<span class="quote">&gt; &gt; @@ -68,6 +69,9 @@ static struct page_ext_operations *page_ext_ops[] = {</span>
<span class="quote">&gt; &gt;  #if defined(CONFIG_IDLE_PAGE_TRACKING) &amp;&amp; !defined(CONFIG_64BIT)</span>
<span class="quote">&gt; &gt;  	&amp;page_idle_ops,</span>
<span class="quote">&gt; &gt;  #endif</span>
<span class="quote">&gt; &gt; +#ifdef CONFIG_XPFO</span>
<span class="quote">&gt; &gt; +	&amp;page_xpfo_ops,</span>
<span class="quote">&gt; &gt; +#endif</span>
<span class="quote">&gt; &gt;  };</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  static unsigned long total_usage;</span>
<span class="quote">&gt; &gt; diff --git a/mm/xpfo.c b/mm/xpfo.c</span>
<span class="quote">&gt; &gt; new file mode 100644</span>
<span class="quote">&gt; &gt; index 000000000000..8e3a6a694b6a</span>
<span class="quote">&gt; &gt; --- /dev/null</span>
<span class="quote">&gt; &gt; +++ b/mm/xpfo.c</span>
<span class="quote">&gt; &gt; @@ -0,0 +1,206 @@</span>
<span class="quote">&gt; &gt; +/*</span>
<span class="quote">&gt; &gt; + * Copyright (C) 2016 Hewlett Packard Enterprise Development, L.P.</span>
<span class="quote">&gt; &gt; + * Copyright (C) 2016 Brown University. All rights reserved.</span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; &gt; + * Authors:</span>
<span class="quote">&gt; &gt; + *   Juerg Haefliger &lt;juerg.haefliger@hpe.com&gt;</span>
<span class="quote">&gt; &gt; + *   Vasileios P. Kemerlis &lt;vpk@cs.brown.edu&gt;</span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; &gt; + * This program is free software; you can redistribute it and/or modify it</span>
<span class="quote">&gt; &gt; + * under the terms of the GNU General Public License version 2 as published by</span>
<span class="quote">&gt; &gt; + * the Free Software Foundation.</span>
<span class="quote">&gt; &gt; + */</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +#include &lt;linux/mm.h&gt;</span>
<span class="quote">&gt; &gt; +#include &lt;linux/module.h&gt;</span>
<span class="quote">&gt; &gt; +#include &lt;linux/page_ext.h&gt;</span>
<span class="quote">&gt; &gt; +#include &lt;linux/xpfo.h&gt;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +#include &lt;asm/tlbflush.h&gt;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +DEFINE_STATIC_KEY_FALSE(xpfo_inited);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static bool need_xpfo(void)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	return true;</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static void init_xpfo(void)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	printk(KERN_INFO &quot;XPFO enabled\n&quot;);</span>
<span class="quote">&gt; &gt; +	static_branch_enable(&amp;xpfo_inited);</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +struct page_ext_operations page_xpfo_ops = {</span>
<span class="quote">&gt; &gt; +	.need = need_xpfo,</span>
<span class="quote">&gt; &gt; +	.init = init_xpfo,</span>
<span class="quote">&gt; &gt; +};</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +/*</span>
<span class="quote">&gt; &gt; + * Update a single kernel page table entry</span>
<span class="quote">&gt; &gt; + */</span>
<span class="quote">&gt; &gt; +static inline void set_kpte(struct page *page, unsigned long kaddr,</span>
<span class="quote">&gt; &gt; +			    pgprot_t prot) {</span>
<span class="quote">&gt; &gt; +	unsigned int level;</span>
<span class="quote">&gt; &gt; +	pte_t *kpte = lookup_address(kaddr, &amp;level);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	/* We only support 4k pages for now */</span>
<span class="quote">&gt; &gt; +	BUG_ON(!kpte || level != PG_LEVEL_4K);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	set_pte_atomic(kpte, pfn_pte(page_to_pfn(page), canon_pgprot(prot)));</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; As lookup_address() and set_pte_atomic() (and PG_LEVEL_4K), are arch-specific,</span>
<span class="quote">&gt; would it be better to put the whole definition into arch-specific part?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +void xpfo_alloc_page(struct page *page, int order, gfp_t gfp)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	int i, flush_tlb = 0;</span>
<span class="quote">&gt; &gt; +	struct page_ext *page_ext;</span>
<span class="quote">&gt; &gt; +	unsigned long kaddr;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	if (!static_branch_unlikely(&amp;xpfo_inited))</span>
<span class="quote">&gt; &gt; +		return;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	for (i = 0; i &lt; (1 &lt;&lt; order); i++)  {</span>
<span class="quote">&gt; &gt; +		page_ext = lookup_page_ext(page + i);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		BUG_ON(test_bit(PAGE_EXT_XPFO_UNMAPPED, &amp;page_ext-&gt;flags));</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		/* Initialize the map lock and map counter */</span>
<span class="quote">&gt; &gt; +		if (!page_ext-&gt;inited) {</span>
<span class="quote">&gt; &gt; +			spin_lock_init(&amp;page_ext-&gt;maplock);</span>
<span class="quote">&gt; &gt; +			atomic_set(&amp;page_ext-&gt;mapcount, 0);</span>
<span class="quote">&gt; &gt; +			page_ext-&gt;inited = 1;</span>
<span class="quote">&gt; &gt; +		}</span>
<span class="quote">&gt; &gt; +		BUG_ON(atomic_read(&amp;page_ext-&gt;mapcount));</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		if ((gfp &amp; GFP_HIGHUSER) == GFP_HIGHUSER) {</span>
<span class="quote">&gt; &gt; +			/*</span>
<span class="quote">&gt; &gt; +			 * Flush the TLB if the page was previously allocated</span>
<span class="quote">&gt; &gt; +			 * to the kernel.</span>
<span class="quote">&gt; &gt; +			 */</span>
<span class="quote">&gt; &gt; +			if (test_and_clear_bit(PAGE_EXT_XPFO_KERNEL,</span>
<span class="quote">&gt; &gt; +					       &amp;page_ext-&gt;flags))</span>
<span class="quote">&gt; &gt; +				flush_tlb = 1;</span>
<span class="quote">&gt; &gt; +		} else {</span>
<span class="quote">&gt; &gt; +			/* Tag the page as a kernel page */</span>
<span class="quote">&gt; &gt; +			set_bit(PAGE_EXT_XPFO_KERNEL, &amp;page_ext-&gt;flags);</span>
<span class="quote">&gt; &gt; +		}</span>
<span class="quote">&gt; &gt; +	}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	if (flush_tlb) {</span>
<span class="quote">&gt; &gt; +		kaddr = (unsigned long)page_address(page);</span>
<span class="quote">&gt; &gt; +		flush_tlb_kernel_range(kaddr, kaddr + (1 &lt;&lt; order) *</span>
<span class="quote">&gt; &gt; +				       PAGE_SIZE);</span>
<span class="quote">&gt; &gt; +	}</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +void xpfo_free_page(struct page *page, int order)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	int i;</span>
<span class="quote">&gt; &gt; +	struct page_ext *page_ext;</span>
<span class="quote">&gt; &gt; +	unsigned long kaddr;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	if (!static_branch_unlikely(&amp;xpfo_inited))</span>
<span class="quote">&gt; &gt; +		return;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	for (i = 0; i &lt; (1 &lt;&lt; order); i++) {</span>
<span class="quote">&gt; &gt; +		page_ext = lookup_page_ext(page + i);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		if (!page_ext-&gt;inited) {</span>
<span class="quote">&gt; &gt; +			/*</span>
<span class="quote">&gt; &gt; +			 * The page was allocated before page_ext was</span>
<span class="quote">&gt; &gt; +			 * initialized, so it is a kernel page and it needs to</span>
<span class="quote">&gt; &gt; +			 * be tagged accordingly.</span>
<span class="quote">&gt; &gt; +			 */</span>
<span class="quote">&gt; &gt; +			set_bit(PAGE_EXT_XPFO_KERNEL, &amp;page_ext-&gt;flags);</span>
<span class="quote">&gt; &gt; +			continue;</span>
<span class="quote">&gt; &gt; +		}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		/*</span>
<span class="quote">&gt; &gt; +		 * Map the page back into the kernel if it was previously</span>
<span class="quote">&gt; &gt; +		 * allocated to user space.</span>
<span class="quote">&gt; &gt; +		 */</span>
<span class="quote">&gt; &gt; +		if (test_and_clear_bit(PAGE_EXT_XPFO_UNMAPPED,</span>
<span class="quote">&gt; &gt; +				       &amp;page_ext-&gt;flags)) {</span>
<span class="quote">&gt; &gt; +			kaddr = (unsigned long)page_address(page + i);</span>
<span class="quote">&gt; &gt; +			set_kpte(page + i,  kaddr, __pgprot(__PAGE_KERNEL));</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Why not PAGE_KERNEL?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; +		}</span>
<span class="quote">&gt; &gt; +	}</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +void xpfo_kmap(void *kaddr, struct page *page)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	struct page_ext *page_ext;</span>
<span class="quote">&gt; &gt; +	unsigned long flags;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	if (!static_branch_unlikely(&amp;xpfo_inited))</span>
<span class="quote">&gt; &gt; +		return;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	page_ext = lookup_page_ext(page);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	/*</span>
<span class="quote">&gt; &gt; +	 * The page was allocated before page_ext was initialized (which means</span>
<span class="quote">&gt; &gt; +	 * it&#39;s a kernel page) or it&#39;s allocated to the kernel, so nothing to</span>
<span class="quote">&gt; &gt; +	 * do.</span>
<span class="quote">&gt; &gt; +	 */</span>
<span class="quote">&gt; &gt; +	if (!page_ext-&gt;inited ||</span>
<span class="quote">&gt; &gt; +	    test_bit(PAGE_EXT_XPFO_KERNEL, &amp;page_ext-&gt;flags))</span>
<span class="quote">&gt; &gt; +		return;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	spin_lock_irqsave(&amp;page_ext-&gt;maplock, flags);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	/*</span>
<span class="quote">&gt; &gt; +	 * The page was previously allocated to user space, so map it back</span>
<span class="quote">&gt; &gt; +	 * into the kernel. No TLB flush required.</span>
<span class="quote">&gt; &gt; +	 */</span>
<span class="quote">&gt; &gt; +	if ((atomic_inc_return(&amp;page_ext-&gt;mapcount) == 1) &amp;&amp;</span>
<span class="quote">&gt; &gt; +	    test_and_clear_bit(PAGE_EXT_XPFO_UNMAPPED, &amp;page_ext-&gt;flags))</span>
<span class="quote">&gt; &gt; +		set_kpte(page, (unsigned long)kaddr, __pgprot(__PAGE_KERNEL));</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	spin_unlock_irqrestore(&amp;page_ext-&gt;maplock, flags);</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +EXPORT_SYMBOL(xpfo_kmap);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +void xpfo_kunmap(void *kaddr, struct page *page)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	struct page_ext *page_ext;</span>
<span class="quote">&gt; &gt; +	unsigned long flags;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	if (!static_branch_unlikely(&amp;xpfo_inited))</span>
<span class="quote">&gt; &gt; +		return;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	page_ext = lookup_page_ext(page);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	/*</span>
<span class="quote">&gt; &gt; +	 * The page was allocated before page_ext was initialized (which means</span>
<span class="quote">&gt; &gt; +	 * it&#39;s a kernel page) or it&#39;s allocated to the kernel, so nothing to</span>
<span class="quote">&gt; &gt; +	 * do.</span>
<span class="quote">&gt; &gt; +	 */</span>
<span class="quote">&gt; &gt; +	if (!page_ext-&gt;inited ||</span>
<span class="quote">&gt; &gt; +	    test_bit(PAGE_EXT_XPFO_KERNEL, &amp;page_ext-&gt;flags))</span>
<span class="quote">&gt; &gt; +		return;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	spin_lock_irqsave(&amp;page_ext-&gt;maplock, flags);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	/*</span>
<span class="quote">&gt; &gt; +	 * The page is to be allocated back to user space, so unmap it from the</span>
<span class="quote">&gt; &gt; +	 * kernel, flush the TLB and tag it as a user page.</span>
<span class="quote">&gt; &gt; +	 */</span>
<span class="quote">&gt; &gt; +	if (atomic_dec_return(&amp;page_ext-&gt;mapcount) == 0) {</span>
<span class="quote">&gt; &gt; +		BUG_ON(test_bit(PAGE_EXT_XPFO_UNMAPPED, &amp;page_ext-&gt;flags));</span>
<span class="quote">&gt; &gt; +		set_bit(PAGE_EXT_XPFO_UNMAPPED, &amp;page_ext-&gt;flags);</span>
<span class="quote">&gt; &gt; +		set_kpte(page, (unsigned long)kaddr, __pgprot(0));</span>
<span class="quote">&gt; &gt; +		__flush_tlb_one((unsigned long)kaddr);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Again __flush_tlb_one() is x86-specific.</span>
<span class="quote">&gt; flush_tlb_kernel_range() instead?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Thanks,</span>
<span class="quote">&gt; -Takahiro AKASHI</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; +	}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	spin_unlock_irqrestore(&amp;page_ext-&gt;maplock, flags);</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +EXPORT_SYMBOL(xpfo_kunmap);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +inline bool xpfo_page_is_unmapped(struct page *page)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	if (!static_branch_unlikely(&amp;xpfo_inited))</span>
<span class="quote">&gt; &gt; +		return false;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	return test_bit(PAGE_EXT_XPFO_UNMAPPED, &amp;lookup_page_ext(page)-&gt;flags);</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +EXPORT_SYMBOL(xpfo_page_is_unmapped);</span>
<span class="quote">&gt; &gt; diff --git a/security/Kconfig b/security/Kconfig</span>
<span class="quote">&gt; &gt; index 118f4549404e..4502e15c8419 100644</span>
<span class="quote">&gt; &gt; --- a/security/Kconfig</span>
<span class="quote">&gt; &gt; +++ b/security/Kconfig</span>
<span class="quote">&gt; &gt; @@ -6,6 +6,25 @@ menu &quot;Security options&quot;</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  source security/keys/Kconfig</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; +config ARCH_SUPPORTS_XPFO</span>
<span class="quote">&gt; &gt; +	bool</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +config XPFO</span>
<span class="quote">&gt; &gt; +	bool &quot;Enable eXclusive Page Frame Ownership (XPFO)&quot;</span>
<span class="quote">&gt; &gt; +	default n</span>
<span class="quote">&gt; &gt; +	depends on ARCH_SUPPORTS_XPFO</span>
<span class="quote">&gt; &gt; +	select PAGE_EXTENSION</span>
<span class="quote">&gt; &gt; +	help</span>
<span class="quote">&gt; &gt; +	  This option offers protection against &#39;ret2dir&#39; kernel attacks.</span>
<span class="quote">&gt; &gt; +	  When enabled, every time a page frame is allocated to user space, it</span>
<span class="quote">&gt; &gt; +	  is unmapped from the direct mapped RAM region in kernel space</span>
<span class="quote">&gt; &gt; +	  (physmap). Similarly, when a page frame is freed/reclaimed, it is</span>
<span class="quote">&gt; &gt; +	  mapped back to physmap.</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	  There is a slight performance impact when this option is enabled.</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	  If in doubt, say &quot;N&quot;.</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt;  config SECURITY_DMESG_RESTRICT</span>
<span class="quote">&gt; &gt;  	bool &quot;Restrict unprivileged access to the kernel syslog&quot;</span>
<span class="quote">&gt; &gt;  	default n</span>
<span class="quote">&gt; &gt; -- </span>
<span class="quote">&gt; &gt; 2.10.1</span>
<span class="quote">&gt; &gt;</span>
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig</span>
<span class="p_header">index bada636d1065..38b334f8fde5 100644</span>
<span class="p_header">--- a/arch/x86/Kconfig</span>
<span class="p_header">+++ b/arch/x86/Kconfig</span>
<span class="p_chunk">@@ -165,6 +165,7 @@</span> <span class="p_context"> config X86</span>
 	select HAVE_STACK_VALIDATION		if X86_64
 	select ARCH_USES_HIGH_VMA_FLAGS		if X86_INTEL_MEMORY_PROTECTION_KEYS
 	select ARCH_HAS_PKEYS			if X86_INTEL_MEMORY_PROTECTION_KEYS
<span class="p_add">+	select ARCH_SUPPORTS_XPFO		if X86_64</span>
 
 config INSTRUCTION_DECODER
 	def_bool y
<span class="p_chunk">@@ -1361,7 +1362,7 @@</span> <span class="p_context"> config ARCH_DMA_ADDR_T_64BIT</span>
 
 config X86_DIRECT_GBPAGES
 	def_bool y
<span class="p_del">-	depends on X86_64 &amp;&amp; !DEBUG_PAGEALLOC &amp;&amp; !KMEMCHECK</span>
<span class="p_add">+	depends on X86_64 &amp;&amp; !DEBUG_PAGEALLOC &amp;&amp; !KMEMCHECK &amp;&amp; !XPFO</span>
 	---help---
 	  Certain kernel features effectively disable kernel
 	  linear 1 GB mappings (even if the CPU otherwise
<span class="p_header">diff --git a/arch/x86/mm/init.c b/arch/x86/mm/init.c</span>
<span class="p_header">index 22af912d66d2..a6fafbae02bb 100644</span>
<span class="p_header">--- a/arch/x86/mm/init.c</span>
<span class="p_header">+++ b/arch/x86/mm/init.c</span>
<span class="p_chunk">@@ -161,7 +161,7 @@</span> <span class="p_context"> static int page_size_mask;</span>
 
 static void __init probe_page_size_mask(void)
 {
<span class="p_del">-#if !defined(CONFIG_KMEMCHECK)</span>
<span class="p_add">+#if !defined(CONFIG_KMEMCHECK) &amp;&amp; !defined(CONFIG_XPFO)</span>
 	/*
 	 * For CONFIG_KMEMCHECK or pagealloc debugging, identity mapping will
 	 * use small pages.
<span class="p_header">diff --git a/drivers/ata/libata-sff.c b/drivers/ata/libata-sff.c</span>
<span class="p_header">index 051b6158d1b7..58af734be25d 100644</span>
<span class="p_header">--- a/drivers/ata/libata-sff.c</span>
<span class="p_header">+++ b/drivers/ata/libata-sff.c</span>
<span class="p_chunk">@@ -715,7 +715,7 @@</span> <span class="p_context"> static void ata_pio_sector(struct ata_queued_cmd *qc)</span>
 
 	DPRINTK(&quot;data %s\n&quot;, qc-&gt;tf.flags &amp; ATA_TFLAG_WRITE ? &quot;write&quot; : &quot;read&quot;);
 
<span class="p_del">-	if (PageHighMem(page)) {</span>
<span class="p_add">+	if (PageHighMem(page) || xpfo_page_is_unmapped(page)) {</span>
 		unsigned long flags;
 
 		/* FIXME: use a bounce buffer */
<span class="p_chunk">@@ -860,7 +860,7 @@</span> <span class="p_context"> static int __atapi_pio_bytes(struct ata_queued_cmd *qc, unsigned int bytes)</span>
 
 	DPRINTK(&quot;data %s\n&quot;, qc-&gt;tf.flags &amp; ATA_TFLAG_WRITE ? &quot;write&quot; : &quot;read&quot;);
 
<span class="p_del">-	if (PageHighMem(page)) {</span>
<span class="p_add">+	if (PageHighMem(page) || xpfo_page_is_unmapped(page)) {</span>
 		unsigned long flags;
 
 		/* FIXME: use bounce buffer */
<span class="p_header">diff --git a/include/linux/highmem.h b/include/linux/highmem.h</span>
<span class="p_header">index bb3f3297062a..7a17c166532f 100644</span>
<span class="p_header">--- a/include/linux/highmem.h</span>
<span class="p_header">+++ b/include/linux/highmem.h</span>
<span class="p_chunk">@@ -7,6 +7,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/mm.h&gt;
 #include &lt;linux/uaccess.h&gt;
 #include &lt;linux/hardirq.h&gt;
<span class="p_add">+#include &lt;linux/xpfo.h&gt;</span>
 
 #include &lt;asm/cacheflush.h&gt;
 
<span class="p_chunk">@@ -55,24 +56,34 @@</span> <span class="p_context"> static inline struct page *kmap_to_page(void *addr)</span>
 #ifndef ARCH_HAS_KMAP
 static inline void *kmap(struct page *page)
 {
<span class="p_add">+	void *kaddr;</span>
<span class="p_add">+</span>
 	might_sleep();
<span class="p_del">-	return page_address(page);</span>
<span class="p_add">+	kaddr = page_address(page);</span>
<span class="p_add">+	xpfo_kmap(kaddr, page);</span>
<span class="p_add">+	return kaddr;</span>
 }
 
 static inline void kunmap(struct page *page)
 {
<span class="p_add">+	xpfo_kunmap(page_address(page), page);</span>
 }
 
 static inline void *kmap_atomic(struct page *page)
 {
<span class="p_add">+	void *kaddr;</span>
<span class="p_add">+</span>
 	preempt_disable();
 	pagefault_disable();
<span class="p_del">-	return page_address(page);</span>
<span class="p_add">+	kaddr = page_address(page);</span>
<span class="p_add">+	xpfo_kmap(kaddr, page);</span>
<span class="p_add">+	return kaddr;</span>
 }
 #define kmap_atomic_prot(page, prot)	kmap_atomic(page)
 
 static inline void __kunmap_atomic(void *addr)
 {
<span class="p_add">+	xpfo_kunmap(addr, virt_to_page(addr));</span>
 	pagefault_enable();
 	preempt_enable();
 }
<span class="p_header">diff --git a/include/linux/page_ext.h b/include/linux/page_ext.h</span>
<span class="p_header">index 9298c393ddaa..0e451a42e5a3 100644</span>
<span class="p_header">--- a/include/linux/page_ext.h</span>
<span class="p_header">+++ b/include/linux/page_ext.h</span>
<span class="p_chunk">@@ -29,6 +29,8 @@</span> <span class="p_context"> enum page_ext_flags {</span>
 	PAGE_EXT_DEBUG_POISON,		/* Page is poisoned */
 	PAGE_EXT_DEBUG_GUARD,
 	PAGE_EXT_OWNER,
<span class="p_add">+	PAGE_EXT_XPFO_KERNEL,		/* Page is a kernel page */</span>
<span class="p_add">+	PAGE_EXT_XPFO_UNMAPPED,		/* Page is unmapped */</span>
 #if defined(CONFIG_IDLE_PAGE_TRACKING) &amp;&amp; !defined(CONFIG_64BIT)
 	PAGE_EXT_YOUNG,
 	PAGE_EXT_IDLE,
<span class="p_chunk">@@ -44,6 +46,11 @@</span> <span class="p_context"> enum page_ext_flags {</span>
  */
 struct page_ext {
 	unsigned long flags;
<span class="p_add">+#ifdef CONFIG_XPFO</span>
<span class="p_add">+	int inited;		/* Map counter and lock initialized */</span>
<span class="p_add">+	atomic_t mapcount;	/* Counter for balancing map/unmap requests */</span>
<span class="p_add">+	spinlock_t maplock;	/* Lock to serialize map/unmap requests */</span>
<span class="p_add">+#endif</span>
 };
 
 extern void pgdat_page_ext_init(struct pglist_data *pgdat);
<span class="p_header">diff --git a/include/linux/xpfo.h b/include/linux/xpfo.h</span>
new file mode 100644
<span class="p_header">index 000000000000..77187578ca33</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/include/linux/xpfo.h</span>
<span class="p_chunk">@@ -0,0 +1,39 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2016 Hewlett Packard Enterprise Development, L.P.</span>
<span class="p_add">+ * Copyright (C) 2016 Brown University. All rights reserved.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Authors:</span>
<span class="p_add">+ *   Juerg Haefliger &lt;juerg.haefliger@hpe.com&gt;</span>
<span class="p_add">+ *   Vasileios P. Kemerlis &lt;vpk@cs.brown.edu&gt;</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify it</span>
<span class="p_add">+ * under the terms of the GNU General Public License version 2 as published by</span>
<span class="p_add">+ * the Free Software Foundation.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _LINUX_XPFO_H</span>
<span class="p_add">+#define _LINUX_XPFO_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_XPFO</span>
<span class="p_add">+</span>
<span class="p_add">+extern struct page_ext_operations page_xpfo_ops;</span>
<span class="p_add">+</span>
<span class="p_add">+extern void xpfo_kmap(void *kaddr, struct page *page);</span>
<span class="p_add">+extern void xpfo_kunmap(void *kaddr, struct page *page);</span>
<span class="p_add">+extern void xpfo_alloc_page(struct page *page, int order, gfp_t gfp);</span>
<span class="p_add">+extern void xpfo_free_page(struct page *page, int order);</span>
<span class="p_add">+</span>
<span class="p_add">+extern bool xpfo_page_is_unmapped(struct page *page);</span>
<span class="p_add">+</span>
<span class="p_add">+#else /* !CONFIG_XPFO */</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void xpfo_kmap(void *kaddr, struct page *page) { }</span>
<span class="p_add">+static inline void xpfo_kunmap(void *kaddr, struct page *page) { }</span>
<span class="p_add">+static inline void xpfo_alloc_page(struct page *page, int order, gfp_t gfp) { }</span>
<span class="p_add">+static inline void xpfo_free_page(struct page *page, int order) { }</span>
<span class="p_add">+</span>
<span class="p_add">+static inline bool xpfo_page_is_unmapped(struct page *page) { return false; }</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* CONFIG_XPFO */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _LINUX_XPFO_H */</span>
<span class="p_header">diff --git a/lib/swiotlb.c b/lib/swiotlb.c</span>
<span class="p_header">index 22e13a0e19d7..455eff44604e 100644</span>
<span class="p_header">--- a/lib/swiotlb.c</span>
<span class="p_header">+++ b/lib/swiotlb.c</span>
<span class="p_chunk">@@ -390,8 +390,9 @@</span> <span class="p_context"> static void swiotlb_bounce(phys_addr_t orig_addr, phys_addr_t tlb_addr,</span>
 {
 	unsigned long pfn = PFN_DOWN(orig_addr);
 	unsigned char *vaddr = phys_to_virt(tlb_addr);
<span class="p_add">+	struct page *page = pfn_to_page(pfn);</span>
 
<span class="p_del">-	if (PageHighMem(pfn_to_page(pfn))) {</span>
<span class="p_add">+	if (PageHighMem(page) || xpfo_page_is_unmapped(page)) {</span>
 		/* The buffer does not have a mapping.  Map it in and copy */
 		unsigned int offset = orig_addr &amp; ~PAGE_MASK;
 		char *buffer;
<span class="p_header">diff --git a/mm/Makefile b/mm/Makefile</span>
<span class="p_header">index 295bd7a9f76b..175680f516aa 100644</span>
<span class="p_header">--- a/mm/Makefile</span>
<span class="p_header">+++ b/mm/Makefile</span>
<span class="p_chunk">@@ -100,3 +100,4 @@</span> <span class="p_context"> obj-$(CONFIG_IDLE_PAGE_TRACKING) += page_idle.o</span>
 obj-$(CONFIG_FRAME_VECTOR) += frame_vector.o
 obj-$(CONFIG_DEBUG_PAGE_REF) += debug_page_ref.o
 obj-$(CONFIG_HARDENED_USERCOPY) += usercopy.o
<span class="p_add">+obj-$(CONFIG_XPFO) += xpfo.o</span>
<span class="p_header">diff --git a/mm/page_alloc.c b/mm/page_alloc.c</span>
<span class="p_header">index 8fd42aa7c4bd..100e80e008e2 100644</span>
<span class="p_header">--- a/mm/page_alloc.c</span>
<span class="p_header">+++ b/mm/page_alloc.c</span>
<span class="p_chunk">@@ -1045,6 +1045,7 @@</span> <span class="p_context"> static __always_inline bool free_pages_prepare(struct page *page,</span>
 	kernel_poison_pages(page, 1 &lt;&lt; order, 0);
 	kernel_map_pages(page, 1 &lt;&lt; order, 0);
 	kasan_free_pages(page, order);
<span class="p_add">+	xpfo_free_page(page, order);</span>
 
 	return true;
 }
<span class="p_chunk">@@ -1745,6 +1746,7 @@</span> <span class="p_context"> inline void post_alloc_hook(struct page *page, unsigned int order,</span>
 	kernel_map_pages(page, 1 &lt;&lt; order, 1);
 	kernel_poison_pages(page, 1 &lt;&lt; order, 1);
 	kasan_alloc_pages(page, order);
<span class="p_add">+	xpfo_alloc_page(page, order, gfp_flags);</span>
 	set_page_owner(page, order, gfp_flags);
 }
 
<span class="p_header">diff --git a/mm/page_ext.c b/mm/page_ext.c</span>
<span class="p_header">index 121dcffc4ec1..ba6dbcacc2db 100644</span>
<span class="p_header">--- a/mm/page_ext.c</span>
<span class="p_header">+++ b/mm/page_ext.c</span>
<span class="p_chunk">@@ -7,6 +7,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/kmemleak.h&gt;
 #include &lt;linux/page_owner.h&gt;
 #include &lt;linux/page_idle.h&gt;
<span class="p_add">+#include &lt;linux/xpfo.h&gt;</span>
 
 /*
  * struct page extension
<span class="p_chunk">@@ -68,6 +69,9 @@</span> <span class="p_context"> static struct page_ext_operations *page_ext_ops[] = {</span>
 #if defined(CONFIG_IDLE_PAGE_TRACKING) &amp;&amp; !defined(CONFIG_64BIT)
 	&amp;page_idle_ops,
 #endif
<span class="p_add">+#ifdef CONFIG_XPFO</span>
<span class="p_add">+	&amp;page_xpfo_ops,</span>
<span class="p_add">+#endif</span>
 };
 
 static unsigned long total_usage;
<span class="p_header">diff --git a/mm/xpfo.c b/mm/xpfo.c</span>
new file mode 100644
<span class="p_header">index 000000000000..8e3a6a694b6a</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/mm/xpfo.c</span>
<span class="p_chunk">@@ -0,0 +1,206 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2016 Hewlett Packard Enterprise Development, L.P.</span>
<span class="p_add">+ * Copyright (C) 2016 Brown University. All rights reserved.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Authors:</span>
<span class="p_add">+ *   Juerg Haefliger &lt;juerg.haefliger@hpe.com&gt;</span>
<span class="p_add">+ *   Vasileios P. Kemerlis &lt;vpk@cs.brown.edu&gt;</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify it</span>
<span class="p_add">+ * under the terms of the GNU General Public License version 2 as published by</span>
<span class="p_add">+ * the Free Software Foundation.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/mm.h&gt;</span>
<span class="p_add">+#include &lt;linux/module.h&gt;</span>
<span class="p_add">+#include &lt;linux/page_ext.h&gt;</span>
<span class="p_add">+#include &lt;linux/xpfo.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/tlbflush.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+DEFINE_STATIC_KEY_FALSE(xpfo_inited);</span>
<span class="p_add">+</span>
<span class="p_add">+static bool need_xpfo(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return true;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void init_xpfo(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	printk(KERN_INFO &quot;XPFO enabled\n&quot;);</span>
<span class="p_add">+	static_branch_enable(&amp;xpfo_inited);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+struct page_ext_operations page_xpfo_ops = {</span>
<span class="p_add">+	.need = need_xpfo,</span>
<span class="p_add">+	.init = init_xpfo,</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Update a single kernel page table entry</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void set_kpte(struct page *page, unsigned long kaddr,</span>
<span class="p_add">+			    pgprot_t prot) {</span>
<span class="p_add">+	unsigned int level;</span>
<span class="p_add">+	pte_t *kpte = lookup_address(kaddr, &amp;level);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* We only support 4k pages for now */</span>
<span class="p_add">+	BUG_ON(!kpte || level != PG_LEVEL_4K);</span>
<span class="p_add">+</span>
<span class="p_add">+	set_pte_atomic(kpte, pfn_pte(page_to_pfn(page), canon_pgprot(prot)));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void xpfo_alloc_page(struct page *page, int order, gfp_t gfp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int i, flush_tlb = 0;</span>
<span class="p_add">+	struct page_ext *page_ext;</span>
<span class="p_add">+	unsigned long kaddr;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!static_branch_unlikely(&amp;xpfo_inited))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i &lt; (1 &lt;&lt; order); i++)  {</span>
<span class="p_add">+		page_ext = lookup_page_ext(page + i);</span>
<span class="p_add">+</span>
<span class="p_add">+		BUG_ON(test_bit(PAGE_EXT_XPFO_UNMAPPED, &amp;page_ext-&gt;flags));</span>
<span class="p_add">+</span>
<span class="p_add">+		/* Initialize the map lock and map counter */</span>
<span class="p_add">+		if (!page_ext-&gt;inited) {</span>
<span class="p_add">+			spin_lock_init(&amp;page_ext-&gt;maplock);</span>
<span class="p_add">+			atomic_set(&amp;page_ext-&gt;mapcount, 0);</span>
<span class="p_add">+			page_ext-&gt;inited = 1;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		BUG_ON(atomic_read(&amp;page_ext-&gt;mapcount));</span>
<span class="p_add">+</span>
<span class="p_add">+		if ((gfp &amp; GFP_HIGHUSER) == GFP_HIGHUSER) {</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * Flush the TLB if the page was previously allocated</span>
<span class="p_add">+			 * to the kernel.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			if (test_and_clear_bit(PAGE_EXT_XPFO_KERNEL,</span>
<span class="p_add">+					       &amp;page_ext-&gt;flags))</span>
<span class="p_add">+				flush_tlb = 1;</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			/* Tag the page as a kernel page */</span>
<span class="p_add">+			set_bit(PAGE_EXT_XPFO_KERNEL, &amp;page_ext-&gt;flags);</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (flush_tlb) {</span>
<span class="p_add">+		kaddr = (unsigned long)page_address(page);</span>
<span class="p_add">+		flush_tlb_kernel_range(kaddr, kaddr + (1 &lt;&lt; order) *</span>
<span class="p_add">+				       PAGE_SIZE);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void xpfo_free_page(struct page *page, int order)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+	struct page_ext *page_ext;</span>
<span class="p_add">+	unsigned long kaddr;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!static_branch_unlikely(&amp;xpfo_inited))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i &lt; (1 &lt;&lt; order); i++) {</span>
<span class="p_add">+		page_ext = lookup_page_ext(page + i);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!page_ext-&gt;inited) {</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * The page was allocated before page_ext was</span>
<span class="p_add">+			 * initialized, so it is a kernel page and it needs to</span>
<span class="p_add">+			 * be tagged accordingly.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			set_bit(PAGE_EXT_XPFO_KERNEL, &amp;page_ext-&gt;flags);</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Map the page back into the kernel if it was previously</span>
<span class="p_add">+		 * allocated to user space.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (test_and_clear_bit(PAGE_EXT_XPFO_UNMAPPED,</span>
<span class="p_add">+				       &amp;page_ext-&gt;flags)) {</span>
<span class="p_add">+			kaddr = (unsigned long)page_address(page + i);</span>
<span class="p_add">+			set_kpte(page + i,  kaddr, __pgprot(__PAGE_KERNEL));</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void xpfo_kmap(void *kaddr, struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct page_ext *page_ext;</span>
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!static_branch_unlikely(&amp;xpfo_inited))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	page_ext = lookup_page_ext(page);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The page was allocated before page_ext was initialized (which means</span>
<span class="p_add">+	 * it&#39;s a kernel page) or it&#39;s allocated to the kernel, so nothing to</span>
<span class="p_add">+	 * do.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (!page_ext-&gt;inited ||</span>
<span class="p_add">+	    test_bit(PAGE_EXT_XPFO_KERNEL, &amp;page_ext-&gt;flags))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	spin_lock_irqsave(&amp;page_ext-&gt;maplock, flags);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The page was previously allocated to user space, so map it back</span>
<span class="p_add">+	 * into the kernel. No TLB flush required.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if ((atomic_inc_return(&amp;page_ext-&gt;mapcount) == 1) &amp;&amp;</span>
<span class="p_add">+	    test_and_clear_bit(PAGE_EXT_XPFO_UNMAPPED, &amp;page_ext-&gt;flags))</span>
<span class="p_add">+		set_kpte(page, (unsigned long)kaddr, __pgprot(__PAGE_KERNEL));</span>
<span class="p_add">+</span>
<span class="p_add">+	spin_unlock_irqrestore(&amp;page_ext-&gt;maplock, flags);</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(xpfo_kmap);</span>
<span class="p_add">+</span>
<span class="p_add">+void xpfo_kunmap(void *kaddr, struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct page_ext *page_ext;</span>
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!static_branch_unlikely(&amp;xpfo_inited))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	page_ext = lookup_page_ext(page);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The page was allocated before page_ext was initialized (which means</span>
<span class="p_add">+	 * it&#39;s a kernel page) or it&#39;s allocated to the kernel, so nothing to</span>
<span class="p_add">+	 * do.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (!page_ext-&gt;inited ||</span>
<span class="p_add">+	    test_bit(PAGE_EXT_XPFO_KERNEL, &amp;page_ext-&gt;flags))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	spin_lock_irqsave(&amp;page_ext-&gt;maplock, flags);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The page is to be allocated back to user space, so unmap it from the</span>
<span class="p_add">+	 * kernel, flush the TLB and tag it as a user page.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (atomic_dec_return(&amp;page_ext-&gt;mapcount) == 0) {</span>
<span class="p_add">+		BUG_ON(test_bit(PAGE_EXT_XPFO_UNMAPPED, &amp;page_ext-&gt;flags));</span>
<span class="p_add">+		set_bit(PAGE_EXT_XPFO_UNMAPPED, &amp;page_ext-&gt;flags);</span>
<span class="p_add">+		set_kpte(page, (unsigned long)kaddr, __pgprot(0));</span>
<span class="p_add">+		__flush_tlb_one((unsigned long)kaddr);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	spin_unlock_irqrestore(&amp;page_ext-&gt;maplock, flags);</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(xpfo_kunmap);</span>
<span class="p_add">+</span>
<span class="p_add">+inline bool xpfo_page_is_unmapped(struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!static_branch_unlikely(&amp;xpfo_inited))</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+</span>
<span class="p_add">+	return test_bit(PAGE_EXT_XPFO_UNMAPPED, &amp;lookup_page_ext(page)-&gt;flags);</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(xpfo_page_is_unmapped);</span>
<span class="p_header">diff --git a/security/Kconfig b/security/Kconfig</span>
<span class="p_header">index 118f4549404e..4502e15c8419 100644</span>
<span class="p_header">--- a/security/Kconfig</span>
<span class="p_header">+++ b/security/Kconfig</span>
<span class="p_chunk">@@ -6,6 +6,25 @@</span> <span class="p_context"> menu &quot;Security options&quot;</span>
 
 source security/keys/Kconfig
 
<span class="p_add">+config ARCH_SUPPORTS_XPFO</span>
<span class="p_add">+	bool</span>
<span class="p_add">+</span>
<span class="p_add">+config XPFO</span>
<span class="p_add">+	bool &quot;Enable eXclusive Page Frame Ownership (XPFO)&quot;</span>
<span class="p_add">+	default n</span>
<span class="p_add">+	depends on ARCH_SUPPORTS_XPFO</span>
<span class="p_add">+	select PAGE_EXTENSION</span>
<span class="p_add">+	help</span>
<span class="p_add">+	  This option offers protection against &#39;ret2dir&#39; kernel attacks.</span>
<span class="p_add">+	  When enabled, every time a page frame is allocated to user space, it</span>
<span class="p_add">+	  is unmapped from the direct mapped RAM region in kernel space</span>
<span class="p_add">+	  (physmap). Similarly, when a page frame is freed/reclaimed, it is</span>
<span class="p_add">+	  mapped back to physmap.</span>
<span class="p_add">+</span>
<span class="p_add">+	  There is a slight performance impact when this option is enabled.</span>
<span class="p_add">+</span>
<span class="p_add">+	  If in doubt, say &quot;N&quot;.</span>
<span class="p_add">+</span>
 config SECURITY_DMESG_RESTRICT
 	bool &quot;Restrict unprivileged access to the kernel syslog&quot;
 	default n

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



