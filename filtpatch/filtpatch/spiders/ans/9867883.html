
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[2/3] mm: fix MADV_[FREE|DONTNEED] TLB flush miss problem - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [2/3] mm: fix MADV_[FREE|DONTNEED] TLB flush miss problem</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=36811">Minchan Kim</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>July 28, 2017, 6:41 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1501224112-23656-3-git-send-email-minchan@kernel.org&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9867883/mbox/"
   >mbox</a>
|
   <a href="/patch/9867883/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9867883/">/patch/9867883/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	98CD3603F9 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 28 Jul 2017 06:42:13 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 8B8902889F
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 28 Jul 2017 06:42:13 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 7FEBA288A2; Fri, 28 Jul 2017 06:42:13 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=unavailable version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id AC52D2889F
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 28 Jul 2017 06:42:12 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1751774AbdG1GmK (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Fri, 28 Jul 2017 02:42:10 -0400
Received: from LGEAMRELO12.lge.com ([156.147.23.52]:57848 &quot;EHLO
	lgeamrelo12.lge.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1751738AbdG1GmG (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Fri, 28 Jul 2017 02:42:06 -0400
Received: from unknown (HELO lgeamrelo04.lge.com) (156.147.1.127)
	by 156.147.23.52 with ESMTP; 28 Jul 2017 15:42:03 +0900
X-Original-SENDERIP: 156.147.1.127
X-Original-MAILFROM: minchan@kernel.org
Received: from unknown (HELO localhost.localdomain) (10.177.220.163)
	by 156.147.1.127 with ESMTP; 28 Jul 2017 15:42:02 +0900
X-Original-SENDERIP: 10.177.220.163
X-Original-MAILFROM: minchan@kernel.org
From: Minchan Kim &lt;minchan@kernel.org&gt;
To: Andrew Morton &lt;akpm@linux-foundation.org&gt;
Cc: kernel-team &lt;kernel-team@lge.com&gt;, linux-mm@kvack.org,
	linux-kernel@vger.kernel.org, Rik van Riel &lt;riel@redhat.com&gt;,
	Minchan Kim &lt;minchan@kernel.org&gt;,
	Ingo Molnar &lt;mingo@redhat.com&gt;, x86@kernel.org,
	Russell King &lt;linux@armlinux.org.uk&gt;,
	linux-arm-kernel@lists.infradead.org,
	Tony Luck &lt;tony.luck@intel.com&gt;, linux-ia64@vger.kernel.org,
	Martin Schwidefsky &lt;schwidefsky@de.ibm.com&gt;,
	&quot;David S. Miller&quot; &lt;davem@davemloft.net&gt;,
	Heiko Carstens &lt;heiko.carstens@de.ibm.com&gt;, linux-s390@vger.kernel.org,
	Yoshinori Sato &lt;ysato@users.sourceforge.jp&gt;,
	linux-sh@vger.kernel.org, Jeff Dike &lt;jdike@addtoit.com&gt;,
	user-mode-linux-devel@lists.sourceforge.net,
	linux-arch@vger.kernel.org, Nadav Amit &lt;nadav.amit@gmail.com&gt;,
	Mel Gorman &lt;mgorman@techsingularity.net&gt;
Subject: [PATCH 2/3] mm: fix MADV_[FREE|DONTNEED] TLB flush miss problem
Date: Fri, 28 Jul 2017 15:41:51 +0900
Message-Id: &lt;1501224112-23656-3-git-send-email-minchan@kernel.org&gt;
X-Mailer: git-send-email 2.7.4
In-Reply-To: &lt;1501224112-23656-1-git-send-email-minchan@kernel.org&gt;
References: &lt;1501224112-23656-1-git-send-email-minchan@kernel.org&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36811">Minchan Kim</a> - July 28, 2017, 6:41 a.m.</div>
<pre class="content">
Nadav reported parallel MADV_DONTNEED on same range has a stale TLB
problem and Mel fixed it[1] and found same problem on MADV_FREE[2].

Quote from Mel Gorman

&quot;The race in question is CPU 0 running madv_free and updating some PTEs
while CPU 1 is also running madv_free and looking at the same PTEs.
CPU 1 may have writable TLB entries for a page but fail the pte_dirty
check (because CPU 0 has updated it already) and potentially fail to flush.
Hence, when madv_free on CPU 1 returns, there are still potentially writable
TLB entries and the underlying PTE is still present so that a subsequent write
does not necessarily propagate the dirty bit to the underlying PTE any more.
Reclaim at some unknown time at the future may then see that the PTE is still
clean and discard the page even though a write has happened in the meantime.
I think this is possible but I could have missed some protection in madv_free
that prevents it happening.&quot;

This patch aims for solving both problems all at once and is ready for
other problem with KSM, MADV_FREE and soft-dirty story[3].

TLB batch API(tlb_[gather|finish]_mmu] uses [set|clear]_tlb_flush_pending
and mmu_tlb_flush_pending so that when tlb_finish_mmu is called, we can catch
there are parallel threads going on. In that case, flush TLB to prevent
for user to access memory via stale TLB entry although it fail to gather
pte entry.

I confiremd this patch works with [4] test program Nadav gave so this patch
supersedes &quot;mm: Always flush VMA ranges affected by zap_page_range v2&quot;
in current mmotm.

NOTE:
This patch modifies arch-specific TLB gathering interface(x86, ia64,
s390, sh, um). It seems most of architecture are straightforward but s390
need to be careful because tlb_flush_mmu works only if mm-&gt;context.flush_mm
is set to non-zero which happens only a pte entry really is cleared by
ptep_get_and_clear and friends. However, this problem never changes the
pte entries but need to flush to prevent memory access from stale tlb.

Any thoughts?

[1] http://lkml.kernel.org/r/20170725101230.5v7gvnjmcnkzzql3@techsingularity.net
[2] http://lkml.kernel.org/r/20170725100722.2dxnmgypmwnrfawp@suse.de
[3] http://lkml.kernel.org/r/BD3A0EBE-ECF4-41D4-87FA-C755EA9AB6BD@gmail.com
[4] https://patchwork.kernel.org/patch/9861621/

Cc: Ingo Molnar &lt;mingo@redhat.com&gt;
Cc: x86@kernel.org
Cc: Russell King &lt;linux@armlinux.org.uk&gt;
Cc: linux-arm-kernel@lists.infradead.org
Cc: Tony Luck &lt;tony.luck@intel.com&gt;
Cc: linux-ia64@vger.kernel.org
Cc: Martin Schwidefsky &lt;schwidefsky@de.ibm.com&gt;
Cc: &quot;David S. Miller&quot; &lt;davem@davemloft.net&gt;
Cc: Heiko Carstens &lt;heiko.carstens@de.ibm.com&gt;
Cc: linux-s390@vger.kernel.org
Cc: Yoshinori Sato &lt;ysato@users.sourceforge.jp&gt;
Cc: linux-sh@vger.kernel.org
Cc: Jeff Dike &lt;jdike@addtoit.com&gt;
Cc: user-mode-linux-devel@lists.sourceforge.net
Cc: linux-arch@vger.kernel.org
Cc: Nadav Amit &lt;nadav.amit@gmail.com&gt;
Reported-by: Mel Gorman &lt;mgorman@techsingularity.net&gt;
<span class="signed-off-by">Signed-off-by: Minchan Kim &lt;minchan@kernel.org&gt;</span>
---
 arch/arm/include/asm/tlb.h  | 15 ++++++++++++++-
 arch/ia64/include/asm/tlb.h | 12 ++++++++++++
 arch/s390/include/asm/tlb.h | 15 +++++++++++++++
 arch/sh/include/asm/tlb.h   |  4 +++-
 arch/um/include/asm/tlb.h   |  8 ++++++++
 include/linux/mm_types.h    |  7 +++++--
 mm/memory.c                 | 24 ++++++++++++------------
 7 files changed, 69 insertions(+), 16 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=138281">Mel Gorman</a> - July 28, 2017, 8:46 a.m.</div>
<pre class="content">
On Fri, Jul 28, 2017 at 03:41:51PM +0900, Minchan Kim wrote:
<span class="quote">&gt; Nadav reported parallel MADV_DONTNEED on same range has a stale TLB</span>
<span class="quote">&gt; problem and Mel fixed it[1] and found same problem on MADV_FREE[2].</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Quote from Mel Gorman</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &quot;The race in question is CPU 0 running madv_free and updating some PTEs</span>
<span class="quote">&gt; while CPU 1 is also running madv_free and looking at the same PTEs.</span>
<span class="quote">&gt; CPU 1 may have writable TLB entries for a page but fail the pte_dirty</span>
<span class="quote">&gt; check (because CPU 0 has updated it already) and potentially fail to flush.</span>
<span class="quote">&gt; Hence, when madv_free on CPU 1 returns, there are still potentially writable</span>
<span class="quote">&gt; TLB entries and the underlying PTE is still present so that a subsequent write</span>
<span class="quote">&gt; does not necessarily propagate the dirty bit to the underlying PTE any more.</span>
<span class="quote">&gt; Reclaim at some unknown time at the future may then see that the PTE is still</span>
<span class="quote">&gt; clean and discard the page even though a write has happened in the meantime.</span>
<span class="quote">&gt; I think this is possible but I could have missed some protection in madv_free</span>
<span class="quote">&gt; that prevents it happening.&quot;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This patch aims for solving both problems all at once and is ready for</span>
<span class="quote">&gt; other problem with KSM, MADV_FREE and soft-dirty story[3].</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; TLB batch API(tlb_[gather|finish]_mmu] uses [set|clear]_tlb_flush_pending</span>
<span class="quote">&gt; and mmu_tlb_flush_pending so that when tlb_finish_mmu is called, we can catch</span>
<span class="quote">&gt; there are parallel threads going on. In that case, flush TLB to prevent</span>
<span class="quote">&gt; for user to access memory via stale TLB entry although it fail to gather</span>
<span class="quote">&gt; pte entry.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I confiremd this patch works with [4] test program Nadav gave so this patch</span>
<span class="quote">&gt; supersedes &quot;mm: Always flush VMA ranges affected by zap_page_range v2&quot;</span>
<span class="quote">&gt; in current mmotm.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; NOTE:</span>
<span class="quote">&gt; This patch modifies arch-specific TLB gathering interface(x86, ia64,</span>
<span class="quote">&gt; s390, sh, um). It seems most of architecture are straightforward but s390</span>
<span class="quote">&gt; need to be careful because tlb_flush_mmu works only if mm-&gt;context.flush_mm</span>
<span class="quote">&gt; is set to non-zero which happens only a pte entry really is cleared by</span>
<span class="quote">&gt; ptep_get_and_clear and friends. However, this problem never changes the</span>
<span class="quote">&gt; pte entries but need to flush to prevent memory access from stale tlb.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Any thoughts?</span>
<span class="quote">&gt; </span>

The cc list is somewhat ..... extensive, given the topic. Trim it if
there is another version.
<span class="quote">
&gt; index 3f2eb76243e3..8c26961f0503 100644</span>
<span class="quote">&gt; --- a/arch/arm/include/asm/tlb.h</span>
<span class="quote">&gt; +++ b/arch/arm/include/asm/tlb.h</span>
<span class="quote">&gt; @@ -163,13 +163,26 @@ tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm, unsigned long start</span>
<span class="quote">&gt;  #ifdef CONFIG_HAVE_RCU_TABLE_FREE</span>
<span class="quote">&gt;  	tlb-&gt;batch = NULL;</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt; +	set_tlb_flush_pending(tlb-&gt;mm);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static inline void</span>
<span class="quote">&gt;  tlb_finish_mmu(struct mmu_gather *tlb, unsigned long start, unsigned long end)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	tlb_flush_mmu(tlb);</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * If there are parallel threads are doing PTE changes on same range</span>
<span class="quote">&gt; +	 * under non-exclusive lock(e.g., mmap_sem read-side) but defer TLB</span>
<span class="quote">&gt; +	 * flush by batching, a thread has stable TLB entry can fail to flush</span>
<span class="quote">&gt; +	 * the TLB by observing pte_none|!pte_dirty, for example so flush TLB</span>
<span class="quote">&gt; +	 * if we detect parallel PTE batching threads.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	if (mm_tlb_flush_pending(tlb-&gt;mm, false) &gt; 1) {</span>
<span class="quote">&gt; +		tlb-&gt;range_start = start;</span>
<span class="quote">&gt; +		tlb-&gt;range_end = end;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +	tlb_flush_mmu(tlb);</span>
<span class="quote">&gt; +	clear_tlb_flush_pending(tlb-&gt;mm);</span>
<span class="quote">&gt;  	/* keep the page table cache within bounds */</span>
<span class="quote">&gt;  	check_pgt_cache();</span>
<span class="quote">&gt;  </span>

mm_tlb_flush_pending shouldn&#39;t be taking a barrier specific arg. I expect
this to change in the future and cause a conflict. At least I think in
this context, it&#39;s the conditional barrier stuff.

That aside, it&#39;s very unfortunate that the return value of
mm_tlb_flush_pending really matters. Knowing why 1 is magic there requires
knowledge of the internals on a per-arch basis which is a bit nuts.
Consider renaming this to mm_tlb_flush_parallel() to return true if there
is a nr_pending &gt; 1 with comments explaining why. I don&#39;t think any of
the callers expect a nr_pending of 0 ever. That removes some knowledge of
the specifics.

The arch-specific changes to tlb_gather_mmu are almost all identical.
It&#39;s a little tricky to split the arch layer and core mm to have all
the set/clear of mm_tlb_flush_pending handled by the core mm.  It&#39;s not
required but it would be preferred. The set one is obvious. rename
tlb_gather_mmu to arch_tlb_gather_mmu (including the generic implementation)
and create a tlb_gather_mmu alias that calls arch_tlb_gather_mmu and
set_tlb_flush_pending.

The clear is not as straight-forward but can be done by creating a new
arch helper that handles this hunk on a per-arch basis
<span class="quote">
&gt; +     if (mm_tlb_flush_pending(tlb-&gt;mm, false) &gt; 1) {</span>
<span class="quote">&gt; +             tlb-&gt;start = start;</span>
<span class="quote">&gt; +             tlb-&gt;end = end;</span>
<span class="quote">&gt; +     }</span>

It&#39;ll be churn initially but it means any different handling in the TLB
batching area will be mostly a core concern.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36811">Minchan Kim</a> - July 28, 2017, 3:12 p.m.</div>
<pre class="content">
On Fri, Jul 28, 2017 at 09:46:34AM +0100, Mel Gorman wrote:
<span class="quote">&gt; On Fri, Jul 28, 2017 at 03:41:51PM +0900, Minchan Kim wrote:</span>
<span class="quote">&gt; &gt; Nadav reported parallel MADV_DONTNEED on same range has a stale TLB</span>
<span class="quote">&gt; &gt; problem and Mel fixed it[1] and found same problem on MADV_FREE[2].</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Quote from Mel Gorman</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; &quot;The race in question is CPU 0 running madv_free and updating some PTEs</span>
<span class="quote">&gt; &gt; while CPU 1 is also running madv_free and looking at the same PTEs.</span>
<span class="quote">&gt; &gt; CPU 1 may have writable TLB entries for a page but fail the pte_dirty</span>
<span class="quote">&gt; &gt; check (because CPU 0 has updated it already) and potentially fail to flush.</span>
<span class="quote">&gt; &gt; Hence, when madv_free on CPU 1 returns, there are still potentially writable</span>
<span class="quote">&gt; &gt; TLB entries and the underlying PTE is still present so that a subsequent write</span>
<span class="quote">&gt; &gt; does not necessarily propagate the dirty bit to the underlying PTE any more.</span>
<span class="quote">&gt; &gt; Reclaim at some unknown time at the future may then see that the PTE is still</span>
<span class="quote">&gt; &gt; clean and discard the page even though a write has happened in the meantime.</span>
<span class="quote">&gt; &gt; I think this is possible but I could have missed some protection in madv_free</span>
<span class="quote">&gt; &gt; that prevents it happening.&quot;</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; This patch aims for solving both problems all at once and is ready for</span>
<span class="quote">&gt; &gt; other problem with KSM, MADV_FREE and soft-dirty story[3].</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; TLB batch API(tlb_[gather|finish]_mmu] uses [set|clear]_tlb_flush_pending</span>
<span class="quote">&gt; &gt; and mmu_tlb_flush_pending so that when tlb_finish_mmu is called, we can catch</span>
<span class="quote">&gt; &gt; there are parallel threads going on. In that case, flush TLB to prevent</span>
<span class="quote">&gt; &gt; for user to access memory via stale TLB entry although it fail to gather</span>
<span class="quote">&gt; &gt; pte entry.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I confiremd this patch works with [4] test program Nadav gave so this patch</span>
<span class="quote">&gt; &gt; supersedes &quot;mm: Always flush VMA ranges affected by zap_page_range v2&quot;</span>
<span class="quote">&gt; &gt; in current mmotm.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; NOTE:</span>
<span class="quote">&gt; &gt; This patch modifies arch-specific TLB gathering interface(x86, ia64,</span>
<span class="quote">&gt; &gt; s390, sh, um). It seems most of architecture are straightforward but s390</span>
<span class="quote">&gt; &gt; need to be careful because tlb_flush_mmu works only if mm-&gt;context.flush_mm</span>
<span class="quote">&gt; &gt; is set to non-zero which happens only a pte entry really is cleared by</span>
<span class="quote">&gt; &gt; ptep_get_and_clear and friends. However, this problem never changes the</span>
<span class="quote">&gt; &gt; pte entries but need to flush to prevent memory access from stale tlb.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Any thoughts?</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The cc list is somewhat ..... extensive, given the topic. Trim it if</span>
<span class="quote">&gt; there is another version.</span>

Most of them are maintainers and mailling list for each architecures
I am changing. I&#39;m not sure what I can trim. As you said it&#39;s rather
extensive, I will trim mailing list for each arch but keep maintainers
and linux-arch.
<span class="quote">
&gt; </span>
<span class="quote">&gt; &gt; index 3f2eb76243e3..8c26961f0503 100644</span>
<span class="quote">&gt; &gt; --- a/arch/arm/include/asm/tlb.h</span>
<span class="quote">&gt; &gt; +++ b/arch/arm/include/asm/tlb.h</span>
<span class="quote">&gt; &gt; @@ -163,13 +163,26 @@ tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm, unsigned long start</span>
<span class="quote">&gt; &gt;  #ifdef CONFIG_HAVE_RCU_TABLE_FREE</span>
<span class="quote">&gt; &gt;  	tlb-&gt;batch = NULL;</span>
<span class="quote">&gt; &gt;  #endif</span>
<span class="quote">&gt; &gt; +	set_tlb_flush_pending(tlb-&gt;mm);</span>
<span class="quote">&gt; &gt;  }</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  static inline void</span>
<span class="quote">&gt; &gt;  tlb_finish_mmu(struct mmu_gather *tlb, unsigned long start, unsigned long end)</span>
<span class="quote">&gt; &gt;  {</span>
<span class="quote">&gt; &gt; -	tlb_flush_mmu(tlb);</span>
<span class="quote">&gt; &gt; +	/*</span>
<span class="quote">&gt; &gt; +	 * If there are parallel threads are doing PTE changes on same range</span>
<span class="quote">&gt; &gt; +	 * under non-exclusive lock(e.g., mmap_sem read-side) but defer TLB</span>
<span class="quote">&gt; &gt; +	 * flush by batching, a thread has stable TLB entry can fail to flush</span>
<span class="quote">&gt; &gt; +	 * the TLB by observing pte_none|!pte_dirty, for example so flush TLB</span>
<span class="quote">&gt; &gt; +	 * if we detect parallel PTE batching threads.</span>
<span class="quote">&gt; &gt; +	 */</span>
<span class="quote">&gt; &gt; +	if (mm_tlb_flush_pending(tlb-&gt;mm, false) &gt; 1) {</span>
<span class="quote">&gt; &gt; +		tlb-&gt;range_start = start;</span>
<span class="quote">&gt; &gt; +		tlb-&gt;range_end = end;</span>
<span class="quote">&gt; &gt; +	}</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; +	tlb_flush_mmu(tlb);</span>
<span class="quote">&gt; &gt; +	clear_tlb_flush_pending(tlb-&gt;mm);</span>
<span class="quote">&gt; &gt;  	/* keep the page table cache within bounds */</span>
<span class="quote">&gt; &gt;  	check_pgt_cache();</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; mm_tlb_flush_pending shouldn&#39;t be taking a barrier specific arg. I expect</span>
<span class="quote">&gt; this to change in the future and cause a conflict. At least I think in</span>
<span class="quote">&gt; this context, it&#39;s the conditional barrier stuff.</span>
<span class="quote">&gt; </span>

Yub. I saw your comment to Nadav so I expect you want mm_tlb_flush_pending
be called under pte lock. However, I will use it out of pte lock where
tlb_finish_mmu, however, in that case, atomic op and barrier
to prevent compiler reordering between tlb flush and atomic_read
in mm_tlb_flush_pending are enough to work.
<span class="quote">
&gt; That aside, it&#39;s very unfortunate that the return value of</span>
<span class="quote">&gt; mm_tlb_flush_pending really matters. Knowing why 1 is magic there requires</span>
<span class="quote">&gt; knowledge of the internals on a per-arch basis which is a bit nuts.</span>
<span class="quote">&gt; Consider renaming this to mm_tlb_flush_parallel() to return true if there</span>
<span class="quote">&gt; is a nr_pending &gt; 1 with comments explaining why. I don&#39;t think any of</span>
<span class="quote">&gt; the callers expect a nr_pending of 0 ever. That removes some knowledge of</span>
<span class="quote">&gt; the specifics.</span>

Okay. If you are not strong against, I prefer mm_tlb_flush_nested
which returns true if nr_pending &gt; 1.
<span class="quote">
&gt; </span>
<span class="quote">&gt; The arch-specific changes to tlb_gather_mmu are almost all identical.</span>
<span class="quote">&gt; It&#39;s a little tricky to split the arch layer and core mm to have all</span>
<span class="quote">&gt; the set/clear of mm_tlb_flush_pending handled by the core mm.  It&#39;s not</span>
<span class="quote">&gt; required but it would be preferred. The set one is obvious. rename</span>
<span class="quote">&gt; tlb_gather_mmu to arch_tlb_gather_mmu (including the generic implementation)</span>
<span class="quote">&gt; and create a tlb_gather_mmu alias that calls arch_tlb_gather_mmu and</span>
<span class="quote">&gt; set_tlb_flush_pending.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The clear is not as straight-forward but can be done by creating a new</span>
<span class="quote">&gt; arch helper that handles this hunk on a per-arch basis</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; +     if (mm_tlb_flush_pending(tlb-&gt;mm, false) &gt; 1) {</span>
<span class="quote">&gt; &gt; +             tlb-&gt;start = start;</span>
<span class="quote">&gt; &gt; +             tlb-&gt;end = end;</span>
<span class="quote">&gt; &gt; +     }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It&#39;ll be churn initially but it means any different handling in the TLB</span>
<span class="quote">&gt; batching area will be mostly a core concern.</span>

Fair enough. I will respin next week.
Thanks for the review, Mel.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/arm/include/asm/tlb.h b/arch/arm/include/asm/tlb.h</span>
<span class="p_header">index 3f2eb76243e3..8c26961f0503 100644</span>
<span class="p_header">--- a/arch/arm/include/asm/tlb.h</span>
<span class="p_header">+++ b/arch/arm/include/asm/tlb.h</span>
<span class="p_chunk">@@ -163,13 +163,26 @@</span> <span class="p_context"> tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm, unsigned long start</span>
 #ifdef CONFIG_HAVE_RCU_TABLE_FREE
 	tlb-&gt;batch = NULL;
 #endif
<span class="p_add">+	set_tlb_flush_pending(tlb-&gt;mm);</span>
 }
 
 static inline void
 tlb_finish_mmu(struct mmu_gather *tlb, unsigned long start, unsigned long end)
 {
<span class="p_del">-	tlb_flush_mmu(tlb);</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If there are parallel threads are doing PTE changes on same range</span>
<span class="p_add">+	 * under non-exclusive lock(e.g., mmap_sem read-side) but defer TLB</span>
<span class="p_add">+	 * flush by batching, a thread has stable TLB entry can fail to flush</span>
<span class="p_add">+	 * the TLB by observing pte_none|!pte_dirty, for example so flush TLB</span>
<span class="p_add">+	 * if we detect parallel PTE batching threads.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (mm_tlb_flush_pending(tlb-&gt;mm, false) &gt; 1) {</span>
<span class="p_add">+		tlb-&gt;range_start = start;</span>
<span class="p_add">+		tlb-&gt;range_end = end;</span>
<span class="p_add">+	}</span>
 
<span class="p_add">+	tlb_flush_mmu(tlb);</span>
<span class="p_add">+	clear_tlb_flush_pending(tlb-&gt;mm);</span>
 	/* keep the page table cache within bounds */
 	check_pgt_cache();
 
<span class="p_header">diff --git a/arch/ia64/include/asm/tlb.h b/arch/ia64/include/asm/tlb.h</span>
<span class="p_header">index fced197b9626..22fe976a4693 100644</span>
<span class="p_header">--- a/arch/ia64/include/asm/tlb.h</span>
<span class="p_header">+++ b/arch/ia64/include/asm/tlb.h</span>
<span class="p_chunk">@@ -178,6 +178,7 @@</span> <span class="p_context"> tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm, unsigned long start</span>
 	tlb-&gt;start = start;
 	tlb-&gt;end = end;
 	tlb-&gt;start_addr = ~0UL;
<span class="p_add">+	set_tlb_flush_pending(tlb-&gt;mm);</span>
 }
 
 /*
<span class="p_chunk">@@ -188,10 +189,21 @@</span> <span class="p_context"> static inline void</span>
 tlb_finish_mmu(struct mmu_gather *tlb, unsigned long start, unsigned long end)
 {
 	/*
<span class="p_add">+	 * If there are parallel threads are doing PTE changes on same range</span>
<span class="p_add">+	 * under non-exclusive lock(e.g., mmap_sem read-side) but defer TLB</span>
<span class="p_add">+	 * flush by batching, a thread has stable TLB entry can fail to flush</span>
<span class="p_add">+	 * the TLB by observing pte_none|!pte_dirty, for example so flush TLB</span>
<span class="p_add">+	 * if we detect parallel PTE batching threads.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (mm_tlb_flush_pending(tlb-&gt;mm, false) &gt; 1)</span>
<span class="p_add">+		tlb-&gt;need_flush = 1;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
 	 * Note: tlb-&gt;nr may be 0 at this point, so we can&#39;t rely on tlb-&gt;start_addr and
 	 * tlb-&gt;end_addr.
 	 */
 	ia64_tlb_flush_mmu(tlb, start, end);
<span class="p_add">+	clear_tlb_flush_pending(tlb-&gt;mm);</span>
 
 	/* keep the page table cache within bounds */
 	check_pgt_cache();
<span class="p_header">diff --git a/arch/s390/include/asm/tlb.h b/arch/s390/include/asm/tlb.h</span>
<span class="p_header">index 950af48e88be..69eede9f31e5 100644</span>
<span class="p_header">--- a/arch/s390/include/asm/tlb.h</span>
<span class="p_header">+++ b/arch/s390/include/asm/tlb.h</span>
<span class="p_chunk">@@ -57,6 +57,8 @@</span> <span class="p_context"> static inline void tlb_gather_mmu(struct mmu_gather *tlb,</span>
 	tlb-&gt;end = end;
 	tlb-&gt;fullmm = !(start | (end+1));
 	tlb-&gt;batch = NULL;
<span class="p_add">+</span>
<span class="p_add">+	set_tlb_flush_pending(tlb-&gt;mm);</span>
 }
 
 static inline void tlb_flush_mmu_tlbonly(struct mmu_gather *tlb)
<span class="p_chunk">@@ -79,7 +81,20 @@</span> <span class="p_context"> static inline void tlb_flush_mmu(struct mmu_gather *tlb)</span>
 static inline void tlb_finish_mmu(struct mmu_gather *tlb,
 				  unsigned long start, unsigned long end)
 {
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If there are parallel threads are doing PTE changes on same range</span>
<span class="p_add">+	 * under non-exclusive lock(e.g., mmap_sem read-side) but defer TLB</span>
<span class="p_add">+	 * flush by batching, a thread has stable TLB entry can fail to flush</span>
<span class="p_add">+	 * the TLB by observing pte_none|!pte_dirty, for example so flush TLB</span>
<span class="p_add">+	 * if we detect parallel PTE batching threads.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (mm_tlb_flush_pending(tlb-&gt;mm, false) &gt; 1) {</span>
<span class="p_add">+		tlb-&gt;start = start;</span>
<span class="p_add">+		tlb-&gt;end = end;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	tlb_flush_mmu(tlb);
<span class="p_add">+	clear_tlb_flush_pending(tlb-&gt;mm);</span>
 }
 
 /*
<span class="p_header">diff --git a/arch/sh/include/asm/tlb.h b/arch/sh/include/asm/tlb.h</span>
<span class="p_header">index 46e0d635e36f..37d1e247f0dc 100644</span>
<span class="p_header">--- a/arch/sh/include/asm/tlb.h</span>
<span class="p_header">+++ b/arch/sh/include/asm/tlb.h</span>
<span class="p_chunk">@@ -44,14 +44,16 @@</span> <span class="p_context"> tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm, unsigned long start</span>
 	tlb-&gt;fullmm = !(start | (end+1));
 
 	init_tlb_gather(tlb);
<span class="p_add">+	set_tlb_flush_pending(tlb-&gt;mm);</span>
 }
 
 static inline void
 tlb_finish_mmu(struct mmu_gather *tlb, unsigned long start, unsigned long end)
 {
<span class="p_del">-	if (tlb-&gt;fullmm)</span>
<span class="p_add">+	if (tlb-&gt;fullmm || mm_tlb_flush_pending(tlb-&gt;mm, false) &gt; 1)</span>
 		flush_tlb_mm(tlb-&gt;mm);
 
<span class="p_add">+	clear_tlb_flush_pending(tlb-&gt;mm);</span>
 	/* keep the page table cache within bounds */
 	check_pgt_cache();
 }
<span class="p_header">diff --git a/arch/um/include/asm/tlb.h b/arch/um/include/asm/tlb.h</span>
<span class="p_header">index 600a2e9bfee2..8938c4914bd0 100644</span>
<span class="p_header">--- a/arch/um/include/asm/tlb.h</span>
<span class="p_header">+++ b/arch/um/include/asm/tlb.h</span>
<span class="p_chunk">@@ -53,6 +53,7 @@</span> <span class="p_context"> tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm, unsigned long start</span>
 	tlb-&gt;fullmm = !(start | (end+1));
 
 	init_tlb_gather(tlb);
<span class="p_add">+	set_tlb_flush_pending(tlb-&gt;mm);</span>
 }
 
 extern void flush_tlb_mm_range(struct mm_struct *mm, unsigned long start,
<span class="p_chunk">@@ -87,7 +88,14 @@</span> <span class="p_context"> tlb_flush_mmu(struct mmu_gather *tlb)</span>
 static inline void
 tlb_finish_mmu(struct mmu_gather *tlb, unsigned long start, unsigned long end)
 {
<span class="p_add">+	if (mm_tlb_flush_pending(tlb-&gt;mm, false) &gt; 1) {</span>
<span class="p_add">+		tlb-&gt;start = start;</span>
<span class="p_add">+		tlb-&gt;end = end;</span>
<span class="p_add">+		tlb-&gt;need_flush = 1;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	tlb_flush_mmu(tlb);
<span class="p_add">+	clear_tlb_flush_pending(tlb-&gt;mm);</span>
 
 	/* keep the page table cache within bounds */
 	check_pgt_cache();
<span class="p_header">diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h</span>
<span class="p_header">index 6953d2c706fe..8bb0dfc004be 100644</span>
<span class="p_header">--- a/include/linux/mm_types.h</span>
<span class="p_header">+++ b/include/linux/mm_types.h</span>
<span class="p_chunk">@@ -527,8 +527,9 @@</span> <span class="p_context"> static inline cpumask_t *mm_cpumask(struct mm_struct *mm)</span>
  * which happen while the lock is not taken, and the PTE updates, which happen
  * while the lock is taken, are serialized.
  */
<span class="p_del">-static inline bool mm_tlb_flush_pending(struct mm_struct *mm, bool pt_locked)</span>
<span class="p_add">+static inline int mm_tlb_flush_pending(struct mm_struct *mm, bool pt_locked)</span>
 {
<span class="p_add">+	int nr_pending;</span>
 	/*
 	 * mm_tlb_flush_pending() is safe if it is executed while the page-table
 	 * lock is taken. But if the lock was already released, there does not
<span class="p_chunk">@@ -538,8 +539,10 @@</span> <span class="p_context"> static inline bool mm_tlb_flush_pending(struct mm_struct *mm, bool pt_locked)</span>
 	if (!pt_locked)
 		smp_mb__after_unlock_lock();
 
<span class="p_del">-	return atomic_read(&amp;mm-&gt;tlb_flush_pending) &gt; 0;</span>
<span class="p_add">+	nr_pending = atomic_read(&amp;mm-&gt;tlb_flush_pending);</span>
<span class="p_add">+	return nr_pending;</span>
 }
<span class="p_add">+</span>
 static inline void set_tlb_flush_pending(struct mm_struct *mm)
 {
 	atomic_inc(&amp;mm-&gt;tlb_flush_pending);
<span class="p_header">diff --git a/mm/memory.c b/mm/memory.c</span>
<span class="p_header">index ea9f28e44b81..7861d3556c6e 100644</span>
<span class="p_header">--- a/mm/memory.c</span>
<span class="p_header">+++ b/mm/memory.c</span>
<span class="p_chunk">@@ -239,6 +239,7 @@</span> <span class="p_context"> void tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm, unsigned long</span>
 	tlb-&gt;page_size = 0;
 
 	__tlb_reset_range(tlb);
<span class="p_add">+	set_tlb_flush_pending(tlb-&gt;mm);</span>
 }
 
 static void tlb_flush_mmu_tlbonly(struct mmu_gather *tlb)
<span class="p_chunk">@@ -278,8 +279,18 @@</span> <span class="p_context"> void tlb_flush_mmu(struct mmu_gather *tlb)</span>
 void tlb_finish_mmu(struct mmu_gather *tlb, unsigned long start, unsigned long end)
 {
 	struct mmu_gather_batch *batch, *next;
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If there are parallel threads are doing PTE changes on same range</span>
<span class="p_add">+	 * under non-exclusive lock(e.g., mmap_sem read-side) but defer TLB</span>
<span class="p_add">+	 * flush by batching, a thread has stable TLB entry can fail to flush</span>
<span class="p_add">+	 * the TLB by observing pte_none|!pte_dirty, for example so flush TLB</span>
<span class="p_add">+	 * if we detect parallel PTE batching threads.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (mm_tlb_flush_pending(tlb-&gt;mm, false) &gt; 1)</span>
<span class="p_add">+		__tlb_adjust_range(tlb, start, end - start);</span>
 
 	tlb_flush_mmu(tlb);
<span class="p_add">+	clear_tlb_flush_pending(tlb-&gt;mm);</span>
 
 	/* keep the page table cache within bounds */
 	check_pgt_cache();
<span class="p_chunk">@@ -1485,20 +1496,9 @@</span> <span class="p_context"> void zap_page_range(struct vm_area_struct *vma, unsigned long start,</span>
 	tlb_gather_mmu(&amp;tlb, mm, start, end);
 	update_hiwater_rss(mm);
 	mmu_notifier_invalidate_range_start(mm, start, end);
<span class="p_del">-	for ( ; vma &amp;&amp; vma-&gt;vm_start &lt; end; vma = vma-&gt;vm_next) {</span>
<span class="p_add">+	for ( ; vma &amp;&amp; vma-&gt;vm_start &lt; end; vma = vma-&gt;vm_next)</span>
 		unmap_single_vma(&amp;tlb, vma, start, end, NULL);
 
<span class="p_del">-		/*</span>
<span class="p_del">-		 * zap_page_range does not specify whether mmap_sem should be</span>
<span class="p_del">-		 * held for read or write. That allows parallel zap_page_range</span>
<span class="p_del">-		 * operations to unmap a PTE and defer a flush meaning that</span>
<span class="p_del">-		 * this call observes pte_none and fails to flush the TLB.</span>
<span class="p_del">-		 * Rather than adding a complex API, ensure that no stale</span>
<span class="p_del">-		 * TLB entries exist when this call returns.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		flush_tlb_range(vma, start, end);</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
 	mmu_notifier_invalidate_range_end(mm, start, end);
 	tlb_finish_mmu(&amp;tlb, start, end);
 }

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



