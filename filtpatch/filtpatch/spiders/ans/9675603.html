
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[4/5] Hot-remove implementation for arm64 - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [4/5] Hot-remove implementation for arm64</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=171743">Andrea Reale</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>April 11, 2017, 2:55 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;897973dd5d3fc91c70aba4b44350099a61c3a12c.1491920513.git.ar@linux.vnet.ibm.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9675603/mbox/"
   >mbox</a>
|
   <a href="/patch/9675603/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9675603/">/patch/9675603/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	E60A6600CB for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 11 Apr 2017 14:56:07 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id D512828505
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 11 Apr 2017 14:56:07 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id C9CC6285AF; Tue, 11 Apr 2017 14:56:07 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id BA2AC28505
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 11 Apr 2017 14:56:06 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1753555AbdDKOz7 (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 11 Apr 2017 10:55:59 -0400
Received: from mx0b-001b2d01.pphosted.com ([148.163.158.5]:49337 &quot;EHLO
	mx0a-001b2d01.pphosted.com&quot; rhost-flags-OK-OK-OK-FAIL)
	by vger.kernel.org with ESMTP id S1752385AbdDKOz4 (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 11 Apr 2017 10:55:56 -0400
Received: from pps.filterd (m0098417.ppops.net [127.0.0.1])
	by mx0a-001b2d01.pphosted.com (8.16.0.20/8.16.0.20) with SMTP id
	v3BEmqFt025774
	for &lt;linux-kernel@vger.kernel.org&gt;; Tue, 11 Apr 2017 10:55:56 -0400
Received: from e06smtp13.uk.ibm.com (e06smtp13.uk.ibm.com [195.75.94.109])
	by mx0a-001b2d01.pphosted.com with ESMTP id 29rkcv4cf2-1
	(version=TLSv1.2 cipher=AES256-SHA bits=256 verify=NOT)
	for &lt;linux-kernel@vger.kernel.org&gt;; Tue, 11 Apr 2017 10:55:55 -0400
Received: from localhost
	by e06smtp13.uk.ibm.com with IBM ESMTP SMTP Gateway: Authorized Use
	Only! Violators will be prosecuted
	for &lt;linux-kernel@vger.kernel.org&gt; from &lt;ar@linux.vnet.ibm.com&gt;;
	Tue, 11 Apr 2017 15:55:53 +0100
Received: from b06cxnps4076.portsmouth.uk.ibm.com (9.149.109.198)
	by e06smtp13.uk.ibm.com (192.168.101.143) with IBM ESMTP SMTP
	Gateway: Authorized Use Only! Violators will be prosecuted; 
	Tue, 11 Apr 2017 15:55:49 +0100
Received: from d06av22.portsmouth.uk.ibm.com (d06av22.portsmouth.uk.ibm.com
	[9.149.105.58])
	by b06cxnps4076.portsmouth.uk.ibm.com (8.14.9/8.14.9/NCO v10.0) with
	ESMTP id v3BEtnlq38928512; Tue, 11 Apr 2017 14:55:49 GMT
Received: from d06av22.portsmouth.uk.ibm.com (unknown [127.0.0.1])
	by IMSVA (Postfix) with ESMTP id 266A34C046;
	Tue, 11 Apr 2017 15:55:13 +0100 (BST)
Received: from d06av22.portsmouth.uk.ibm.com (unknown [127.0.0.1])
	by IMSVA (Postfix) with ESMTP id AAB804C044;
	Tue, 11 Apr 2017 15:55:12 +0100 (BST)
Received: from samekh (unknown [9.162.48.51])
	by d06av22.portsmouth.uk.ibm.com (Postfix) with ESMTPS;
	Tue, 11 Apr 2017 15:55:12 +0100 (BST)
Date: Tue, 11 Apr 2017 15:55:42 +0100
From: Andrea Reale &lt;ar@linux.vnet.ibm.com&gt;
To: linux-arm-kernel@lists.infradead.org
Cc: m.bielski@virtualopensystems.com, ar@linux.vnet.ibm.com,
	scott.branden@broadcom.com, will.deacon@arm.com,
	qiuxishi@huawei.com, f.fainelli@gmail.com, linux-kernel@vger.kernel.org
Subject: [PATCH 4/5] Hot-remove implementation for arm64
References: &lt;cover.1491920513.git.ar@linux.vnet.ibm.com&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Disposition: inline
In-Reply-To: &lt;cover.1491920513.git.ar@linux.vnet.ibm.com&gt;
User-Agent: Mutt/1.5.21 (2010-09-15)
X-TM-AS-GCONF: 00
x-cbid: 17041114-0012-0000-0000-00000508183C
X-IBM-AV-DETECTION: SAVI=unused REMOTE=unused XFE=unused
x-cbparentid: 17041114-0013-0000-0000-000018038F64
Message-Id: &lt;897973dd5d3fc91c70aba4b44350099a61c3a12c.1491920513.git.ar@linux.vnet.ibm.com&gt;
X-Proofpoint-Virus-Version: vendor=fsecure engine=2.50.10432:, ,
	definitions=2017-04-11_13:, , signatures=0
X-Proofpoint-Spam-Details: rule=outbound_notspam policy=outbound score=0
	spamscore=0 suspectscore=3
	malwarescore=0 phishscore=0 adultscore=0 bulkscore=0 classifier=spam
	adjust=0 reason=mlx scancount=1 engine=8.0.1-1702020001
	definitions=main-1704110115
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=171743">Andrea Reale</a> - April 11, 2017, 2:55 p.m.</div>
<pre class="content">
- arch_remove_memory interface
- kernel page tables cleanup
- vmemmap_free implementation for arm64
<span class="signed-off-by">
Signed-off-by: Andrea Reale &lt;ar@linux.vnet.ibm.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Maciej Bielski &lt;m.bielski@virtualopensystems.com&gt;</span>
---
 arch/arm64/Kconfig               |   3 +
 arch/arm64/include/asm/mmu.h     |   4 +
 arch/arm64/include/asm/pgtable.h |  15 ++
 arch/arm64/mm/init.c             |  32 +++-
 arch/arm64/mm/mmu.c              | 390 ++++++++++++++++++++++++++++++++++++++-
 5 files changed, 438 insertions(+), 6 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=30282">Mark Rutland</a> - April 11, 2017, 5:12 p.m.</div>
<pre class="content">
Hi,

On Tue, Apr 11, 2017 at 03:55:42PM +0100, Andrea Reale wrote:
<span class="quote">&gt; +static inline unsigned long pmd_page_vaddr(pmd_t pmd)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return (unsigned long) __va(pmd_page_paddr(pmd));</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  /* Find an entry in the third-level page table. */</span>
<span class="quote">&gt;  #define pte_index(addr)		(((addr) &gt;&gt; PAGE_SHIFT) &amp; (PTRS_PER_PTE - 1))</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -450,6 +455,11 @@ static inline phys_addr_t pud_page_paddr(pud_t pud)</span>
<span class="quote">&gt;  	return pud_val(pud) &amp; PHYS_MASK &amp; (s32)PAGE_MASK;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +static inline unsigned long pud_page_vaddr(pud_t pud)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return (unsigned long) __va(pud_page_paddr(pud));</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  /* Find an entry in the second-level page table. */</span>
<span class="quote">&gt;  #define pmd_index(addr)		(((addr) &gt;&gt; PMD_SHIFT) &amp; (PTRS_PER_PMD - 1))</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -502,6 +512,11 @@ static inline phys_addr_t pgd_page_paddr(pgd_t pgd)</span>
<span class="quote">&gt;  	return pgd_val(pgd) &amp; PHYS_MASK &amp; (s32)PAGE_MASK;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +static inline unsigned long pgd_page_vaddr(pgd_t pgd)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return (unsigned long) __va(pgd_page_paddr(pgd));</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>

These duplicate the existing p*d_offset*() functions, and I do not think
they are necessary. More on that below.

[...]
<span class="quote">
&gt; @@ -551,7 +551,6 @@ int arch_add_memory(int nid, u64 start, u64 size, bool for_device)</span>
<span class="quote">&gt;  	unsigned long nr_pages = size &gt;&gt; PAGE_SHIFT;</span>
<span class="quote">&gt;  	unsigned long end_pfn = start_pfn + nr_pages;</span>
<span class="quote">&gt;  	unsigned long max_sparsemem_pfn = 1UL &lt;&lt; (MAX_PHYSMEM_BITS-PAGE_SHIFT);</span>
<span class="quote">&gt; -	unsigned long pfn;</span>
<span class="quote">&gt;  	int ret;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (end_pfn &gt; max_sparsemem_pfn) {</span>

Should this have been part of a prior patch?

This patch doesn&#39;t remove any users of this variable.

[...]
<span class="quote">
&gt; +static void  free_pagetable(struct page *page, int order, bool direct)</span>

This &quot;direct&quot; parameter needs a better name, and a description in a
comment block above this function.
<span class="quote">
&gt; +{</span>
<span class="quote">&gt; +	unsigned long magic;</span>
<span class="quote">&gt; +	unsigned int nr_pages = 1 &lt;&lt; order;</span>
<span class="quote">&gt; +	struct vmem_altmap *altmap = to_vmem_altmap((unsigned long) page);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (altmap) {</span>
<span class="quote">&gt; +		vmem_altmap_free(altmap, nr_pages);</span>
<span class="quote">&gt; +		return;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/* bootmem page has reserved flag */</span>
<span class="quote">&gt; +	if (PageReserved(page)) {</span>
<span class="quote">&gt; +		__ClearPageReserved(page);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		magic = (unsigned long)page-&gt;lru.next;</span>
<span class="quote">&gt; +		if (magic == SECTION_INFO || magic == MIX_SECTION_INFO) {</span>
<span class="quote">&gt; +			while (nr_pages--)</span>
<span class="quote">&gt; +				put_page_bootmem(page++);</span>
<span class="quote">&gt; +		} else {</span>
<span class="quote">&gt; +			while (nr_pages--)</span>
<span class="quote">&gt; +				free_reserved_page(page++);</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +	} else {</span>
<span class="quote">&gt; +		/*</span>
<span class="quote">&gt; +		 * Only direct pagetable allocation (those allocated via</span>
<span class="quote">&gt; +		 * hotplug) call the pgtable_page_ctor; vmemmap pgtable</span>
<span class="quote">&gt; +		 * allocations don&#39;t.</span>
<span class="quote">&gt; +		 */</span>
<span class="quote">&gt; +		if (direct)</span>
<span class="quote">&gt; +			pgtable_page_dtor(page);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		free_pages((unsigned long)page_address(page), order);</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +}</span>

This largely looks like a copy of the x86 code. Why can that not be
factored out to an arch-neutral location?
<span class="quote">
&gt; +</span>
<span class="quote">&gt; +static void free_pte_table(pmd_t *pmd, bool direct)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	pte_t *pte_start, *pte;</span>
<span class="quote">&gt; +	struct page *page;</span>
<span class="quote">&gt; +	int i;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	pte_start =  (pte_t *) pmd_page_vaddr(*pmd);</span>
<span class="quote">&gt; +	/* Check if there is no valid entry in the PMD */</span>
<span class="quote">&gt; +	for (i = 0; i &lt; PTRS_PER_PTE; i++) {</span>
<span class="quote">&gt; +		pte = pte_start + i;</span>
<span class="quote">&gt; +		if (!pte_none(*pte))</span>
<span class="quote">&gt; +			return;</span>
<span class="quote">&gt; +	}</span>

If we must walk the tables in this way, please use the existing pattern
from arch/arm64/mm/dump.c.

e.g. 

	pte_t *pte;

	pte = pte_offset_kernel(pmd, 0UL);
	for (i = 0; i &lt; PTRS_PER_PTE; i++, pte++)
		if (!pte_none)
			return;
<span class="quote">
&gt; +</span>
<span class="quote">&gt; +	page = pmd_page(*pmd);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	free_pagetable(page, 0, direct);</span>

The page has been freed here, and may be subject to arbitrary
modification...
<span class="quote">
&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * This spin lock could be only taken in _pte_aloc_kernel</span>
<span class="quote">&gt; +	 * in mm/memory.c and nowhere else (for arm64). Not sure if</span>
<span class="quote">&gt; +	 * the function above can be called concurrently. In doubt,</span>
<span class="quote">&gt; +	 * I am living it here for now, but it probably can be removed</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	spin_lock(&amp;init_mm.page_table_lock);</span>
<span class="quote">&gt; +	pmd_clear(pmd);</span>

... but we only remove it from the page tables here, so the page table
walkers can see junk in the tables, were the page reused in the mean
timer.

After clearing the PMD, it needs to be cleared from TLBs. We allow
partial walks to be cached, so the TLBs may still start walking this
entry or beyond.
<span class="quote">
&gt; +	spin_unlock(&amp;init_mm.page_table_lock);</span>
<span class="quote">&gt; +}</span>

The same comments apply to all the free_p*d_table() functions, so I
shan&#39;t repeat them.

[...]
<span class="quote">
&gt; +static void remove_pte_table(pte_t *pte, unsigned long addr,</span>
<span class="quote">&gt; +	unsigned long end, bool direct)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	unsigned long next;</span>
<span class="quote">&gt; +	void *page_addr;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	for (; addr &lt; end; addr = next, pte++) {</span>
<span class="quote">&gt; +		next = (addr + PAGE_SIZE) &amp; PAGE_MASK;</span>
<span class="quote">&gt; +		if (next &gt; end)</span>
<span class="quote">&gt; +			next = end;</span>

Please use the usual p*d_addr_end() functions. See alloc_init_pmd() and
friends in arch/arm64/mm/mmu.c for examples of how to use them.
<span class="quote">
&gt; +</span>
<span class="quote">&gt; +		if (!pte_present(*pte))</span>
<span class="quote">&gt; +			continue;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		if (PAGE_ALIGNED(addr) &amp;&amp; PAGE_ALIGNED(next)) {</span>

When would those addresses *not* be page-aligned? By construction, next
must be. Unplugging partial pages of memory makes no sense, so surely
addr is page-aligned when passed in?
<span class="quote">
&gt; +			/*</span>
<span class="quote">&gt; +			 * Do not free direct mapping pages since they were</span>
<span class="quote">&gt; +			 * freed when offlining, or simplely not in use.</span>
<span class="quote">&gt; +			 */</span>
<span class="quote">&gt; +			if (!direct)</span>
<span class="quote">&gt; +				free_pagetable(pte_page(*pte), 0, direct);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			/*</span>
<span class="quote">&gt; +			 * This spin lock could be only</span>
<span class="quote">&gt; +			 * taken in _pte_aloc_kernel in</span>
<span class="quote">&gt; +			 * mm/memory.c and nowhere else</span>
<span class="quote">&gt; +			 * (for arm64). Not sure if the</span>
<span class="quote">&gt; +			 * function above can be called</span>
<span class="quote">&gt; +			 * concurrently. In doubt,</span>
<span class="quote">&gt; +			 * I am living it here for now,</span>
<span class="quote">&gt; +			 * but it probably can be removed.</span>
<span class="quote">&gt; +			 */</span>
<span class="quote">&gt; +			spin_lock(&amp;init_mm.page_table_lock);</span>
<span class="quote">&gt; +			pte_clear(&amp;init_mm, addr, pte);</span>
<span class="quote">&gt; +			spin_unlock(&amp;init_mm.page_table_lock);</span>
<span class="quote">&gt; +		} else {</span>
<span class="quote">&gt; +			/*</span>
<span class="quote">&gt; +			 * If we are here, we are freeing vmemmap pages since</span>
<span class="quote">&gt; +			 * direct mapped memory ranges to be freed are aligned.</span>
<span class="quote">&gt; +			 *</span>
<span class="quote">&gt; +			 * If we are not removing the whole page, it means</span>
<span class="quote">&gt; +			 * other page structs in this page are being used and</span>
<span class="quote">&gt; +			 * we canot remove them. So fill the unused page_structs</span>
<span class="quote">&gt; +			 * with 0xFD, and remove the page when it is wholly</span>
<span class="quote">&gt; +			 * filled with 0xFD.</span>
<span class="quote">&gt; +			 */</span>
<span class="quote">&gt; +			memset((void *)addr, PAGE_INUSE, next - addr);</span>

What&#39;s special about 0xFD?

Why do we need to mess with the page array in this manner? Why can&#39;t we
detect when a range is free by querying memblock, for example?
<span class="quote">
&gt; +</span>
<span class="quote">&gt; +			page_addr = page_address(pte_page(*pte));</span>
<span class="quote">&gt; +			if (!memchr_inv(page_addr, PAGE_INUSE, PAGE_SIZE)) {</span>
<span class="quote">&gt; +				free_pagetable(pte_page(*pte), 0, direct);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +				/*</span>
<span class="quote">&gt; +				 * This spin lock could be only</span>
<span class="quote">&gt; +				 * taken in _pte_aloc_kernel in</span>
<span class="quote">&gt; +				 * mm/memory.c and nowhere else</span>
<span class="quote">&gt; +				 * (for arm64). Not sure if the</span>
<span class="quote">&gt; +				 * function above can be called</span>
<span class="quote">&gt; +				 * concurrently. In doubt,</span>
<span class="quote">&gt; +				 * I am living it here for now,</span>
<span class="quote">&gt; +				 * but it probably can be removed.</span>
<span class="quote">&gt; +				 */</span>
<span class="quote">&gt; +				spin_lock(&amp;init_mm.page_table_lock);</span>
<span class="quote">&gt; +				pte_clear(&amp;init_mm, addr, pte);</span>
<span class="quote">&gt; +				spin_unlock(&amp;init_mm.page_table_lock);</span>

This logic appears to be duplicated with the free_*_table functions, and
looks incredibly suspicious.

To me, it doesn&#39;t make sense that we&#39;d need the lock *only* to alter the
leaf entries.
<span class="quote">
&gt; +			}</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	// I am adding this flush here in simmetry to the x86 code.</span>
<span class="quote">&gt; +	// Why do I need to call it here and not in remove_p[mu]d</span>
<span class="quote">&gt; +	flush_tlb_all();</span>

If the page tables weren&#39;t freed until this point, it might be that this
is just amortizing the cost of removing them from the TLBs.

Given that they&#39;re freed first, this makes no sense to me.
<span class="quote">
&gt; +}</span>

The same commenst apply to all the remove_p*d_table() functions, so I
shan&#39;t repeat them.
<span class="quote">
&gt; +</span>
<span class="quote">&gt; +static void remove_pmd_table(pmd_t *pmd, unsigned long addr,</span>
<span class="quote">&gt; +	unsigned long end, bool direct)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	unsigned long next;</span>
<span class="quote">&gt; +	void *page_addr;</span>
<span class="quote">&gt; +	pte_t *pte;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	for (; addr &lt; end; addr = next, pmd++) {</span>
<span class="quote">&gt; +		next = pmd_addr_end(addr, end);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		if (!pmd_present(*pmd))</span>
<span class="quote">&gt; +			continue;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		// check if we are using 2MB section mappings</span>
<span class="quote">&gt; +		if (pmd_sect(*pmd)) {</span>
<span class="quote">&gt; +			if (PAGE_ALIGNED(addr) &amp;&amp; PAGE_ALIGNED(next)) {</span>

Surely you&#39;re intending to check if you can free the whole pmd? i.e.
that addr and next are pmd-aligned?

Can we ever be in a situation where we&#39;re requested to free a partial
pmd that could be section mapped?

If that&#39;s the case, we&#39;ll *need* to split the pmd, which we can&#39;t do on
live page tables.
<span class="quote">
&gt; +				if (!direct) {</span>
<span class="quote">&gt; +					free_pagetable(pmd_page(*pmd),</span>
<span class="quote">&gt; +						get_order(PMD_SIZE), direct);</span>
<span class="quote">&gt; +				}</span>
<span class="quote">&gt; +				/*</span>
<span class="quote">&gt; +				 * This spin lock could be only</span>
<span class="quote">&gt; +				 * taken in _pte_aloc_kernel in</span>
<span class="quote">&gt; +				 * mm/memory.c and nowhere else</span>
<span class="quote">&gt; +				 * (for arm64). Not sure if the</span>
<span class="quote">&gt; +				 * function above can be called</span>
<span class="quote">&gt; +				 * concurrently. In doubt,</span>
<span class="quote">&gt; +				 * I am living it here for now,</span>
<span class="quote">&gt; +				 * but it probably can be removed.</span>
<span class="quote">&gt; +				 */</span>
<span class="quote">&gt; +				spin_lock(&amp;init_mm.page_table_lock);</span>
<span class="quote">&gt; +				pmd_clear(pmd);</span>
<span class="quote">&gt; +				spin_unlock(&amp;init_mm.page_table_lock);</span>
<span class="quote">&gt; +			} else {</span>
<span class="quote">&gt; +				/* If here, we are freeing vmemmap pages. */</span>
<span class="quote">&gt; +				memset((void *)addr, PAGE_INUSE, next - addr);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +				page_addr = page_address(pmd_page(*pmd));</span>
<span class="quote">&gt; +				if (!memchr_inv(page_addr, PAGE_INUSE,</span>
<span class="quote">&gt; +						PMD_SIZE)) {</span>
<span class="quote">&gt; +					free_pagetable(pmd_page(*pmd),</span>
<span class="quote">&gt; +						get_order(PMD_SIZE), direct);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +					/*</span>
<span class="quote">&gt; +					 * This spin lock could be only</span>
<span class="quote">&gt; +					 * taken in _pte_aloc_kernel in</span>
<span class="quote">&gt; +					 * mm/memory.c and nowhere else</span>
<span class="quote">&gt; +					 * (for arm64). Not sure if the</span>
<span class="quote">&gt; +					 * function above can be called</span>
<span class="quote">&gt; +					 * concurrently. In doubt,</span>
<span class="quote">&gt; +					 * I am living it here for now,</span>
<span class="quote">&gt; +					 * but it probably can be removed.</span>
<span class="quote">&gt; +					 */</span>
<span class="quote">&gt; +					spin_lock(&amp;init_mm.page_table_lock);</span>
<span class="quote">&gt; +					pmd_clear(pmd);</span>
<span class="quote">&gt; +					spin_unlock(&amp;init_mm.page_table_lock);</span>
<span class="quote">&gt; +				}</span>

I don&#39;t think this is correct.

If we&#39;re getting rid of a partial pmd, we *must* split the pmd.
Otherwise, we&#39;re leaving bits mapped that should not be. If we split the
pmd, we can free the individual pages as we would for a non-section
mapping.

As above, we can&#39;t split block entries within live tables, so that will
be painful at best.

If we can&#39;t split a pmd, hen we cannot free a partial pmd, and will need
to reject request to do so.

The same comments (with s/pmu/pud/, etc) apply for the higher level
remove_p*d_table functions.

[...]
<span class="quote">
&gt; +void remove_pagetable(unsigned long start, unsigned long end, bool direct)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	unsigned long next;</span>
<span class="quote">&gt; +	unsigned long addr;</span>
<span class="quote">&gt; +	pgd_t *pgd;</span>
<span class="quote">&gt; +	pud_t *pud;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	for (addr = start; addr &lt; end; addr = next) {</span>
<span class="quote">&gt; +		next = pgd_addr_end(addr, end);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		pgd = pgd_offset_k(addr);</span>
<span class="quote">&gt; +		if (pgd_none(*pgd))</span>
<span class="quote">&gt; +			continue;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		pud = pud_offset(pgd, addr);</span>
<span class="quote">&gt; +		remove_pud_table(pud, addr, next, direct);</span>
<span class="quote">&gt; +		/*</span>
<span class="quote">&gt; +		 * When the PUD is folded on the PGD (three levels of paging),</span>
<span class="quote">&gt; +		 * I did already clear the PMD page in free_pmd_table,</span>
<span class="quote">&gt; +		 * and reset the corresponding PGD==PUD entry.</span>
<span class="quote">&gt; +		 */</span>
<span class="quote">&gt; +#if CONFIG_PGTABLE_LEVELS &gt; 3</span>
<span class="quote">&gt; +		free_pud_table(pgd, direct);</span>
<span class="quote">&gt;  #endif</span>

This looks suspicious. Shouldn&#39;t we have a similar check for PMD, to
cater for CONFIG_PGTABLE_LEVELS == 2? e.g. 64K pages, 42-bit VA.

We should be able to hide this distinction in helpers for both cases.
<span class="quote">
&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	flush_tlb_all();</span>

This is too late to be useful.

Thanks,
Mark.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=171743">Andrea Reale</a> - April 14, 2017, 2:01 p.m.</div>
<pre class="content">
Hi Mark,

thanks for your thorough feedback, it is really appreciated.

I have a few comments and I&#39;d like to ask for further clarification,
if you don&#39;t mind, in order to help us work on a new version of the
patch.

Before I go into details, let me point out that, as you have noted
yourself, the structure of the patch is largely derived from the
x86_64 hot-remove code (mostly introduced in commit ae9aae9eda2db7
&quot;memory-hotplug: common APIs to support page tables hot-remove&quot;) trying
to account for architectural differences. In this process, I guess it
is likely that I might have made assumptions that are true for x86_64
but do not hold for arm64. Whenever you feel this is the case, I would
be really grateful if you could help identify them.

Please, find detailed replies to your comments in-line.

On Tue, Apr 11, 2017 at 06:12:11PM +0100, Mark Rutland wrote:
<span class="quote">&gt; Hi,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; On Tue, Apr 11, 2017 at 03:55:42PM +0100, Andrea Reale wrote:</span>
<span class="quote">&gt; &gt; +static inline unsigned long pmd_page_vaddr(pmd_t pmd)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	return (unsigned long) __va(pmd_page_paddr(pmd));</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt;  /* Find an entry in the third-level page table. */</span>
<span class="quote">&gt; &gt;  #define pte_index(addr)		(((addr) &gt;&gt; PAGE_SHIFT) &amp; (PTRS_PER_PTE - 1))</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; @@ -450,6 +455,11 @@ static inline phys_addr_t pud_page_paddr(pud_t pud)</span>
<span class="quote">&gt; &gt;  	return pud_val(pud) &amp; PHYS_MASK &amp; (s32)PAGE_MASK;</span>
<span class="quote">&gt; &gt;  }</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; +static inline unsigned long pud_page_vaddr(pud_t pud)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	return (unsigned long) __va(pud_page_paddr(pud));</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt;  /* Find an entry in the second-level page table. */</span>
<span class="quote">&gt; &gt;  #define pmd_index(addr)		(((addr) &gt;&gt; PMD_SHIFT) &amp; (PTRS_PER_PMD - 1))</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; @@ -502,6 +512,11 @@ static inline phys_addr_t pgd_page_paddr(pgd_t pgd)</span>
<span class="quote">&gt; &gt;  	return pgd_val(pgd) &amp; PHYS_MASK &amp; (s32)PAGE_MASK;</span>
<span class="quote">&gt; &gt;  }</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; +static inline unsigned long pgd_page_vaddr(pgd_t pgd)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	return (unsigned long) __va(pgd_page_paddr(pgd));</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; These duplicate the existing p*d_offset*() functions, and I do not think</span>
<span class="quote">&gt; they are necessary. More on that below.</span>

I agree they are redundant. It just seemed cleaner rather than
offsetting 0, but fair enough. 
<span class="quote">
&gt; [...]</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; @@ -551,7 +551,6 @@ int arch_add_memory(int nid, u64 start, u64 size, bool for_device)</span>
<span class="quote">&gt; &gt;  	unsigned long nr_pages = size &gt;&gt; PAGE_SHIFT;</span>
<span class="quote">&gt; &gt;  	unsigned long end_pfn = start_pfn + nr_pages;</span>
<span class="quote">&gt; &gt;  	unsigned long max_sparsemem_pfn = 1UL &lt;&lt; (MAX_PHYSMEM_BITS-PAGE_SHIFT);</span>
<span class="quote">&gt; &gt; -	unsigned long pfn;</span>
<span class="quote">&gt; &gt;  	int ret;</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  	if (end_pfn &gt; max_sparsemem_pfn) {</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Should this have been part of a prior patch?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This patch doesn&#39;t remove any users of this variable.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; [...]</span>

Indeed, the unused pfn variable is a leftover of patch 3/5. We will
fix that in the next version.
<span class="quote">
&gt; &gt; +static void  free_pagetable(struct page *page, int order, bool direct)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This &quot;direct&quot; parameter needs a better name, and a description in a</span>
<span class="quote">&gt; comment block above this function.</span>
 
The name direct is inherited directly from the x86_64 hot remove code.
It serves to distinguish if we are removing either a pagetable page that
is mapping to the direct mapping space (I think it is called also linear
mapping area somewhere else) or a pagetable page or a data page 
from vmemmap.

In this specific set of functions, the flag is needed because the various
alloc_init_p*d used for allocating entries for direct memory mapping
rely on pgd_pgtable_alloc, which in its turn calls  pgtable_page_ctor;
hence, we need to call the dtor.  On the contrary, vmemmap entries
are created using vmemmap_alloc_block (from within vmemmap_populate),
which does not call pgtable_page_ctor when allocating pages.

I am not sure I understand why the pgtable_page_ctor is not called when
allocating vmemmap page tables, but that&#39;s the current situation.
<span class="quote">
&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	unsigned long magic;</span>
<span class="quote">&gt; &gt; +	unsigned int nr_pages = 1 &lt;&lt; order;</span>
<span class="quote">&gt; &gt; +	struct vmem_altmap *altmap = to_vmem_altmap((unsigned long) page);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	if (altmap) {</span>
<span class="quote">&gt; &gt; +		vmem_altmap_free(altmap, nr_pages);</span>
<span class="quote">&gt; &gt; +		return;</span>
<span class="quote">&gt; &gt; +	}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	/* bootmem page has reserved flag */</span>
<span class="quote">&gt; &gt; +	if (PageReserved(page)) {</span>
<span class="quote">&gt; &gt; +		__ClearPageReserved(page);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		magic = (unsigned long)page-&gt;lru.next;</span>
<span class="quote">&gt; &gt; +		if (magic == SECTION_INFO || magic == MIX_SECTION_INFO) {</span>
<span class="quote">&gt; &gt; +			while (nr_pages--)</span>
<span class="quote">&gt; &gt; +				put_page_bootmem(page++);</span>
<span class="quote">&gt; &gt; +		} else {</span>
<span class="quote">&gt; &gt; +			while (nr_pages--)</span>
<span class="quote">&gt; &gt; +				free_reserved_page(page++);</span>
<span class="quote">&gt; &gt; +		}</span>
<span class="quote">&gt; &gt; +	} else {</span>
<span class="quote">&gt; &gt; +		/*</span>
<span class="quote">&gt; &gt; +		 * Only direct pagetable allocation (those allocated via</span>
<span class="quote">&gt; &gt; +		 * hotplug) call the pgtable_page_ctor; vmemmap pgtable</span>
<span class="quote">&gt; &gt; +		 * allocations don&#39;t.</span>
<span class="quote">&gt; &gt; +		 */</span>
<span class="quote">&gt; &gt; +		if (direct)</span>
<span class="quote">&gt; &gt; +			pgtable_page_dtor(page);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		free_pages((unsigned long)page_address(page), order);</span>
<span class="quote">&gt; &gt; +	}</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This largely looks like a copy of the x86 code. Why can that not be</span>
<span class="quote">&gt; factored out to an arch-neutral location?</span>

Yes it probably can - the only difference being calling
pgtable_page_dtor when it needs to - but I am not confident enough to
say that it would really be architecture neutral or just specific to
only arm64 and x86.  For example, I don&#39;t see this used anywhere else
for hot-removing memory.

(Actually, also a large part of remove_*_table and free_*_table could
probably be factored, but I wouldn&#39;t be sure how to deal with the
differences in the pgtable.h macros used throughout)
<span class="quote">
&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static void free_pte_table(pmd_t *pmd, bool direct)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	pte_t *pte_start, *pte;</span>
<span class="quote">&gt; &gt; +	struct page *page;</span>
<span class="quote">&gt; &gt; +	int i;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	pte_start =  (pte_t *) pmd_page_vaddr(*pmd);</span>
<span class="quote">&gt; &gt; +	/* Check if there is no valid entry in the PMD */</span>
<span class="quote">&gt; &gt; +	for (i = 0; i &lt; PTRS_PER_PTE; i++) {</span>
<span class="quote">&gt; &gt; +		pte = pte_start + i;</span>
<span class="quote">&gt; &gt; +		if (!pte_none(*pte))</span>
<span class="quote">&gt; &gt; +			return;</span>
<span class="quote">&gt; &gt; +	}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; If we must walk the tables in this way, please use the existing pattern</span>
<span class="quote">&gt; from arch/arm64/mm/dump.c.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; e.g. </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	pte_t *pte;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	pte = pte_offset_kernel(pmd, 0UL);</span>
<span class="quote">&gt; 	for (i = 0; i &lt; PTRS_PER_PTE; i++, pte++)</span>
<span class="quote">&gt; 		if (!pte_none)</span>
<span class="quote">&gt; 			return;</span>

Thanks.
<span class="quote">
&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	page = pmd_page(*pmd);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	free_pagetable(page, 0, direct);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The page has been freed here, and may be subject to arbitrary</span>
<span class="quote">&gt; modification...</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	/*</span>
<span class="quote">&gt; &gt; +	 * This spin lock could be only taken in _pte_aloc_kernel</span>
<span class="quote">&gt; &gt; +	 * in mm/memory.c and nowhere else (for arm64). Not sure if</span>
<span class="quote">&gt; &gt; +	 * the function above can be called concurrently. In doubt,</span>
<span class="quote">&gt; &gt; +	 * I am living it here for now, but it probably can be removed</span>
<span class="quote">&gt; &gt; +	 */</span>
<span class="quote">&gt; &gt; +	spin_lock(&amp;init_mm.page_table_lock);</span>
<span class="quote">&gt; &gt; +	pmd_clear(pmd);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; ... but we only remove it from the page tables here, so the page table</span>
<span class="quote">&gt; walkers can see junk in the tables, were the page reused in the mean</span>
<span class="quote">&gt; timer.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; After clearing the PMD, it needs to be cleared from TLBs. We allow</span>
<span class="quote">&gt; partial walks to be cached, so the TLBs may still start walking this</span>
<span class="quote">&gt; entry or beyond.</span>

I guess that the safe approach would be something along the lines:
1. clear the page table 
2. flush the tlbs
3. free the page

am I mistaken? When I am flushing intermediate p*d entries, would it be
more appropriate to use something like __flush_tlb_pgtable() to clear
cached partial walks rather than flushing the whole table? I mean,
hot-remove is not going to be a frequent operation anyway, so I don&#39;t
think that flushing the whole tlb would be a great deal of harm
anyway.

My question at this point would be: how come that the code structure above
works for x86_64 hot-remove? My assumption, when I was writing this, was
that there would be no problem since this code is called when we are sure
that all the memory mapped by these entries has been already offlined,
so nobody should be using those VAs anyway (also considering that there
cannot be any two mem hot-plug/remove actions running concurrently).
Is that correct?
<span class="quote"> 
&gt; &gt; +	spin_unlock(&amp;init_mm.page_table_lock);</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The same comments apply to all the free_p*d_table() functions, so I</span>
<span class="quote">&gt; shan&#39;t repeat them.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; [...]</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; +static void remove_pte_table(pte_t *pte, unsigned long addr,</span>
<span class="quote">&gt; &gt; +	unsigned long end, bool direct)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	unsigned long next;</span>
<span class="quote">&gt; &gt; +	void *page_addr;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	for (; addr &lt; end; addr = next, pte++) {</span>
<span class="quote">&gt; &gt; +		next = (addr + PAGE_SIZE) &amp; PAGE_MASK;</span>
<span class="quote">&gt; &gt; +		if (next &gt; end)</span>
<span class="quote">&gt; &gt; +			next = end;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Please use the usual p*d_addr_end() functions. See alloc_init_pmd() and</span>
<span class="quote">&gt; friends in arch/arm64/mm/mmu.c for examples of how to use them.</span>

we used the p*d_addr_end family of functions when dealing with p*d(s). I
cannot identify an equivalent for pte entries. Would you recommend adding
a pte_addr_end macro in pgtable.h?
<span class="quote">
&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		if (!pte_present(*pte))</span>
<span class="quote">&gt; &gt; +			continue;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		if (PAGE_ALIGNED(addr) &amp;&amp; PAGE_ALIGNED(next)) {</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; When would those addresses *not* be page-aligned? By construction, next</span>
<span class="quote">&gt; must be. Unplugging partial pages of memory makes no sense, so surely</span>
<span class="quote">&gt; addr is page-aligned when passed in?</span>

The issue here is that this function is called in one of two cases:
1. to clear pagetables of directly mapped (linear) memory 
2. Pagetables (and corresponding data pages) for vmemmap. 

It is my understanding that, in the second case, we might be clearing
only part of the page content (i.e, only a few struct pages). Note that
next is page aligned by construction only if next &lt;= end.
<span class="quote">
&gt; &gt; +			/*</span>
<span class="quote">&gt; &gt; +			 * Do not free direct mapping pages since they were</span>
<span class="quote">&gt; &gt; +			 * freed when offlining, or simplely not in use.</span>
<span class="quote">&gt; &gt; +			 */</span>
<span class="quote">&gt; &gt; +			if (!direct)</span>
<span class="quote">&gt; &gt; +				free_pagetable(pte_page(*pte), 0, direct);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +			/*</span>
<span class="quote">&gt; &gt; +			 * This spin lock could be only</span>
<span class="quote">&gt; &gt; +			 * taken in _pte_aloc_kernel in</span>
<span class="quote">&gt; &gt; +			 * mm/memory.c and nowhere else</span>
<span class="quote">&gt; &gt; +			 * (for arm64). Not sure if the</span>
<span class="quote">&gt; &gt; +			 * function above can be called</span>
<span class="quote">&gt; &gt; +			 * concurrently. In doubt,</span>
<span class="quote">&gt; &gt; +			 * I am living it here for now,</span>
<span class="quote">&gt; &gt; +			 * but it probably can be removed.</span>
<span class="quote">&gt; &gt; +			 */</span>
<span class="quote">&gt; &gt; +			spin_lock(&amp;init_mm.page_table_lock);</span>
<span class="quote">&gt; &gt; +			pte_clear(&amp;init_mm, addr, pte);</span>
<span class="quote">&gt; &gt; +			spin_unlock(&amp;init_mm.page_table_lock);</span>
<span class="quote">&gt; &gt; +		} else {</span>
<span class="quote">&gt; &gt; +			/*</span>
<span class="quote">&gt; &gt; +			 * If we are here, we are freeing vmemmap pages since</span>
<span class="quote">&gt; &gt; +			 * direct mapped memory ranges to be freed are aligned.</span>
<span class="quote">&gt; &gt; +			 *</span>
<span class="quote">&gt; &gt; +			 * If we are not removing the whole page, it means</span>
<span class="quote">&gt; &gt; +			 * other page structs in this page are being used and</span>
<span class="quote">&gt; &gt; +			 * we canot remove them. So fill the unused page_structs</span>
<span class="quote">&gt; &gt; +			 * with 0xFD, and remove the page when it is wholly</span>
<span class="quote">&gt; &gt; +			 * filled with 0xFD.</span>
<span class="quote">&gt; &gt; +			 */</span>
<span class="quote">&gt; &gt; +			memset((void *)addr, PAGE_INUSE, next - addr);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; What&#39;s special about 0xFD?</span>

Just used it as a constant symmetrically to x86_64 code.
<span class="quote">
&gt; Why do we need to mess with the page array in this manner? Why can&#39;t we</span>
<span class="quote">&gt; detect when a range is free by querying memblock, for example?</span>

I am not sure I get your suggestion. I guess that the logic here
is that I cannot be sure that I can free the full page because other
entries might be in use for active vmemmap mappings. So we just &quot;mark&quot;
the unused once and only free the page when all of it is marked. See
again commit ae9aae9eda2db71, where all this comes from.
<span class="quote">
&gt; &gt; +</span>
<span class="quote">&gt; &gt; +			page_addr = page_address(pte_page(*pte));</span>
<span class="quote">&gt; &gt; +			if (!memchr_inv(page_addr, PAGE_INUSE, PAGE_SIZE)) {</span>
<span class="quote">&gt; &gt; +				free_pagetable(pte_page(*pte), 0, direct);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +				/*</span>
<span class="quote">&gt; &gt; +				 * This spin lock could be only</span>
<span class="quote">&gt; &gt; +				 * taken in _pte_aloc_kernel in</span>
<span class="quote">&gt; &gt; +				 * mm/memory.c and nowhere else</span>
<span class="quote">&gt; &gt; +				 * (for arm64). Not sure if the</span>
<span class="quote">&gt; &gt; +				 * function above can be called</span>
<span class="quote">&gt; &gt; +				 * concurrently. In doubt,</span>
<span class="quote">&gt; &gt; +				 * I am living it here for now,</span>
<span class="quote">&gt; &gt; +				 * but it probably can be removed.</span>
<span class="quote">&gt; &gt; +				 */</span>
<span class="quote">&gt; &gt; +				spin_lock(&amp;init_mm.page_table_lock);</span>
<span class="quote">&gt; &gt; +				pte_clear(&amp;init_mm, addr, pte);</span>
<span class="quote">&gt; &gt; +				spin_unlock(&amp;init_mm.page_table_lock);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This logic appears to be duplicated with the free_*_table functions, and</span>
<span class="quote">&gt; looks incredibly suspicious.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; To me, it doesn&#39;t make sense that we&#39;d need the lock *only* to alter the</span>
<span class="quote">&gt; leaf entries.</span>

I admit I am still confused by the use of the page_table_lock and when
and where it should be used. Any hint on that would be more than
welcome.
<span class="quote">
&gt; &gt; +			}</span>
<span class="quote">&gt; &gt; +		}</span>
<span class="quote">&gt; &gt; +	}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	// I am adding this flush here in simmetry to the x86 code.</span>
<span class="quote">&gt; &gt; +	// Why do I need to call it here and not in remove_p[mu]d</span>
<span class="quote">&gt; &gt; +	flush_tlb_all();</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; If the page tables weren&#39;t freed until this point, it might be that this</span>
<span class="quote">&gt; is just amortizing the cost of removing them from the TLBs.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Given that they&#39;re freed first, this makes no sense to me.</span>

Please, see above.
<span class="quote">
&gt; &gt; +}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The same commenst apply to all the remove_p*d_table() functions, so I</span>
<span class="quote">&gt; shan&#39;t repeat them.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static void remove_pmd_table(pmd_t *pmd, unsigned long addr,</span>
<span class="quote">&gt; &gt; +	unsigned long end, bool direct)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	unsigned long next;</span>
<span class="quote">&gt; &gt; +	void *page_addr;</span>
<span class="quote">&gt; &gt; +	pte_t *pte;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	for (; addr &lt; end; addr = next, pmd++) {</span>
<span class="quote">&gt; &gt; +		next = pmd_addr_end(addr, end);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		if (!pmd_present(*pmd))</span>
<span class="quote">&gt; &gt; +			continue;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		// check if we are using 2MB section mappings</span>
<span class="quote">&gt; &gt; +		if (pmd_sect(*pmd)) {</span>
<span class="quote">&gt; &gt; +			if (PAGE_ALIGNED(addr) &amp;&amp; PAGE_ALIGNED(next)) {</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Surely you&#39;re intending to check if you can free the whole pmd? i.e.</span>
<span class="quote">&gt; that addr and next are pmd-aligned?</span>

Indeed, that&#39;s a mistake. It should have been IS_ALIGNED(addr, PMD_SIZE).
<span class="quote">
&gt; Can we ever be in a situation where we&#39;re requested to free a partial</span>
<span class="quote">&gt; pmd that could be section mapped?</span>

Yes, as I said above, for vmemmap mappings.
<span class="quote"> 
&gt; If that&#39;s the case, we&#39;ll *need* to split the pmd, which we can&#39;t do on</span>
<span class="quote">&gt; live page tables.</span>

Please, see below.
<span class="quote">
&gt; &gt; +				if (!direct) {</span>
<span class="quote">&gt; &gt; +					free_pagetable(pmd_page(*pmd),</span>
<span class="quote">&gt; &gt; +						get_order(PMD_SIZE), direct);</span>
<span class="quote">&gt; &gt; +				}</span>
<span class="quote">&gt; &gt; +				/*</span>
<span class="quote">&gt; &gt; +				 * This spin lock could be only</span>
<span class="quote">&gt; &gt; +				 * taken in _pte_aloc_kernel in</span>
<span class="quote">&gt; &gt; +				 * mm/memory.c and nowhere else</span>
<span class="quote">&gt; &gt; +				 * (for arm64). Not sure if the</span>
<span class="quote">&gt; &gt; +				 * function above can be called</span>
<span class="quote">&gt; &gt; +				 * concurrently. In doubt,</span>
<span class="quote">&gt; &gt; +				 * I am living it here for now,</span>
<span class="quote">&gt; &gt; +				 * but it probably can be removed.</span>
<span class="quote">&gt; &gt; +				 */</span>
<span class="quote">&gt; &gt; +				spin_lock(&amp;init_mm.page_table_lock);</span>
<span class="quote">&gt; &gt; +				pmd_clear(pmd);</span>
<span class="quote">&gt; &gt; +				spin_unlock(&amp;init_mm.page_table_lock);</span>
<span class="quote">&gt; &gt; +			} else {</span>
<span class="quote">&gt; &gt; +				/* If here, we are freeing vmemmap pages. */</span>
<span class="quote">&gt; &gt; +				memset((void *)addr, PAGE_INUSE, next - addr);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +				page_addr = page_address(pmd_page(*pmd));</span>
<span class="quote">&gt; &gt; +				if (!memchr_inv(page_addr, PAGE_INUSE,</span>
<span class="quote">&gt; &gt; +						PMD_SIZE)) {</span>
<span class="quote">&gt; &gt; +					free_pagetable(pmd_page(*pmd),</span>
<span class="quote">&gt; &gt; +						get_order(PMD_SIZE), direct);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +					/*</span>
<span class="quote">&gt; &gt; +					 * This spin lock could be only</span>
<span class="quote">&gt; &gt; +					 * taken in _pte_aloc_kernel in</span>
<span class="quote">&gt; &gt; +					 * mm/memory.c and nowhere else</span>
<span class="quote">&gt; &gt; +					 * (for arm64). Not sure if the</span>
<span class="quote">&gt; &gt; +					 * function above can be called</span>
<span class="quote">&gt; &gt; +					 * concurrently. In doubt,</span>
<span class="quote">&gt; &gt; +					 * I am living it here for now,</span>
<span class="quote">&gt; &gt; +					 * but it probably can be removed.</span>
<span class="quote">&gt; &gt; +					 */</span>
<span class="quote">&gt; &gt; +					spin_lock(&amp;init_mm.page_table_lock);</span>
<span class="quote">&gt; &gt; +					pmd_clear(pmd);</span>
<span class="quote">&gt; &gt; +					spin_unlock(&amp;init_mm.page_table_lock);</span>
<span class="quote">&gt; &gt; +				}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I don&#39;t think this is correct.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; If we&#39;re getting rid of a partial pmd, we *must* split the pmd.</span>
<span class="quote">&gt; Otherwise, we&#39;re leaving bits mapped that should not be. If we split the</span>
<span class="quote">&gt; pmd, we can free the individual pages as we would for a non-section</span>
<span class="quote">&gt; mapping.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; As above, we can&#39;t split block entries within live tables, so that will</span>
<span class="quote">&gt; be painful at best.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; If we can&#39;t split a pmd, hen we cannot free a partial pmd, and will need</span>
<span class="quote">&gt; to reject request to do so.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The same comments (with s/pmu/pud/, etc) apply for the higher level</span>
<span class="quote">&gt; remove_p*d_table functions.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; [...]</span>

This only happens when we are clearing vmemmap memory.  My understanding
is that the whole hack of marking the content of partially unused areas
with the 0xFD constant is exactly to avoid splitting the PMD, but instead
to only clear the full area when we realize that there&#39;s no valid struct
page in it anymore. When would this kind of use be source of problems?

I am also realizing that vememmaps never use section maps at pud level, 
so this code would only need to stay when clearing pmds and ptes.
<span class="quote">
&gt; &gt; +void remove_pagetable(unsigned long start, unsigned long end, bool direct)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	unsigned long next;</span>
<span class="quote">&gt; &gt; +	unsigned long addr;</span>
<span class="quote">&gt; &gt; +	pgd_t *pgd;</span>
<span class="quote">&gt; &gt; +	pud_t *pud;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	for (addr = start; addr &lt; end; addr = next) {</span>
<span class="quote">&gt; &gt; +		next = pgd_addr_end(addr, end);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		pgd = pgd_offset_k(addr);</span>
<span class="quote">&gt; &gt; +		if (pgd_none(*pgd))</span>
<span class="quote">&gt; &gt; +			continue;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		pud = pud_offset(pgd, addr);</span>
<span class="quote">&gt; &gt; +		remove_pud_table(pud, addr, next, direct);</span>
<span class="quote">&gt; &gt; +		/*</span>
<span class="quote">&gt; &gt; +		 * When the PUD is folded on the PGD (three levels of paging),</span>
<span class="quote">&gt; &gt; +		 * I did already clear the PMD page in free_pmd_table,</span>
<span class="quote">&gt; &gt; +		 * and reset the corresponding PGD==PUD entry.</span>
<span class="quote">&gt; &gt; +		 */</span>
<span class="quote">&gt; &gt; +#if CONFIG_PGTABLE_LEVELS &gt; 3</span>
<span class="quote">&gt; &gt; +		free_pud_table(pgd, direct);</span>
<span class="quote">&gt; &gt;  #endif</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This looks suspicious. Shouldn&#39;t we have a similar check for PMD, to</span>
<span class="quote">&gt; cater for CONFIG_PGTABLE_LEVELS == 2? e.g. 64K pages, 42-bit VA.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; We should be able to hide this distinction in helpers for both cases.</span>

True, we will fix it in the next version.
<span class="quote">
&gt; &gt; +	}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	flush_tlb_all();</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This is too late to be useful.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Thanks,</span>
<span class="quote">&gt; Mark.</span>
 
Thanks again for your comments.

Best,
Andrea
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=30282">Mark Rutland</a> - April 18, 2017, 6:21 p.m.</div>
<pre class="content">
On Fri, Apr 14, 2017 at 03:01:58PM +0100, Andrea Reale wrote:
<span class="quote">&gt; I guess it is likely that I might have made assumptions that are true</span>
<span class="quote">&gt; for x86_64 but do not hold for arm64. Whenever you feel this is the</span>
<span class="quote">&gt; case, I would be really grateful if you could help identify them.</span>

Sure thing.
<span class="quote">
&gt; On Tue, Apr 11, 2017 at 06:12:11PM +0100, Mark Rutland wrote:</span>
<span class="quote">&gt; &gt; On Tue, Apr 11, 2017 at 03:55:42PM +0100, Andrea Reale wrote:</span>
<span class="quote">
&gt; &gt; &gt; +static void  free_pagetable(struct page *page, int order, bool direct)</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; This &quot;direct&quot; parameter needs a better name, and a description in a</span>
<span class="quote">&gt; &gt; comment block above this function.</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; The name direct is inherited directly from the x86_64 hot remove code.</span>
<span class="quote">&gt; It serves to distinguish if we are removing either a pagetable page that</span>
<span class="quote">&gt; is mapping to the direct mapping space (I think it is called also linear</span>
<span class="quote">&gt; mapping area somewhere else) or a pagetable page or a data page </span>
<span class="quote">&gt; from vmemmap.</span>

FWIW, I&#39;ve largely heard the folk call that the &quot;linear mapping&quot;, and
x86 folk call that the &quot;direct mapping&quot;. The two are interchangeable.
<span class="quote">
&gt; In this specific set of functions, the flag is needed because the various</span>
<span class="quote">&gt; alloc_init_p*d used for allocating entries for direct memory mapping</span>
<span class="quote">&gt; rely on pgd_pgtable_alloc, which in its turn calls  pgtable_page_ctor;</span>
<span class="quote">&gt; hence, we need to call the dtor. </span>

AFAICT, that&#39;s not true for the arm64 linear map, since that&#39;s created
with our early_pgtable_alloc(), which doesn&#39;t call pgtable_page_ctor().

Judging by commit:

  1378dc3d4ba07ccd (&quot;arm64: mm: run pgtable_page_ctor() on non-swapper
                     translation table pages&quot;)

... we only do this for non-swapper page tables.
<span class="quote">
&gt; On the contrary, vmemmap entries are created using vmemmap_alloc_block</span>
<span class="quote">&gt; (from within vmemmap_populate), which does not call pgtable_page_ctor</span>
<span class="quote">&gt; when allocating pages.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I am not sure I understand why the pgtable_page_ctor is not called when</span>
<span class="quote">&gt; allocating vmemmap page tables, but that&#39;s the current situation.</span>

From a quick scan, I see that it&#39;s necessary to use pgtable_page_ctor()
for pages that will be used for userspace page tables, but it&#39;s not
clear to me if it&#39;s ever necessary for pages used for kernel page
tables.

If it is, we appear to have a bug on arm64.

Laura, Ard, thoughts?
<span class="quote">
&gt; &gt; &gt; +{</span>
<span class="quote">&gt; &gt; &gt; +	unsigned long magic;</span>
<span class="quote">&gt; &gt; &gt; +	unsigned int nr_pages = 1 &lt;&lt; order;</span>
<span class="quote">&gt; &gt; &gt; +	struct vmem_altmap *altmap = to_vmem_altmap((unsigned long) page);</span>
<span class="quote">&gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; &gt; +	if (altmap) {</span>
<span class="quote">&gt; &gt; &gt; +		vmem_altmap_free(altmap, nr_pages);</span>
<span class="quote">&gt; &gt; &gt; +		return;</span>
<span class="quote">&gt; &gt; &gt; +	}</span>
<span class="quote">&gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; &gt; +	/* bootmem page has reserved flag */</span>
<span class="quote">&gt; &gt; &gt; +	if (PageReserved(page)) {</span>
<span class="quote">&gt; &gt; &gt; +		__ClearPageReserved(page);</span>
<span class="quote">&gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; &gt; +		magic = (unsigned long)page-&gt;lru.next;</span>
<span class="quote">&gt; &gt; &gt; +		if (magic == SECTION_INFO || magic == MIX_SECTION_INFO) {</span>
<span class="quote">&gt; &gt; &gt; +			while (nr_pages--)</span>
<span class="quote">&gt; &gt; &gt; +				put_page_bootmem(page++);</span>
<span class="quote">&gt; &gt; &gt; +		} else {</span>
<span class="quote">&gt; &gt; &gt; +			while (nr_pages--)</span>
<span class="quote">&gt; &gt; &gt; +				free_reserved_page(page++);</span>
<span class="quote">&gt; &gt; &gt; +		}</span>
<span class="quote">&gt; &gt; &gt; +	} else {</span>
<span class="quote">&gt; &gt; &gt; +		/*</span>
<span class="quote">&gt; &gt; &gt; +		 * Only direct pagetable allocation (those allocated via</span>
<span class="quote">&gt; &gt; &gt; +		 * hotplug) call the pgtable_page_ctor; vmemmap pgtable</span>
<span class="quote">&gt; &gt; &gt; +		 * allocations don&#39;t.</span>
<span class="quote">&gt; &gt; &gt; +		 */</span>
<span class="quote">&gt; &gt; &gt; +		if (direct)</span>
<span class="quote">&gt; &gt; &gt; +			pgtable_page_dtor(page);</span>
<span class="quote">&gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; &gt; +		free_pages((unsigned long)page_address(page), order);</span>
<span class="quote">&gt; &gt; &gt; +	}</span>
<span class="quote">&gt; &gt; &gt; +}</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; This largely looks like a copy of the x86 code. Why can that not be</span>
<span class="quote">&gt; &gt; factored out to an arch-neutral location?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Yes it probably can - the only difference being calling</span>
<span class="quote">&gt; pgtable_page_dtor when it needs to - but I am not confident enough to</span>
<span class="quote">&gt; say that it would really be architecture neutral or just specific to</span>
<span class="quote">&gt; only arm64 and x86.  For example, I don&#39;t see this used anywhere else</span>
<span class="quote">&gt; for hot-removing memory.</span>

Mhmmm. As above, it&#39;s also not clear to me if the ctor()/dtor() dance is
necessary in the first place.

I&#39;m also not clear on why x86_64 and powerpc are the only architectures
that appear to clear up their page tables after __remove_pages(). Do
other architectures not have &quot;real&quot; hot-remove?
<span class="quote">
&gt; (Actually, also a large part of remove_*_table and free_*_table could</span>
<span class="quote">&gt; probably be factored, but I wouldn&#39;t be sure how to deal with the</span>
<span class="quote">&gt; differences in the pgtable.h macros used throughout)</span>

Let&#39;s figure out what&#39;s necessary first. Then we&#39;ll know if/how we can
align on a common pattern.

[...]
<span class="quote">
&gt;&gt; &gt; +	page = pmd_page(*pmd);</span>
<span class="quote">&gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; &gt; +	free_pagetable(page, 0, direct);</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; The page has been freed here, and may be subject to arbitrary</span>
<span class="quote">&gt; &gt; modification...</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; &gt; +	/*</span>
<span class="quote">&gt; &gt; &gt; +	 * This spin lock could be only taken in _pte_aloc_kernel</span>
<span class="quote">&gt; &gt; &gt; +	 * in mm/memory.c and nowhere else (for arm64). Not sure if</span>
<span class="quote">&gt; &gt; &gt; +	 * the function above can be called concurrently. In doubt,</span>
<span class="quote">&gt; &gt; &gt; +	 * I am living it here for now, but it probably can be removed</span>
<span class="quote">&gt; &gt; &gt; +	 */</span>
<span class="quote">&gt; &gt; &gt; +	spin_lock(&amp;init_mm.page_table_lock);</span>
<span class="quote">&gt; &gt; &gt; +	pmd_clear(pmd);</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; ... but we only remove it from the page tables here, so the page table</span>
<span class="quote">&gt; &gt; walkers can see junk in the tables, were the page reused in the mean</span>
<span class="quote">&gt; &gt; timer.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; After clearing the PMD, it needs to be cleared from TLBs. We allow</span>
<span class="quote">&gt; &gt; partial walks to be cached, so the TLBs may still start walking this</span>
<span class="quote">&gt; &gt; entry or beyond.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I guess that the safe approach would be something along the lines:</span>
<span class="quote">&gt; 1. clear the page table </span>
<span class="quote">&gt; 2. flush the tlbs</span>
<span class="quote">&gt; 3. free the page</span>

Yes. That&#39;s the sequence we follow elsewhere.
<span class="quote">
&gt; When I am flushing intermediate p*d entries, would it be</span>
<span class="quote">&gt; more appropriate to use something like __flush_tlb_pgtable() to clear</span>
<span class="quote">&gt; cached partial walks rather than flushing the whole table? I mean,</span>
<span class="quote">&gt; hot-remove is not going to be a frequent operation anyway, so I don&#39;t</span>
<span class="quote">&gt; think that flushing the whole tlb would be a great deal of harm</span>
<span class="quote">&gt; anyway.</span>

Using __flush_tlb_pgtable() sounds sane to me. That&#39;s what we do when
tearing down user mappings.
<span class="quote">
&gt; My question at this point would be: how come that the code structure above</span>
<span class="quote">&gt; works for x86_64 hot-remove?</span>

I don&#39;t know enough about x86 to say.
<span class="quote">
&gt; My assumption, when I was writing this, was</span>
<span class="quote">&gt; that there would be no problem since this code is called when we are sure</span>
<span class="quote">&gt; that all the memory mapped by these entries has been already offlined,</span>
<span class="quote">&gt; so nobody should be using those VAs anyway (also considering that there</span>
<span class="quote">&gt; cannot be any two mem hot-plug/remove actions running concurrently).</span>
<span class="quote">&gt; Is that correct?</span>

The problem is that speculation, Out-of-Order execution, HW prefetching,
and other such things *can* result in those VAs being accessed,
regardless of what code explicitly does in a sequential execution.

If any table relevant to one of those VAs has been freed (and
potentially modified by the allocator, or in another thread), it&#39;s
possible that the CPU performs a page table walk and sees junk as a
valid page table entry. As a result, a number of bad things can happen.

If the entry was a pgd, pud, or pmd, the CPU may try to continue to walk
to the relevant pte, accessing a junk address. That could change the
state of MMIO, trigger an SError, etc, or allocate more junk into the
TLBs.

For any level, the CPU might allocate the entry into a TLB, regardless
of whether an existing entry existed. The new entry can conflict with
the existing one, either leading to a TLB conflict abort, or to the TLB
returning junk for that address. Speculation and so on can now access
junk based on that, etc.

[...]
<span class="quote">
&gt; &gt; &gt; +static void remove_pte_table(pte_t *pte, unsigned long addr,</span>
<span class="quote">&gt; &gt; &gt; +	unsigned long end, bool direct)</span>
<span class="quote">&gt; &gt; &gt; +{</span>
<span class="quote">&gt; &gt; &gt; +	unsigned long next;</span>
<span class="quote">&gt; &gt; &gt; +	void *page_addr;</span>
<span class="quote">&gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; &gt; +	for (; addr &lt; end; addr = next, pte++) {</span>
<span class="quote">&gt; &gt; &gt; +		next = (addr + PAGE_SIZE) &amp; PAGE_MASK;</span>
<span class="quote">&gt; &gt; &gt; +		if (next &gt; end)</span>
<span class="quote">&gt; &gt; &gt; +			next = end;</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Please use the usual p*d_addr_end() functions. See alloc_init_pmd() and</span>
<span class="quote">&gt; &gt; friends in arch/arm64/mm/mmu.c for examples of how to use them.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; we used the p*d_addr_end family of functions when dealing with p*d(s). I</span>
<span class="quote">&gt; cannot identify an equivalent for pte entries.</span>

Ah; my bad.

I guess we should follow the same pattern as alloc_init_pte() does here,
assuming we use p*d_addr_end() for all the levels above (as for
alloc_init_p*d()).
<span class="quote">
&gt; Would you recommend adding a pte_addr_end macro in pgtable.h?</span>

That shouldn&#39;t be necessary. Sorry for the confusion.
<span class="quote">
&gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; &gt; +		if (!pte_present(*pte))</span>
<span class="quote">&gt; &gt; &gt; +			continue;</span>
<span class="quote">&gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; &gt; +		if (PAGE_ALIGNED(addr) &amp;&amp; PAGE_ALIGNED(next)) {</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; When would those addresses *not* be page-aligned? By construction, next</span>
<span class="quote">&gt; &gt; must be. Unplugging partial pages of memory makes no sense, so surely</span>
<span class="quote">&gt; &gt; addr is page-aligned when passed in?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The issue here is that this function is called in one of two cases:</span>
<span class="quote">&gt; 1. to clear pagetables of directly mapped (linear) memory </span>
<span class="quote">&gt; 2. Pagetables (and corresponding data pages) for vmemmap. </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It is my understanding that, in the second case, we might be clearing</span>
<span class="quote">&gt; only part of the page content (i.e, only a few struct pages). Note that</span>
<span class="quote">&gt; next is page aligned by construction only if next &lt;= end.</span>

Ok. A comment to that effect immediately above this check would be
helpful.
<span class="quote">
&gt; &gt; &gt; +			/*</span>
<span class="quote">&gt; &gt; &gt; +			 * Do not free direct mapping pages since they were</span>
<span class="quote">&gt; &gt; &gt; +			 * freed when offlining, or simplely not in use.</span>
<span class="quote">&gt; &gt; &gt; +			 */</span>
<span class="quote">&gt; &gt; &gt; +			if (!direct)</span>
<span class="quote">&gt; &gt; &gt; +				free_pagetable(pte_page(*pte), 0, direct);</span>
<span class="quote">&gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; &gt; +			/*</span>
<span class="quote">&gt; &gt; &gt; +			 * This spin lock could be only</span>
<span class="quote">&gt; &gt; &gt; +			 * taken in _pte_aloc_kernel in</span>
<span class="quote">&gt; &gt; &gt; +			 * mm/memory.c and nowhere else</span>
<span class="quote">&gt; &gt; &gt; +			 * (for arm64). Not sure if the</span>
<span class="quote">&gt; &gt; &gt; +			 * function above can be called</span>
<span class="quote">&gt; &gt; &gt; +			 * concurrently. In doubt,</span>
<span class="quote">&gt; &gt; &gt; +			 * I am living it here for now,</span>
<span class="quote">&gt; &gt; &gt; +			 * but it probably can be removed.</span>
<span class="quote">&gt; &gt; &gt; +			 */</span>
<span class="quote">&gt; &gt; &gt; +			spin_lock(&amp;init_mm.page_table_lock);</span>
<span class="quote">&gt; &gt; &gt; +			pte_clear(&amp;init_mm, addr, pte);</span>
<span class="quote">&gt; &gt; &gt; +			spin_unlock(&amp;init_mm.page_table_lock);</span>
<span class="quote">&gt; &gt; &gt; +		} else {</span>
<span class="quote">&gt; &gt; &gt; +			/*</span>
<span class="quote">&gt; &gt; &gt; +			 * If we are here, we are freeing vmemmap pages since</span>
<span class="quote">&gt; &gt; &gt; +			 * direct mapped memory ranges to be freed are aligned.</span>
<span class="quote">&gt; &gt; &gt; +			 *</span>
<span class="quote">&gt; &gt; &gt; +			 * If we are not removing the whole page, it means</span>
<span class="quote">&gt; &gt; &gt; +			 * other page structs in this page are being used and</span>
<span class="quote">&gt; &gt; &gt; +			 * we canot remove them. So fill the unused page_structs</span>
<span class="quote">&gt; &gt; &gt; +			 * with 0xFD, and remove the page when it is wholly</span>
<span class="quote">&gt; &gt; &gt; +			 * filled with 0xFD.</span>
<span class="quote">&gt; &gt; &gt; +			 */</span>
<span class="quote">&gt; &gt; &gt; +			memset((void *)addr, PAGE_INUSE, next - addr);</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; What&#39;s special about 0xFD?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Just used it as a constant symmetrically to x86_64 code.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; Why do we need to mess with the page array in this manner? Why can&#39;t we</span>
<span class="quote">&gt; &gt; detect when a range is free by querying memblock, for example?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I am not sure I get your suggestion. I guess that the logic here</span>
<span class="quote">&gt; is that I cannot be sure that I can free the full page because other</span>
<span class="quote">&gt; entries might be in use for active vmemmap mappings. So we just &quot;mark&quot;</span>
<span class="quote">&gt; the unused once and only free the page when all of it is marked. See</span>
<span class="quote">&gt; again commit ae9aae9eda2db71, where all this comes from.</span>

I understood that this is deferring freeing until a whole page of struct
pages has been freed.

My concern is that filling the unused memory with an array of junk chars
feels like a hack. We don&#39;t do this at the edges when we allocate
memblock today, AFAICT, so this doesn&#39;t seem complete.

Is there no &quot;real&quot; datastructure we can use to keep track of what memory
is present? e.g. memblock?

[...]
<span class="quote">
&gt; &gt; &gt; +static void remove_pmd_table(pmd_t *pmd, unsigned long addr,</span>
<span class="quote">&gt; &gt; &gt; +	unsigned long end, bool direct)</span>
<span class="quote">&gt; &gt; &gt; +{</span>
<span class="quote">&gt; &gt; &gt; +	unsigned long next;</span>
<span class="quote">&gt; &gt; &gt; +	void *page_addr;</span>
<span class="quote">&gt; &gt; &gt; +	pte_t *pte;</span>
<span class="quote">&gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; &gt; +	for (; addr &lt; end; addr = next, pmd++) {</span>
<span class="quote">&gt; &gt; &gt; +		next = pmd_addr_end(addr, end);</span>
<span class="quote">&gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; &gt; +		if (!pmd_present(*pmd))</span>
<span class="quote">&gt; &gt; &gt; +			continue;</span>
<span class="quote">&gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; &gt; +		// check if we are using 2MB section mappings</span>
<span class="quote">&gt; &gt; &gt; +		if (pmd_sect(*pmd)) {</span>
<span class="quote">&gt; &gt; &gt; +			if (PAGE_ALIGNED(addr) &amp;&amp; PAGE_ALIGNED(next)) {</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Surely you&#39;re intending to check if you can free the whole pmd? i.e.</span>
<span class="quote">&gt; &gt; that addr and next are pmd-aligned?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Indeed, that&#39;s a mistake. It should have been IS_ALIGNED(addr, PMD_SIZE).</span>

Ok.
<span class="quote">
&gt; &gt; Can we ever be in a situation where we&#39;re requested to free a partial</span>
<span class="quote">&gt; &gt; pmd that could be section mapped?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Yes, as I said above, for vmemmap mappings.</span>

Ok.
<span class="quote">
&gt; &gt; If that&#39;s the case, we&#39;ll *need* to split the pmd, which we can&#39;t do on</span>
<span class="quote">&gt; &gt; live page tables.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Please, see below.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; &gt; +				if (!direct) {</span>
<span class="quote">&gt; &gt; &gt; +					free_pagetable(pmd_page(*pmd),</span>
<span class="quote">&gt; &gt; &gt; +						get_order(PMD_SIZE), direct);</span>
<span class="quote">&gt; &gt; &gt; +				}</span>
<span class="quote">&gt; &gt; &gt; +				/*</span>
<span class="quote">&gt; &gt; &gt; +				 * This spin lock could be only</span>
<span class="quote">&gt; &gt; &gt; +				 * taken in _pte_aloc_kernel in</span>
<span class="quote">&gt; &gt; &gt; +				 * mm/memory.c and nowhere else</span>
<span class="quote">&gt; &gt; &gt; +				 * (for arm64). Not sure if the</span>
<span class="quote">&gt; &gt; &gt; +				 * function above can be called</span>
<span class="quote">&gt; &gt; &gt; +				 * concurrently. In doubt,</span>
<span class="quote">&gt; &gt; &gt; +				 * I am living it here for now,</span>
<span class="quote">&gt; &gt; &gt; +				 * but it probably can be removed.</span>
<span class="quote">&gt; &gt; &gt; +				 */</span>
<span class="quote">&gt; &gt; &gt; +				spin_lock(&amp;init_mm.page_table_lock);</span>
<span class="quote">&gt; &gt; &gt; +				pmd_clear(pmd);</span>
<span class="quote">&gt; &gt; &gt; +				spin_unlock(&amp;init_mm.page_table_lock);</span>
<span class="quote">&gt; &gt; &gt; +			} else {</span>
<span class="quote">&gt; &gt; &gt; +				/* If here, we are freeing vmemmap pages. */</span>
<span class="quote">&gt; &gt; &gt; +				memset((void *)addr, PAGE_INUSE, next - addr);</span>
<span class="quote">&gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; &gt; +				page_addr = page_address(pmd_page(*pmd));</span>
<span class="quote">&gt; &gt; &gt; +				if (!memchr_inv(page_addr, PAGE_INUSE,</span>
<span class="quote">&gt; &gt; &gt; +						PMD_SIZE)) {</span>
<span class="quote">&gt; &gt; &gt; +					free_pagetable(pmd_page(*pmd),</span>
<span class="quote">&gt; &gt; &gt; +						get_order(PMD_SIZE), direct);</span>
<span class="quote">&gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; &gt; +					/*</span>
<span class="quote">&gt; &gt; &gt; +					 * This spin lock could be only</span>
<span class="quote">&gt; &gt; &gt; +					 * taken in _pte_aloc_kernel in</span>
<span class="quote">&gt; &gt; &gt; +					 * mm/memory.c and nowhere else</span>
<span class="quote">&gt; &gt; &gt; +					 * (for arm64). Not sure if the</span>
<span class="quote">&gt; &gt; &gt; +					 * function above can be called</span>
<span class="quote">&gt; &gt; &gt; +					 * concurrently. In doubt,</span>
<span class="quote">&gt; &gt; &gt; +					 * I am living it here for now,</span>
<span class="quote">&gt; &gt; &gt; +					 * but it probably can be removed.</span>
<span class="quote">&gt; &gt; &gt; +					 */</span>
<span class="quote">&gt; &gt; &gt; +					spin_lock(&amp;init_mm.page_table_lock);</span>
<span class="quote">&gt; &gt; &gt; +					pmd_clear(pmd);</span>
<span class="quote">&gt; &gt; &gt; +					spin_unlock(&amp;init_mm.page_table_lock);</span>
<span class="quote">&gt; &gt; &gt; +				}</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I don&#39;t think this is correct.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; If we&#39;re getting rid of a partial pmd, we *must* split the pmd.</span>
<span class="quote">&gt; &gt; Otherwise, we&#39;re leaving bits mapped that should not be. If we split the</span>
<span class="quote">&gt; &gt; pmd, we can free the individual pages as we would for a non-section</span>
<span class="quote">&gt; &gt; mapping.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; As above, we can&#39;t split block entries within live tables, so that will</span>
<span class="quote">&gt; &gt; be painful at best.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; If we can&#39;t split a pmd, hen we cannot free a partial pmd, and will need</span>
<span class="quote">&gt; &gt; to reject request to do so.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; The same comments (with s/pmu/pud/, etc) apply for the higher level</span>
<span class="quote">&gt; &gt; remove_p*d_table functions.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; [...]</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This only happens when we are clearing vmemmap memory. </span>

Is that definitely the case?

Currently, I can&#39;t see what prevents adding 2M of memory, and then
removing the first 4K of that. We&#39;ll use a 2M section for the linear map
of that, but won&#39;t unmap the 4K when removing.

Likewise for the next level up, with s/2M/1G/ and s/4K/2M/.
<span class="quote">
&gt; My understanding</span>
<span class="quote">&gt; is that the whole hack of marking the content of partially unused areas</span>
<span class="quote">&gt; with the 0xFD constant is exactly to avoid splitting the PMD, but instead</span>
<span class="quote">&gt; to only clear the full area when we realize that there&#39;s no valid struct</span>
<span class="quote">&gt; page in it anymore. When would this kind of use be source of problems?</span>

I understand what&#39;s going on for the vmemmap. So long as we don&#39;t use
hotpluggable memory to back the vmemmap, that&#39;s fine.

As above, my concern is whether splitting a section can ever occur for
the linear map.

Thanks,
Mark.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=66681">Ard Biesheuvel</a> - April 18, 2017, 6:48 p.m.</div>
<pre class="content">
On 18 April 2017 at 19:21, Mark Rutland &lt;mark.rutland@arm.com&gt; wrote:
<span class="quote">&gt; On Fri, Apr 14, 2017 at 03:01:58PM +0100, Andrea Reale wrote:</span>
<span class="quote">&gt;&gt; I guess it is likely that I might have made assumptions that are true</span>
<span class="quote">&gt;&gt; for x86_64 but do not hold for arm64. Whenever you feel this is the</span>
<span class="quote">&gt;&gt; case, I would be really grateful if you could help identify them.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Sure thing.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; On Tue, Apr 11, 2017 at 06:12:11PM +0100, Mark Rutland wrote:</span>
<span class="quote">&gt;&gt; &gt; On Tue, Apr 11, 2017 at 03:55:42PM +0100, Andrea Reale wrote:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; &gt; &gt; +static void  free_pagetable(struct page *page, int order, bool direct)</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; This &quot;direct&quot; parameter needs a better name, and a description in a</span>
<span class="quote">&gt;&gt; &gt; comment block above this function.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; The name direct is inherited directly from the x86_64 hot remove code.</span>
<span class="quote">&gt;&gt; It serves to distinguish if we are removing either a pagetable page that</span>
<span class="quote">&gt;&gt; is mapping to the direct mapping space (I think it is called also linear</span>
<span class="quote">&gt;&gt; mapping area somewhere else) or a pagetable page or a data page</span>
<span class="quote">&gt;&gt; from vmemmap.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; FWIW, I&#39;ve largely heard the folk call that the &quot;linear mapping&quot;, and</span>
<span class="quote">&gt; x86 folk call that the &quot;direct mapping&quot;. The two are interchangeable.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; In this specific set of functions, the flag is needed because the various</span>
<span class="quote">&gt;&gt; alloc_init_p*d used for allocating entries for direct memory mapping</span>
<span class="quote">&gt;&gt; rely on pgd_pgtable_alloc, which in its turn calls  pgtable_page_ctor;</span>
<span class="quote">&gt;&gt; hence, we need to call the dtor.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; AFAICT, that&#39;s not true for the arm64 linear map, since that&#39;s created</span>
<span class="quote">&gt; with our early_pgtable_alloc(), which doesn&#39;t call pgtable_page_ctor().</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Judging by commit:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;   1378dc3d4ba07ccd (&quot;arm64: mm: run pgtable_page_ctor() on non-swapper</span>
<span class="quote">&gt;                      translation table pages&quot;)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; ... we only do this for non-swapper page tables.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; On the contrary, vmemmap entries are created using vmemmap_alloc_block</span>
<span class="quote">&gt;&gt; (from within vmemmap_populate), which does not call pgtable_page_ctor</span>
<span class="quote">&gt;&gt; when allocating pages.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; I am not sure I understand why the pgtable_page_ctor is not called when</span>
<span class="quote">&gt;&gt; allocating vmemmap page tables, but that&#39;s the current situation.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; From a quick scan, I see that it&#39;s necessary to use pgtable_page_ctor()</span>
<span class="quote">&gt; for pages that will be used for userspace page tables, but it&#39;s not</span>
<span class="quote">&gt; clear to me if it&#39;s ever necessary for pages used for kernel page</span>
<span class="quote">&gt; tables.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; If it is, we appear to have a bug on arm64.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Laura, Ard, thoughts?</span>
<span class="quote">&gt;</span>

The generic apply_to_page_range() will expect the PTE lock to be
initialized for page table pages that are not part of init_mm. For
arm64, that is precisely efi_mm as far as I am aware. For EFI, the
locking is unnecessary but does no harm (the permissions are set once
via apply_to_page_range() at boot), so I added this call when adding
support for strict permissions in EFI rt services mappings.

So I think it is appropriate for create_pgd_mapping() to be in charge
of calling the ctor(). We simply have no destroy_pgd_mapping()
counterpart that would be the place for the dtor() call, given that we
never take down EFI rt services mappings.

Whether it makes sense or not to lock/unlock in apply_to_page_range()
is something I did not spend any brain cycles on at the time.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=130411">Laura Abbott</a> - April 19, 2017, 3:53 p.m.</div>
<pre class="content">
On 04/18/2017 11:48 AM, Ard Biesheuvel wrote:
<span class="quote">&gt; On 18 April 2017 at 19:21, Mark Rutland &lt;mark.rutland@arm.com&gt; wrote:</span>
<span class="quote">&gt;&gt; On Fri, Apr 14, 2017 at 03:01:58PM +0100, Andrea Reale wrote:</span>
<span class="quote">&gt;&gt;&gt; I guess it is likely that I might have made assumptions that are true</span>
<span class="quote">&gt;&gt;&gt; for x86_64 but do not hold for arm64. Whenever you feel this is the</span>
<span class="quote">&gt;&gt;&gt; case, I would be really grateful if you could help identify them.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Sure thing.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; On Tue, Apr 11, 2017 at 06:12:11PM +0100, Mark Rutland wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; On Tue, Apr 11, 2017 at 03:55:42PM +0100, Andrea Reale wrote:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; +static void  free_pagetable(struct page *page, int order, bool direct)</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; This &quot;direct&quot; parameter needs a better name, and a description in a</span>
<span class="quote">&gt;&gt;&gt;&gt; comment block above this function.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; The name direct is inherited directly from the x86_64 hot remove code.</span>
<span class="quote">&gt;&gt;&gt; It serves to distinguish if we are removing either a pagetable page that</span>
<span class="quote">&gt;&gt;&gt; is mapping to the direct mapping space (I think it is called also linear</span>
<span class="quote">&gt;&gt;&gt; mapping area somewhere else) or a pagetable page or a data page</span>
<span class="quote">&gt;&gt;&gt; from vmemmap.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; FWIW, I&#39;ve largely heard the folk call that the &quot;linear mapping&quot;, and</span>
<span class="quote">&gt;&gt; x86 folk call that the &quot;direct mapping&quot;. The two are interchangeable.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; In this specific set of functions, the flag is needed because the various</span>
<span class="quote">&gt;&gt;&gt; alloc_init_p*d used for allocating entries for direct memory mapping</span>
<span class="quote">&gt;&gt;&gt; rely on pgd_pgtable_alloc, which in its turn calls  pgtable_page_ctor;</span>
<span class="quote">&gt;&gt;&gt; hence, we need to call the dtor.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; AFAICT, that&#39;s not true for the arm64 linear map, since that&#39;s created</span>
<span class="quote">&gt;&gt; with our early_pgtable_alloc(), which doesn&#39;t call pgtable_page_ctor().</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Judging by commit:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;    1378dc3d4ba07ccd (&quot;arm64: mm: run pgtable_page_ctor() on non-swapper</span>
<span class="quote">&gt;&gt;                       translation table pages&quot;)</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; ... we only do this for non-swapper page tables.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; On the contrary, vmemmap entries are created using vmemmap_alloc_block</span>
<span class="quote">&gt;&gt;&gt; (from within vmemmap_populate), which does not call pgtable_page_ctor</span>
<span class="quote">&gt;&gt;&gt; when allocating pages.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; I am not sure I understand why the pgtable_page_ctor is not called when</span>
<span class="quote">&gt;&gt;&gt; allocating vmemmap page tables, but that&#39;s the current situation.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;  From a quick scan, I see that it&#39;s necessary to use pgtable_page_ctor()</span>
<span class="quote">&gt;&gt; for pages that will be used for userspace page tables, but it&#39;s not</span>
<span class="quote">&gt;&gt; clear to me if it&#39;s ever necessary for pages used for kernel page</span>
<span class="quote">&gt;&gt; tables.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; If it is, we appear to have a bug on arm64.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Laura, Ard, thoughts?</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The generic apply_to_page_range() will expect the PTE lock to be</span>
<span class="quote">&gt; initialized for page table pages that are not part of init_mm. For</span>
<span class="quote">&gt; arm64, that is precisely efi_mm as far as I am aware. For EFI, the</span>
<span class="quote">&gt; locking is unnecessary but does no harm (the permissions are set once</span>
<span class="quote">&gt; via apply_to_page_range() at boot), so I added this call when adding</span>
<span class="quote">&gt; support for strict permissions in EFI rt services mappings.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So I think it is appropriate for create_pgd_mapping() to be in charge</span>
<span class="quote">&gt; of calling the ctor(). We simply have no destroy_pgd_mapping()</span>
<span class="quote">&gt; counterpart that would be the place for the dtor() call, given that we</span>
<span class="quote">&gt; never take down EFI rt services mappi &gt;</span>
<span class="quote">&gt; Whether it makes sense or not to lock/unlock in apply_to_page_range()</span>
<span class="quote">&gt; is something I did not spend any brain cycles on at the time.</span>
<span class="quote">&gt; </span>

Agreed there shouldn&#39;t be a problem right now. I do think the locking is
appropriate in apply_to_page_range given what other functions also get
locked.

I really wish this were less asymmetrical though since it get hard
to reason about. It looks like hotplug_paging will call the ctor,
so is there an issue with calling hot-remove on memory that was once
hot-added or is that not a concern?

Thanks,
Laura
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=171743">Andrea Reale</a> - April 21, 2017, 10:02 a.m.</div>
<pre class="content">
Hi all,

Thanks Mark, Ard and Laura for your comments. Replies in-line.

On Tue, Apr 18, 2017 at 07:21:26PM +0100, Mark Rutland wrote:
[...]
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; The name direct is inherited directly from the x86_64 hot remove code.</span>
<span class="quote">&gt; &gt; It serves to distinguish if we are removing either a pagetable page that</span>
<span class="quote">&gt; &gt; is mapping to the direct mapping space (I think it is called also linear</span>
<span class="quote">&gt; &gt; mapping area somewhere else) or a pagetable page or a data page </span>
<span class="quote">&gt; &gt; from vmemmap.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; FWIW, I&#39;ve largely heard the folk call that the &quot;linear mapping&quot;, and</span>
<span class="quote">&gt; x86 folk call that the &quot;direct mapping&quot;. The two are interchangeable.</span>
<span class="quote">&gt; </span>

Thanks for the clarification; We&#39;ll just call it &quot;linear mapping&quot; then.
<span class="quote">
&gt; &gt; In this specific set of functions, the flag is needed because the various</span>
<span class="quote">&gt; &gt; alloc_init_p*d used for allocating entries for direct memory mapping</span>
<span class="quote">&gt; &gt; rely on pgd_pgtable_alloc, which in its turn calls  pgtable_page_ctor;</span>
<span class="quote">&gt; &gt; hence, we need to call the dtor. </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; AFAICT, that&#39;s not true for the arm64 linear map, since that&#39;s created</span>
<span class="quote">&gt; with our early_pgtable_alloc(), which doesn&#39;t call pgtable_page_ctor().</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Judging by commit:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;   1378dc3d4ba07ccd (&quot;arm64: mm: run pgtable_page_ctor() on non-swapper</span>
<span class="quote">&gt;                      translation table pages&quot;)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; ... we only do this for non-swapper page tables.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; On the contrary, vmemmap entries are created using vmemmap_alloc_block</span>
<span class="quote">&gt; &gt; (from within vmemmap_populate), which does not call pgtable_page_ctor</span>
<span class="quote">&gt; &gt; when allocating pages.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I am not sure I understand why the pgtable_page_ctor is not called when</span>
<span class="quote">&gt; &gt; allocating vmemmap page tables, but that&#39;s the current situation.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; From a quick scan, I see that it&#39;s necessary to use pgtable_page_ctor()</span>
<span class="quote">&gt; for pages that will be used for userspace page tables, but it&#39;s not</span>
<span class="quote">&gt; clear to me if it&#39;s ever necessary for pages used for kernel page</span>
<span class="quote">&gt; tables.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; If it is, we appear to have a bug on arm64.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Laura, Ard, thoughts?</span>
<span class="quote">&gt; </span>

More comments on that as a separate reply to Laura&#39;s and Ard&#39;s messages.

[...]
<span class="quote">
&gt; &gt; </span>
<span class="quote">&gt; &gt; I guess that the safe approach would be something along the lines:</span>
<span class="quote">&gt; &gt; 1. clear the page table </span>
<span class="quote">&gt; &gt; 2. flush the tlbs</span>
<span class="quote">&gt; &gt; 3. free the page</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Yes. That&#39;s the sequence we follow elsewhere.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; When I am flushing intermediate p*d entries, would it be</span>
<span class="quote">&gt; &gt; more appropriate to use something like __flush_tlb_pgtable() to clear</span>
<span class="quote">&gt; &gt; cached partial walks rather than flushing the whole table? I mean,</span>
<span class="quote">&gt; &gt; hot-remove is not going to be a frequent operation anyway, so I don&#39;t</span>
<span class="quote">&gt; &gt; think that flushing the whole tlb would be a great deal of harm</span>
<span class="quote">&gt; &gt; anyway.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Using __flush_tlb_pgtable() sounds sane to me. That&#39;s what we do when</span>
<span class="quote">&gt; tearing down user mappings.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; My question at this point would be: how come that the code structure above</span>
<span class="quote">&gt; &gt; works for x86_64 hot-remove?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I don&#39;t know enough about x86 to say.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; My assumption, when I was writing this, was</span>
<span class="quote">&gt; &gt; that there would be no problem since this code is called when we are sure</span>
<span class="quote">&gt; &gt; that all the memory mapped by these entries has been already offlined,</span>
<span class="quote">&gt; &gt; so nobody should be using those VAs anyway (also considering that there</span>
<span class="quote">&gt; &gt; cannot be any two mem hot-plug/remove actions running concurrently).</span>
<span class="quote">&gt; &gt; Is that correct?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The problem is that speculation, Out-of-Order execution, HW prefetching,</span>
<span class="quote">&gt; and other such things *can* result in those VAs being accessed,</span>
<span class="quote">&gt; regardless of what code explicitly does in a sequential execution.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; If any table relevant to one of those VAs has been freed (and</span>
<span class="quote">&gt; potentially modified by the allocator, or in another thread), it&#39;s</span>
<span class="quote">&gt; possible that the CPU performs a page table walk and sees junk as a</span>
<span class="quote">&gt; valid page table entry. As a result, a number of bad things can happen.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; If the entry was a pgd, pud, or pmd, the CPU may try to continue to walk</span>
<span class="quote">&gt; to the relevant pte, accessing a junk address. That could change the</span>
<span class="quote">&gt; state of MMIO, trigger an SError, etc, or allocate more junk into the</span>
<span class="quote">&gt; TLBs.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; For any level, the CPU might allocate the entry into a TLB, regardless</span>
<span class="quote">&gt; of whether an existing entry existed. The new entry can conflict with</span>
<span class="quote">&gt; the existing one, either leading to a TLB conflict abort, or to the TLB</span>
<span class="quote">&gt; returning junk for that address. Speculation and so on can now access</span>
<span class="quote">&gt; junk based on that, etc.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; [...]</span>
<span class="quote">&gt; </span>

Thanks, we&#39;ll just clear it the proper way.

[...]
<span class="quote">
&gt; &gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; &gt; &gt; +		if (!pte_present(*pte))</span>
<span class="quote">&gt; &gt; &gt; &gt; +			continue;</span>
<span class="quote">&gt; &gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; &gt; &gt; +		if (PAGE_ALIGNED(addr) &amp;&amp; PAGE_ALIGNED(next)) {</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; When would those addresses *not* be page-aligned? By construction, next</span>
<span class="quote">&gt; &gt; &gt; must be. Unplugging partial pages of memory makes no sense, so surely</span>
<span class="quote">&gt; &gt; &gt; addr is page-aligned when passed in?</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; The issue here is that this function is called in one of two cases:</span>
<span class="quote">&gt; &gt; 1. to clear pagetables of directly mapped (linear) memory </span>
<span class="quote">&gt; &gt; 2. Pagetables (and corresponding data pages) for vmemmap. </span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; It is my understanding that, in the second case, we might be clearing</span>
<span class="quote">&gt; &gt; only part of the page content (i.e, only a few struct pages). Note that</span>
<span class="quote">&gt; &gt; next is page aligned by construction only if next &lt;= end.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Ok. A comment to that effect immediately above this check would be</span>
<span class="quote">&gt; helpful.</span>
<span class="quote">&gt; </span>

Ok, thanks.
<span class="quote">
&gt; &gt; &gt; &gt; +			/*</span>
<span class="quote">&gt; &gt; &gt; &gt; +			 * Do not free direct mapping pages since they were</span>
<span class="quote">&gt; &gt; &gt; &gt; +			 * freed when offlining, or simplely not in use.</span>
<span class="quote">&gt; &gt; &gt; &gt; +			 */</span>
<span class="quote">&gt; &gt; &gt; &gt; +			if (!direct)</span>
<span class="quote">&gt; &gt; &gt; &gt; +				free_pagetable(pte_page(*pte), 0, direct);</span>
<span class="quote">&gt; &gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; &gt; &gt; +			/*</span>
<span class="quote">&gt; &gt; &gt; &gt; +			 * This spin lock could be only</span>
<span class="quote">&gt; &gt; &gt; &gt; +			 * taken in _pte_aloc_kernel in</span>
<span class="quote">&gt; &gt; &gt; &gt; +			 * mm/memory.c and nowhere else</span>
<span class="quote">&gt; &gt; &gt; &gt; +			 * (for arm64). Not sure if the</span>
<span class="quote">&gt; &gt; &gt; &gt; +			 * function above can be called</span>
<span class="quote">&gt; &gt; &gt; &gt; +			 * concurrently. In doubt,</span>
<span class="quote">&gt; &gt; &gt; &gt; +			 * I am living it here for now,</span>
<span class="quote">&gt; &gt; &gt; &gt; +			 * but it probably can be removed.</span>
<span class="quote">&gt; &gt; &gt; &gt; +			 */</span>
<span class="quote">&gt; &gt; &gt; &gt; +			spin_lock(&amp;init_mm.page_table_lock);</span>
<span class="quote">&gt; &gt; &gt; &gt; +			pte_clear(&amp;init_mm, addr, pte);</span>
<span class="quote">&gt; &gt; &gt; &gt; +			spin_unlock(&amp;init_mm.page_table_lock);</span>
<span class="quote">&gt; &gt; &gt; &gt; +		} else {</span>
<span class="quote">&gt; &gt; &gt; &gt; +			/*</span>
<span class="quote">&gt; &gt; &gt; &gt; +			 * If we are here, we are freeing vmemmap pages since</span>
<span class="quote">&gt; &gt; &gt; &gt; +			 * direct mapped memory ranges to be freed are aligned.</span>
<span class="quote">&gt; &gt; &gt; &gt; +			 *</span>
<span class="quote">&gt; &gt; &gt; &gt; +			 * If we are not removing the whole page, it means</span>
<span class="quote">&gt; &gt; &gt; &gt; +			 * other page structs in this page are being used and</span>
<span class="quote">&gt; &gt; &gt; &gt; +			 * we canot remove them. So fill the unused page_structs</span>
<span class="quote">&gt; &gt; &gt; &gt; +			 * with 0xFD, and remove the page when it is wholly</span>
<span class="quote">&gt; &gt; &gt; &gt; +			 * filled with 0xFD.</span>
<span class="quote">&gt; &gt; &gt; &gt; +			 */</span>
<span class="quote">&gt; &gt; &gt; &gt; +			memset((void *)addr, PAGE_INUSE, next - addr);</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; What&#39;s special about 0xFD?</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Just used it as a constant symmetrically to x86_64 code.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Why do we need to mess with the page array in this manner? Why can&#39;t we</span>
<span class="quote">&gt; &gt; &gt; detect when a range is free by querying memblock, for example?</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I am not sure I get your suggestion. I guess that the logic here</span>
<span class="quote">&gt; &gt; is that I cannot be sure that I can free the full page because other</span>
<span class="quote">&gt; &gt; entries might be in use for active vmemmap mappings. So we just &quot;mark&quot;</span>
<span class="quote">&gt; &gt; the unused once and only free the page when all of it is marked. See</span>
<span class="quote">&gt; &gt; again commit ae9aae9eda2db71, where all this comes from.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I understood that this is deferring freeing until a whole page of struct</span>
<span class="quote">&gt; pages has been freed.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; My concern is that filling the unused memory with an array of junk chars</span>
<span class="quote">&gt; feels like a hack. We don&#39;t do this at the edges when we allocate</span>
<span class="quote">&gt; memblock today, AFAICT, so this doesn&#39;t seem complete.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Is there no &quot;real&quot; datastructure we can use to keep track of what memory</span>
<span class="quote">&gt; is present? e.g. memblock?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; [...]</span>

We could add a MEMBLOCK_VMEMMAP_UNUSED flag in memblock and mark the
partially unused memblock range with that instead of using the 0xFD
hack. Eventually that might even be backported to x86. Is that what
you are suggesting? I am not confident we can reuse an existing flag
for the purpose without breaking something else.

[...]
<span class="quote">
&gt; &gt; &gt; If that&#39;s the case, we&#39;ll *need* to split the pmd, which we can&#39;t do on</span>
<span class="quote">&gt; &gt; &gt; live page tables.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Please, see below.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; +				if (!direct) {</span>
<span class="quote">&gt; &gt; &gt; &gt; +					free_pagetable(pmd_page(*pmd),</span>
<span class="quote">&gt; &gt; &gt; &gt; +						get_order(PMD_SIZE), direct);</span>
<span class="quote">&gt; &gt; &gt; &gt; +				}</span>
<span class="quote">&gt; &gt; &gt; &gt; +				/*</span>
<span class="quote">&gt; &gt; &gt; &gt; +				 * This spin lock could be only</span>
<span class="quote">&gt; &gt; &gt; &gt; +				 * taken in _pte_aloc_kernel in</span>
<span class="quote">&gt; &gt; &gt; &gt; +				 * mm/memory.c and nowhere else</span>
<span class="quote">&gt; &gt; &gt; &gt; +				 * (for arm64). Not sure if the</span>
<span class="quote">&gt; &gt; &gt; &gt; +				 * function above can be called</span>
<span class="quote">&gt; &gt; &gt; &gt; +				 * concurrently. In doubt,</span>
<span class="quote">&gt; &gt; &gt; &gt; +				 * I am living it here for now,</span>
<span class="quote">&gt; &gt; &gt; &gt; +				 * but it probably can be removed.</span>
<span class="quote">&gt; &gt; &gt; &gt; +				 */</span>
<span class="quote">&gt; &gt; &gt; &gt; +				spin_lock(&amp;init_mm.page_table_lock);</span>
<span class="quote">&gt; &gt; &gt; &gt; +				pmd_clear(pmd);</span>
<span class="quote">&gt; &gt; &gt; &gt; +				spin_unlock(&amp;init_mm.page_table_lock);</span>
<span class="quote">&gt; &gt; &gt; &gt; +			} else {</span>
<span class="quote">&gt; &gt; &gt; &gt; +				/* If here, we are freeing vmemmap pages. */</span>
<span class="quote">&gt; &gt; &gt; &gt; +				memset((void *)addr, PAGE_INUSE, next - addr);</span>
<span class="quote">&gt; &gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; &gt; &gt; +				page_addr = page_address(pmd_page(*pmd));</span>
<span class="quote">&gt; &gt; &gt; &gt; +				if (!memchr_inv(page_addr, PAGE_INUSE,</span>
<span class="quote">&gt; &gt; &gt; &gt; +						PMD_SIZE)) {</span>
<span class="quote">&gt; &gt; &gt; &gt; +					free_pagetable(pmd_page(*pmd),</span>
<span class="quote">&gt; &gt; &gt; &gt; +						get_order(PMD_SIZE), direct);</span>
<span class="quote">&gt; &gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; &gt; &gt; +					/*</span>
<span class="quote">&gt; &gt; &gt; &gt; +					 * This spin lock could be only</span>
<span class="quote">&gt; &gt; &gt; &gt; +					 * taken in _pte_aloc_kernel in</span>
<span class="quote">&gt; &gt; &gt; &gt; +					 * mm/memory.c and nowhere else</span>
<span class="quote">&gt; &gt; &gt; &gt; +					 * (for arm64). Not sure if the</span>
<span class="quote">&gt; &gt; &gt; &gt; +					 * function above can be called</span>
<span class="quote">&gt; &gt; &gt; &gt; +					 * concurrently. In doubt,</span>
<span class="quote">&gt; &gt; &gt; &gt; +					 * I am living it here for now,</span>
<span class="quote">&gt; &gt; &gt; &gt; +					 * but it probably can be removed.</span>
<span class="quote">&gt; &gt; &gt; &gt; +					 */</span>
<span class="quote">&gt; &gt; &gt; &gt; +					spin_lock(&amp;init_mm.page_table_lock);</span>
<span class="quote">&gt; &gt; &gt; &gt; +					pmd_clear(pmd);</span>
<span class="quote">&gt; &gt; &gt; &gt; +					spin_unlock(&amp;init_mm.page_table_lock);</span>
<span class="quote">&gt; &gt; &gt; &gt; +				}</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; I don&#39;t think this is correct.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; If we&#39;re getting rid of a partial pmd, we *must* split the pmd.</span>
<span class="quote">&gt; &gt; &gt; Otherwise, we&#39;re leaving bits mapped that should not be. If we split the</span>
<span class="quote">&gt; &gt; &gt; pmd, we can free the individual pages as we would for a non-section</span>
<span class="quote">&gt; &gt; &gt; mapping.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; As above, we can&#39;t split block entries within live tables, so that will</span>
<span class="quote">&gt; &gt; &gt; be painful at best.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; If we can&#39;t split a pmd, hen we cannot free a partial pmd, and will need</span>
<span class="quote">&gt; &gt; &gt; to reject request to do so.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; The same comments (with s/pmu/pud/, etc) apply for the higher level</span>
<span class="quote">&gt; &gt; &gt; remove_p*d_table functions.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; [...]</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; This only happens when we are clearing vmemmap memory. </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Is that definitely the case?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Currently, I can&#39;t see what prevents adding 2M of memory, and then</span>
<span class="quote">&gt; removing the first 4K of that. We&#39;ll use a 2M section for the linear map</span>
<span class="quote">&gt; of that, but won&#39;t unmap the 4K when removing.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Likewise for the next level up, with s/2M/1G/ and s/4K/2M/.</span>
<span class="quote">&gt; </span>

You&#39;re right. The confusion comes from the fact that we were only
considering the case where we are hot-removing memory that was previously
hot-added. In that case, the memory granularity of hot-add and hot-remove
is the same and fixed at compile time to get_memory_block_size() ==
1&lt;&lt;SECTION_SIZE_BITS (1GB by default).

In case we are removing memory that was in the linear mapping since boot,
then we might get in the situation you are describing above if someone has
manually changed SECTION_SIZE_BITS to a different value in sparsemem.h.
If not, I believe that the fact that we are removing one aligned GB per
shot should guarantee that we never have to split a PUD.

However, we should definitely handle all the cases. Since splitting a
P[UM]D might be a nightmare (if possible at all), two more or less clean
solutions I can think of are:
1. Only allow to hot-remove memory that was previously hot-added or,
2. Detect when we are trying to only partially remove a section mapped
   area of the linear mapping and just fail (and roll back) the remove
   process.

I think I prefer no. 2; it has the advantage of not forbidding a-priori
to remove non-hot-added memory; and its only disadvantage is that in some
(uncommon?) cases you would be just forbidden to hot-remove some memory,
with no distructive side effects.  What do you think?

[...]
<span class="quote"> 
&gt; Thanks,</span>
<span class="quote">&gt; Mark.</span>
<span class="quote">&gt; </span>

Thanks again and best regards,
Andrea
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=171743">Andrea Reale</a> - April 21, 2017, 10:05 a.m.</div>
<pre class="content">
Hi all,

thanks for taking the time to comment. Replies in-line.

On Wed, Apr 19, 2017 at 08:53:13AM -0700, Laura Abbott wrote:
<span class="quote">&gt; On 04/18/2017 11:48 AM, Ard Biesheuvel wrote:</span>
<span class="quote">&gt; &gt;On 18 April 2017 at 19:21, Mark Rutland &lt;mark.rutland@arm.com&gt; wrote:</span>
<span class="quote">&gt; &gt;&gt;On Fri, Apr 14, 2017 at 03:01:58PM +0100, Andrea Reale wrote:</span>

[...]
<span class="quote">
&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; From a quick scan, I see that it&#39;s necessary to use pgtable_page_ctor()</span>
<span class="quote">&gt; &gt;&gt;for pages that will be used for userspace page tables, but it&#39;s not</span>
<span class="quote">&gt; &gt;&gt;clear to me if it&#39;s ever necessary for pages used for kernel page</span>
<span class="quote">&gt; &gt;&gt;tables.</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;If it is, we appear to have a bug on arm64.</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;Laura, Ard, thoughts?</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;The generic apply_to_page_range() will expect the PTE lock to be</span>
<span class="quote">&gt; &gt;initialized for page table pages that are not part of init_mm. For</span>
<span class="quote">&gt; &gt;arm64, that is precisely efi_mm as far as I am aware. For EFI, the</span>
<span class="quote">&gt; &gt;locking is unnecessary but does no harm (the permissions are set once</span>
<span class="quote">&gt; &gt;via apply_to_page_range() at boot), so I added this call when adding</span>
<span class="quote">&gt; &gt;support for strict permissions in EFI rt services mappings.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;So I think it is appropriate for create_pgd_mapping() to be in charge</span>
<span class="quote">&gt; &gt;of calling the ctor(). We simply have no destroy_pgd_mapping()</span>
<span class="quote">&gt; &gt;counterpart that would be the place for the dtor() call, given that we</span>
<span class="quote">&gt; &gt;never take down EFI rt services mappi &gt;</span>
<span class="quote">&gt; &gt;Whether it makes sense or not to lock/unlock in apply_to_page_range()</span>
<span class="quote">&gt; &gt;is something I did not spend any brain cycles on at the time.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Agreed there shouldn&#39;t be a problem right now. I do think the locking is</span>
<span class="quote">&gt; appropriate in apply_to_page_range given what other functions also get</span>
<span class="quote">&gt; locked.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I really wish this were less asymmetrical though since it get hard</span>
<span class="quote">&gt; to reason about. It looks like hotplug_paging will call the ctor,</span>
<span class="quote">&gt; so is there an issue with calling hot-remove on memory that was once</span>
<span class="quote">&gt; hot-added or is that not a concern?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Thanks,</span>
<span class="quote">&gt; Laura</span>

I think the confusion comes from the fact that, in hotplug_paging, we are
passing pgd_pgtable_alloc as the page allocator for __create_pgd_mapping,
which always calls the ctor.

If I got things right (but, please, correct me if I am wrong), we don&#39;t
need to get the pte_lock that the ctor gets since - in hotplug - we are
adding to init_mm.

Moreover, I am just realizing that calling the dtor while hot-removing
might create problems when removing memory that *was not* previously
hotplugged, as we are calling a dtor on something that was never
ctor&#39;ed. Is that what you were hinting at, Laura?

Thanks and best regards,
Andrea
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=130411">Laura Abbott</a> - April 24, 2017, 11:59 p.m.</div>
<pre class="content">
On 04/21/2017 03:05 AM, Andrea Reale wrote:
<span class="quote">&gt; Hi all,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; thanks for taking the time to comment. Replies in-line.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; On Wed, Apr 19, 2017 at 08:53:13AM -0700, Laura Abbott wrote:</span>
<span class="quote">&gt;&gt; On 04/18/2017 11:48 AM, Ard Biesheuvel wrote:</span>
<span class="quote">&gt;&gt;&gt; On 18 April 2017 at 19:21, Mark Rutland &lt;mark.rutland@arm.com&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; On Fri, Apr 14, 2017 at 03:01:58PM +0100, Andrea Reale wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; [...]</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; From a quick scan, I see that it&#39;s necessary to use pgtable_page_ctor()</span>
<span class="quote">&gt;&gt;&gt;&gt; for pages that will be used for userspace page tables, but it&#39;s not</span>
<span class="quote">&gt;&gt;&gt;&gt; clear to me if it&#39;s ever necessary for pages used for kernel page</span>
<span class="quote">&gt;&gt;&gt;&gt; tables.</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; If it is, we appear to have a bug on arm64.</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; Laura, Ard, thoughts?</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; The generic apply_to_page_range() will expect the PTE lock to be</span>
<span class="quote">&gt;&gt;&gt; initialized for page table pages that are not part of init_mm. For</span>
<span class="quote">&gt;&gt;&gt; arm64, that is precisely efi_mm as far as I am aware. For EFI, the</span>
<span class="quote">&gt;&gt;&gt; locking is unnecessary but does no harm (the permissions are set once</span>
<span class="quote">&gt;&gt;&gt; via apply_to_page_range() at boot), so I added this call when adding</span>
<span class="quote">&gt;&gt;&gt; support for strict permissions in EFI rt services mappings.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; So I think it is appropriate for create_pgd_mapping() to be in charge</span>
<span class="quote">&gt;&gt;&gt; of calling the ctor(). We simply have no destroy_pgd_mapping()</span>
<span class="quote">&gt;&gt;&gt; counterpart that would be the place for the dtor() call, given that we</span>
<span class="quote">&gt;&gt;&gt; never take down EFI rt services mappi &gt;</span>
<span class="quote">&gt;&gt;&gt; Whether it makes sense or not to lock/unlock in apply_to_page_range()</span>
<span class="quote">&gt;&gt;&gt; is something I did not spend any brain cycles on at the time.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Agreed there shouldn&#39;t be a problem right now. I do think the locking is</span>
<span class="quote">&gt;&gt; appropriate in apply_to_page_range given what other functions also get</span>
<span class="quote">&gt;&gt; locked.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; I really wish this were less asymmetrical though since it get hard</span>
<span class="quote">&gt;&gt; to reason about. It looks like hotplug_paging will call the ctor,</span>
<span class="quote">&gt;&gt; so is there an issue with calling hot-remove on memory that was once</span>
<span class="quote">&gt;&gt; hot-added or is that not a concern?</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Thanks,</span>
<span class="quote">&gt;&gt; Laura</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I think the confusion comes from the fact that, in hotplug_paging, we are</span>
<span class="quote">&gt; passing pgd_pgtable_alloc as the page allocator for __create_pgd_mapping,</span>
<span class="quote">&gt; which always calls the ctor.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; If I got things right (but, please, correct me if I am wrong), we don&#39;t</span>
<span class="quote">&gt; need to get the pte_lock that the ctor gets since - in hotplug - we are</span>
<span class="quote">&gt; adding to init_mm.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Moreover, I am just realizing that calling the dtor while hot-removing</span>
<span class="quote">&gt; might create problems when removing memory that *was not* previously</span>
<span class="quote">&gt; hotplugged, as we are calling a dtor on something that was never</span>
<span class="quote">&gt; ctor&#39;ed. Is that what you were hinting at, Laura?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Thanks and best regards,</span>
<span class="quote">&gt; Andrea</span>
<span class="quote">&gt; </span>

Yes, that was what I was thinking.

Thanks,
Laura
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/arm64/Kconfig b/arch/arm64/Kconfig</span>
<span class="p_header">index fa71d94..83b8bb5 100644</span>
<span class="p_header">--- a/arch/arm64/Kconfig</span>
<span class="p_header">+++ b/arch/arm64/Kconfig</span>
<span class="p_chunk">@@ -624,6 +624,9 @@</span> <span class="p_context"> config ARCH_ENABLE_MEMORY_HOTPLUG</span>
     depends on !NUMA
 	def_bool y
 
<span class="p_add">+config ARCH_ENABLE_MEMORY_HOTREMOVE</span>
<span class="p_add">+	def_bool y</span>
<span class="p_add">+</span>
 # Common NUMA Features
 config NUMA
 	bool &quot;Numa Memory Allocation and Scheduler Support&quot;
<span class="p_header">diff --git a/arch/arm64/include/asm/mmu.h b/arch/arm64/include/asm/mmu.h</span>
<span class="p_header">index 8eb31db..2cf2115 100644</span>
<span class="p_header">--- a/arch/arm64/include/asm/mmu.h</span>
<span class="p_header">+++ b/arch/arm64/include/asm/mmu.h</span>
<span class="p_chunk">@@ -39,6 +39,10 @@</span> <span class="p_context"> extern void create_pgd_mapping(struct mm_struct *mm, phys_addr_t phys,</span>
 extern void *fixmap_remap_fdt(phys_addr_t dt_phys);
 #ifdef CONFIG_MEMORY_HOTPLUG
 extern void hotplug_paging(phys_addr_t start, phys_addr_t size);
<span class="p_add">+#ifdef CONFIG_MEMORY_HOTREMOVE</span>
<span class="p_add">+extern void remove_pagetable(unsigned long start,</span>
<span class="p_add">+	unsigned long end, bool direct);</span>
<span class="p_add">+#endif</span>
 #endif
 
 #endif
<span class="p_header">diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h</span>
<span class="p_header">index 0eef606..194cb3e 100644</span>
<span class="p_header">--- a/arch/arm64/include/asm/pgtable.h</span>
<span class="p_header">+++ b/arch/arm64/include/asm/pgtable.h</span>
<span class="p_chunk">@@ -399,6 +399,11 @@</span> <span class="p_context"> static inline phys_addr_t pmd_page_paddr(pmd_t pmd)</span>
 	return pmd_val(pmd) &amp; PHYS_MASK &amp; (s32)PAGE_MASK;
 }
 
<span class="p_add">+static inline unsigned long pmd_page_vaddr(pmd_t pmd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (unsigned long) __va(pmd_page_paddr(pmd));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /* Find an entry in the third-level page table. */
 #define pte_index(addr)		(((addr) &gt;&gt; PAGE_SHIFT) &amp; (PTRS_PER_PTE - 1))
 
<span class="p_chunk">@@ -450,6 +455,11 @@</span> <span class="p_context"> static inline phys_addr_t pud_page_paddr(pud_t pud)</span>
 	return pud_val(pud) &amp; PHYS_MASK &amp; (s32)PAGE_MASK;
 }
 
<span class="p_add">+static inline unsigned long pud_page_vaddr(pud_t pud)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (unsigned long) __va(pud_page_paddr(pud));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /* Find an entry in the second-level page table. */
 #define pmd_index(addr)		(((addr) &gt;&gt; PMD_SHIFT) &amp; (PTRS_PER_PMD - 1))
 
<span class="p_chunk">@@ -502,6 +512,11 @@</span> <span class="p_context"> static inline phys_addr_t pgd_page_paddr(pgd_t pgd)</span>
 	return pgd_val(pgd) &amp; PHYS_MASK &amp; (s32)PAGE_MASK;
 }
 
<span class="p_add">+static inline unsigned long pgd_page_vaddr(pgd_t pgd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (unsigned long) __va(pgd_page_paddr(pgd));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /* Find an entry in the frst-level page table. */
 #define pud_index(addr)		(((addr) &gt;&gt; PUD_SHIFT) &amp; (PTRS_PER_PUD - 1))
 
<span class="p_header">diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c</span>
<span class="p_header">index 259bb6e..c12983b 100644</span>
<span class="p_header">--- a/arch/arm64/mm/init.c</span>
<span class="p_header">+++ b/arch/arm64/mm/init.c</span>
<span class="p_chunk">@@ -551,7 +551,6 @@</span> <span class="p_context"> int arch_add_memory(int nid, u64 start, u64 size, bool for_device)</span>
 	unsigned long nr_pages = size &gt;&gt; PAGE_SHIFT;
 	unsigned long end_pfn = start_pfn + nr_pages;
 	unsigned long max_sparsemem_pfn = 1UL &lt;&lt; (MAX_PHYSMEM_BITS-PAGE_SHIFT);
<span class="p_del">-	unsigned long pfn;</span>
 	int ret;
 
 	if (end_pfn &gt; max_sparsemem_pfn) {
<span class="p_chunk">@@ -624,5 +623,34 @@</span> <span class="p_context"> int arch_add_memory(int nid, u64 start, u64 size, bool for_device)</span>
 
 	return ret;
 }
<span class="p_del">-#endif</span>
 
<span class="p_add">+#ifdef CONFIG_MEMORY_HOTREMOVE</span>
<span class="p_add">+static void kernel_physical_mapping_remove(unsigned long start,</span>
<span class="p_add">+	unsigned long end)</span>
<span class="p_add">+{</span>
<span class="p_add">+	start = (unsigned long)__va(start);</span>
<span class="p_add">+	end = (unsigned long)__va(end);</span>
<span class="p_add">+</span>
<span class="p_add">+	remove_pagetable(start, end, true);</span>
<span class="p_add">+</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+int arch_remove_memory(u64 start, u64 size)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long start_pfn = start &gt;&gt; PAGE_SHIFT;</span>
<span class="p_add">+	unsigned long nr_pages = size &gt;&gt; PAGE_SHIFT;</span>
<span class="p_add">+	struct page *page = pfn_to_page(start_pfn);</span>
<span class="p_add">+	struct zone *zone;</span>
<span class="p_add">+	int ret = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	zone = page_zone(page);</span>
<span class="p_add">+	ret = __remove_pages(zone, start_pfn, nr_pages);</span>
<span class="p_add">+	WARN_ON_ONCE(ret);</span>
<span class="p_add">+</span>
<span class="p_add">+	kernel_physical_mapping_remove(start, start + size);</span>
<span class="p_add">+</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* CONFIG_MEMORY_HOTREMOVE */</span>
<span class="p_add">+#endif /* CONFIG_MEMORY_HOTPLUG */</span>
<span class="p_header">diff --git a/arch/arm64/mm/mmu.c b/arch/arm64/mm/mmu.c</span>
<span class="p_header">index 8882187..e129d7c 100644</span>
<span class="p_header">--- a/arch/arm64/mm/mmu.c</span>
<span class="p_header">+++ b/arch/arm64/mm/mmu.c</span>
<span class="p_chunk">@@ -1,4 +1,3 @@</span> <span class="p_context"></span>
<span class="p_del">-#define pr_fmt(fmt) KBUILD_MODNAME &quot;: &quot; fmt</span>
 /*
  * Based on arch/arm/mm/mmu.c
  *
<span class="p_chunk">@@ -24,6 +23,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/errno.h&gt;
 #include &lt;linux/init.h&gt;
 #include &lt;linux/libfdt.h&gt;
<span class="p_add">+#include &lt;linux/memremap.h&gt;</span>
 #include &lt;linux/mman.h&gt;
 #include &lt;linux/nodemask.h&gt;
 #include &lt;linux/memblock.h&gt;
<span class="p_chunk">@@ -119,7 +119,6 @@</span> <span class="p_context"> static void alloc_init_pte(pmd_t *pmd, unsigned long addr,</span>
 		phys_addr_t pte_phys;
 		BUG_ON(!pgtable_alloc);
 		pte_phys = pgtable_alloc();
<span class="p_del">-		pr_debug(&quot;Allocating PTE at %p\n&quot;, __va(pte_phys));</span>
 		pte = pte_set_fixmap(pte_phys);
 		__pmd_populate(pmd, pte_phys, PMD_TYPE_TABLE);
 		pte_clear_fixmap();
<span class="p_chunk">@@ -160,7 +159,6 @@</span> <span class="p_context"> static void alloc_init_pmd(pud_t *pud, unsigned long addr, unsigned long end,</span>
 		phys_addr_t pmd_phys;
 		BUG_ON(!pgtable_alloc);
 		pmd_phys = pgtable_alloc();
<span class="p_del">-		pr_debug(&quot;Allocating PMD at %p\n&quot;, __va(pmd_phys));</span>
 		pmd = pmd_set_fixmap(pmd_phys);
 		__pud_populate(pud, pmd_phys, PUD_TYPE_TABLE);
 		pmd_clear_fixmap();
<span class="p_chunk">@@ -221,7 +219,6 @@</span> <span class="p_context"> static void alloc_init_pud(pgd_t *pgd, unsigned long addr, unsigned long end,</span>
 		phys_addr_t pud_phys;
 		BUG_ON(!pgtable_alloc);
 		pud_phys = pgtable_alloc();
<span class="p_del">-		pr_debug(&quot;Allocating PUD at %p\n&quot;, __va(pud_phys));</span>
 		__pgd_populate(pgd, pud_phys, PUD_TYPE_TABLE);
 	}
 	BUG_ON(pgd_bad(*pgd));
<span class="p_chunk">@@ -546,7 +543,389 @@</span> <span class="p_context"> void hotplug_paging(phys_addr_t start, phys_addr_t size)</span>
 	__free_pages(pg, 0);
 }
 
<span class="p_add">+#ifdef CONFIG_MEMORY_HOTREMOVE</span>
<span class="p_add">+#define PAGE_INUSE 0xFD</span>
<span class="p_add">+</span>
<span class="p_add">+static void  free_pagetable(struct page *page, int order, bool direct)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long magic;</span>
<span class="p_add">+	unsigned int nr_pages = 1 &lt;&lt; order;</span>
<span class="p_add">+	struct vmem_altmap *altmap = to_vmem_altmap((unsigned long) page);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (altmap) {</span>
<span class="p_add">+		vmem_altmap_free(altmap, nr_pages);</span>
<span class="p_add">+		return;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/* bootmem page has reserved flag */</span>
<span class="p_add">+	if (PageReserved(page)) {</span>
<span class="p_add">+		__ClearPageReserved(page);</span>
<span class="p_add">+</span>
<span class="p_add">+		magic = (unsigned long)page-&gt;lru.next;</span>
<span class="p_add">+		if (magic == SECTION_INFO || magic == MIX_SECTION_INFO) {</span>
<span class="p_add">+			while (nr_pages--)</span>
<span class="p_add">+				put_page_bootmem(page++);</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			while (nr_pages--)</span>
<span class="p_add">+				free_reserved_page(page++);</span>
<span class="p_add">+		}</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Only direct pagetable allocation (those allocated via</span>
<span class="p_add">+		 * hotplug) call the pgtable_page_ctor; vmemmap pgtable</span>
<span class="p_add">+		 * allocations don&#39;t.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (direct)</span>
<span class="p_add">+			pgtable_page_dtor(page);</span>
<span class="p_add">+</span>
<span class="p_add">+		free_pages((unsigned long)page_address(page), order);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void free_pte_table(pmd_t *pmd, bool direct)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pte_t *pte_start, *pte;</span>
<span class="p_add">+	struct page *page;</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+</span>
<span class="p_add">+	pte_start =  (pte_t *) pmd_page_vaddr(*pmd);</span>
<span class="p_add">+	/* Check if there is no valid entry in the PMD */</span>
<span class="p_add">+	for (i = 0; i &lt; PTRS_PER_PTE; i++) {</span>
<span class="p_add">+		pte = pte_start + i;</span>
<span class="p_add">+		if (!pte_none(*pte))</span>
<span class="p_add">+			return;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	page = pmd_page(*pmd);</span>
<span class="p_add">+</span>
<span class="p_add">+	free_pagetable(page, 0, direct);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * This spin lock could be only taken in _pte_aloc_kernel</span>
<span class="p_add">+	 * in mm/memory.c and nowhere else (for arm64). Not sure if</span>
<span class="p_add">+	 * the function above can be called concurrently. In doubt,</span>
<span class="p_add">+	 * I am living it here for now, but it probably can be removed</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	spin_lock(&amp;init_mm.page_table_lock);</span>
<span class="p_add">+	pmd_clear(pmd);</span>
<span class="p_add">+	spin_unlock(&amp;init_mm.page_table_lock);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void free_pmd_table(pud_t *pud, bool direct)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pmd_t *pmd_start, *pmd;</span>
<span class="p_add">+	struct page *page;</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+</span>
<span class="p_add">+	pmd_start = (pmd_t *) pud_page_vaddr(*pud);</span>
<span class="p_add">+	/* Check if there is no valid entry in the PMD */</span>
<span class="p_add">+	for (i = 0; i &lt; PTRS_PER_PMD; i++) {</span>
<span class="p_add">+		pmd = pmd_start + i;</span>
<span class="p_add">+		if (!pmd_none(*pmd))</span>
<span class="p_add">+			return;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	page = pud_page(*pud);</span>
<span class="p_add">+</span>
<span class="p_add">+	free_pagetable(page, 0, direct);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * This spin lock could be only taken in _pte_aloc_kernel</span>
<span class="p_add">+	 * in mm/memory.c and nowhere else (for arm64). Not sure if</span>
<span class="p_add">+	 * the function above can be called concurrently. In doubt,</span>
<span class="p_add">+	 * I am living it here for now, but it probably can be removed</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	spin_lock(&amp;init_mm.page_table_lock);</span>
<span class="p_add">+	pud_clear(pud);</span>
<span class="p_add">+	spin_unlock(&amp;init_mm.page_table_lock);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * When the PUD is folded on the PGD (three levels of paging),</span>
<span class="p_add">+ * there&#39;s no need to free PUDs</span>
<span class="p_add">+ */</span>
<span class="p_add">+#if CONFIG_PGTABLE_LEVELS &gt; 3</span>
<span class="p_add">+static void free_pud_table(pgd_t *pgd, bool direct)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pud_t *pud_start, *pud;</span>
<span class="p_add">+	struct page *page;</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+</span>
<span class="p_add">+	pud_start = (pud_t *) pgd_page_vaddr(*pgd);</span>
<span class="p_add">+	/* Check if there is no valid entry in the PUD */</span>
<span class="p_add">+	for (i = 0; i &lt; PTRS_PER_PUD; i++) {</span>
<span class="p_add">+		pud = pud_start + i;</span>
<span class="p_add">+		if (!pud_none(*pud))</span>
<span class="p_add">+			return;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	page = pgd_page(*pgd);</span>
<span class="p_add">+</span>
<span class="p_add">+	free_pagetable(page, 0, direct);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * This spin lock could be only</span>
<span class="p_add">+	 * taken in _pte_aloc_kernel in</span>
<span class="p_add">+	 * mm/memory.c and nowhere else</span>
<span class="p_add">+	 * (for arm64). Not sure if the</span>
<span class="p_add">+	 * function above can be called</span>
<span class="p_add">+	 * concurrently. In doubt,</span>
<span class="p_add">+	 * I am living it here for now,</span>
<span class="p_add">+	 * but it probably can be removed.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	spin_lock(&amp;init_mm.page_table_lock);</span>
<span class="p_add">+	pgd_clear(pgd);</span>
<span class="p_add">+	spin_unlock(&amp;init_mm.page_table_lock);</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+static void remove_pte_table(pte_t *pte, unsigned long addr,</span>
<span class="p_add">+	unsigned long end, bool direct)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long next;</span>
<span class="p_add">+	void *page_addr;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (; addr &lt; end; addr = next, pte++) {</span>
<span class="p_add">+		next = (addr + PAGE_SIZE) &amp; PAGE_MASK;</span>
<span class="p_add">+		if (next &gt; end)</span>
<span class="p_add">+			next = end;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!pte_present(*pte))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (PAGE_ALIGNED(addr) &amp;&amp; PAGE_ALIGNED(next)) {</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * Do not free direct mapping pages since they were</span>
<span class="p_add">+			 * freed when offlining, or simplely not in use.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			if (!direct)</span>
<span class="p_add">+				free_pagetable(pte_page(*pte), 0, direct);</span>
<span class="p_add">+</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * This spin lock could be only</span>
<span class="p_add">+			 * taken in _pte_aloc_kernel in</span>
<span class="p_add">+			 * mm/memory.c and nowhere else</span>
<span class="p_add">+			 * (for arm64). Not sure if the</span>
<span class="p_add">+			 * function above can be called</span>
<span class="p_add">+			 * concurrently. In doubt,</span>
<span class="p_add">+			 * I am living it here for now,</span>
<span class="p_add">+			 * but it probably can be removed.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			spin_lock(&amp;init_mm.page_table_lock);</span>
<span class="p_add">+			pte_clear(&amp;init_mm, addr, pte);</span>
<span class="p_add">+			spin_unlock(&amp;init_mm.page_table_lock);</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * If we are here, we are freeing vmemmap pages since</span>
<span class="p_add">+			 * direct mapped memory ranges to be freed are aligned.</span>
<span class="p_add">+			 *</span>
<span class="p_add">+			 * If we are not removing the whole page, it means</span>
<span class="p_add">+			 * other page structs in this page are being used and</span>
<span class="p_add">+			 * we canot remove them. So fill the unused page_structs</span>
<span class="p_add">+			 * with 0xFD, and remove the page when it is wholly</span>
<span class="p_add">+			 * filled with 0xFD.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			memset((void *)addr, PAGE_INUSE, next - addr);</span>
<span class="p_add">+</span>
<span class="p_add">+			page_addr = page_address(pte_page(*pte));</span>
<span class="p_add">+			if (!memchr_inv(page_addr, PAGE_INUSE, PAGE_SIZE)) {</span>
<span class="p_add">+				free_pagetable(pte_page(*pte), 0, direct);</span>
<span class="p_add">+</span>
<span class="p_add">+				/*</span>
<span class="p_add">+				 * This spin lock could be only</span>
<span class="p_add">+				 * taken in _pte_aloc_kernel in</span>
<span class="p_add">+				 * mm/memory.c and nowhere else</span>
<span class="p_add">+				 * (for arm64). Not sure if the</span>
<span class="p_add">+				 * function above can be called</span>
<span class="p_add">+				 * concurrently. In doubt,</span>
<span class="p_add">+				 * I am living it here for now,</span>
<span class="p_add">+				 * but it probably can be removed.</span>
<span class="p_add">+				 */</span>
<span class="p_add">+				spin_lock(&amp;init_mm.page_table_lock);</span>
<span class="p_add">+				pte_clear(&amp;init_mm, addr, pte);</span>
<span class="p_add">+				spin_unlock(&amp;init_mm.page_table_lock);</span>
<span class="p_add">+			}</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	// I am adding this flush here in simmetry to the x86 code.</span>
<span class="p_add">+	// Why do I need to call it here and not in remove_p[mu]d</span>
<span class="p_add">+	flush_tlb_all();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void remove_pmd_table(pmd_t *pmd, unsigned long addr,</span>
<span class="p_add">+	unsigned long end, bool direct)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long next;</span>
<span class="p_add">+	void *page_addr;</span>
<span class="p_add">+	pte_t *pte;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (; addr &lt; end; addr = next, pmd++) {</span>
<span class="p_add">+		next = pmd_addr_end(addr, end);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!pmd_present(*pmd))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		// check if we are using 2MB section mappings</span>
<span class="p_add">+		if (pmd_sect(*pmd)) {</span>
<span class="p_add">+			if (PAGE_ALIGNED(addr) &amp;&amp; PAGE_ALIGNED(next)) {</span>
<span class="p_add">+				if (!direct) {</span>
<span class="p_add">+					free_pagetable(pmd_page(*pmd),</span>
<span class="p_add">+						get_order(PMD_SIZE), direct);</span>
<span class="p_add">+				}</span>
<span class="p_add">+				/*</span>
<span class="p_add">+				 * This spin lock could be only</span>
<span class="p_add">+				 * taken in _pte_aloc_kernel in</span>
<span class="p_add">+				 * mm/memory.c and nowhere else</span>
<span class="p_add">+				 * (for arm64). Not sure if the</span>
<span class="p_add">+				 * function above can be called</span>
<span class="p_add">+				 * concurrently. In doubt,</span>
<span class="p_add">+				 * I am living it here for now,</span>
<span class="p_add">+				 * but it probably can be removed.</span>
<span class="p_add">+				 */</span>
<span class="p_add">+				spin_lock(&amp;init_mm.page_table_lock);</span>
<span class="p_add">+				pmd_clear(pmd);</span>
<span class="p_add">+				spin_unlock(&amp;init_mm.page_table_lock);</span>
<span class="p_add">+			} else {</span>
<span class="p_add">+				/* If here, we are freeing vmemmap pages. */</span>
<span class="p_add">+				memset((void *)addr, PAGE_INUSE, next - addr);</span>
<span class="p_add">+</span>
<span class="p_add">+				page_addr = page_address(pmd_page(*pmd));</span>
<span class="p_add">+				if (!memchr_inv(page_addr, PAGE_INUSE,</span>
<span class="p_add">+						PMD_SIZE)) {</span>
<span class="p_add">+					free_pagetable(pmd_page(*pmd),</span>
<span class="p_add">+						get_order(PMD_SIZE), direct);</span>
<span class="p_add">+</span>
<span class="p_add">+					/*</span>
<span class="p_add">+					 * This spin lock could be only</span>
<span class="p_add">+					 * taken in _pte_aloc_kernel in</span>
<span class="p_add">+					 * mm/memory.c and nowhere else</span>
<span class="p_add">+					 * (for arm64). Not sure if the</span>
<span class="p_add">+					 * function above can be called</span>
<span class="p_add">+					 * concurrently. In doubt,</span>
<span class="p_add">+					 * I am living it here for now,</span>
<span class="p_add">+					 * but it probably can be removed.</span>
<span class="p_add">+					 */</span>
<span class="p_add">+					spin_lock(&amp;init_mm.page_table_lock);</span>
<span class="p_add">+					pmd_clear(pmd);</span>
<span class="p_add">+					spin_unlock(&amp;init_mm.page_table_lock);</span>
<span class="p_add">+				}</span>
<span class="p_add">+			}</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		BUG_ON(!pmd_table(*pmd));</span>
<span class="p_add">+</span>
<span class="p_add">+		pte = pte_offset_map(pmd, addr);</span>
<span class="p_add">+		remove_pte_table(pte, addr, next, direct);</span>
<span class="p_add">+		free_pte_table(pmd, direct);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void remove_pud_table(pud_t *pud, unsigned long addr,</span>
<span class="p_add">+	unsigned long end, bool direct)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long next;</span>
<span class="p_add">+	pmd_t *pmd;</span>
<span class="p_add">+	void *page_addr;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (; addr &lt; end; addr = next, pud++) {</span>
<span class="p_add">+		next = pud_addr_end(addr, end);</span>
<span class="p_add">+		if (!pud_present(*pud))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * If we are using 4K granules, check if we are using</span>
<span class="p_add">+		 * 1GB section mapping.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (pud_sect(*pud)) {</span>
<span class="p_add">+			if (PAGE_ALIGNED(addr) &amp;&amp; PAGE_ALIGNED(next)) {</span>
<span class="p_add">+				if (!direct) {</span>
<span class="p_add">+					free_pagetable(pud_page(*pud),</span>
<span class="p_add">+						get_order(PUD_SIZE), direct);</span>
<span class="p_add">+				}</span>
<span class="p_add">+</span>
<span class="p_add">+				/*</span>
<span class="p_add">+				 * This spin lock could be only</span>
<span class="p_add">+				 * taken in _pte_aloc_kernel in</span>
<span class="p_add">+				 * mm/memory.c and nowhere else</span>
<span class="p_add">+				 * (for arm64). Not sure if the</span>
<span class="p_add">+				 * function above can be called</span>
<span class="p_add">+				 * concurrently. In doubt,</span>
<span class="p_add">+				 * I am living it here for now,</span>
<span class="p_add">+				 * but it probably can be removed.</span>
<span class="p_add">+				 */</span>
<span class="p_add">+				spin_lock(&amp;init_mm.page_table_lock);</span>
<span class="p_add">+				pud_clear(pud);</span>
<span class="p_add">+				spin_unlock(&amp;init_mm.page_table_lock);</span>
<span class="p_add">+			} else {</span>
<span class="p_add">+				/* If here, we are freeing vmemmap pages. */</span>
<span class="p_add">+				memset((void *)addr, PAGE_INUSE, next - addr);</span>
<span class="p_add">+</span>
<span class="p_add">+				page_addr = page_address(pud_page(*pud));</span>
<span class="p_add">+				if (!memchr_inv(page_addr, PAGE_INUSE,</span>
<span class="p_add">+						PUD_SIZE)) {</span>
<span class="p_add">+</span>
<span class="p_add">+					free_pagetable(pud_page(*pud),</span>
<span class="p_add">+						get_order(PUD_SIZE), direct);</span>
<span class="p_add">+</span>
<span class="p_add">+					/*</span>
<span class="p_add">+					 * This spin lock could be only</span>
<span class="p_add">+					 * taken in _pte_aloc_kernel in</span>
<span class="p_add">+					 * mm/memory.c and nowhere else</span>
<span class="p_add">+					 * (for arm64). Not sure if the</span>
<span class="p_add">+					 * function above can be called</span>
<span class="p_add">+					 * concurrently. In doubt,</span>
<span class="p_add">+					 * I am living it here for now,</span>
<span class="p_add">+					 * but it probably can be removed.</span>
<span class="p_add">+					 */</span>
<span class="p_add">+					spin_lock(&amp;init_mm.page_table_lock);</span>
<span class="p_add">+					pud_clear(pud);</span>
<span class="p_add">+					spin_unlock(&amp;init_mm.page_table_lock);</span>
<span class="p_add">+				}</span>
<span class="p_add">+			}</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		BUG_ON(!pud_table(*pud));</span>
<span class="p_add">+</span>
<span class="p_add">+		pmd = pmd_offset(pud, addr);</span>
<span class="p_add">+		remove_pmd_table(pmd, addr, next, direct);</span>
<span class="p_add">+		free_pmd_table(pud, direct);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void remove_pagetable(unsigned long start, unsigned long end, bool direct)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long next;</span>
<span class="p_add">+	unsigned long addr;</span>
<span class="p_add">+	pgd_t *pgd;</span>
<span class="p_add">+	pud_t *pud;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (addr = start; addr &lt; end; addr = next) {</span>
<span class="p_add">+		next = pgd_addr_end(addr, end);</span>
<span class="p_add">+</span>
<span class="p_add">+		pgd = pgd_offset_k(addr);</span>
<span class="p_add">+		if (pgd_none(*pgd))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		pud = pud_offset(pgd, addr);</span>
<span class="p_add">+		remove_pud_table(pud, addr, next, direct);</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * When the PUD is folded on the PGD (three levels of paging),</span>
<span class="p_add">+		 * I did already clear the PMD page in free_pmd_table,</span>
<span class="p_add">+		 * and reset the corresponding PGD==PUD entry.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+#if CONFIG_PGTABLE_LEVELS &gt; 3</span>
<span class="p_add">+		free_pud_table(pgd, direct);</span>
 #endif
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	flush_tlb_all();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* CONFIG_MEMORY_HOTREMOVE */</span>
<span class="p_add">+#endif /* CONFIG_MEMORY_HOTPLUG */</span>
 
 /*
  * Check whether a kernel address is valid (derived from arch/x86/).
<span class="p_chunk">@@ -629,6 +1008,9 @@</span> <span class="p_context"> int __meminit vmemmap_populate(unsigned long start, unsigned long end, int node)</span>
 #endif	/* CONFIG_ARM64_64K_PAGES */
 void vmemmap_free(unsigned long start, unsigned long end)
 {
<span class="p_add">+#ifdef CONFIG_MEMORY_HOTREMOVE</span>
<span class="p_add">+	remove_pagetable(start, end, false);</span>
<span class="p_add">+#endif</span>
 }
 #endif	/* CONFIG_SPARSEMEM_VMEMMAP */
 

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



