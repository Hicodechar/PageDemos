
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[GIT,pull] x86/pti: Preparatory changes - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [GIT,pull] x86/pti: Preparatory changes</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=107">Thomas Gleixner</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Dec. 23, 2017, 7:23 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;alpine.DEB.2.20.1712231808040.1853@nanos&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10131659/mbox/"
   >mbox</a>
|
   <a href="/patch/10131659/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10131659/">/patch/10131659/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	0FE0A6019D for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Sat, 23 Dec 2017 19:23:30 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id AD3F029017
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Sat, 23 Dec 2017 19:23:29 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id AA7FD294C3; Sat, 23 Dec 2017 19:23:29 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id C24E629017
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Sat, 23 Dec 2017 19:23:25 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1757076AbdLWTXQ (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Sat, 23 Dec 2017 14:23:16 -0500
Received: from Galois.linutronix.de ([146.0.238.70]:58823 &quot;EHLO
	Galois.linutronix.de&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1752791AbdLWTXL (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Sat, 23 Dec 2017 14:23:11 -0500
Received: from p4fea5f09.dip0.t-ipconnect.de ([79.234.95.9] helo=nanos)
	by Galois.linutronix.de with esmtpsa
	(TLS1.2:DHE_RSA_AES_256_CBC_SHA256:256) (Exim 4.80)
	(envelope-from &lt;tglx@linutronix.de&gt;)
	id 1eSpMF-0003ua-Cn; Sat, 23 Dec 2017 20:21:20 +0100
Date: Sat, 23 Dec 2017 20:23:05 +0100 (CET)
From: Thomas Gleixner &lt;tglx@linutronix.de&gt;
To: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;
cc: LKML &lt;linux-kernel@vger.kernel.org&gt;, Ingo Molnar &lt;mingo@kernel.org&gt;,
	&quot;H. Peter Anvin&quot; &lt;hpa@zytor.com&gt;, Borislav Petkov &lt;bp@alien8.de&gt;,
	Greg KH &lt;gregkh@linuxfoundation.org&gt;, Andy Lutomirski &lt;luto@kernel.org&gt;
Subject: [GIT pull] x86/pti: Preparatory changes
Message-ID: &lt;alpine.DEB.2.20.1712231808040.1853@nanos&gt;
User-Agent: Alpine 2.20 (DEB 67 2015-01-07)
MIME-Version: 1.0
Content-Type: text/plain; charset=US-ASCII
X-Linutronix-Spam-Score: -1.0
X-Linutronix-Spam-Level: -
X-Linutronix-Spam-Status: No , -1.0 points, 5.0 required, ALL_TRUSTED=-1,
	SHORTCIRCUIT=-0.0001
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=107">Thomas Gleixner</a> - Dec. 23, 2017, 7:23 p.m.</div>
<pre class="content">
Linus,

please pull the latest x86-pti-for-linus git tree from:

   git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip.git x86-pti-for-linus

Todays Advent calendar window contains twentyfour easy to digest
patches. The original plan was to have twenty three matching the date, but
a late fixup made that moot.

  - Move the cpu_entry_area mapping out of the fixmap into a separate
    address space. That&#39;s necessary because the fixmap becomes too big with
    NRCPUS=8192 and this caused already subtle and hard to diagnose
    failures.

    The top most patch is fresh from today and cures a brain slip of that
    tall grumpy german greybeard, who ignored the intricacies of 32bit
    wraparounds.

  - Limit the number of CPUs on 32bit to 64. That&#39;s insane big already, but
    at least it&#39;s small enough to prevent address space issues with the
    cpu_entry_area map, which have been observed and debugged with the
    fixmap code

  - A few TLB flush fixes in various places plus documentation which of the
    TLB functions should be used for what.

  - Rename the SYSENTER stack to CPU_ENTRY_AREA stack as it is used for
    more than sysenter now and keeping the name makes backtraces confusing.

  - Prevent LDT inheritance on exec() by moving it to arch_dup_mmap(),
    which is only invoked on fork().

  - Make vysycall more robust.

  - A few fixes and cleanups of the debug_pagetables code. Check
    PAGE_PRESENT instead of checking the PTE for 0 and a cleanup of the C89
    initialization of the address hint array which already was out of sync
    with the index enums.

  - Move the ESPFIX init to a different place to prepare for PTI.

  - Several code moves with no functional change to make PTI integration
    simpler and header files less convoluted.
    
  - Documentation fixes and clarifications.

Thanks,

	tglx

------------------&gt;
Andy Lutomirski (3):
      x86/vsyscall/64: Explicitly set _PAGE_USER in the pagetable hierarchy
      x86/vsyscall/64: Warn and fail vsyscall emulation in NATIVE mode
      x86/mm/64: Improve the memory map documentation

Dave Hansen (4):
      x86/entry: Rename SYSENTER_stack to CPU_ENTRY_AREA_entry_stack
      x86/mm: Move the CR3 construction functions to tlbflush.h
      x86/mm: Remove hard-coded ASID limit checks
      x86/mm: Put MMU to hardware ASID translation in one place

Peter Zijlstra (8):
      x86/ldt: Rework locking
      x86/doc: Remove obvious weirdnesses from the x86 MM layout documentation
      x86/uv: Use the right TLB-flush API
      x86/microcode: Dont abuse the TLB-flush interface
      x86/mm: Use __flush_tlb_one() for kernel memory
      x86/mm: Remove superfluous barriers
      x86/mm: Add comments to clarify which TLB-flush functions are supposed to flush what
      x86/mm: Create asm/invpcid.h

Thomas Gleixner (9):
      x86/Kconfig: Limit NR_CPUS on 32-bit to a sane amount
      x86/mm/dump_pagetables: Check PAGE_PRESENT for real
      x86/mm/dump_pagetables: Make the address hints correct and readable
      arch, mm: Allow arch_dup_mmap() to fail
      x86/ldt: Prevent LDT inheritance on exec
      x86/cpu_entry_area: Move it to a separate unit
      x86/cpu_entry_area: Move it out of the fixmap
      init: Invoke init_espfix_bsp() from mm_init()
      x86/cpu_entry_area: Prevent wraparound in setup_cpu_entry_area_ptes() on 32bit


 Documentation/x86/x86_64/mm.txt          |  24 +++---
 arch/powerpc/include/asm/mmu_context.h   |   5 +-
 arch/um/include/asm/mmu_context.h        |   3 +-
 arch/unicore32/include/asm/mmu_context.h |   5 +-
 arch/x86/Kconfig                         |   3 +-
 arch/x86/entry/entry_32.S                |  12 +--
 arch/x86/entry/entry_64.S                |   4 +-
 arch/x86/entry/vsyscall/vsyscall_64.c    |  38 ++++++++-
 arch/x86/include/asm/cpu_entry_area.h    |  68 +++++++++++++++
 arch/x86/include/asm/desc.h              |   1 +
 arch/x86/include/asm/espfix.h            |   7 +-
 arch/x86/include/asm/fixmap.h            |  71 +---------------
 arch/x86/include/asm/invpcid.h           |  53 ++++++++++++
 arch/x86/include/asm/mmu.h               |   4 +-
 arch/x86/include/asm/mmu_context.h       |  54 ++++--------
 arch/x86/include/asm/pgtable_32_types.h  |  15 +++-
 arch/x86/include/asm/pgtable_64_types.h  |  47 ++++++-----
 arch/x86/include/asm/processor.h         |   6 +-
 arch/x86/include/asm/stacktrace.h        |   4 +-
 arch/x86/include/asm/tlbflush.h          | 136 ++++++++++++++++--------------
 arch/x86/kernel/asm-offsets.c            |   4 +-
 arch/x86/kernel/asm-offsets_32.c         |   2 +-
 arch/x86/kernel/cpu/common.c             | 100 +---------------------
 arch/x86/kernel/cpu/microcode/intel.c    |  13 ---
 arch/x86/kernel/dumpstack.c              |  11 +--
 arch/x86/kernel/dumpstack_32.c           |   6 +-
 arch/x86/kernel/dumpstack_64.c           |  12 ++-
 arch/x86/kernel/ldt.c                    |  47 ++++++-----
 arch/x86/kernel/smpboot.c                |   6 +-
 arch/x86/kernel/traps.c                  |   6 +-
 arch/x86/mm/Makefile                     |   2 +-
 arch/x86/mm/cpu_entry_area.c             | 139 +++++++++++++++++++++++++++++++
 arch/x86/mm/dump_pagetables.c            |  98 +++++++++++++---------
 arch/x86/mm/init_32.c                    |   6 ++
 arch/x86/mm/kasan_init_64.c              |  29 ++++---
 arch/x86/mm/pgtable_32.c                 |   1 +
 arch/x86/mm/tlb.c                        |  10 +--
 arch/x86/platform/uv/tlb_uv.c            |   2 +-
 arch/x86/xen/mmu_pv.c                    |   2 -
 include/asm-generic/mm_hooks.h           |   5 +-
 include/asm-generic/pgtable.h            |   5 ++
 init/main.c                              |   6 +-
 kernel/fork.c                            |   3 +-
 tools/testing/selftests/x86/ldt_gdt.c    |   9 +-
 44 files changed, 626 insertions(+), 458 deletions(-)
 create mode 100644 arch/x86/include/asm/cpu_entry_area.h
 create mode 100644 arch/x86/include/asm/invpcid.h
 create mode 100644 arch/x86/mm/cpu_entry_area.c
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=77">Linus Torvalds</a> - Dec. 23, 2017, 8:12 p.m.</div>
<pre class="content">
On Sat, Dec 23, 2017 at 11:23 AM, Thomas Gleixner &lt;tglx@linutronix.de&gt; wrote:
<span class="quote">&gt;</span>
<span class="quote">&gt; Todays Advent calendar window contains twentyfour easy to digest</span>
<span class="quote">&gt; patches.</span>

Thanks, this was nice and clear and I saw nothing odd at all.

My only reaction ended up being that I don&#39;t much like how complex the
NR_CPUS config entry has become, and how confusing that is.

For example, we now have

        range 2 64 if SMP &amp;&amp; X86_32 &amp;&amp; X86_BIGSMP

but then we have

        default &quot;8192&quot; if MAXSMP

which seems to make no sense, and unlike some of the other defaults
it&#39;s not clear that those things aren&#39;t compatible. It turns out that
MAXSMP is limited to X86_64, but that&#39;s not at all obvious within that
config entry.

So I think that could be simplified by introducing separate
MAX_CONFIG_CPUS etc entries (that aren&#39;t user choice, but just codify
the limits), so that there would be some more abstraction there.

So the NR_CPUS thing would become something like

    config NR_CPUS
        int &quot;Maximum number of CPUs&quot; if SMP &amp;&amp; !MAXSMP
        range MIN_CONFIG_CPUS MAX_CONFIG_CPUS
        default DEF_CONFIG_CPUS

and then have separate (simpler) config expressions for those
MIN/MAX/DEF values, rather than making it one big complex config
entry.

But that was not new complexity (just added complexity to an already
confusing case), and it&#39;s purely bike-shedding.

So I&#39;ve pulled this stuff, and will push out once it passes my trivial
build test (which I obviously expect it to do, since my final build
test is much more limited than what you guys do).

                   Linus
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/Documentation/x86/x86_64/mm.txt b/Documentation/x86/x86_64/mm.txt</span>
<span class="p_header">index 3448e675b462..51101708a03a 100644</span>
<span class="p_header">--- a/Documentation/x86/x86_64/mm.txt</span>
<span class="p_header">+++ b/Documentation/x86/x86_64/mm.txt</span>
<span class="p_chunk">@@ -1,6 +1,4 @@</span> <span class="p_context"></span>
 
<span class="p_del">-&lt;previous description obsolete, deleted&gt;</span>
<span class="p_del">-</span>
 Virtual memory map with 4 level page tables:
 
 0000000000000000 - 00007fffffffffff (=47 bits) user space, different per mm
<span class="p_chunk">@@ -14,13 +12,15 @@</span> <span class="p_context"> ffffea0000000000 - ffffeaffffffffff (=40 bits) virtual memory map (1TB)</span>
 ... unused hole ...
 ffffec0000000000 - fffffbffffffffff (=44 bits) kasan shadow memory (16TB)
 ... unused hole ...
<span class="p_add">+fffffe8000000000 - fffffeffffffffff (=39 bits) cpu_entry_area mapping</span>
 ffffff0000000000 - ffffff7fffffffff (=39 bits) %esp fixup stacks
 ... unused hole ...
 ffffffef00000000 - fffffffeffffffff (=64 GB) EFI region mapping space
 ... unused hole ...
 ffffffff80000000 - ffffffff9fffffff (=512 MB)  kernel text mapping, from phys 0
<span class="p_del">-ffffffffa0000000 - ffffffffff5fffff (=1526 MB) module mapping space (variable)</span>
<span class="p_del">-ffffffffff600000 - ffffffffffdfffff (=8 MB) vsyscalls</span>
<span class="p_add">+ffffffffa0000000 - [fixmap start]   (~1526 MB) module mapping space (variable)</span>
<span class="p_add">+[fixmap start]   - ffffffffff5fffff kernel-internal fixmap range</span>
<span class="p_add">+ffffffffff600000 - ffffffffff600fff (=4 kB) legacy vsyscall ABI</span>
 ffffffffffe00000 - ffffffffffffffff (=2 MB) unused hole
 
 Virtual memory map with 5 level page tables:
<span class="p_chunk">@@ -36,19 +36,22 @@</span> <span class="p_context"> ffd4000000000000 - ffd5ffffffffffff (=49 bits) virtual memory map (512TB)</span>
 ... unused hole ...
 ffdf000000000000 - fffffc0000000000 (=53 bits) kasan shadow memory (8PB)
 ... unused hole ...
<span class="p_add">+fffffe8000000000 - fffffeffffffffff (=39 bits) cpu_entry_area mapping</span>
 ffffff0000000000 - ffffff7fffffffff (=39 bits) %esp fixup stacks
 ... unused hole ...
 ffffffef00000000 - fffffffeffffffff (=64 GB) EFI region mapping space
 ... unused hole ...
 ffffffff80000000 - ffffffff9fffffff (=512 MB)  kernel text mapping, from phys 0
<span class="p_del">-ffffffffa0000000 - ffffffffff5fffff (=1526 MB) module mapping space</span>
<span class="p_del">-ffffffffff600000 - ffffffffffdfffff (=8 MB) vsyscalls</span>
<span class="p_add">+ffffffffa0000000 - [fixmap start]   (~1526 MB) module mapping space</span>
<span class="p_add">+[fixmap start]   - ffffffffff5fffff kernel-internal fixmap range</span>
<span class="p_add">+ffffffffff600000 - ffffffffff600fff (=4 kB) legacy vsyscall ABI</span>
 ffffffffffe00000 - ffffffffffffffff (=2 MB) unused hole
 
 Architecture defines a 64-bit virtual address. Implementations can support
 less. Currently supported are 48- and 57-bit virtual addresses. Bits 63
<span class="p_del">-through to the most-significant implemented bit are set to either all ones</span>
<span class="p_del">-or all zero. This causes hole between user space and kernel addresses.</span>
<span class="p_add">+through to the most-significant implemented bit are sign extended.</span>
<span class="p_add">+This causes hole between user space and kernel addresses if you interpret them</span>
<span class="p_add">+as unsigned.</span>
 
 The direct mapping covers all memory in the system up to the highest
 memory address (this means in some cases it can also include PCI memory
<span class="p_chunk">@@ -58,9 +61,6 @@</span> <span class="p_context"> vmalloc space is lazily synchronized into the different PML4/PML5 pages of</span>
 the processes using the page fault handler, with init_top_pgt as
 reference.
 
<span class="p_del">-Current X86-64 implementations support up to 46 bits of address space (64 TB),</span>
<span class="p_del">-which is our current limit. This expands into MBZ space in the page tables.</span>
<span class="p_del">-</span>
 We map EFI runtime services in the &#39;efi_pgd&#39; PGD in a 64Gb large virtual
 memory window (this size is arbitrary, it can be raised later if needed).
 The mappings are not part of any other kernel PGD and are only available
<span class="p_chunk">@@ -72,5 +72,3 @@</span> <span class="p_context"> following fixmap section.</span>
 Note that if CONFIG_RANDOMIZE_MEMORY is enabled, the direct mapping of all
 physical memory, vmalloc/ioremap space and virtual memory map are randomized.
 Their order is preserved but their base will be offset early at boot time.
<span class="p_del">-</span>
<span class="p_del">--Andi Kleen, Jul 2004</span>
<span class="p_header">diff --git a/arch/powerpc/include/asm/mmu_context.h b/arch/powerpc/include/asm/mmu_context.h</span>
<span class="p_header">index 492d8140a395..44fdf4786638 100644</span>
<span class="p_header">--- a/arch/powerpc/include/asm/mmu_context.h</span>
<span class="p_header">+++ b/arch/powerpc/include/asm/mmu_context.h</span>
<span class="p_chunk">@@ -114,9 +114,10 @@</span> <span class="p_context"> static inline void enter_lazy_tlb(struct mm_struct *mm,</span>
 #endif
 }
 
<span class="p_del">-static inline void arch_dup_mmap(struct mm_struct *oldmm,</span>
<span class="p_del">-				 struct mm_struct *mm)</span>
<span class="p_add">+static inline int arch_dup_mmap(struct mm_struct *oldmm,</span>
<span class="p_add">+				struct mm_struct *mm)</span>
 {
<span class="p_add">+	return 0;</span>
 }
 
 static inline void arch_exit_mmap(struct mm_struct *mm)
<span class="p_header">diff --git a/arch/um/include/asm/mmu_context.h b/arch/um/include/asm/mmu_context.h</span>
<span class="p_header">index b668e351fd6c..fca34b2177e2 100644</span>
<span class="p_header">--- a/arch/um/include/asm/mmu_context.h</span>
<span class="p_header">+++ b/arch/um/include/asm/mmu_context.h</span>
<span class="p_chunk">@@ -15,9 +15,10 @@</span> <span class="p_context"> extern void uml_setup_stubs(struct mm_struct *mm);</span>
 /*
  * Needed since we do not use the asm-generic/mm_hooks.h:
  */
<span class="p_del">-static inline void arch_dup_mmap(struct mm_struct *oldmm, struct mm_struct *mm)</span>
<span class="p_add">+static inline int arch_dup_mmap(struct mm_struct *oldmm, struct mm_struct *mm)</span>
 {
 	uml_setup_stubs(mm);
<span class="p_add">+	return 0;</span>
 }
 extern void arch_exit_mmap(struct mm_struct *mm);
 static inline void arch_unmap(struct mm_struct *mm,
<span class="p_header">diff --git a/arch/unicore32/include/asm/mmu_context.h b/arch/unicore32/include/asm/mmu_context.h</span>
<span class="p_header">index 59b06b48f27d..5c205a9cb5a6 100644</span>
<span class="p_header">--- a/arch/unicore32/include/asm/mmu_context.h</span>
<span class="p_header">+++ b/arch/unicore32/include/asm/mmu_context.h</span>
<span class="p_chunk">@@ -81,9 +81,10 @@</span> <span class="p_context"> do { \</span>
 	} \
 } while (0)
 
<span class="p_del">-static inline void arch_dup_mmap(struct mm_struct *oldmm,</span>
<span class="p_del">-				 struct mm_struct *mm)</span>
<span class="p_add">+static inline int arch_dup_mmap(struct mm_struct *oldmm,</span>
<span class="p_add">+				struct mm_struct *mm)</span>
 {
<span class="p_add">+	return 0;</span>
 }
 
 static inline void arch_unmap(struct mm_struct *mm,
<span class="p_header">diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig</span>
<span class="p_header">index 665eba1b6103..cd5199de231e 100644</span>
<span class="p_header">--- a/arch/x86/Kconfig</span>
<span class="p_header">+++ b/arch/x86/Kconfig</span>
<span class="p_chunk">@@ -925,7 +925,8 @@</span> <span class="p_context"> config MAXSMP</span>
 config NR_CPUS
 	int &quot;Maximum number of CPUs&quot; if SMP &amp;&amp; !MAXSMP
 	range 2 8 if SMP &amp;&amp; X86_32 &amp;&amp; !X86_BIGSMP
<span class="p_del">-	range 2 512 if SMP &amp;&amp; !MAXSMP &amp;&amp; !CPUMASK_OFFSTACK</span>
<span class="p_add">+	range 2 64 if SMP &amp;&amp; X86_32 &amp;&amp; X86_BIGSMP</span>
<span class="p_add">+	range 2 512 if SMP &amp;&amp; !MAXSMP &amp;&amp; !CPUMASK_OFFSTACK &amp;&amp; X86_64</span>
 	range 2 8192 if SMP &amp;&amp; !MAXSMP &amp;&amp; CPUMASK_OFFSTACK &amp;&amp; X86_64
 	default &quot;1&quot; if !SMP
 	default &quot;8192&quot; if MAXSMP
<span class="p_header">diff --git a/arch/x86/entry/entry_32.S b/arch/x86/entry/entry_32.S</span>
<span class="p_header">index bd8b57a5c874..ace8f321a5a1 100644</span>
<span class="p_header">--- a/arch/x86/entry/entry_32.S</span>
<span class="p_header">+++ b/arch/x86/entry/entry_32.S</span>
<span class="p_chunk">@@ -942,9 +942,9 @@</span> <span class="p_context"> ENTRY(debug)</span>
 
 	/* Are we currently on the SYSENTER stack? */
 	movl	PER_CPU_VAR(cpu_entry_area), %ecx
<span class="p_del">-	addl	$CPU_ENTRY_AREA_SYSENTER_stack + SIZEOF_SYSENTER_stack, %ecx</span>
<span class="p_del">-	subl	%eax, %ecx	/* ecx = (end of SYSENTER_stack) - esp */</span>
<span class="p_del">-	cmpl	$SIZEOF_SYSENTER_stack, %ecx</span>
<span class="p_add">+	addl	$CPU_ENTRY_AREA_entry_stack + SIZEOF_entry_stack, %ecx</span>
<span class="p_add">+	subl	%eax, %ecx	/* ecx = (end of entry_stack) - esp */</span>
<span class="p_add">+	cmpl	$SIZEOF_entry_stack, %ecx</span>
 	jb	.Ldebug_from_sysenter_stack
 
 	TRACE_IRQS_OFF
<span class="p_chunk">@@ -986,9 +986,9 @@</span> <span class="p_context"> ENTRY(nmi)</span>
 
 	/* Are we currently on the SYSENTER stack? */
 	movl	PER_CPU_VAR(cpu_entry_area), %ecx
<span class="p_del">-	addl	$CPU_ENTRY_AREA_SYSENTER_stack + SIZEOF_SYSENTER_stack, %ecx</span>
<span class="p_del">-	subl	%eax, %ecx	/* ecx = (end of SYSENTER_stack) - esp */</span>
<span class="p_del">-	cmpl	$SIZEOF_SYSENTER_stack, %ecx</span>
<span class="p_add">+	addl	$CPU_ENTRY_AREA_entry_stack + SIZEOF_entry_stack, %ecx</span>
<span class="p_add">+	subl	%eax, %ecx	/* ecx = (end of entry_stack) - esp */</span>
<span class="p_add">+	cmpl	$SIZEOF_entry_stack, %ecx</span>
 	jb	.Lnmi_from_sysenter_stack
 
 	/* Not on SYSENTER stack. */
<span class="p_header">diff --git a/arch/x86/entry/entry_64.S b/arch/x86/entry/entry_64.S</span>
<span class="p_header">index 2812ce043a7a..87cebe78bbef 100644</span>
<span class="p_header">--- a/arch/x86/entry/entry_64.S</span>
<span class="p_header">+++ b/arch/x86/entry/entry_64.S</span>
<span class="p_chunk">@@ -154,8 +154,8 @@</span> <span class="p_context"> END(native_usergs_sysret64)</span>
 	_entry_trampoline - CPU_ENTRY_AREA_entry_trampoline(%rip)
 
 /* The top word of the SYSENTER stack is hot and is usable as scratch space. */
<span class="p_del">-#define RSP_SCRATCH	CPU_ENTRY_AREA_SYSENTER_stack + \</span>
<span class="p_del">-			SIZEOF_SYSENTER_stack - 8 + CPU_ENTRY_AREA</span>
<span class="p_add">+#define RSP_SCRATCH	CPU_ENTRY_AREA_entry_stack + \</span>
<span class="p_add">+			SIZEOF_entry_stack - 8 + CPU_ENTRY_AREA</span>
 
 ENTRY(entry_SYSCALL_64_trampoline)
 	UNWIND_HINT_EMPTY
<span class="p_header">diff --git a/arch/x86/entry/vsyscall/vsyscall_64.c b/arch/x86/entry/vsyscall/vsyscall_64.c</span>
<span class="p_header">index f279ba2643dc..1faf40f2dda9 100644</span>
<span class="p_header">--- a/arch/x86/entry/vsyscall/vsyscall_64.c</span>
<span class="p_header">+++ b/arch/x86/entry/vsyscall/vsyscall_64.c</span>
<span class="p_chunk">@@ -37,6 +37,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/unistd.h&gt;
 #include &lt;asm/fixmap.h&gt;
 #include &lt;asm/traps.h&gt;
<span class="p_add">+#include &lt;asm/paravirt.h&gt;</span>
 
 #define CREATE_TRACE_POINTS
 #include &quot;vsyscall_trace.h&quot;
<span class="p_chunk">@@ -138,6 +139,10 @@</span> <span class="p_context"> bool emulate_vsyscall(struct pt_regs *regs, unsigned long address)</span>
 
 	WARN_ON_ONCE(address != regs-&gt;ip);
 
<span class="p_add">+	/* This should be unreachable in NATIVE mode. */</span>
<span class="p_add">+	if (WARN_ON(vsyscall_mode == NATIVE))</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+</span>
 	if (vsyscall_mode == NONE) {
 		warn_bad_vsyscall(KERN_INFO, regs,
 				  &quot;vsyscall attempted with vsyscall=none&quot;);
<span class="p_chunk">@@ -329,16 +334,47 @@</span> <span class="p_context"> int in_gate_area_no_mm(unsigned long addr)</span>
 	return vsyscall_mode != NONE &amp;&amp; (addr &amp; PAGE_MASK) == VSYSCALL_ADDR;
 }
 
<span class="p_add">+/*</span>
<span class="p_add">+ * The VSYSCALL page is the only user-accessible page in the kernel address</span>
<span class="p_add">+ * range.  Normally, the kernel page tables can have _PAGE_USER clear, but</span>
<span class="p_add">+ * the tables covering VSYSCALL_ADDR need _PAGE_USER set if vsyscalls</span>
<span class="p_add">+ * are enabled.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Some day we may create a &quot;minimal&quot; vsyscall mode in which we emulate</span>
<span class="p_add">+ * vsyscalls but leave the page not present.  If so, we skip calling</span>
<span class="p_add">+ * this.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static void __init set_vsyscall_pgtable_user_bits(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pgd_t *pgd;</span>
<span class="p_add">+	p4d_t *p4d;</span>
<span class="p_add">+	pud_t *pud;</span>
<span class="p_add">+	pmd_t *pmd;</span>
<span class="p_add">+</span>
<span class="p_add">+	pgd = pgd_offset_k(VSYSCALL_ADDR);</span>
<span class="p_add">+	set_pgd(pgd, __pgd(pgd_val(*pgd) | _PAGE_USER));</span>
<span class="p_add">+	p4d = p4d_offset(pgd, VSYSCALL_ADDR);</span>
<span class="p_add">+#if CONFIG_PGTABLE_LEVELS &gt;= 5</span>
<span class="p_add">+	p4d-&gt;p4d |= _PAGE_USER;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	pud = pud_offset(p4d, VSYSCALL_ADDR);</span>
<span class="p_add">+	set_pud(pud, __pud(pud_val(*pud) | _PAGE_USER));</span>
<span class="p_add">+	pmd = pmd_offset(pud, VSYSCALL_ADDR);</span>
<span class="p_add">+	set_pmd(pmd, __pmd(pmd_val(*pmd) | _PAGE_USER));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 void __init map_vsyscall(void)
 {
 	extern char __vsyscall_page;
 	unsigned long physaddr_vsyscall = __pa_symbol(&amp;__vsyscall_page);
 
<span class="p_del">-	if (vsyscall_mode != NONE)</span>
<span class="p_add">+	if (vsyscall_mode != NONE) {</span>
 		__set_fixmap(VSYSCALL_PAGE, physaddr_vsyscall,
 			     vsyscall_mode == NATIVE
 			     ? PAGE_KERNEL_VSYSCALL
 			     : PAGE_KERNEL_VVAR);
<span class="p_add">+		set_vsyscall_pgtable_user_bits();</span>
<span class="p_add">+	}</span>
 
 	BUILD_BUG_ON((unsigned long)__fix_to_virt(VSYSCALL_PAGE) !=
 		     (unsigned long)VSYSCALL_ADDR);
<span class="p_header">diff --git a/arch/x86/include/asm/cpu_entry_area.h b/arch/x86/include/asm/cpu_entry_area.h</span>
new file mode 100644
<span class="p_header">index 000000000000..2fbc69a0916e</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/x86/include/asm/cpu_entry_area.h</span>
<span class="p_chunk">@@ -0,0 +1,68 @@</span> <span class="p_context"></span>
<span class="p_add">+// SPDX-License-Identifier: GPL-2.0</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_X86_CPU_ENTRY_AREA_H</span>
<span class="p_add">+#define _ASM_X86_CPU_ENTRY_AREA_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/percpu-defs.h&gt;</span>
<span class="p_add">+#include &lt;asm/processor.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * cpu_entry_area is a percpu region that contains things needed by the CPU</span>
<span class="p_add">+ * and early entry/exit code.  Real types aren&#39;t used for all fields here</span>
<span class="p_add">+ * to avoid circular header dependencies.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Every field is a virtual alias of some other allocated backing store.</span>
<span class="p_add">+ * There is no direct allocation of a struct cpu_entry_area.</span>
<span class="p_add">+ */</span>
<span class="p_add">+struct cpu_entry_area {</span>
<span class="p_add">+	char gdt[PAGE_SIZE];</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The GDT is just below entry_stack and thus serves (on x86_64) as</span>
<span class="p_add">+	 * a a read-only guard page.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	struct entry_stack_page entry_stack_page;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * On x86_64, the TSS is mapped RO.  On x86_32, it&#39;s mapped RW because</span>
<span class="p_add">+	 * we need task switches to work, and task switches write to the TSS.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	struct tss_struct tss;</span>
<span class="p_add">+</span>
<span class="p_add">+	char entry_trampoline[PAGE_SIZE];</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_X86_64</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Exception stacks used for IST entries.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * In the future, this should have a separate slot for each stack</span>
<span class="p_add">+	 * with guard pages between them.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	char exception_stacks[(N_EXCEPTION_STACKS - 1) * EXCEPTION_STKSZ + DEBUG_STKSZ];</span>
<span class="p_add">+#endif</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+#define CPU_ENTRY_AREA_SIZE	(sizeof(struct cpu_entry_area))</span>
<span class="p_add">+#define CPU_ENTRY_AREA_TOT_SIZE	(CPU_ENTRY_AREA_SIZE * NR_CPUS)</span>
<span class="p_add">+</span>
<span class="p_add">+DECLARE_PER_CPU(struct cpu_entry_area *, cpu_entry_area);</span>
<span class="p_add">+</span>
<span class="p_add">+extern void setup_cpu_entry_areas(void);</span>
<span class="p_add">+extern void cea_set_pte(void *cea_vaddr, phys_addr_t pa, pgprot_t flags);</span>
<span class="p_add">+</span>
<span class="p_add">+#define	CPU_ENTRY_AREA_RO_IDT		CPU_ENTRY_AREA_BASE</span>
<span class="p_add">+#define CPU_ENTRY_AREA_PER_CPU		(CPU_ENTRY_AREA_RO_IDT + PAGE_SIZE)</span>
<span class="p_add">+</span>
<span class="p_add">+#define CPU_ENTRY_AREA_RO_IDT_VADDR	((void *)CPU_ENTRY_AREA_RO_IDT)</span>
<span class="p_add">+</span>
<span class="p_add">+#define CPU_ENTRY_AREA_MAP_SIZE			\</span>
<span class="p_add">+	(CPU_ENTRY_AREA_PER_CPU + CPU_ENTRY_AREA_TOT_SIZE - CPU_ENTRY_AREA_BASE)</span>
<span class="p_add">+</span>
<span class="p_add">+extern struct cpu_entry_area *get_cpu_entry_area(int cpu);</span>
<span class="p_add">+</span>
<span class="p_add">+static inline struct entry_stack *cpu_entry_stack(int cpu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return &amp;get_cpu_entry_area(cpu)-&gt;entry_stack_page.stack;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#endif</span>
<span class="p_header">diff --git a/arch/x86/include/asm/desc.h b/arch/x86/include/asm/desc.h</span>
<span class="p_header">index 2ace1f90d138..bc359dd2f7f6 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/desc.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/desc.h</span>
<span class="p_chunk">@@ -7,6 +7,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/mmu.h&gt;
 #include &lt;asm/fixmap.h&gt;
 #include &lt;asm/irq_vectors.h&gt;
<span class="p_add">+#include &lt;asm/cpu_entry_area.h&gt;</span>
 
 #include &lt;linux/smp.h&gt;
 #include &lt;linux/percpu.h&gt;
<span class="p_header">diff --git a/arch/x86/include/asm/espfix.h b/arch/x86/include/asm/espfix.h</span>
<span class="p_header">index 0211029076ea..6777480d8a42 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/espfix.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/espfix.h</span>
<span class="p_chunk">@@ -2,7 +2,7 @@</span> <span class="p_context"></span>
 #ifndef _ASM_X86_ESPFIX_H
 #define _ASM_X86_ESPFIX_H
 
<span class="p_del">-#ifdef CONFIG_X86_64</span>
<span class="p_add">+#ifdef CONFIG_X86_ESPFIX64</span>
 
 #include &lt;asm/percpu.h&gt;
 
<span class="p_chunk">@@ -11,7 +11,8 @@</span> <span class="p_context"> DECLARE_PER_CPU_READ_MOSTLY(unsigned long, espfix_waddr);</span>
 
 extern void init_espfix_bsp(void);
 extern void init_espfix_ap(int cpu);
<span class="p_del">-</span>
<span class="p_del">-#endif /* CONFIG_X86_64 */</span>
<span class="p_add">+#else</span>
<span class="p_add">+static inline void init_espfix_ap(int cpu) { }</span>
<span class="p_add">+#endif</span>
 
 #endif /* _ASM_X86_ESPFIX_H */
<span class="p_header">diff --git a/arch/x86/include/asm/fixmap.h b/arch/x86/include/asm/fixmap.h</span>
<span class="p_header">index 94fc4fa14127..64c4a30e0d39 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/fixmap.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/fixmap.h</span>
<span class="p_chunk">@@ -45,46 +45,6 @@</span> <span class="p_context"> extern unsigned long __FIXADDR_TOP;</span>
 #endif
 
 /*
<span class="p_del">- * cpu_entry_area is a percpu region in the fixmap that contains things</span>
<span class="p_del">- * needed by the CPU and early entry/exit code.  Real types aren&#39;t used</span>
<span class="p_del">- * for all fields here to avoid circular header dependencies.</span>
<span class="p_del">- *</span>
<span class="p_del">- * Every field is a virtual alias of some other allocated backing store.</span>
<span class="p_del">- * There is no direct allocation of a struct cpu_entry_area.</span>
<span class="p_del">- */</span>
<span class="p_del">-struct cpu_entry_area {</span>
<span class="p_del">-	char gdt[PAGE_SIZE];</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * The GDT is just below SYSENTER_stack and thus serves (on x86_64) as</span>
<span class="p_del">-	 * a a read-only guard page.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	struct SYSENTER_stack_page SYSENTER_stack_page;</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * On x86_64, the TSS is mapped RO.  On x86_32, it&#39;s mapped RW because</span>
<span class="p_del">-	 * we need task switches to work, and task switches write to the TSS.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	struct tss_struct tss;</span>
<span class="p_del">-</span>
<span class="p_del">-	char entry_trampoline[PAGE_SIZE];</span>
<span class="p_del">-</span>
<span class="p_del">-#ifdef CONFIG_X86_64</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Exception stacks used for IST entries.</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 * In the future, this should have a separate slot for each stack</span>
<span class="p_del">-	 * with guard pages between them.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	char exception_stacks[(N_EXCEPTION_STACKS - 1) * EXCEPTION_STKSZ + DEBUG_STKSZ];</span>
<span class="p_del">-#endif</span>
<span class="p_del">-};</span>
<span class="p_del">-</span>
<span class="p_del">-#define CPU_ENTRY_AREA_PAGES (sizeof(struct cpu_entry_area) / PAGE_SIZE)</span>
<span class="p_del">-</span>
<span class="p_del">-extern void setup_cpu_entry_areas(void);</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
  * Here we define all the compile-time &#39;special&#39; virtual
  * addresses. The point is to have a constant address at
  * compile time, but to set the physical address only
<span class="p_chunk">@@ -123,7 +83,6 @@</span> <span class="p_context"> enum fixed_addresses {</span>
 	FIX_IO_APIC_BASE_0,
 	FIX_IO_APIC_BASE_END = FIX_IO_APIC_BASE_0 + MAX_IO_APICS - 1,
 #endif
<span class="p_del">-	FIX_RO_IDT,	/* Virtual mapping for read-only IDT */</span>
 #ifdef CONFIG_X86_32
 	FIX_KMAP_BEGIN,	/* reserved pte&#39;s for temporary kernel mappings */
 	FIX_KMAP_END = FIX_KMAP_BEGIN+(KM_TYPE_NR*NR_CPUS)-1,
<span class="p_chunk">@@ -139,9 +98,6 @@</span> <span class="p_context"> enum fixed_addresses {</span>
 #ifdef	CONFIG_X86_INTEL_MID
 	FIX_LNW_VRTC,
 #endif
<span class="p_del">-	/* Fixmap entries to remap the GDTs, one per processor. */</span>
<span class="p_del">-	FIX_CPU_ENTRY_AREA_TOP,</span>
<span class="p_del">-	FIX_CPU_ENTRY_AREA_BOTTOM = FIX_CPU_ENTRY_AREA_TOP + (CPU_ENTRY_AREA_PAGES * NR_CPUS) - 1,</span>
 
 #ifdef CONFIG_ACPI_APEI_GHES
 	/* Used for GHES mapping from assorted contexts */
<span class="p_chunk">@@ -182,7 +138,7 @@</span> <span class="p_context"> enum fixed_addresses {</span>
 extern void reserve_top_address(unsigned long reserve);
 
 #define FIXADDR_SIZE	(__end_of_permanent_fixed_addresses &lt;&lt; PAGE_SHIFT)
<span class="p_del">-#define FIXADDR_START		(FIXADDR_TOP - FIXADDR_SIZE)</span>
<span class="p_add">+#define FIXADDR_START	(FIXADDR_TOP - FIXADDR_SIZE)</span>
 
 extern int fixmaps_set;
 
<span class="p_chunk">@@ -230,30 +186,5 @@</span> <span class="p_context"> void __init *early_memremap_decrypted_wp(resource_size_t phys_addr,</span>
 void __early_set_fixmap(enum fixed_addresses idx,
 			phys_addr_t phys, pgprot_t flags);
 
<span class="p_del">-static inline unsigned int __get_cpu_entry_area_page_index(int cpu, int page)</span>
<span class="p_del">-{</span>
<span class="p_del">-	BUILD_BUG_ON(sizeof(struct cpu_entry_area) % PAGE_SIZE != 0);</span>
<span class="p_del">-</span>
<span class="p_del">-	return FIX_CPU_ENTRY_AREA_BOTTOM - cpu*CPU_ENTRY_AREA_PAGES - page;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-#define __get_cpu_entry_area_offset_index(cpu, offset) ({		\</span>
<span class="p_del">-	BUILD_BUG_ON(offset % PAGE_SIZE != 0);				\</span>
<span class="p_del">-	__get_cpu_entry_area_page_index(cpu, offset / PAGE_SIZE);	\</span>
<span class="p_del">-	})</span>
<span class="p_del">-</span>
<span class="p_del">-#define get_cpu_entry_area_index(cpu, field)				\</span>
<span class="p_del">-	__get_cpu_entry_area_offset_index((cpu), offsetof(struct cpu_entry_area, field))</span>
<span class="p_del">-</span>
<span class="p_del">-static inline struct cpu_entry_area *get_cpu_entry_area(int cpu)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return (struct cpu_entry_area *)__fix_to_virt(__get_cpu_entry_area_page_index(cpu, 0));</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline struct SYSENTER_stack *cpu_SYSENTER_stack(int cpu)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return &amp;get_cpu_entry_area(cpu)-&gt;SYSENTER_stack_page.stack;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 #endif /* !__ASSEMBLY__ */
 #endif /* _ASM_X86_FIXMAP_H */
<span class="p_header">diff --git a/arch/x86/include/asm/invpcid.h b/arch/x86/include/asm/invpcid.h</span>
new file mode 100644
<span class="p_header">index 000000000000..989cfa86de85</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/x86/include/asm/invpcid.h</span>
<span class="p_chunk">@@ -0,0 +1,53 @@</span> <span class="p_context"></span>
<span class="p_add">+/* SPDX-License-Identifier: GPL-2.0 */</span>
<span class="p_add">+#ifndef _ASM_X86_INVPCID</span>
<span class="p_add">+#define _ASM_X86_INVPCID</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void __invpcid(unsigned long pcid, unsigned long addr,</span>
<span class="p_add">+			     unsigned long type)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct { u64 d[2]; } desc = { { pcid, addr } };</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The memory clobber is because the whole point is to invalidate</span>
<span class="p_add">+	 * stale TLB entries and, especially if we&#39;re flushing global</span>
<span class="p_add">+	 * mappings, we don&#39;t want the compiler to reorder any subsequent</span>
<span class="p_add">+	 * memory accesses before the TLB flush.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * The hex opcode is invpcid (%ecx), %eax in 32-bit mode and</span>
<span class="p_add">+	 * invpcid (%rcx), %rax in long mode.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	asm volatile (&quot;.byte 0x66, 0x0f, 0x38, 0x82, 0x01&quot;</span>
<span class="p_add">+		      : : &quot;m&quot; (desc), &quot;a&quot; (type), &quot;c&quot; (&amp;desc) : &quot;memory&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define INVPCID_TYPE_INDIV_ADDR		0</span>
<span class="p_add">+#define INVPCID_TYPE_SINGLE_CTXT	1</span>
<span class="p_add">+#define INVPCID_TYPE_ALL_INCL_GLOBAL	2</span>
<span class="p_add">+#define INVPCID_TYPE_ALL_NON_GLOBAL	3</span>
<span class="p_add">+</span>
<span class="p_add">+/* Flush all mappings for a given pcid and addr, not including globals. */</span>
<span class="p_add">+static inline void invpcid_flush_one(unsigned long pcid,</span>
<span class="p_add">+				     unsigned long addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__invpcid(pcid, addr, INVPCID_TYPE_INDIV_ADDR);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Flush all mappings for a given PCID, not including globals. */</span>
<span class="p_add">+static inline void invpcid_flush_single_context(unsigned long pcid)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__invpcid(pcid, 0, INVPCID_TYPE_SINGLE_CTXT);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Flush all mappings, including globals, for all PCIDs. */</span>
<span class="p_add">+static inline void invpcid_flush_all(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__invpcid(0, 0, INVPCID_TYPE_ALL_INCL_GLOBAL);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Flush all mappings for all PCIDs except globals. */</span>
<span class="p_add">+static inline void invpcid_flush_all_nonglobals(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__invpcid(0, 0, INVPCID_TYPE_ALL_NON_GLOBAL);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_X86_INVPCID */</span>
<span class="p_header">diff --git a/arch/x86/include/asm/mmu.h b/arch/x86/include/asm/mmu.h</span>
<span class="p_header">index 9ea26f167497..5ff3e8af2c20 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/mmu.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/mmu.h</span>
<span class="p_chunk">@@ -3,6 +3,7 @@</span> <span class="p_context"></span>
 #define _ASM_X86_MMU_H
 
 #include &lt;linux/spinlock.h&gt;
<span class="p_add">+#include &lt;linux/rwsem.h&gt;</span>
 #include &lt;linux/mutex.h&gt;
 #include &lt;linux/atomic.h&gt;
 
<span class="p_chunk">@@ -27,7 +28,8 @@</span> <span class="p_context"> typedef struct {</span>
 	atomic64_t tlb_gen;
 
 #ifdef CONFIG_MODIFY_LDT_SYSCALL
<span class="p_del">-	struct ldt_struct *ldt;</span>
<span class="p_add">+	struct rw_semaphore	ldt_usr_sem;</span>
<span class="p_add">+	struct ldt_struct	*ldt;</span>
 #endif
 
 #ifdef CONFIG_X86_64
<span class="p_header">diff --git a/arch/x86/include/asm/mmu_context.h b/arch/x86/include/asm/mmu_context.h</span>
<span class="p_header">index 6d16d15d09a0..5ede7cae1d67 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/mmu_context.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/mmu_context.h</span>
<span class="p_chunk">@@ -57,11 +57,17 @@</span> <span class="p_context"> struct ldt_struct {</span>
 /*
  * Used for LDT copy/destruction.
  */
<span class="p_del">-int init_new_context_ldt(struct task_struct *tsk, struct mm_struct *mm);</span>
<span class="p_add">+static inline void init_new_context_ldt(struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+	mm-&gt;context.ldt = NULL;</span>
<span class="p_add">+	init_rwsem(&amp;mm-&gt;context.ldt_usr_sem);</span>
<span class="p_add">+}</span>
<span class="p_add">+int ldt_dup_context(struct mm_struct *oldmm, struct mm_struct *mm);</span>
 void destroy_context_ldt(struct mm_struct *mm);
 #else	/* CONFIG_MODIFY_LDT_SYSCALL */
<span class="p_del">-static inline int init_new_context_ldt(struct task_struct *tsk,</span>
<span class="p_del">-				       struct mm_struct *mm)</span>
<span class="p_add">+static inline void init_new_context_ldt(struct mm_struct *mm) { }</span>
<span class="p_add">+static inline int ldt_dup_context(struct mm_struct *oldmm,</span>
<span class="p_add">+				  struct mm_struct *mm)</span>
 {
 	return 0;
 }
<span class="p_chunk">@@ -132,18 +138,21 @@</span> <span class="p_context"> void enter_lazy_tlb(struct mm_struct *mm, struct task_struct *tsk);</span>
 static inline int init_new_context(struct task_struct *tsk,
 				   struct mm_struct *mm)
 {
<span class="p_add">+	mutex_init(&amp;mm-&gt;context.lock);</span>
<span class="p_add">+</span>
 	mm-&gt;context.ctx_id = atomic64_inc_return(&amp;last_mm_ctx_id);
 	atomic64_set(&amp;mm-&gt;context.tlb_gen, 0);
 
<span class="p_del">-	#ifdef CONFIG_X86_INTEL_MEMORY_PROTECTION_KEYS</span>
<span class="p_add">+#ifdef CONFIG_X86_INTEL_MEMORY_PROTECTION_KEYS</span>
 	if (cpu_feature_enabled(X86_FEATURE_OSPKE)) {
 		/* pkey 0 is the default and always allocated */
 		mm-&gt;context.pkey_allocation_map = 0x1;
 		/* -1 means unallocated or invalid */
 		mm-&gt;context.execute_only_pkey = -1;
 	}
<span class="p_del">-	#endif</span>
<span class="p_del">-	return init_new_context_ldt(tsk, mm);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	init_new_context_ldt(mm);</span>
<span class="p_add">+	return 0;</span>
 }
 static inline void destroy_context(struct mm_struct *mm)
 {
<span class="p_chunk">@@ -176,10 +185,10 @@</span> <span class="p_context"> do {						\</span>
 } while (0)
 #endif
 
<span class="p_del">-static inline void arch_dup_mmap(struct mm_struct *oldmm,</span>
<span class="p_del">-				 struct mm_struct *mm)</span>
<span class="p_add">+static inline int arch_dup_mmap(struct mm_struct *oldmm, struct mm_struct *mm)</span>
 {
 	paravirt_arch_dup_mmap(oldmm, mm);
<span class="p_add">+	return ldt_dup_context(oldmm, mm);</span>
 }
 
 static inline void arch_exit_mmap(struct mm_struct *mm)
<span class="p_chunk">@@ -282,33 +291,6 @@</span> <span class="p_context"> static inline bool arch_vma_access_permitted(struct vm_area_struct *vma,</span>
 }
 
 /*
<span class="p_del">- * If PCID is on, ASID-aware code paths put the ASID+1 into the PCID</span>
<span class="p_del">- * bits.  This serves two purposes.  It prevents a nasty situation in</span>
<span class="p_del">- * which PCID-unaware code saves CR3, loads some other value (with PCID</span>
<span class="p_del">- * == 0), and then restores CR3, thus corrupting the TLB for ASID 0 if</span>
<span class="p_del">- * the saved ASID was nonzero.  It also means that any bugs involving</span>
<span class="p_del">- * loading a PCID-enabled CR3 with CR4.PCIDE off will trigger</span>
<span class="p_del">- * deterministically.</span>
<span class="p_del">- */</span>
<span class="p_del">-</span>
<span class="p_del">-static inline unsigned long build_cr3(struct mm_struct *mm, u16 asid)</span>
<span class="p_del">-{</span>
<span class="p_del">-	if (static_cpu_has(X86_FEATURE_PCID)) {</span>
<span class="p_del">-		VM_WARN_ON_ONCE(asid &gt; 4094);</span>
<span class="p_del">-		return __sme_pa(mm-&gt;pgd) | (asid + 1);</span>
<span class="p_del">-	} else {</span>
<span class="p_del">-		VM_WARN_ON_ONCE(asid != 0);</span>
<span class="p_del">-		return __sme_pa(mm-&gt;pgd);</span>
<span class="p_del">-	}</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline unsigned long build_cr3_noflush(struct mm_struct *mm, u16 asid)</span>
<span class="p_del">-{</span>
<span class="p_del">-	VM_WARN_ON_ONCE(asid &gt; 4094);</span>
<span class="p_del">-	return __sme_pa(mm-&gt;pgd) | (asid + 1) | CR3_NOFLUSH;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
  * This can be used from process context to figure out what the value of
  * CR3 is without needing to do a (slow) __read_cr3().
  *
<span class="p_chunk">@@ -317,7 +299,7 @@</span> <span class="p_context"> static inline unsigned long build_cr3_noflush(struct mm_struct *mm, u16 asid)</span>
  */
 static inline unsigned long __get_current_cr3_fast(void)
 {
<span class="p_del">-	unsigned long cr3 = build_cr3(this_cpu_read(cpu_tlbstate.loaded_mm),</span>
<span class="p_add">+	unsigned long cr3 = build_cr3(this_cpu_read(cpu_tlbstate.loaded_mm)-&gt;pgd,</span>
 		this_cpu_read(cpu_tlbstate.loaded_mm_asid));
 
 	/* For now, be very restrictive about when this can be called. */
<span class="p_header">diff --git a/arch/x86/include/asm/pgtable_32_types.h b/arch/x86/include/asm/pgtable_32_types.h</span>
<span class="p_header">index f2ca9b28fd68..ce245b0cdfca 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/pgtable_32_types.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/pgtable_32_types.h</span>
<span class="p_chunk">@@ -38,13 +38,22 @@</span> <span class="p_context"> extern bool __vmalloc_start_set; /* set once high_memory is set */</span>
 #define LAST_PKMAP 1024
 #endif
 
<span class="p_del">-#define PKMAP_BASE ((FIXADDR_START - PAGE_SIZE * (LAST_PKMAP + 1))	\</span>
<span class="p_del">-		    &amp; PMD_MASK)</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Define this here and validate with BUILD_BUG_ON() in pgtable_32.c</span>
<span class="p_add">+ * to avoid include recursion hell</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define CPU_ENTRY_AREA_PAGES	(NR_CPUS * 40)</span>
<span class="p_add">+</span>
<span class="p_add">+#define CPU_ENTRY_AREA_BASE				\</span>
<span class="p_add">+	((FIXADDR_START - PAGE_SIZE * (CPU_ENTRY_AREA_PAGES + 1)) &amp; PMD_MASK)</span>
<span class="p_add">+</span>
<span class="p_add">+#define PKMAP_BASE		\</span>
<span class="p_add">+	((CPU_ENTRY_AREA_BASE - PAGE_SIZE) &amp; PMD_MASK)</span>
 
 #ifdef CONFIG_HIGHMEM
 # define VMALLOC_END	(PKMAP_BASE - 2 * PAGE_SIZE)
 #else
<span class="p_del">-# define VMALLOC_END	(FIXADDR_START - 2 * PAGE_SIZE)</span>
<span class="p_add">+# define VMALLOC_END	(CPU_ENTRY_AREA_BASE - 2 * PAGE_SIZE)</span>
 #endif
 
 #define MODULES_VADDR	VMALLOC_START
<span class="p_header">diff --git a/arch/x86/include/asm/pgtable_64_types.h b/arch/x86/include/asm/pgtable_64_types.h</span>
<span class="p_header">index 6d5f45dcd4a1..3d27831bc58d 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/pgtable_64_types.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/pgtable_64_types.h</span>
<span class="p_chunk">@@ -76,32 +76,41 @@</span> <span class="p_context"> typedef struct { pteval_t pte; } pte_t;</span>
 #define PGDIR_MASK	(~(PGDIR_SIZE - 1))
 
 /* See Documentation/x86/x86_64/mm.txt for a description of the memory map. */
<span class="p_del">-#define MAXMEM		_AC(__AC(1, UL) &lt;&lt; MAX_PHYSMEM_BITS, UL)</span>
<span class="p_add">+#define MAXMEM			_AC(__AC(1, UL) &lt;&lt; MAX_PHYSMEM_BITS, UL)</span>
<span class="p_add">+</span>
 #ifdef CONFIG_X86_5LEVEL
<span class="p_del">-#define VMALLOC_SIZE_TB _AC(16384, UL)</span>
<span class="p_del">-#define __VMALLOC_BASE	_AC(0xff92000000000000, UL)</span>
<span class="p_del">-#define __VMEMMAP_BASE	_AC(0xffd4000000000000, UL)</span>
<span class="p_add">+# define VMALLOC_SIZE_TB	_AC(16384, UL)</span>
<span class="p_add">+# define __VMALLOC_BASE		_AC(0xff92000000000000, UL)</span>
<span class="p_add">+# define __VMEMMAP_BASE		_AC(0xffd4000000000000, UL)</span>
 #else
<span class="p_del">-#define VMALLOC_SIZE_TB	_AC(32, UL)</span>
<span class="p_del">-#define __VMALLOC_BASE	_AC(0xffffc90000000000, UL)</span>
<span class="p_del">-#define __VMEMMAP_BASE	_AC(0xffffea0000000000, UL)</span>
<span class="p_add">+# define VMALLOC_SIZE_TB	_AC(32, UL)</span>
<span class="p_add">+# define __VMALLOC_BASE		_AC(0xffffc90000000000, UL)</span>
<span class="p_add">+# define __VMEMMAP_BASE		_AC(0xffffea0000000000, UL)</span>
 #endif
<span class="p_add">+</span>
 #ifdef CONFIG_RANDOMIZE_MEMORY
<span class="p_del">-#define VMALLOC_START	vmalloc_base</span>
<span class="p_del">-#define VMEMMAP_START	vmemmap_base</span>
<span class="p_add">+# define VMALLOC_START		vmalloc_base</span>
<span class="p_add">+# define VMEMMAP_START		vmemmap_base</span>
 #else
<span class="p_del">-#define VMALLOC_START	__VMALLOC_BASE</span>
<span class="p_del">-#define VMEMMAP_START	__VMEMMAP_BASE</span>
<span class="p_add">+# define VMALLOC_START		__VMALLOC_BASE</span>
<span class="p_add">+# define VMEMMAP_START		__VMEMMAP_BASE</span>
 #endif /* CONFIG_RANDOMIZE_MEMORY */
<span class="p_del">-#define VMALLOC_END	(VMALLOC_START + _AC((VMALLOC_SIZE_TB &lt;&lt; 40) - 1, UL))</span>
<span class="p_del">-#define MODULES_VADDR    (__START_KERNEL_map + KERNEL_IMAGE_SIZE)</span>
<span class="p_add">+</span>
<span class="p_add">+#define VMALLOC_END		(VMALLOC_START + _AC((VMALLOC_SIZE_TB &lt;&lt; 40) - 1, UL))</span>
<span class="p_add">+</span>
<span class="p_add">+#define MODULES_VADDR		(__START_KERNEL_map + KERNEL_IMAGE_SIZE)</span>
 /* The module sections ends with the start of the fixmap */
<span class="p_del">-#define MODULES_END   __fix_to_virt(__end_of_fixed_addresses + 1)</span>
<span class="p_del">-#define MODULES_LEN   (MODULES_END - MODULES_VADDR)</span>
<span class="p_del">-#define ESPFIX_PGD_ENTRY _AC(-2, UL)</span>
<span class="p_del">-#define ESPFIX_BASE_ADDR (ESPFIX_PGD_ENTRY &lt;&lt; P4D_SHIFT)</span>
<span class="p_del">-#define EFI_VA_START	 ( -4 * (_AC(1, UL) &lt;&lt; 30))</span>
<span class="p_del">-#define EFI_VA_END	 (-68 * (_AC(1, UL) &lt;&lt; 30))</span>
<span class="p_add">+#define MODULES_END		__fix_to_virt(__end_of_fixed_addresses + 1)</span>
<span class="p_add">+#define MODULES_LEN		(MODULES_END - MODULES_VADDR)</span>
<span class="p_add">+</span>
<span class="p_add">+#define ESPFIX_PGD_ENTRY	_AC(-2, UL)</span>
<span class="p_add">+#define ESPFIX_BASE_ADDR	(ESPFIX_PGD_ENTRY &lt;&lt; P4D_SHIFT)</span>
<span class="p_add">+</span>
<span class="p_add">+#define CPU_ENTRY_AREA_PGD	_AC(-3, UL)</span>
<span class="p_add">+#define CPU_ENTRY_AREA_BASE	(CPU_ENTRY_AREA_PGD &lt;&lt; P4D_SHIFT)</span>
<span class="p_add">+</span>
<span class="p_add">+#define EFI_VA_START		( -4 * (_AC(1, UL) &lt;&lt; 30))</span>
<span class="p_add">+#define EFI_VA_END		(-68 * (_AC(1, UL) &lt;&lt; 30))</span>
 
 #define EARLY_DYNAMIC_PAGE_TABLES	64
 
<span class="p_header">diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h</span>
<span class="p_header">index da943411d3d8..9e482d8b0b97 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/processor.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/processor.h</span>
<span class="p_chunk">@@ -336,12 +336,12 @@</span> <span class="p_context"> struct x86_hw_tss {</span>
 #define IO_BITMAP_OFFSET		(offsetof(struct tss_struct, io_bitmap) - offsetof(struct tss_struct, x86_tss))
 #define INVALID_IO_BITMAP_OFFSET	0x8000
 
<span class="p_del">-struct SYSENTER_stack {</span>
<span class="p_add">+struct entry_stack {</span>
 	unsigned long		words[64];
 };
 
<span class="p_del">-struct SYSENTER_stack_page {</span>
<span class="p_del">-	struct SYSENTER_stack stack;</span>
<span class="p_add">+struct entry_stack_page {</span>
<span class="p_add">+	struct entry_stack stack;</span>
 } __aligned(PAGE_SIZE);
 
 struct tss_struct {
<span class="p_header">diff --git a/arch/x86/include/asm/stacktrace.h b/arch/x86/include/asm/stacktrace.h</span>
<span class="p_header">index f8062bfd43a0..f73706878772 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/stacktrace.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/stacktrace.h</span>
<span class="p_chunk">@@ -16,7 +16,7 @@</span> <span class="p_context"> enum stack_type {</span>
 	STACK_TYPE_TASK,
 	STACK_TYPE_IRQ,
 	STACK_TYPE_SOFTIRQ,
<span class="p_del">-	STACK_TYPE_SYSENTER,</span>
<span class="p_add">+	STACK_TYPE_ENTRY,</span>
 	STACK_TYPE_EXCEPTION,
 	STACK_TYPE_EXCEPTION_LAST = STACK_TYPE_EXCEPTION + N_EXCEPTION_STACKS-1,
 };
<span class="p_chunk">@@ -29,7 +29,7 @@</span> <span class="p_context"> struct stack_info {</span>
 bool in_task_stack(unsigned long *stack, struct task_struct *task,
 		   struct stack_info *info);
 
<span class="p_del">-bool in_sysenter_stack(unsigned long *stack, struct stack_info *info);</span>
<span class="p_add">+bool in_entry_stack(unsigned long *stack, struct stack_info *info);</span>
 
 int get_stack_info(unsigned long *stack, struct task_struct *task,
 		   struct stack_info *info, unsigned long *visit_mask);
<span class="p_header">diff --git a/arch/x86/include/asm/tlbflush.h b/arch/x86/include/asm/tlbflush.h</span>
<span class="p_header">index 509046cfa5ce..171b429f43a2 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/tlbflush.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/tlbflush.h</span>
<span class="p_chunk">@@ -9,70 +9,66 @@</span> <span class="p_context"></span>
 #include &lt;asm/cpufeature.h&gt;
 #include &lt;asm/special_insns.h&gt;
 #include &lt;asm/smp.h&gt;
<span class="p_add">+#include &lt;asm/invpcid.h&gt;</span>
 
<span class="p_del">-static inline void __invpcid(unsigned long pcid, unsigned long addr,</span>
<span class="p_del">-			     unsigned long type)</span>
<span class="p_add">+static inline u64 inc_mm_tlb_gen(struct mm_struct *mm)</span>
 {
<span class="p_del">-	struct { u64 d[2]; } desc = { { pcid, addr } };</span>
<span class="p_del">-</span>
 	/*
<span class="p_del">-	 * The memory clobber is because the whole point is to invalidate</span>
<span class="p_del">-	 * stale TLB entries and, especially if we&#39;re flushing global</span>
<span class="p_del">-	 * mappings, we don&#39;t want the compiler to reorder any subsequent</span>
<span class="p_del">-	 * memory accesses before the TLB flush.</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 * The hex opcode is invpcid (%ecx), %eax in 32-bit mode and</span>
<span class="p_del">-	 * invpcid (%rcx), %rax in long mode.</span>
<span class="p_add">+	 * Bump the generation count.  This also serves as a full barrier</span>
<span class="p_add">+	 * that synchronizes with switch_mm(): callers are required to order</span>
<span class="p_add">+	 * their read of mm_cpumask after their writes to the paging</span>
<span class="p_add">+	 * structures.</span>
 	 */
<span class="p_del">-	asm volatile (&quot;.byte 0x66, 0x0f, 0x38, 0x82, 0x01&quot;</span>
<span class="p_del">-		      : : &quot;m&quot; (desc), &quot;a&quot; (type), &quot;c&quot; (&amp;desc) : &quot;memory&quot;);</span>
<span class="p_add">+	return atomic64_inc_return(&amp;mm-&gt;context.tlb_gen);</span>
 }
 
<span class="p_del">-#define INVPCID_TYPE_INDIV_ADDR		0</span>
<span class="p_del">-#define INVPCID_TYPE_SINGLE_CTXT	1</span>
<span class="p_del">-#define INVPCID_TYPE_ALL_INCL_GLOBAL	2</span>
<span class="p_del">-#define INVPCID_TYPE_ALL_NON_GLOBAL	3</span>
<span class="p_del">-</span>
<span class="p_del">-/* Flush all mappings for a given pcid and addr, not including globals. */</span>
<span class="p_del">-static inline void invpcid_flush_one(unsigned long pcid,</span>
<span class="p_del">-				     unsigned long addr)</span>
<span class="p_del">-{</span>
<span class="p_del">-	__invpcid(pcid, addr, INVPCID_TYPE_INDIV_ADDR);</span>
<span class="p_del">-}</span>
<span class="p_add">+/* There are 12 bits of space for ASIDS in CR3 */</span>
<span class="p_add">+#define CR3_HW_ASID_BITS		12</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * When enabled, PAGE_TABLE_ISOLATION consumes a single bit for</span>
<span class="p_add">+ * user/kernel switches</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define PTI_CONSUMED_ASID_BITS		0</span>
 
<span class="p_del">-/* Flush all mappings for a given PCID, not including globals. */</span>
<span class="p_del">-static inline void invpcid_flush_single_context(unsigned long pcid)</span>
<span class="p_del">-{</span>
<span class="p_del">-	__invpcid(pcid, 0, INVPCID_TYPE_SINGLE_CTXT);</span>
<span class="p_del">-}</span>
<span class="p_add">+#define CR3_AVAIL_ASID_BITS (CR3_HW_ASID_BITS - PTI_CONSUMED_ASID_BITS)</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * ASIDs are zero-based: 0-&gt;MAX_AVAIL_ASID are valid.  -1 below to account</span>
<span class="p_add">+ * for them being zero-based.  Another -1 is because ASID 0 is reserved for</span>
<span class="p_add">+ * use by non-PCID-aware users.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define MAX_ASID_AVAILABLE ((1 &lt;&lt; CR3_AVAIL_ASID_BITS) - 2)</span>
 
<span class="p_del">-/* Flush all mappings, including globals, for all PCIDs. */</span>
<span class="p_del">-static inline void invpcid_flush_all(void)</span>
<span class="p_add">+static inline u16 kern_pcid(u16 asid)</span>
 {
<span class="p_del">-	__invpcid(0, 0, INVPCID_TYPE_ALL_INCL_GLOBAL);</span>
<span class="p_add">+	VM_WARN_ON_ONCE(asid &gt; MAX_ASID_AVAILABLE);</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If PCID is on, ASID-aware code paths put the ASID+1 into the</span>
<span class="p_add">+	 * PCID bits.  This serves two purposes.  It prevents a nasty</span>
<span class="p_add">+	 * situation in which PCID-unaware code saves CR3, loads some other</span>
<span class="p_add">+	 * value (with PCID == 0), and then restores CR3, thus corrupting</span>
<span class="p_add">+	 * the TLB for ASID 0 if the saved ASID was nonzero.  It also means</span>
<span class="p_add">+	 * that any bugs involving loading a PCID-enabled CR3 with</span>
<span class="p_add">+	 * CR4.PCIDE off will trigger deterministically.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	return asid + 1;</span>
 }
 
<span class="p_del">-/* Flush all mappings for all PCIDs except globals. */</span>
<span class="p_del">-static inline void invpcid_flush_all_nonglobals(void)</span>
<span class="p_add">+struct pgd_t;</span>
<span class="p_add">+static inline unsigned long build_cr3(pgd_t *pgd, u16 asid)</span>
 {
<span class="p_del">-	__invpcid(0, 0, INVPCID_TYPE_ALL_NON_GLOBAL);</span>
<span class="p_add">+	if (static_cpu_has(X86_FEATURE_PCID)) {</span>
<span class="p_add">+		return __sme_pa(pgd) | kern_pcid(asid);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		VM_WARN_ON_ONCE(asid != 0);</span>
<span class="p_add">+		return __sme_pa(pgd);</span>
<span class="p_add">+	}</span>
 }
 
<span class="p_del">-static inline u64 inc_mm_tlb_gen(struct mm_struct *mm)</span>
<span class="p_add">+static inline unsigned long build_cr3_noflush(pgd_t *pgd, u16 asid)</span>
 {
<span class="p_del">-	u64 new_tlb_gen;</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Bump the generation count.  This also serves as a full barrier</span>
<span class="p_del">-	 * that synchronizes with switch_mm(): callers are required to order</span>
<span class="p_del">-	 * their read of mm_cpumask after their writes to the paging</span>
<span class="p_del">-	 * structures.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	smp_mb__before_atomic();</span>
<span class="p_del">-	new_tlb_gen = atomic64_inc_return(&amp;mm-&gt;context.tlb_gen);</span>
<span class="p_del">-	smp_mb__after_atomic();</span>
<span class="p_del">-</span>
<span class="p_del">-	return new_tlb_gen;</span>
<span class="p_add">+	VM_WARN_ON_ONCE(asid &gt; MAX_ASID_AVAILABLE);</span>
<span class="p_add">+	VM_WARN_ON_ONCE(!this_cpu_has(X86_FEATURE_PCID));</span>
<span class="p_add">+	return __sme_pa(pgd) | kern_pcid(asid) | CR3_NOFLUSH;</span>
 }
 
 #ifdef CONFIG_PARAVIRT
<span class="p_chunk">@@ -234,6 +230,9 @@</span> <span class="p_context"> static inline void cr4_set_bits_and_update_boot(unsigned long mask)</span>
 
 extern void initialize_tlbstate_and_flush(void);
 
<span class="p_add">+/*</span>
<span class="p_add">+ * flush the entire current user mapping</span>
<span class="p_add">+ */</span>
 static inline void __native_flush_tlb(void)
 {
 	/*
<span class="p_chunk">@@ -246,20 +245,12 @@</span> <span class="p_context"> static inline void __native_flush_tlb(void)</span>
 	preempt_enable();
 }
 
<span class="p_del">-static inline void __native_flush_tlb_global_irq_disabled(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	unsigned long cr4;</span>
<span class="p_del">-</span>
<span class="p_del">-	cr4 = this_cpu_read(cpu_tlbstate.cr4);</span>
<span class="p_del">-	/* clear PGE */</span>
<span class="p_del">-	native_write_cr4(cr4 &amp; ~X86_CR4_PGE);</span>
<span class="p_del">-	/* write old PGE again and flush TLBs */</span>
<span class="p_del">-	native_write_cr4(cr4);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * flush everything</span>
<span class="p_add">+ */</span>
 static inline void __native_flush_tlb_global(void)
 {
<span class="p_del">-	unsigned long flags;</span>
<span class="p_add">+	unsigned long cr4, flags;</span>
 
 	if (static_cpu_has(X86_FEATURE_INVPCID)) {
 		/*
<span class="p_chunk">@@ -277,22 +268,36 @@</span> <span class="p_context"> static inline void __native_flush_tlb_global(void)</span>
 	 */
 	raw_local_irq_save(flags);
 
<span class="p_del">-	__native_flush_tlb_global_irq_disabled();</span>
<span class="p_add">+	cr4 = this_cpu_read(cpu_tlbstate.cr4);</span>
<span class="p_add">+	/* toggle PGE */</span>
<span class="p_add">+	native_write_cr4(cr4 ^ X86_CR4_PGE);</span>
<span class="p_add">+	/* write old PGE again and flush TLBs */</span>
<span class="p_add">+	native_write_cr4(cr4);</span>
 
 	raw_local_irq_restore(flags);
 }
 
<span class="p_add">+/*</span>
<span class="p_add">+ * flush one page in the user mapping</span>
<span class="p_add">+ */</span>
 static inline void __native_flush_tlb_single(unsigned long addr)
 {
 	asm volatile(&quot;invlpg (%0)&quot; ::&quot;r&quot; (addr) : &quot;memory&quot;);
 }
 
<span class="p_add">+/*</span>
<span class="p_add">+ * flush everything</span>
<span class="p_add">+ */</span>
 static inline void __flush_tlb_all(void)
 {
<span class="p_del">-	if (boot_cpu_has(X86_FEATURE_PGE))</span>
<span class="p_add">+	if (boot_cpu_has(X86_FEATURE_PGE)) {</span>
 		__flush_tlb_global();
<span class="p_del">-	else</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * !PGE -&gt; !PCID (setup_pcid()), thus every flush is total.</span>
<span class="p_add">+		 */</span>
 		__flush_tlb();
<span class="p_add">+	}</span>
 
 	/*
 	 * Note: if we somehow had PCID but not PGE, then this wouldn&#39;t work --
<span class="p_chunk">@@ -303,6 +308,9 @@</span> <span class="p_context"> static inline void __flush_tlb_all(void)</span>
 	 */
 }
 
<span class="p_add">+/*</span>
<span class="p_add">+ * flush one page in the kernel mapping</span>
<span class="p_add">+ */</span>
 static inline void __flush_tlb_one(unsigned long addr)
 {
 	count_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ONE);
<span class="p_header">diff --git a/arch/x86/kernel/asm-offsets.c b/arch/x86/kernel/asm-offsets.c</span>
<span class="p_header">index cd360a5e0dca..676b7cf4b62b 100644</span>
<span class="p_header">--- a/arch/x86/kernel/asm-offsets.c</span>
<span class="p_header">+++ b/arch/x86/kernel/asm-offsets.c</span>
<span class="p_chunk">@@ -97,6 +97,6 @@</span> <span class="p_context"> void common(void) {</span>
 	/* Layout info for cpu_entry_area */
 	OFFSET(CPU_ENTRY_AREA_tss, cpu_entry_area, tss);
 	OFFSET(CPU_ENTRY_AREA_entry_trampoline, cpu_entry_area, entry_trampoline);
<span class="p_del">-	OFFSET(CPU_ENTRY_AREA_SYSENTER_stack, cpu_entry_area, SYSENTER_stack_page);</span>
<span class="p_del">-	DEFINE(SIZEOF_SYSENTER_stack, sizeof(struct SYSENTER_stack));</span>
<span class="p_add">+	OFFSET(CPU_ENTRY_AREA_entry_stack, cpu_entry_area, entry_stack_page);</span>
<span class="p_add">+	DEFINE(SIZEOF_entry_stack, sizeof(struct entry_stack));</span>
 }
<span class="p_header">diff --git a/arch/x86/kernel/asm-offsets_32.c b/arch/x86/kernel/asm-offsets_32.c</span>
<span class="p_header">index 7d20d9c0b3d6..fa1261eefa16 100644</span>
<span class="p_header">--- a/arch/x86/kernel/asm-offsets_32.c</span>
<span class="p_header">+++ b/arch/x86/kernel/asm-offsets_32.c</span>
<span class="p_chunk">@@ -48,7 +48,7 @@</span> <span class="p_context"> void foo(void)</span>
 
 	/* Offset from the sysenter stack to tss.sp0 */
 	DEFINE(TSS_sysenter_sp0, offsetof(struct cpu_entry_area, tss.x86_tss.sp0) -
<span class="p_del">-	       offsetofend(struct cpu_entry_area, SYSENTER_stack_page.stack));</span>
<span class="p_add">+	       offsetofend(struct cpu_entry_area, entry_stack_page.stack));</span>
 
 #ifdef CONFIG_CC_STACKPROTECTOR
 	BLANK();
<span class="p_header">diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c</span>
<span class="p_header">index 034900623adf..8ddcfa4d4165 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/common.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/common.c</span>
<span class="p_chunk">@@ -482,102 +482,8 @@</span> <span class="p_context"> static const unsigned int exception_stack_sizes[N_EXCEPTION_STACKS] = {</span>
 	  [0 ... N_EXCEPTION_STACKS - 1]	= EXCEPTION_STKSZ,
 	  [DEBUG_STACK - 1]			= DEBUG_STKSZ
 };
<span class="p_del">-</span>
<span class="p_del">-static DEFINE_PER_CPU_PAGE_ALIGNED(char, exception_stacks</span>
<span class="p_del">-	[(N_EXCEPTION_STACKS - 1) * EXCEPTION_STKSZ + DEBUG_STKSZ]);</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
<span class="p_del">-static DEFINE_PER_CPU_PAGE_ALIGNED(struct SYSENTER_stack_page,</span>
<span class="p_del">-				   SYSENTER_stack_storage);</span>
<span class="p_del">-</span>
<span class="p_del">-static void __init</span>
<span class="p_del">-set_percpu_fixmap_pages(int idx, void *ptr, int pages, pgprot_t prot)</span>
<span class="p_del">-{</span>
<span class="p_del">-	for ( ; pages; pages--, idx--, ptr += PAGE_SIZE)</span>
<span class="p_del">-		__set_fixmap(idx, per_cpu_ptr_to_phys(ptr), prot);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-/* Setup the fixmap mappings only once per-processor */</span>
<span class="p_del">-static void __init setup_cpu_entry_area(int cpu)</span>
<span class="p_del">-{</span>
<span class="p_del">-#ifdef CONFIG_X86_64</span>
<span class="p_del">-	extern char _entry_trampoline[];</span>
<span class="p_del">-</span>
<span class="p_del">-	/* On 64-bit systems, we use a read-only fixmap GDT and TSS. */</span>
<span class="p_del">-	pgprot_t gdt_prot = PAGE_KERNEL_RO;</span>
<span class="p_del">-	pgprot_t tss_prot = PAGE_KERNEL_RO;</span>
<span class="p_del">-#else</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * On native 32-bit systems, the GDT cannot be read-only because</span>
<span class="p_del">-	 * our double fault handler uses a task gate, and entering through</span>
<span class="p_del">-	 * a task gate needs to change an available TSS to busy.  If the</span>
<span class="p_del">-	 * GDT is read-only, that will triple fault.  The TSS cannot be</span>
<span class="p_del">-	 * read-only because the CPU writes to it on task switches.</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 * On Xen PV, the GDT must be read-only because the hypervisor</span>
<span class="p_del">-	 * requires it.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	pgprot_t gdt_prot = boot_cpu_has(X86_FEATURE_XENPV) ?</span>
<span class="p_del">-		PAGE_KERNEL_RO : PAGE_KERNEL;</span>
<span class="p_del">-	pgprot_t tss_prot = PAGE_KERNEL;</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
<span class="p_del">-	__set_fixmap(get_cpu_entry_area_index(cpu, gdt), get_cpu_gdt_paddr(cpu), gdt_prot);</span>
<span class="p_del">-	set_percpu_fixmap_pages(get_cpu_entry_area_index(cpu, SYSENTER_stack_page),</span>
<span class="p_del">-				per_cpu_ptr(&amp;SYSENTER_stack_storage, cpu), 1,</span>
<span class="p_del">-				PAGE_KERNEL);</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * The Intel SDM says (Volume 3, 7.2.1):</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 *  Avoid placing a page boundary in the part of the TSS that the</span>
<span class="p_del">-	 *  processor reads during a task switch (the first 104 bytes). The</span>
<span class="p_del">-	 *  processor may not correctly perform address translations if a</span>
<span class="p_del">-	 *  boundary occurs in this area. During a task switch, the processor</span>
<span class="p_del">-	 *  reads and writes into the first 104 bytes of each TSS (using</span>
<span class="p_del">-	 *  contiguous physical addresses beginning with the physical address</span>
<span class="p_del">-	 *  of the first byte of the TSS). So, after TSS access begins, if</span>
<span class="p_del">-	 *  part of the 104 bytes is not physically contiguous, the processor</span>
<span class="p_del">-	 *  will access incorrect information without generating a page-fault</span>
<span class="p_del">-	 *  exception.</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 * There are also a lot of errata involving the TSS spanning a page</span>
<span class="p_del">-	 * boundary.  Assert that we&#39;re not doing that.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	BUILD_BUG_ON((offsetof(struct tss_struct, x86_tss) ^</span>
<span class="p_del">-		      offsetofend(struct tss_struct, x86_tss)) &amp; PAGE_MASK);</span>
<span class="p_del">-	BUILD_BUG_ON(sizeof(struct tss_struct) % PAGE_SIZE != 0);</span>
<span class="p_del">-	set_percpu_fixmap_pages(get_cpu_entry_area_index(cpu, tss),</span>
<span class="p_del">-				&amp;per_cpu(cpu_tss_rw, cpu),</span>
<span class="p_del">-				sizeof(struct tss_struct) / PAGE_SIZE,</span>
<span class="p_del">-				tss_prot);</span>
<span class="p_del">-</span>
<span class="p_del">-#ifdef CONFIG_X86_32</span>
<span class="p_del">-	per_cpu(cpu_entry_area, cpu) = get_cpu_entry_area(cpu);</span>
 #endif
 
<span class="p_del">-#ifdef CONFIG_X86_64</span>
<span class="p_del">-	BUILD_BUG_ON(sizeof(exception_stacks) % PAGE_SIZE != 0);</span>
<span class="p_del">-	BUILD_BUG_ON(sizeof(exception_stacks) !=</span>
<span class="p_del">-		     sizeof(((struct cpu_entry_area *)0)-&gt;exception_stacks));</span>
<span class="p_del">-	set_percpu_fixmap_pages(get_cpu_entry_area_index(cpu, exception_stacks),</span>
<span class="p_del">-				&amp;per_cpu(exception_stacks, cpu),</span>
<span class="p_del">-				sizeof(exception_stacks) / PAGE_SIZE,</span>
<span class="p_del">-				PAGE_KERNEL);</span>
<span class="p_del">-</span>
<span class="p_del">-	__set_fixmap(get_cpu_entry_area_index(cpu, entry_trampoline),</span>
<span class="p_del">-		     __pa_symbol(_entry_trampoline), PAGE_KERNEL_RX);</span>
<span class="p_del">-#endif</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-void __init setup_cpu_entry_areas(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	unsigned int cpu;</span>
<span class="p_del">-</span>
<span class="p_del">-	for_each_possible_cpu(cpu)</span>
<span class="p_del">-		setup_cpu_entry_area(cpu);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 /* Load the original GDT from the per-cpu structure */
 void load_direct_gdt(int cpu)
 {
<span class="p_chunk">@@ -1323,7 +1229,7 @@</span> <span class="p_context"> void enable_sep_cpu(void)</span>
 
 	tss-&gt;x86_tss.ss1 = __KERNEL_CS;
 	wrmsr(MSR_IA32_SYSENTER_CS, tss-&gt;x86_tss.ss1, 0);
<span class="p_del">-	wrmsr(MSR_IA32_SYSENTER_ESP, (unsigned long)(cpu_SYSENTER_stack(cpu) + 1), 0);</span>
<span class="p_add">+	wrmsr(MSR_IA32_SYSENTER_ESP, (unsigned long)(cpu_entry_stack(cpu) + 1), 0);</span>
 	wrmsr(MSR_IA32_SYSENTER_EIP, (unsigned long)entry_SYSENTER_32, 0);
 
 	put_cpu();
<span class="p_chunk">@@ -1440,7 +1346,7 @@</span> <span class="p_context"> void syscall_init(void)</span>
 	 * AMD doesn&#39;t allow SYSENTER in long mode (either 32- or 64-bit).
 	 */
 	wrmsrl_safe(MSR_IA32_SYSENTER_CS, (u64)__KERNEL_CS);
<span class="p_del">-	wrmsrl_safe(MSR_IA32_SYSENTER_ESP, (unsigned long)(cpu_SYSENTER_stack(cpu) + 1));</span>
<span class="p_add">+	wrmsrl_safe(MSR_IA32_SYSENTER_ESP, (unsigned long)(cpu_entry_stack(cpu) + 1));</span>
 	wrmsrl_safe(MSR_IA32_SYSENTER_EIP, (u64)entry_SYSENTER_compat);
 #else
 	wrmsrl(MSR_CSTAR, (unsigned long)ignore_sysret);
<span class="p_chunk">@@ -1655,7 +1561,7 @@</span> <span class="p_context"> void cpu_init(void)</span>
 	 */
 	set_tss_desc(cpu, &amp;get_cpu_entry_area(cpu)-&gt;tss.x86_tss);
 	load_TR_desc();
<span class="p_del">-	load_sp0((unsigned long)(cpu_SYSENTER_stack(cpu) + 1));</span>
<span class="p_add">+	load_sp0((unsigned long)(cpu_entry_stack(cpu) + 1));</span>
 
 	load_mm_ldt(&amp;init_mm);
 
<span class="p_header">diff --git a/arch/x86/kernel/cpu/microcode/intel.c b/arch/x86/kernel/cpu/microcode/intel.c</span>
<span class="p_header">index 7dbcb7adf797..8ccdca6d3f9e 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/microcode/intel.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/microcode/intel.c</span>
<span class="p_chunk">@@ -565,15 +565,6 @@</span> <span class="p_context"> static void print_ucode(struct ucode_cpu_info *uci)</span>
 }
 #else
 
<span class="p_del">-/*</span>
<span class="p_del">- * Flush global tlb. We only do this in x86_64 where paging has been enabled</span>
<span class="p_del">- * already and PGE should be enabled as well.</span>
<span class="p_del">- */</span>
<span class="p_del">-static inline void flush_tlb_early(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	__native_flush_tlb_global_irq_disabled();</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 static inline void print_ucode(struct ucode_cpu_info *uci)
 {
 	struct microcode_intel *mc;
<span class="p_chunk">@@ -602,10 +593,6 @@</span> <span class="p_context"> static int apply_microcode_early(struct ucode_cpu_info *uci, bool early)</span>
 	if (rev != mc-&gt;hdr.rev)
 		return -1;
 
<span class="p_del">-#ifdef CONFIG_X86_64</span>
<span class="p_del">-	/* Flush global tlb. This is precaution. */</span>
<span class="p_del">-	flush_tlb_early();</span>
<span class="p_del">-#endif</span>
 	uci-&gt;cpu_sig.rev = rev;
 
 	if (early)
<span class="p_header">diff --git a/arch/x86/kernel/dumpstack.c b/arch/x86/kernel/dumpstack.c</span>
<span class="p_header">index bbd6d986e2d0..36b17e0febe8 100644</span>
<span class="p_header">--- a/arch/x86/kernel/dumpstack.c</span>
<span class="p_header">+++ b/arch/x86/kernel/dumpstack.c</span>
<span class="p_chunk">@@ -18,6 +18,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/nmi.h&gt;
 #include &lt;linux/sysfs.h&gt;
 
<span class="p_add">+#include &lt;asm/cpu_entry_area.h&gt;</span>
 #include &lt;asm/stacktrace.h&gt;
 #include &lt;asm/unwind.h&gt;
 
<span class="p_chunk">@@ -43,9 +44,9 @@</span> <span class="p_context"> bool in_task_stack(unsigned long *stack, struct task_struct *task,</span>
 	return true;
 }
 
<span class="p_del">-bool in_sysenter_stack(unsigned long *stack, struct stack_info *info)</span>
<span class="p_add">+bool in_entry_stack(unsigned long *stack, struct stack_info *info)</span>
 {
<span class="p_del">-	struct SYSENTER_stack *ss = cpu_SYSENTER_stack(smp_processor_id());</span>
<span class="p_add">+	struct entry_stack *ss = cpu_entry_stack(smp_processor_id());</span>
 
 	void *begin = ss;
 	void *end = ss + 1;
<span class="p_chunk">@@ -53,7 +54,7 @@</span> <span class="p_context"> bool in_sysenter_stack(unsigned long *stack, struct stack_info *info)</span>
 	if ((void *)stack &lt; begin || (void *)stack &gt;= end)
 		return false;
 
<span class="p_del">-	info-&gt;type	= STACK_TYPE_SYSENTER;</span>
<span class="p_add">+	info-&gt;type	= STACK_TYPE_ENTRY;</span>
 	info-&gt;begin	= begin;
 	info-&gt;end	= end;
 	info-&gt;next_sp	= NULL;
<span class="p_chunk">@@ -111,13 +112,13 @@</span> <span class="p_context"> void show_trace_log_lvl(struct task_struct *task, struct pt_regs *regs,</span>
 	 * - task stack
 	 * - interrupt stack
 	 * - HW exception stacks (double fault, nmi, debug, mce)
<span class="p_del">-	 * - SYSENTER stack</span>
<span class="p_add">+	 * - entry stack</span>
 	 *
 	 * x86-32 can have up to four stacks:
 	 * - task stack
 	 * - softirq stack
 	 * - hardirq stack
<span class="p_del">-	 * - SYSENTER stack</span>
<span class="p_add">+	 * - entry stack</span>
 	 */
 	for (regs = NULL; stack; stack = PTR_ALIGN(stack_info.next_sp, sizeof(long))) {
 		const char *stack_name;
<span class="p_header">diff --git a/arch/x86/kernel/dumpstack_32.c b/arch/x86/kernel/dumpstack_32.c</span>
<span class="p_header">index 5ff13a6b3680..04170f63e3a1 100644</span>
<span class="p_header">--- a/arch/x86/kernel/dumpstack_32.c</span>
<span class="p_header">+++ b/arch/x86/kernel/dumpstack_32.c</span>
<span class="p_chunk">@@ -26,8 +26,8 @@</span> <span class="p_context"> const char *stack_type_name(enum stack_type type)</span>
 	if (type == STACK_TYPE_SOFTIRQ)
 		return &quot;SOFTIRQ&quot;;
 
<span class="p_del">-	if (type == STACK_TYPE_SYSENTER)</span>
<span class="p_del">-		return &quot;SYSENTER&quot;;</span>
<span class="p_add">+	if (type == STACK_TYPE_ENTRY)</span>
<span class="p_add">+		return &quot;ENTRY_TRAMPOLINE&quot;;</span>
 
 	return NULL;
 }
<span class="p_chunk">@@ -96,7 +96,7 @@</span> <span class="p_context"> int get_stack_info(unsigned long *stack, struct task_struct *task,</span>
 	if (task != current)
 		goto unknown;
 
<span class="p_del">-	if (in_sysenter_stack(stack, info))</span>
<span class="p_add">+	if (in_entry_stack(stack, info))</span>
 		goto recursion_check;
 
 	if (in_hardirq_stack(stack, info))
<span class="p_header">diff --git a/arch/x86/kernel/dumpstack_64.c b/arch/x86/kernel/dumpstack_64.c</span>
<span class="p_header">index abc828f8c297..563e28d14f2c 100644</span>
<span class="p_header">--- a/arch/x86/kernel/dumpstack_64.c</span>
<span class="p_header">+++ b/arch/x86/kernel/dumpstack_64.c</span>
<span class="p_chunk">@@ -37,8 +37,14 @@</span> <span class="p_context"> const char *stack_type_name(enum stack_type type)</span>
 	if (type == STACK_TYPE_IRQ)
 		return &quot;IRQ&quot;;
 
<span class="p_del">-	if (type == STACK_TYPE_SYSENTER)</span>
<span class="p_del">-		return &quot;SYSENTER&quot;;</span>
<span class="p_add">+	if (type == STACK_TYPE_ENTRY) {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * On 64-bit, we have a generic entry stack that we</span>
<span class="p_add">+		 * use for all the kernel entry points, including</span>
<span class="p_add">+		 * SYSENTER.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		return &quot;ENTRY_TRAMPOLINE&quot;;</span>
<span class="p_add">+	}</span>
 
 	if (type &gt;= STACK_TYPE_EXCEPTION &amp;&amp; type &lt;= STACK_TYPE_EXCEPTION_LAST)
 		return exception_stack_names[type - STACK_TYPE_EXCEPTION];
<span class="p_chunk">@@ -118,7 +124,7 @@</span> <span class="p_context"> int get_stack_info(unsigned long *stack, struct task_struct *task,</span>
 	if (in_irq_stack(stack, info))
 		goto recursion_check;
 
<span class="p_del">-	if (in_sysenter_stack(stack, info))</span>
<span class="p_add">+	if (in_entry_stack(stack, info))</span>
 		goto recursion_check;
 
 	goto unknown;
<span class="p_header">diff --git a/arch/x86/kernel/ldt.c b/arch/x86/kernel/ldt.c</span>
<span class="p_header">index 1c1eae961340..a6b5d62f45a7 100644</span>
<span class="p_header">--- a/arch/x86/kernel/ldt.c</span>
<span class="p_header">+++ b/arch/x86/kernel/ldt.c</span>
<span class="p_chunk">@@ -5,6 +5,11 @@</span> <span class="p_context"></span>
  * Copyright (C) 2002 Andi Kleen
  *
  * This handles calls from both 32bit and 64bit mode.
<span class="p_add">+ *</span>
<span class="p_add">+ * Lock order:</span>
<span class="p_add">+ *	contex.ldt_usr_sem</span>
<span class="p_add">+ *	  mmap_sem</span>
<span class="p_add">+ *	    context.lock</span>
  */
 
 #include &lt;linux/errno.h&gt;
<span class="p_chunk">@@ -42,7 +47,7 @@</span> <span class="p_context"> static void refresh_ldt_segments(void)</span>
 #endif
 }
 
<span class="p_del">-/* context.lock is held for us, so we don&#39;t need any locking. */</span>
<span class="p_add">+/* context.lock is held by the task which issued the smp function call */</span>
 static void flush_ldt(void *__mm)
 {
 	struct mm_struct *mm = __mm;
<span class="p_chunk">@@ -99,15 +104,17 @@</span> <span class="p_context"> static void finalize_ldt_struct(struct ldt_struct *ldt)</span>
 	paravirt_alloc_ldt(ldt-&gt;entries, ldt-&gt;nr_entries);
 }
 
<span class="p_del">-/* context.lock is held */</span>
<span class="p_del">-static void install_ldt(struct mm_struct *current_mm,</span>
<span class="p_del">-			struct ldt_struct *ldt)</span>
<span class="p_add">+static void install_ldt(struct mm_struct *mm, struct ldt_struct *ldt)</span>
 {
<span class="p_add">+	mutex_lock(&amp;mm-&gt;context.lock);</span>
<span class="p_add">+</span>
 	/* Synchronizes with READ_ONCE in load_mm_ldt. */
<span class="p_del">-	smp_store_release(&amp;current_mm-&gt;context.ldt, ldt);</span>
<span class="p_add">+	smp_store_release(&amp;mm-&gt;context.ldt, ldt);</span>
 
<span class="p_del">-	/* Activate the LDT for all CPUs using current_mm. */</span>
<span class="p_del">-	on_each_cpu_mask(mm_cpumask(current_mm), flush_ldt, current_mm, true);</span>
<span class="p_add">+	/* Activate the LDT for all CPUs using currents mm. */</span>
<span class="p_add">+	on_each_cpu_mask(mm_cpumask(mm), flush_ldt, mm, true);</span>
<span class="p_add">+</span>
<span class="p_add">+	mutex_unlock(&amp;mm-&gt;context.lock);</span>
 }
 
 static void free_ldt_struct(struct ldt_struct *ldt)
<span class="p_chunk">@@ -124,27 +131,20 @@</span> <span class="p_context"> static void free_ldt_struct(struct ldt_struct *ldt)</span>
 }
 
 /*
<span class="p_del">- * we do not have to muck with descriptors here, that is</span>
<span class="p_del">- * done in switch_mm() as needed.</span>
<span class="p_add">+ * Called on fork from arch_dup_mmap(). Just copy the current LDT state,</span>
<span class="p_add">+ * the new task is not running, so nothing can be installed.</span>
  */
<span class="p_del">-int init_new_context_ldt(struct task_struct *tsk, struct mm_struct *mm)</span>
<span class="p_add">+int ldt_dup_context(struct mm_struct *old_mm, struct mm_struct *mm)</span>
 {
 	struct ldt_struct *new_ldt;
<span class="p_del">-	struct mm_struct *old_mm;</span>
 	int retval = 0;
 
<span class="p_del">-	mutex_init(&amp;mm-&gt;context.lock);</span>
<span class="p_del">-	old_mm = current-&gt;mm;</span>
<span class="p_del">-	if (!old_mm) {</span>
<span class="p_del">-		mm-&gt;context.ldt = NULL;</span>
<span class="p_add">+	if (!old_mm)</span>
 		return 0;
<span class="p_del">-	}</span>
 
 	mutex_lock(&amp;old_mm-&gt;context.lock);
<span class="p_del">-	if (!old_mm-&gt;context.ldt) {</span>
<span class="p_del">-		mm-&gt;context.ldt = NULL;</span>
<span class="p_add">+	if (!old_mm-&gt;context.ldt)</span>
 		goto out_unlock;
<span class="p_del">-	}</span>
 
 	new_ldt = alloc_ldt_struct(old_mm-&gt;context.ldt-&gt;nr_entries);
 	if (!new_ldt) {
<span class="p_chunk">@@ -180,7 +180,7 @@</span> <span class="p_context"> static int read_ldt(void __user *ptr, unsigned long bytecount)</span>
 	unsigned long entries_size;
 	int retval;
 
<span class="p_del">-	mutex_lock(&amp;mm-&gt;context.lock);</span>
<span class="p_add">+	down_read(&amp;mm-&gt;context.ldt_usr_sem);</span>
 
 	if (!mm-&gt;context.ldt) {
 		retval = 0;
<span class="p_chunk">@@ -209,7 +209,7 @@</span> <span class="p_context"> static int read_ldt(void __user *ptr, unsigned long bytecount)</span>
 	retval = bytecount;
 
 out_unlock:
<span class="p_del">-	mutex_unlock(&amp;mm-&gt;context.lock);</span>
<span class="p_add">+	up_read(&amp;mm-&gt;context.ldt_usr_sem);</span>
 	return retval;
 }
 
<span class="p_chunk">@@ -269,7 +269,8 @@</span> <span class="p_context"> static int write_ldt(void __user *ptr, unsigned long bytecount, int oldmode)</span>
 			ldt.avl = 0;
 	}
 
<span class="p_del">-	mutex_lock(&amp;mm-&gt;context.lock);</span>
<span class="p_add">+	if (down_write_killable(&amp;mm-&gt;context.ldt_usr_sem))</span>
<span class="p_add">+		return -EINTR;</span>
 
 	old_ldt       = mm-&gt;context.ldt;
 	old_nr_entries = old_ldt ? old_ldt-&gt;nr_entries : 0;
<span class="p_chunk">@@ -291,7 +292,7 @@</span> <span class="p_context"> static int write_ldt(void __user *ptr, unsigned long bytecount, int oldmode)</span>
 	error = 0;
 
 out_unlock:
<span class="p_del">-	mutex_unlock(&amp;mm-&gt;context.lock);</span>
<span class="p_add">+	up_write(&amp;mm-&gt;context.ldt_usr_sem);</span>
 out:
 	return error;
 }
<span class="p_header">diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c</span>
<span class="p_header">index d56c1d209283..33d6000265aa 100644</span>
<span class="p_header">--- a/arch/x86/kernel/smpboot.c</span>
<span class="p_header">+++ b/arch/x86/kernel/smpboot.c</span>
<span class="p_chunk">@@ -990,12 +990,8 @@</span> <span class="p_context"> static int do_boot_cpu(int apicid, int cpu, struct task_struct *idle,</span>
 	initial_code = (unsigned long)start_secondary;
 	initial_stack  = idle-&gt;thread.sp;
 
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Enable the espfix hack for this CPU</span>
<span class="p_del">-	*/</span>
<span class="p_del">-#ifdef CONFIG_X86_ESPFIX64</span>
<span class="p_add">+	/* Enable the espfix hack for this CPU */</span>
 	init_espfix_ap(cpu);
<span class="p_del">-#endif</span>
 
 	/* So we see what&#39;s up */
 	announce_cpu(cpu, apicid);
<span class="p_header">diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c</span>
<span class="p_header">index 74136fd16f49..7c16fe0b60c2 100644</span>
<span class="p_header">--- a/arch/x86/kernel/traps.c</span>
<span class="p_header">+++ b/arch/x86/kernel/traps.c</span>
<span class="p_chunk">@@ -52,6 +52,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/traps.h&gt;
 #include &lt;asm/desc.h&gt;
 #include &lt;asm/fpu/internal.h&gt;
<span class="p_add">+#include &lt;asm/cpu_entry_area.h&gt;</span>
 #include &lt;asm/mce.h&gt;
 #include &lt;asm/fixmap.h&gt;
 #include &lt;asm/mach_traps.h&gt;
<span class="p_chunk">@@ -950,8 +951,9 @@</span> <span class="p_context"> void __init trap_init(void)</span>
 	 * &quot;sidt&quot; instruction will not leak the location of the kernel, and
 	 * to defend the IDT against arbitrary memory write vulnerabilities.
 	 * It will be reloaded in cpu_init() */
<span class="p_del">-	__set_fixmap(FIX_RO_IDT, __pa_symbol(idt_table), PAGE_KERNEL_RO);</span>
<span class="p_del">-	idt_descr.address = fix_to_virt(FIX_RO_IDT);</span>
<span class="p_add">+	cea_set_pte(CPU_ENTRY_AREA_RO_IDT_VADDR, __pa_symbol(idt_table),</span>
<span class="p_add">+		    PAGE_KERNEL_RO);</span>
<span class="p_add">+	idt_descr.address = CPU_ENTRY_AREA_RO_IDT;</span>
 
 	/*
 	 * Should be a barrier for any external CPU state:
<span class="p_header">diff --git a/arch/x86/mm/Makefile b/arch/x86/mm/Makefile</span>
<span class="p_header">index 7ba7f3d7f477..2e0017af8f9b 100644</span>
<span class="p_header">--- a/arch/x86/mm/Makefile</span>
<span class="p_header">+++ b/arch/x86/mm/Makefile</span>
<span class="p_chunk">@@ -10,7 +10,7 @@</span> <span class="p_context"> CFLAGS_REMOVE_mem_encrypt.o	= -pg</span>
 endif
 
 obj-y	:=  init.o init_$(BITS).o fault.o ioremap.o extable.o pageattr.o mmap.o \
<span class="p_del">-	    pat.o pgtable.o physaddr.o setup_nx.o tlb.o</span>
<span class="p_add">+	    pat.o pgtable.o physaddr.o setup_nx.o tlb.o cpu_entry_area.o</span>
 
 # Make sure __phys_addr has no stackprotector
 nostackp := $(call cc-option, -fno-stack-protector)
<span class="p_header">diff --git a/arch/x86/mm/cpu_entry_area.c b/arch/x86/mm/cpu_entry_area.c</span>
new file mode 100644
<span class="p_header">index 000000000000..fe814fd5e014</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/x86/mm/cpu_entry_area.c</span>
<span class="p_chunk">@@ -0,0 +1,139 @@</span> <span class="p_context"></span>
<span class="p_add">+// SPDX-License-Identifier: GPL-2.0</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/spinlock.h&gt;</span>
<span class="p_add">+#include &lt;linux/percpu.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/cpu_entry_area.h&gt;</span>
<span class="p_add">+#include &lt;asm/pgtable.h&gt;</span>
<span class="p_add">+#include &lt;asm/fixmap.h&gt;</span>
<span class="p_add">+#include &lt;asm/desc.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+static DEFINE_PER_CPU_PAGE_ALIGNED(struct entry_stack_page, entry_stack_storage);</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_X86_64</span>
<span class="p_add">+static DEFINE_PER_CPU_PAGE_ALIGNED(char, exception_stacks</span>
<span class="p_add">+	[(N_EXCEPTION_STACKS - 1) * EXCEPTION_STKSZ + DEBUG_STKSZ]);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+struct cpu_entry_area *get_cpu_entry_area(int cpu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long va = CPU_ENTRY_AREA_PER_CPU + cpu * CPU_ENTRY_AREA_SIZE;</span>
<span class="p_add">+	BUILD_BUG_ON(sizeof(struct cpu_entry_area) % PAGE_SIZE != 0);</span>
<span class="p_add">+</span>
<span class="p_add">+	return (struct cpu_entry_area *) va;</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(get_cpu_entry_area);</span>
<span class="p_add">+</span>
<span class="p_add">+void cea_set_pte(void *cea_vaddr, phys_addr_t pa, pgprot_t flags)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long va = (unsigned long) cea_vaddr;</span>
<span class="p_add">+</span>
<span class="p_add">+	set_pte_vaddr(va, pfn_pte(pa &gt;&gt; PAGE_SHIFT, flags));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void __init</span>
<span class="p_add">+cea_map_percpu_pages(void *cea_vaddr, void *ptr, int pages, pgprot_t prot)</span>
<span class="p_add">+{</span>
<span class="p_add">+	for ( ; pages; pages--, cea_vaddr+= PAGE_SIZE, ptr += PAGE_SIZE)</span>
<span class="p_add">+		cea_set_pte(cea_vaddr, per_cpu_ptr_to_phys(ptr), prot);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Setup the fixmap mappings only once per-processor */</span>
<span class="p_add">+static void __init setup_cpu_entry_area(int cpu)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifdef CONFIG_X86_64</span>
<span class="p_add">+	extern char _entry_trampoline[];</span>
<span class="p_add">+</span>
<span class="p_add">+	/* On 64-bit systems, we use a read-only fixmap GDT and TSS. */</span>
<span class="p_add">+	pgprot_t gdt_prot = PAGE_KERNEL_RO;</span>
<span class="p_add">+	pgprot_t tss_prot = PAGE_KERNEL_RO;</span>
<span class="p_add">+#else</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * On native 32-bit systems, the GDT cannot be read-only because</span>
<span class="p_add">+	 * our double fault handler uses a task gate, and entering through</span>
<span class="p_add">+	 * a task gate needs to change an available TSS to busy.  If the</span>
<span class="p_add">+	 * GDT is read-only, that will triple fault.  The TSS cannot be</span>
<span class="p_add">+	 * read-only because the CPU writes to it on task switches.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * On Xen PV, the GDT must be read-only because the hypervisor</span>
<span class="p_add">+	 * requires it.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	pgprot_t gdt_prot = boot_cpu_has(X86_FEATURE_XENPV) ?</span>
<span class="p_add">+		PAGE_KERNEL_RO : PAGE_KERNEL;</span>
<span class="p_add">+	pgprot_t tss_prot = PAGE_KERNEL;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+	cea_set_pte(&amp;get_cpu_entry_area(cpu)-&gt;gdt, get_cpu_gdt_paddr(cpu),</span>
<span class="p_add">+		    gdt_prot);</span>
<span class="p_add">+</span>
<span class="p_add">+	cea_map_percpu_pages(&amp;get_cpu_entry_area(cpu)-&gt;entry_stack_page,</span>
<span class="p_add">+			     per_cpu_ptr(&amp;entry_stack_storage, cpu), 1,</span>
<span class="p_add">+			     PAGE_KERNEL);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The Intel SDM says (Volume 3, 7.2.1):</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 *  Avoid placing a page boundary in the part of the TSS that the</span>
<span class="p_add">+	 *  processor reads during a task switch (the first 104 bytes). The</span>
<span class="p_add">+	 *  processor may not correctly perform address translations if a</span>
<span class="p_add">+	 *  boundary occurs in this area. During a task switch, the processor</span>
<span class="p_add">+	 *  reads and writes into the first 104 bytes of each TSS (using</span>
<span class="p_add">+	 *  contiguous physical addresses beginning with the physical address</span>
<span class="p_add">+	 *  of the first byte of the TSS). So, after TSS access begins, if</span>
<span class="p_add">+	 *  part of the 104 bytes is not physically contiguous, the processor</span>
<span class="p_add">+	 *  will access incorrect information without generating a page-fault</span>
<span class="p_add">+	 *  exception.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * There are also a lot of errata involving the TSS spanning a page</span>
<span class="p_add">+	 * boundary.  Assert that we&#39;re not doing that.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	BUILD_BUG_ON((offsetof(struct tss_struct, x86_tss) ^</span>
<span class="p_add">+		      offsetofend(struct tss_struct, x86_tss)) &amp; PAGE_MASK);</span>
<span class="p_add">+	BUILD_BUG_ON(sizeof(struct tss_struct) % PAGE_SIZE != 0);</span>
<span class="p_add">+	cea_map_percpu_pages(&amp;get_cpu_entry_area(cpu)-&gt;tss,</span>
<span class="p_add">+			     &amp;per_cpu(cpu_tss_rw, cpu),</span>
<span class="p_add">+			     sizeof(struct tss_struct) / PAGE_SIZE, tss_prot);</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_X86_32</span>
<span class="p_add">+	per_cpu(cpu_entry_area, cpu) = get_cpu_entry_area(cpu);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_X86_64</span>
<span class="p_add">+	BUILD_BUG_ON(sizeof(exception_stacks) % PAGE_SIZE != 0);</span>
<span class="p_add">+	BUILD_BUG_ON(sizeof(exception_stacks) !=</span>
<span class="p_add">+		     sizeof(((struct cpu_entry_area *)0)-&gt;exception_stacks));</span>
<span class="p_add">+	cea_map_percpu_pages(&amp;get_cpu_entry_area(cpu)-&gt;exception_stacks,</span>
<span class="p_add">+			     &amp;per_cpu(exception_stacks, cpu),</span>
<span class="p_add">+			     sizeof(exception_stacks) / PAGE_SIZE, PAGE_KERNEL);</span>
<span class="p_add">+</span>
<span class="p_add">+	cea_set_pte(&amp;get_cpu_entry_area(cpu)-&gt;entry_trampoline,</span>
<span class="p_add">+		     __pa_symbol(_entry_trampoline), PAGE_KERNEL_RX);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static __init void setup_cpu_entry_area_ptes(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifdef CONFIG_X86_32</span>
<span class="p_add">+	unsigned long start, end;</span>
<span class="p_add">+</span>
<span class="p_add">+	BUILD_BUG_ON(CPU_ENTRY_AREA_PAGES * PAGE_SIZE &lt; CPU_ENTRY_AREA_MAP_SIZE);</span>
<span class="p_add">+	BUG_ON(CPU_ENTRY_AREA_BASE &amp; ~PMD_MASK);</span>
<span class="p_add">+</span>
<span class="p_add">+	start = CPU_ENTRY_AREA_BASE;</span>
<span class="p_add">+	end = start + CPU_ENTRY_AREA_MAP_SIZE;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Careful here: start + PMD_SIZE might wrap around */</span>
<span class="p_add">+	for (; start &lt; end &amp;&amp; start &gt;= CPU_ENTRY_AREA_BASE; start += PMD_SIZE)</span>
<span class="p_add">+		populate_extra_pte(start);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void __init setup_cpu_entry_areas(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned int cpu;</span>
<span class="p_add">+</span>
<span class="p_add">+	setup_cpu_entry_area_ptes();</span>
<span class="p_add">+</span>
<span class="p_add">+	for_each_possible_cpu(cpu)</span>
<span class="p_add">+		setup_cpu_entry_area(cpu);</span>
<span class="p_add">+}</span>
<span class="p_header">diff --git a/arch/x86/mm/dump_pagetables.c b/arch/x86/mm/dump_pagetables.c</span>
<span class="p_header">index 5e3ac6fe6c9e..43dedbfb7257 100644</span>
<span class="p_header">--- a/arch/x86/mm/dump_pagetables.c</span>
<span class="p_header">+++ b/arch/x86/mm/dump_pagetables.c</span>
<span class="p_chunk">@@ -44,10 +44,12 @@</span> <span class="p_context"> struct addr_marker {</span>
 	unsigned long max_lines;
 };
 
<span class="p_del">-/* indices for address_markers; keep sync&#39;d w/ address_markers below */</span>
<span class="p_add">+/* Address space markers hints */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_X86_64</span>
<span class="p_add">+</span>
 enum address_markers_idx {
 	USER_SPACE_NR = 0,
<span class="p_del">-#ifdef CONFIG_X86_64</span>
 	KERNEL_SPACE_NR,
 	LOW_KERNEL_NR,
 	VMALLOC_START_NR,
<span class="p_chunk">@@ -56,56 +58,74 @@</span> <span class="p_context"> enum address_markers_idx {</span>
 	KASAN_SHADOW_START_NR,
 	KASAN_SHADOW_END_NR,
 #endif
<span class="p_del">-# ifdef CONFIG_X86_ESPFIX64</span>
<span class="p_add">+	CPU_ENTRY_AREA_NR,</span>
<span class="p_add">+#ifdef CONFIG_X86_ESPFIX64</span>
 	ESPFIX_START_NR,
<span class="p_del">-# endif</span>
<span class="p_add">+#endif</span>
<span class="p_add">+#ifdef CONFIG_EFI</span>
<span class="p_add">+	EFI_END_NR,</span>
<span class="p_add">+#endif</span>
 	HIGH_KERNEL_NR,
 	MODULES_VADDR_NR,
 	MODULES_END_NR,
<span class="p_del">-#else</span>
<span class="p_add">+	FIXADDR_START_NR,</span>
<span class="p_add">+	END_OF_SPACE_NR,</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+static struct addr_marker address_markers[] = {</span>
<span class="p_add">+	[USER_SPACE_NR]		= { 0,			&quot;User Space&quot; },</span>
<span class="p_add">+	[KERNEL_SPACE_NR]	= { (1UL &lt;&lt; 63),	&quot;Kernel Space&quot; },</span>
<span class="p_add">+	[LOW_KERNEL_NR]		= { 0UL,		&quot;Low Kernel Mapping&quot; },</span>
<span class="p_add">+	[VMALLOC_START_NR]	= { 0UL,		&quot;vmalloc() Area&quot; },</span>
<span class="p_add">+	[VMEMMAP_START_NR]	= { 0UL,		&quot;Vmemmap&quot; },</span>
<span class="p_add">+#ifdef CONFIG_KASAN</span>
<span class="p_add">+	[KASAN_SHADOW_START_NR]	= { KASAN_SHADOW_START,	&quot;KASAN shadow&quot; },</span>
<span class="p_add">+	[KASAN_SHADOW_END_NR]	= { KASAN_SHADOW_END,	&quot;KASAN shadow end&quot; },</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	[CPU_ENTRY_AREA_NR]	= { CPU_ENTRY_AREA_BASE,&quot;CPU entry Area&quot; },</span>
<span class="p_add">+#ifdef CONFIG_X86_ESPFIX64</span>
<span class="p_add">+	[ESPFIX_START_NR]	= { ESPFIX_BASE_ADDR,	&quot;ESPfix Area&quot;, 16 },</span>
<span class="p_add">+#endif</span>
<span class="p_add">+#ifdef CONFIG_EFI</span>
<span class="p_add">+	[EFI_END_NR]		= { EFI_VA_END,		&quot;EFI Runtime Services&quot; },</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	[HIGH_KERNEL_NR]	= { __START_KERNEL_map,	&quot;High Kernel Mapping&quot; },</span>
<span class="p_add">+	[MODULES_VADDR_NR]	= { MODULES_VADDR,	&quot;Modules&quot; },</span>
<span class="p_add">+	[MODULES_END_NR]	= { MODULES_END,	&quot;End Modules&quot; },</span>
<span class="p_add">+	[FIXADDR_START_NR]	= { FIXADDR_START,	&quot;Fixmap Area&quot; },</span>
<span class="p_add">+	[END_OF_SPACE_NR]	= { -1,			NULL }</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+#else /* CONFIG_X86_64 */</span>
<span class="p_add">+</span>
<span class="p_add">+enum address_markers_idx {</span>
<span class="p_add">+	USER_SPACE_NR = 0,</span>
 	KERNEL_SPACE_NR,
 	VMALLOC_START_NR,
 	VMALLOC_END_NR,
<span class="p_del">-# ifdef CONFIG_HIGHMEM</span>
<span class="p_add">+#ifdef CONFIG_HIGHMEM</span>
 	PKMAP_BASE_NR,
<span class="p_del">-# endif</span>
<span class="p_del">-	FIXADDR_START_NR,</span>
 #endif
<span class="p_add">+	CPU_ENTRY_AREA_NR,</span>
<span class="p_add">+	FIXADDR_START_NR,</span>
<span class="p_add">+	END_OF_SPACE_NR,</span>
 };
 
<span class="p_del">-/* Address space markers hints */</span>
 static struct addr_marker address_markers[] = {
<span class="p_del">-	{ 0, &quot;User Space&quot; },</span>
<span class="p_del">-#ifdef CONFIG_X86_64</span>
<span class="p_del">-	{ 0x8000000000000000UL, &quot;Kernel Space&quot; },</span>
<span class="p_del">-	{ 0/* PAGE_OFFSET */,   &quot;Low Kernel Mapping&quot; },</span>
<span class="p_del">-	{ 0/* VMALLOC_START */, &quot;vmalloc() Area&quot; },</span>
<span class="p_del">-	{ 0/* VMEMMAP_START */, &quot;Vmemmap&quot; },</span>
<span class="p_del">-#ifdef CONFIG_KASAN</span>
<span class="p_del">-	{ KASAN_SHADOW_START,	&quot;KASAN shadow&quot; },</span>
<span class="p_del">-	{ KASAN_SHADOW_END,	&quot;KASAN shadow end&quot; },</span>
<span class="p_add">+	[USER_SPACE_NR]		= { 0,			&quot;User Space&quot; },</span>
<span class="p_add">+	[KERNEL_SPACE_NR]	= { PAGE_OFFSET,	&quot;Kernel Mapping&quot; },</span>
<span class="p_add">+	[VMALLOC_START_NR]	= { 0UL,		&quot;vmalloc() Area&quot; },</span>
<span class="p_add">+	[VMALLOC_END_NR]	= { 0UL,		&quot;vmalloc() End&quot; },</span>
<span class="p_add">+#ifdef CONFIG_HIGHMEM</span>
<span class="p_add">+	[PKMAP_BASE_NR]		= { 0UL,		&quot;Persistent kmap() Area&quot; },</span>
 #endif
<span class="p_del">-# ifdef CONFIG_X86_ESPFIX64</span>
<span class="p_del">-	{ ESPFIX_BASE_ADDR,	&quot;ESPfix Area&quot;, 16 },</span>
<span class="p_del">-# endif</span>
<span class="p_del">-# ifdef CONFIG_EFI</span>
<span class="p_del">-	{ EFI_VA_END,		&quot;EFI Runtime Services&quot; },</span>
<span class="p_del">-# endif</span>
<span class="p_del">-	{ __START_KERNEL_map,   &quot;High Kernel Mapping&quot; },</span>
<span class="p_del">-	{ MODULES_VADDR,        &quot;Modules&quot; },</span>
<span class="p_del">-	{ MODULES_END,          &quot;End Modules&quot; },</span>
<span class="p_del">-#else</span>
<span class="p_del">-	{ PAGE_OFFSET,          &quot;Kernel Mapping&quot; },</span>
<span class="p_del">-	{ 0/* VMALLOC_START */, &quot;vmalloc() Area&quot; },</span>
<span class="p_del">-	{ 0/*VMALLOC_END*/,     &quot;vmalloc() End&quot; },</span>
<span class="p_del">-# ifdef CONFIG_HIGHMEM</span>
<span class="p_del">-	{ 0/*PKMAP_BASE*/,      &quot;Persistent kmap() Area&quot; },</span>
<span class="p_del">-# endif</span>
<span class="p_del">-	{ 0/*FIXADDR_START*/,   &quot;Fixmap Area&quot; },</span>
<span class="p_del">-#endif</span>
<span class="p_del">-	{ -1, NULL }		/* End of list */</span>
<span class="p_add">+	[CPU_ENTRY_AREA_NR]	= { 0UL,		&quot;CPU entry area&quot; },</span>
<span class="p_add">+	[FIXADDR_START_NR]	= { 0UL,		&quot;Fixmap area&quot; },</span>
<span class="p_add">+	[END_OF_SPACE_NR]	= { -1,			NULL }</span>
 };
 
<span class="p_add">+#endif /* !CONFIG_X86_64 */</span>
<span class="p_add">+</span>
 /* Multipliers for offsets within the PTEs */
 #define PTE_LEVEL_MULT (PAGE_SIZE)
 #define PMD_LEVEL_MULT (PTRS_PER_PTE * PTE_LEVEL_MULT)
<span class="p_chunk">@@ -140,7 +160,7 @@</span> <span class="p_context"> static void printk_prot(struct seq_file *m, pgprot_t prot, int level, bool dmsg)</span>
 	static const char * const level_name[] =
 		{ &quot;cr3&quot;, &quot;pgd&quot;, &quot;p4d&quot;, &quot;pud&quot;, &quot;pmd&quot;, &quot;pte&quot; };
 
<span class="p_del">-	if (!pgprot_val(prot)) {</span>
<span class="p_add">+	if (!(pr &amp; _PAGE_PRESENT)) {</span>
 		/* Not present */
 		pt_dump_cont_printf(m, dmsg, &quot;                              &quot;);
 	} else {
<span class="p_chunk">@@ -525,8 +545,8 @@</span> <span class="p_context"> static int __init pt_dump_init(void)</span>
 	address_markers[PKMAP_BASE_NR].start_address = PKMAP_BASE;
 # endif
 	address_markers[FIXADDR_START_NR].start_address = FIXADDR_START;
<span class="p_add">+	address_markers[CPU_ENTRY_AREA_NR].start_address = CPU_ENTRY_AREA_BASE;</span>
 #endif
<span class="p_del">-</span>
 	return 0;
 }
 __initcall(pt_dump_init);
<span class="p_header">diff --git a/arch/x86/mm/init_32.c b/arch/x86/mm/init_32.c</span>
<span class="p_header">index 8a64a6f2848d..135c9a7898c7 100644</span>
<span class="p_header">--- a/arch/x86/mm/init_32.c</span>
<span class="p_header">+++ b/arch/x86/mm/init_32.c</span>
<span class="p_chunk">@@ -50,6 +50,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/setup.h&gt;
 #include &lt;asm/set_memory.h&gt;
 #include &lt;asm/page_types.h&gt;
<span class="p_add">+#include &lt;asm/cpu_entry_area.h&gt;</span>
 #include &lt;asm/init.h&gt;
 
 #include &quot;mm_internal.h&quot;
<span class="p_chunk">@@ -766,6 +767,7 @@</span> <span class="p_context"> void __init mem_init(void)</span>
 	mem_init_print_info(NULL);
 	printk(KERN_INFO &quot;virtual kernel memory layout:\n&quot;
 		&quot;    fixmap  : 0x%08lx - 0x%08lx   (%4ld kB)\n&quot;
<span class="p_add">+		&quot;  cpu_entry : 0x%08lx - 0x%08lx   (%4ld kB)\n&quot;</span>
 #ifdef CONFIG_HIGHMEM
 		&quot;    pkmap   : 0x%08lx - 0x%08lx   (%4ld kB)\n&quot;
 #endif
<span class="p_chunk">@@ -777,6 +779,10 @@</span> <span class="p_context"> void __init mem_init(void)</span>
 		FIXADDR_START, FIXADDR_TOP,
 		(FIXADDR_TOP - FIXADDR_START) &gt;&gt; 10,
 
<span class="p_add">+		CPU_ENTRY_AREA_BASE,</span>
<span class="p_add">+		CPU_ENTRY_AREA_BASE + CPU_ENTRY_AREA_MAP_SIZE,</span>
<span class="p_add">+		CPU_ENTRY_AREA_MAP_SIZE &gt;&gt; 10,</span>
<span class="p_add">+</span>
 #ifdef CONFIG_HIGHMEM
 		PKMAP_BASE, PKMAP_BASE+LAST_PKMAP*PAGE_SIZE,
 		(LAST_PKMAP*PAGE_SIZE) &gt;&gt; 10,
<span class="p_header">diff --git a/arch/x86/mm/kasan_init_64.c b/arch/x86/mm/kasan_init_64.c</span>
<span class="p_header">index 9ec70d780f1f..47388f0c0e59 100644</span>
<span class="p_header">--- a/arch/x86/mm/kasan_init_64.c</span>
<span class="p_header">+++ b/arch/x86/mm/kasan_init_64.c</span>
<span class="p_chunk">@@ -15,6 +15,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/tlbflush.h&gt;
 #include &lt;asm/sections.h&gt;
 #include &lt;asm/pgtable.h&gt;
<span class="p_add">+#include &lt;asm/cpu_entry_area.h&gt;</span>
 
 extern struct range pfn_mapped[E820_MAX_ENTRIES];
 
<span class="p_chunk">@@ -322,31 +323,33 @@</span> <span class="p_context"> void __init kasan_init(void)</span>
 		map_range(&amp;pfn_mapped[i]);
 	}
 
<span class="p_del">-	kasan_populate_zero_shadow(</span>
<span class="p_del">-		kasan_mem_to_shadow((void *)PAGE_OFFSET + MAXMEM),</span>
<span class="p_del">-		kasan_mem_to_shadow((void *)__START_KERNEL_map));</span>
<span class="p_del">-</span>
<span class="p_del">-	kasan_populate_shadow((unsigned long)kasan_mem_to_shadow(_stext),</span>
<span class="p_del">-			      (unsigned long)kasan_mem_to_shadow(_end),</span>
<span class="p_del">-			      early_pfn_to_nid(__pa(_stext)));</span>
<span class="p_del">-</span>
<span class="p_del">-	shadow_cpu_entry_begin = (void *)__fix_to_virt(FIX_CPU_ENTRY_AREA_BOTTOM);</span>
<span class="p_add">+	shadow_cpu_entry_begin = (void *)CPU_ENTRY_AREA_BASE;</span>
 	shadow_cpu_entry_begin = kasan_mem_to_shadow(shadow_cpu_entry_begin);
 	shadow_cpu_entry_begin = (void *)round_down((unsigned long)shadow_cpu_entry_begin,
 						PAGE_SIZE);
 
<span class="p_del">-	shadow_cpu_entry_end = (void *)(__fix_to_virt(FIX_CPU_ENTRY_AREA_TOP) + PAGE_SIZE);</span>
<span class="p_add">+	shadow_cpu_entry_end = (void *)(CPU_ENTRY_AREA_BASE +</span>
<span class="p_add">+					CPU_ENTRY_AREA_MAP_SIZE);</span>
 	shadow_cpu_entry_end = kasan_mem_to_shadow(shadow_cpu_entry_end);
 	shadow_cpu_entry_end = (void *)round_up((unsigned long)shadow_cpu_entry_end,
 					PAGE_SIZE);
 
<span class="p_del">-	kasan_populate_zero_shadow(kasan_mem_to_shadow((void *)MODULES_END),</span>
<span class="p_del">-				   shadow_cpu_entry_begin);</span>
<span class="p_add">+	kasan_populate_zero_shadow(</span>
<span class="p_add">+		kasan_mem_to_shadow((void *)PAGE_OFFSET + MAXMEM),</span>
<span class="p_add">+		shadow_cpu_entry_begin);</span>
 
 	kasan_populate_shadow((unsigned long)shadow_cpu_entry_begin,
 			      (unsigned long)shadow_cpu_entry_end, 0);
 
<span class="p_del">-	kasan_populate_zero_shadow(shadow_cpu_entry_end, (void *)KASAN_SHADOW_END);</span>
<span class="p_add">+	kasan_populate_zero_shadow(shadow_cpu_entry_end,</span>
<span class="p_add">+				kasan_mem_to_shadow((void *)__START_KERNEL_map));</span>
<span class="p_add">+</span>
<span class="p_add">+	kasan_populate_shadow((unsigned long)kasan_mem_to_shadow(_stext),</span>
<span class="p_add">+			      (unsigned long)kasan_mem_to_shadow(_end),</span>
<span class="p_add">+			      early_pfn_to_nid(__pa(_stext)));</span>
<span class="p_add">+</span>
<span class="p_add">+	kasan_populate_zero_shadow(kasan_mem_to_shadow((void *)MODULES_END),</span>
<span class="p_add">+				(void *)KASAN_SHADOW_END);</span>
 
 	load_cr3(init_top_pgt);
 	__flush_tlb_all();
<span class="p_header">diff --git a/arch/x86/mm/pgtable_32.c b/arch/x86/mm/pgtable_32.c</span>
<span class="p_header">index 6b9bf023a700..c3c5274410a9 100644</span>
<span class="p_header">--- a/arch/x86/mm/pgtable_32.c</span>
<span class="p_header">+++ b/arch/x86/mm/pgtable_32.c</span>
<span class="p_chunk">@@ -10,6 +10,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/pagemap.h&gt;
 #include &lt;linux/spinlock.h&gt;
 
<span class="p_add">+#include &lt;asm/cpu_entry_area.h&gt;</span>
 #include &lt;asm/pgtable.h&gt;
 #include &lt;asm/pgalloc.h&gt;
 #include &lt;asm/fixmap.h&gt;
<span class="p_header">diff --git a/arch/x86/mm/tlb.c b/arch/x86/mm/tlb.c</span>
<span class="p_header">index 3118392cdf75..0a1be3adc97e 100644</span>
<span class="p_header">--- a/arch/x86/mm/tlb.c</span>
<span class="p_header">+++ b/arch/x86/mm/tlb.c</span>
<span class="p_chunk">@@ -128,7 +128,7 @@</span> <span class="p_context"> void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,</span>
 	 * isn&#39;t free.
 	 */
 #ifdef CONFIG_DEBUG_VM
<span class="p_del">-	if (WARN_ON_ONCE(__read_cr3() != build_cr3(real_prev, prev_asid))) {</span>
<span class="p_add">+	if (WARN_ON_ONCE(__read_cr3() != build_cr3(real_prev-&gt;pgd, prev_asid))) {</span>
 		/*
 		 * If we were to BUG here, we&#39;d be very likely to kill
 		 * the system so hard that we don&#39;t see the call trace.
<span class="p_chunk">@@ -195,7 +195,7 @@</span> <span class="p_context"> void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,</span>
 		if (need_flush) {
 			this_cpu_write(cpu_tlbstate.ctxs[new_asid].ctx_id, next-&gt;context.ctx_id);
 			this_cpu_write(cpu_tlbstate.ctxs[new_asid].tlb_gen, next_tlb_gen);
<span class="p_del">-			write_cr3(build_cr3(next, new_asid));</span>
<span class="p_add">+			write_cr3(build_cr3(next-&gt;pgd, new_asid));</span>
 
 			/*
 			 * NB: This gets called via leave_mm() in the idle path
<span class="p_chunk">@@ -208,7 +208,7 @@</span> <span class="p_context"> void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,</span>
 			trace_tlb_flush_rcuidle(TLB_FLUSH_ON_TASK_SWITCH, TLB_FLUSH_ALL);
 		} else {
 			/* The new ASID is already up to date. */
<span class="p_del">-			write_cr3(build_cr3_noflush(next, new_asid));</span>
<span class="p_add">+			write_cr3(build_cr3_noflush(next-&gt;pgd, new_asid));</span>
 
 			/* See above wrt _rcuidle. */
 			trace_tlb_flush_rcuidle(TLB_FLUSH_ON_TASK_SWITCH, 0);
<span class="p_chunk">@@ -288,7 +288,7 @@</span> <span class="p_context"> void initialize_tlbstate_and_flush(void)</span>
 		!(cr4_read_shadow() &amp; X86_CR4_PCIDE));
 
 	/* Force ASID 0 and force a TLB flush. */
<span class="p_del">-	write_cr3(build_cr3(mm, 0));</span>
<span class="p_add">+	write_cr3(build_cr3(mm-&gt;pgd, 0));</span>
 
 	/* Reinitialize tlbstate. */
 	this_cpu_write(cpu_tlbstate.loaded_mm_asid, 0);
<span class="p_chunk">@@ -551,7 +551,7 @@</span> <span class="p_context"> static void do_kernel_range_flush(void *info)</span>
 
 	/* flush range by one by one &#39;invlpg&#39; */
 	for (addr = f-&gt;start; addr &lt; f-&gt;end; addr += PAGE_SIZE)
<span class="p_del">-		__flush_tlb_single(addr);</span>
<span class="p_add">+		__flush_tlb_one(addr);</span>
 }
 
 void flush_tlb_kernel_range(unsigned long start, unsigned long end)
<span class="p_header">diff --git a/arch/x86/platform/uv/tlb_uv.c b/arch/x86/platform/uv/tlb_uv.c</span>
<span class="p_header">index f44c0bc95aa2..8538a6723171 100644</span>
<span class="p_header">--- a/arch/x86/platform/uv/tlb_uv.c</span>
<span class="p_header">+++ b/arch/x86/platform/uv/tlb_uv.c</span>
<span class="p_chunk">@@ -299,7 +299,7 @@</span> <span class="p_context"> static void bau_process_message(struct msg_desc *mdp, struct bau_control *bcp,</span>
 		local_flush_tlb();
 		stat-&gt;d_alltlb++;
 	} else {
<span class="p_del">-		__flush_tlb_one(msg-&gt;address);</span>
<span class="p_add">+		__flush_tlb_single(msg-&gt;address);</span>
 		stat-&gt;d_onetlb++;
 	}
 	stat-&gt;d_requestee++;
<span class="p_header">diff --git a/arch/x86/xen/mmu_pv.c b/arch/x86/xen/mmu_pv.c</span>
<span class="p_header">index c2454237fa67..a0e2b8c6e5c7 100644</span>
<span class="p_header">--- a/arch/x86/xen/mmu_pv.c</span>
<span class="p_header">+++ b/arch/x86/xen/mmu_pv.c</span>
<span class="p_chunk">@@ -2261,7 +2261,6 @@</span> <span class="p_context"> static void xen_set_fixmap(unsigned idx, phys_addr_t phys, pgprot_t prot)</span>
 
 	switch (idx) {
 	case FIX_BTMAP_END ... FIX_BTMAP_BEGIN:
<span class="p_del">-	case FIX_RO_IDT:</span>
 #ifdef CONFIG_X86_32
 	case FIX_WP_TEST:
 # ifdef CONFIG_HIGHMEM
<span class="p_chunk">@@ -2272,7 +2271,6 @@</span> <span class="p_context"> static void xen_set_fixmap(unsigned idx, phys_addr_t phys, pgprot_t prot)</span>
 #endif
 	case FIX_TEXT_POKE0:
 	case FIX_TEXT_POKE1:
<span class="p_del">-	case FIX_CPU_ENTRY_AREA_TOP ... FIX_CPU_ENTRY_AREA_BOTTOM:</span>
 		/* All local page mappings */
 		pte = pfn_pte(phys, prot);
 		break;
<span class="p_header">diff --git a/include/asm-generic/mm_hooks.h b/include/asm-generic/mm_hooks.h</span>
<span class="p_header">index ea189d88a3cc..8ac4e68a12f0 100644</span>
<span class="p_header">--- a/include/asm-generic/mm_hooks.h</span>
<span class="p_header">+++ b/include/asm-generic/mm_hooks.h</span>
<span class="p_chunk">@@ -7,9 +7,10 @@</span> <span class="p_context"></span>
 #ifndef _ASM_GENERIC_MM_HOOKS_H
 #define _ASM_GENERIC_MM_HOOKS_H
 
<span class="p_del">-static inline void arch_dup_mmap(struct mm_struct *oldmm,</span>
<span class="p_del">-				 struct mm_struct *mm)</span>
<span class="p_add">+static inline int arch_dup_mmap(struct mm_struct *oldmm,</span>
<span class="p_add">+				struct mm_struct *mm)</span>
 {
<span class="p_add">+	return 0;</span>
 }
 
 static inline void arch_exit_mmap(struct mm_struct *mm)
<span class="p_header">diff --git a/include/asm-generic/pgtable.h b/include/asm-generic/pgtable.h</span>
<span class="p_header">index 757dc6ffc7ba..231b35a76dd9 100644</span>
<span class="p_header">--- a/include/asm-generic/pgtable.h</span>
<span class="p_header">+++ b/include/asm-generic/pgtable.h</span>
<span class="p_chunk">@@ -1017,6 +1017,11 @@</span> <span class="p_context"> static inline int pmd_clear_huge(pmd_t *pmd)</span>
 struct file;
 int phys_mem_access_prot_allowed(struct file *file, unsigned long pfn,
 			unsigned long size, pgprot_t *vma_prot);
<span class="p_add">+</span>
<span class="p_add">+#ifndef CONFIG_X86_ESPFIX64</span>
<span class="p_add">+static inline void init_espfix_bsp(void) { }</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 #endif /* !__ASSEMBLY__ */
 
 #ifndef io_remap_pfn_range
<span class="p_header">diff --git a/init/main.c b/init/main.c</span>
<span class="p_header">index 0ee9c6866ada..8a390f60ec81 100644</span>
<span class="p_header">--- a/init/main.c</span>
<span class="p_header">+++ b/init/main.c</span>
<span class="p_chunk">@@ -504,6 +504,8 @@</span> <span class="p_context"> static void __init mm_init(void)</span>
 	pgtable_init();
 	vmalloc_init();
 	ioremap_huge_init();
<span class="p_add">+	/* Should be run before the first non-init thread is created */</span>
<span class="p_add">+	init_espfix_bsp();</span>
 }
 
 asmlinkage __visible void __init start_kernel(void)
<span class="p_chunk">@@ -674,10 +676,6 @@</span> <span class="p_context"> asmlinkage __visible void __init start_kernel(void)</span>
 	if (efi_enabled(EFI_RUNTIME_SERVICES))
 		efi_enter_virtual_mode();
 #endif
<span class="p_del">-#ifdef CONFIG_X86_ESPFIX64</span>
<span class="p_del">-	/* Should be run before the first non-init thread is created */</span>
<span class="p_del">-	init_espfix_bsp();</span>
<span class="p_del">-#endif</span>
 	thread_stack_cache_init();
 	cred_init();
 	fork_init();
<span class="p_header">diff --git a/kernel/fork.c b/kernel/fork.c</span>
<span class="p_header">index 07cc743698d3..500ce64517d9 100644</span>
<span class="p_header">--- a/kernel/fork.c</span>
<span class="p_header">+++ b/kernel/fork.c</span>
<span class="p_chunk">@@ -721,8 +721,7 @@</span> <span class="p_context"> static __latent_entropy int dup_mmap(struct mm_struct *mm,</span>
 			goto out;
 	}
 	/* a new mm has just been created */
<span class="p_del">-	arch_dup_mmap(oldmm, mm);</span>
<span class="p_del">-	retval = 0;</span>
<span class="p_add">+	retval = arch_dup_mmap(oldmm, mm);</span>
 out:
 	up_write(&amp;mm-&gt;mmap_sem);
 	flush_tlb_mm(oldmm);
<span class="p_header">diff --git a/tools/testing/selftests/x86/ldt_gdt.c b/tools/testing/selftests/x86/ldt_gdt.c</span>
<span class="p_header">index 66e5ce5b91f0..0304ffb714f2 100644</span>
<span class="p_header">--- a/tools/testing/selftests/x86/ldt_gdt.c</span>
<span class="p_header">+++ b/tools/testing/selftests/x86/ldt_gdt.c</span>
<span class="p_chunk">@@ -627,13 +627,10 @@</span> <span class="p_context"> static void do_multicpu_tests(void)</span>
 static int finish_exec_test(void)
 {
 	/*
<span class="p_del">-	 * In a sensible world, this would be check_invalid_segment(0, 1);</span>
<span class="p_del">-	 * For better or for worse, though, the LDT is inherited across exec.</span>
<span class="p_del">-	 * We can probably change this safely, but for now we test it.</span>
<span class="p_add">+	 * Older kernel versions did inherit the LDT on exec() which is</span>
<span class="p_add">+	 * wrong because exec() starts from a clean state.</span>
 	 */
<span class="p_del">-	check_valid_segment(0, 1,</span>
<span class="p_del">-			    AR_DPL3 | AR_TYPE_XRCODE | AR_S | AR_P | AR_DB,</span>
<span class="p_del">-			    42, true);</span>
<span class="p_add">+	check_invalid_segment(0, 1);</span>
 
 	return nerrs ? 1 : 0;
 }

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



