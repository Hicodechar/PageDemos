
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v2,3/3] powerpc/powernv: Introduce address translation services for Nvlink2 - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v2,3/3] powerpc/powernv: Introduce address translation services for Nvlink2</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=80071">Alistair Popple</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>April 3, 2017, 9:51 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1491213104-24450-3-git-send-email-alistair@popple.id.au&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9659177/mbox/"
   >mbox</a>
|
   <a href="/patch/9659177/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9659177/">/patch/9659177/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	13B2B60353 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  3 Apr 2017 09:52:08 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id E5AF528448
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  3 Apr 2017 09:52:07 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id DA29E284D3; Mon,  3 Apr 2017 09:52:07 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 8CFC628448
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  3 Apr 2017 09:52:06 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752270AbdDCJvz (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 3 Apr 2017 05:51:55 -0400
Received: from ozlabs.org ([103.22.144.67]:39353 &quot;EHLO ozlabs.org&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1751920AbdDCJvw (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 3 Apr 2017 05:51:52 -0400
Received: from authenticated.ozlabs.org (localhost [127.0.0.1])
	(using TLSv1.2 with cipher ECDHE-RSA-AES128-GCM-SHA256 (128/128
	bits)) (No client certificate requested)
	by ozlabs.org (Postfix) with ESMTPSA id 3vxS7946nGz9s81;
	Mon,  3 Apr 2017 19:51:49 +1000 (AEST)
From: Alistair Popple &lt;alistair@popple.id.au&gt;
To: mpe@ellerman.id.au
Cc: linuxppc-dev@ozlabs.org, linux-kernel@vger.kernel.org,
	devicetree@vger.kernel.org, mhairgrove@nvidia.com,
	robh+dt@kernel.org, shailendras@nvidia.com,
	Alistair Popple &lt;alistair@popple.id.au&gt;
Subject: [PATCH v2 3/3] powerpc/powernv: Introduce address translation
	services for Nvlink2
Date: Mon,  3 Apr 2017 19:51:44 +1000
Message-Id: &lt;1491213104-24450-3-git-send-email-alistair@popple.id.au&gt;
X-Mailer: git-send-email 2.1.4
In-Reply-To: &lt;1491213104-24450-1-git-send-email-alistair@popple.id.au&gt;
References: &lt;1491213104-24450-1-git-send-email-alistair@popple.id.au&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=80071">Alistair Popple</a> - April 3, 2017, 9:51 a.m.</div>
<pre class="content">
Nvlink2 supports address translation services (ATS) allowing devices
to request address translations from an mmu known as the nest MMU
which is setup to walk the CPU page tables.

To access this functionality certain firmware calls are required to
setup and manage hardware context tables in the nvlink processing unit
(NPU). The NPU also manages forwarding of TLB invalidates (known as
address translation shootdowns/ATSDs) to attached devices.

This patch exports several methods to allow device drivers to register
a process id (PASID/PID) in the hardware tables and to receive
notification of when a device should stop issuing address translation
requests (ATRs). It also adds a fault handler to allow device drivers
to demand fault pages in.
<span class="signed-off-by">
Signed-off-by: Alistair Popple &lt;alistair@popple.id.au&gt;</span>
---

Changes since v1:

 - Moved exported function call prototypes to an externally accessable
   header.
 - Fixed invalidation of the NMMU PWC (this also fixes the
   reported build failure).

 arch/powerpc/include/asm/book3s/64/mmu.h       |   6 +
 arch/powerpc/include/asm/opal-api.h            |   5 +-
 arch/powerpc/include/asm/opal.h                |   5 +
 arch/powerpc/include/asm/powernv.h             |  22 ++
 arch/powerpc/mm/mmu_context_book3s64.c         |   1 +
 arch/powerpc/platforms/powernv/npu-dma.c       | 443 +++++++++++++++++++++++++
 arch/powerpc/platforms/powernv/opal-wrappers.S |   3 +
 arch/powerpc/platforms/powernv/pci-ioda.c      |   2 +
 arch/powerpc/platforms/powernv/pci.h           |  15 +-
 9 files changed, 500 insertions(+), 2 deletions(-)

--
2.1.4
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=169345">Michael Ellerman</a> - April 6, 2017, 1:06 p.m.</div>
<pre class="content">
On Mon, 2017-04-03 at 09:51:44 UTC, Alistair Popple wrote:
<span class="quote">&gt; Nvlink2 supports address translation services (ATS) allowing devices</span>
<span class="quote">&gt; to request address translations from an mmu known as the nest MMU</span>
<span class="quote">&gt; which is setup to walk the CPU page tables.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; To access this functionality certain firmware calls are required to</span>
<span class="quote">&gt; setup and manage hardware context tables in the nvlink processing unit</span>
<span class="quote">&gt; (NPU). The NPU also manages forwarding of TLB invalidates (known as</span>
<span class="quote">&gt; address translation shootdowns/ATSDs) to attached devices.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This patch exports several methods to allow device drivers to register</span>
<span class="quote">&gt; a process id (PASID/PID) in the hardware tables and to receive</span>
<span class="quote">&gt; notification of when a device should stop issuing address translation</span>
<span class="quote">&gt; requests (ATRs). It also adds a fault handler to allow device drivers</span>
<span class="quote">&gt; to demand fault pages in.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Alistair Popple &lt;alistair@popple.id.au&gt;</span>

Applied to powerpc next, thanks.

https://git.kernel.org/powerpc/c/1ab66d1fbadad86b1f4a9c7857e193

cheers
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/powerpc/include/asm/book3s/64/mmu.h b/arch/powerpc/include/asm/book3s/64/mmu.h</span>
<span class="p_header">index 805d4105..1676ec8 100644</span>
<span class="p_header">--- a/arch/powerpc/include/asm/book3s/64/mmu.h</span>
<span class="p_header">+++ b/arch/powerpc/include/asm/book3s/64/mmu.h</span>
<span class="p_chunk">@@ -73,10 +73,16 @@</span> <span class="p_context"> extern struct patb_entry *partition_tb;</span>
 typedef unsigned long mm_context_id_t;
 struct spinlock;

<span class="p_add">+/* Maximum possible number of NPUs in a system. */</span>
<span class="p_add">+#define NV_MAX_NPUS 8</span>
<span class="p_add">+</span>
 typedef struct {
 	mm_context_id_t id;
 	u16 user_psize;		/* page size index */

<span class="p_add">+	/* NPU NMMU context */</span>
<span class="p_add">+	struct npu_context *npu_context;</span>
<span class="p_add">+</span>
 #ifdef CONFIG_PPC_MM_SLICES
 	u64 low_slices_psize;	/* SLB page size encodings */
 	unsigned char high_slices_psize[SLICE_ARRAY_SIZE];
<span class="p_header">diff --git a/arch/powerpc/include/asm/opal-api.h b/arch/powerpc/include/asm/opal-api.h</span>
<span class="p_header">index a0aa285..a599a2c 100644</span>
<span class="p_header">--- a/arch/powerpc/include/asm/opal-api.h</span>
<span class="p_header">+++ b/arch/powerpc/include/asm/opal-api.h</span>
<span class="p_chunk">@@ -168,7 +168,10 @@</span> <span class="p_context"></span>
 #define OPAL_INT_SET_MFRR			125
 #define OPAL_PCI_TCE_KILL			126
 #define OPAL_NMMU_SET_PTCR			127
<span class="p_del">-#define OPAL_LAST				127</span>
<span class="p_add">+#define OPAL_NPU_INIT_CONTEXT			146</span>
<span class="p_add">+#define OPAL_NPU_DESTROY_CONTEXT		147</span>
<span class="p_add">+#define OPAL_NPU_MAP_LPAR			148</span>
<span class="p_add">+#define OPAL_LAST				148</span>

 /* Device tree flags */

<span class="p_header">diff --git a/arch/powerpc/include/asm/opal.h b/arch/powerpc/include/asm/opal.h</span>
<span class="p_header">index 1ff03a6..b3b97c4 100644</span>
<span class="p_header">--- a/arch/powerpc/include/asm/opal.h</span>
<span class="p_header">+++ b/arch/powerpc/include/asm/opal.h</span>
<span class="p_chunk">@@ -29,6 +29,11 @@</span> <span class="p_context"> extern struct device_node *opal_node;</span>

 /* API functions */
 int64_t opal_invalid_call(void);
<span class="p_add">+int64_t opal_npu_destroy_context(uint64_t phb_id, uint64_t pid, uint64_t bdf);</span>
<span class="p_add">+int64_t opal_npu_init_context(uint64_t phb_id, int pasid, uint64_t msr,</span>
<span class="p_add">+			uint64_t bdf);</span>
<span class="p_add">+int64_t opal_npu_map_lpar(uint64_t phb_id, uint64_t bdf, uint64_t lparid,</span>
<span class="p_add">+			uint64_t lpcr);</span>
 int64_t opal_console_write(int64_t term_number, __be64 *length,
 			   const uint8_t *buffer);
 int64_t opal_console_read(int64_t term_number, __be64 *length,
<span class="p_header">diff --git a/arch/powerpc/include/asm/powernv.h b/arch/powerpc/include/asm/powernv.h</span>
<span class="p_header">index 0e9c240..f627977 100644</span>
<span class="p_header">--- a/arch/powerpc/include/asm/powernv.h</span>
<span class="p_header">+++ b/arch/powerpc/include/asm/powernv.h</span>
<span class="p_chunk">@@ -11,9 +11,31 @@</span> <span class="p_context"></span>
 #define _ASM_POWERNV_H

 #ifdef CONFIG_PPC_POWERNV
<span class="p_add">+#define NPU2_WRITE 1</span>
 extern void powernv_set_nmmu_ptcr(unsigned long ptcr);
<span class="p_add">+extern struct npu_context *pnv_npu2_init_context(struct pci_dev *gpdev,</span>
<span class="p_add">+			unsigned long flags,</span>
<span class="p_add">+			struct npu_context *(*cb)(struct npu_context *, void *),</span>
<span class="p_add">+			void *priv);</span>
<span class="p_add">+extern void pnv_npu2_destroy_context(struct npu_context *context,</span>
<span class="p_add">+				struct pci_dev *gpdev);</span>
<span class="p_add">+extern int pnv_npu2_handle_fault(struct npu_context *context, uintptr_t *ea,</span>
<span class="p_add">+				unsigned long *flags, unsigned long *status,</span>
<span class="p_add">+				int count);</span>
 #else
 static inline void powernv_set_nmmu_ptcr(unsigned long ptcr) { }
<span class="p_add">+static inline struct npu_context *pnv_npu2_init_context(struct pci_dev *gpdev,</span>
<span class="p_add">+			unsigned long flags,</span>
<span class="p_add">+			struct npu_context *(*cb)(struct npu_context *, void *),</span>
<span class="p_add">+			void *priv) { return ERR_PTR(-ENODEV); }</span>
<span class="p_add">+static inline void pnv_npu2_destroy_context(struct npu_context *context,</span>
<span class="p_add">+					struct pci_dev *gpdev) { }</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int pnv_npu2_handle_fault(struct npu_context *context,</span>
<span class="p_add">+					uintptr_t *ea, unsigned long *flags,</span>
<span class="p_add">+					unsigned long *status, int count) {</span>
<span class="p_add">+	return -ENODEV;</span>
<span class="p_add">+}</span>
 #endif

 #endif /* _ASM_POWERNV_H */
<span class="p_header">diff --git a/arch/powerpc/mm/mmu_context_book3s64.c b/arch/powerpc/mm/mmu_context_book3s64.c</span>
<span class="p_header">index 73bf6e1..eb317f1 100644</span>
<span class="p_header">--- a/arch/powerpc/mm/mmu_context_book3s64.c</span>
<span class="p_header">+++ b/arch/powerpc/mm/mmu_context_book3s64.c</span>
<span class="p_chunk">@@ -67,6 +67,7 @@</span> <span class="p_context"> static int radix__init_new_context(struct mm_struct *mm, int index)</span>
 	 */
 	rts_field = radix__get_tree_size();
 	process_tb[index].prtb0 = cpu_to_be64(rts_field | __pa(mm-&gt;pgd) | RADIX_PGD_INDEX_SIZE);
<span class="p_add">+	mm-&gt;context.npu_context = NULL;</span>
 	return 0;
 }

<span class="p_header">diff --git a/arch/powerpc/platforms/powernv/npu-dma.c b/arch/powerpc/platforms/powernv/npu-dma.c</span>
<span class="p_header">index 050bd5d..7392520 100644</span>
<span class="p_header">--- a/arch/powerpc/platforms/powernv/npu-dma.c</span>
<span class="p_header">+++ b/arch/powerpc/platforms/powernv/npu-dma.c</span>
<span class="p_chunk">@@ -9,11 +9,20 @@</span> <span class="p_context"></span>
  * License as published by the Free Software Foundation.
  */

<span class="p_add">+#include &lt;linux/slab.h&gt;</span>
<span class="p_add">+#include &lt;linux/mmu_notifier.h&gt;</span>
<span class="p_add">+#include &lt;linux/mmu_context.h&gt;</span>
<span class="p_add">+#include &lt;linux/of.h&gt;</span>
 #include &lt;linux/export.h&gt;
 #include &lt;linux/pci.h&gt;
 #include &lt;linux/memblock.h&gt;
 #include &lt;linux/iommu.h&gt;

<span class="p_add">+#include &lt;asm/tlb.h&gt;</span>
<span class="p_add">+#include &lt;asm/powernv.h&gt;</span>
<span class="p_add">+#include &lt;asm/reg.h&gt;</span>
<span class="p_add">+#include &lt;asm/opal.h&gt;</span>
<span class="p_add">+#include &lt;asm/io.h&gt;</span>
 #include &lt;asm/iommu.h&gt;
 #include &lt;asm/pnv-pci.h&gt;
 #include &lt;asm/msi_bitmap.h&gt;
<span class="p_chunk">@@ -22,6 +31,8 @@</span> <span class="p_context"></span>
 #include &quot;powernv.h&quot;
 #include &quot;pci.h&quot;

<span class="p_add">+#define npu_to_phb(x) container_of(x, struct pnv_phb, npu)</span>
<span class="p_add">+</span>
 /*
  * Other types of TCE cache invalidation are not functional in the
  * hardware.
<span class="p_chunk">@@ -371,3 +382,435 @@</span> <span class="p_context"> struct pnv_ioda_pe *pnv_pci_npu_setup_iommu(struct pnv_ioda_pe *npe)</span>

 	return gpe;
 }
<span class="p_add">+</span>
<span class="p_add">+/* Maximum number of nvlinks per npu */</span>
<span class="p_add">+#define NV_MAX_LINKS 6</span>
<span class="p_add">+</span>
<span class="p_add">+/* Maximum index of npu2 hosts in the system. Always &lt; NV_MAX_NPUS */</span>
<span class="p_add">+static int max_npu2_index;</span>
<span class="p_add">+</span>
<span class="p_add">+struct npu_context {</span>
<span class="p_add">+	struct mm_struct *mm;</span>
<span class="p_add">+	struct pci_dev *npdev[NV_MAX_NPUS][NV_MAX_LINKS];</span>
<span class="p_add">+	struct mmu_notifier mn;</span>
<span class="p_add">+	struct kref kref;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Callback to stop translation requests on a given gpu */</span>
<span class="p_add">+	struct npu_context *(*release_cb)(struct npu_context *, void *);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Private pointer passed to the above callback for usage by</span>
<span class="p_add">+	 * device drivers</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	void *priv;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Find a free MMIO ATSD register and mark it in use. Return -ENOSPC</span>
<span class="p_add">+ * if none are available.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static int get_mmio_atsd_reg(struct npu *npu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i &lt; npu-&gt;mmio_atsd_count; i++) {</span>
<span class="p_add">+		if (!test_and_set_bit(i, &amp;npu-&gt;mmio_atsd_usage))</span>
<span class="p_add">+			return i;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return -ENOSPC;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void put_mmio_atsd_reg(struct npu *npu, int reg)</span>
<span class="p_add">+{</span>
<span class="p_add">+	clear_bit(reg, &amp;npu-&gt;mmio_atsd_usage);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* MMIO ATSD register offsets */</span>
<span class="p_add">+#define XTS_ATSD_AVA  1</span>
<span class="p_add">+#define XTS_ATSD_STAT 2</span>
<span class="p_add">+</span>
<span class="p_add">+static int mmio_launch_invalidate(struct npu *npu, unsigned long launch,</span>
<span class="p_add">+				unsigned long va)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int mmio_atsd_reg;</span>
<span class="p_add">+</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		mmio_atsd_reg = get_mmio_atsd_reg(npu);</span>
<span class="p_add">+		cpu_relax();</span>
<span class="p_add">+	} while (mmio_atsd_reg &lt; 0);</span>
<span class="p_add">+</span>
<span class="p_add">+	__raw_writeq(cpu_to_be64(va),</span>
<span class="p_add">+		npu-&gt;mmio_atsd_regs[mmio_atsd_reg] + XTS_ATSD_AVA);</span>
<span class="p_add">+	eieio();</span>
<span class="p_add">+	__raw_writeq(cpu_to_be64(launch), npu-&gt;mmio_atsd_regs[mmio_atsd_reg]);</span>
<span class="p_add">+</span>
<span class="p_add">+	return mmio_atsd_reg;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int mmio_invalidate_pid(struct npu *npu, unsigned long pid)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long launch;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* IS set to invalidate matching PID */</span>
<span class="p_add">+	launch = PPC_BIT(12);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* PRS set to process-scoped */</span>
<span class="p_add">+	launch |= PPC_BIT(13);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* AP */</span>
<span class="p_add">+	launch |= (u64) mmu_get_ap(mmu_virtual_psize) &lt;&lt; PPC_BITLSHIFT(17);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* PID */</span>
<span class="p_add">+	launch |= pid &lt;&lt; PPC_BITLSHIFT(38);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Invalidating the entire process doesn&#39;t use a va */</span>
<span class="p_add">+	return mmio_launch_invalidate(npu, launch, 0);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int mmio_invalidate_va(struct npu *npu, unsigned long va,</span>
<span class="p_add">+			unsigned long pid)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long launch;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* IS set to invalidate target VA */</span>
<span class="p_add">+	launch = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* PRS set to process scoped */</span>
<span class="p_add">+	launch |= PPC_BIT(13);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* AP */</span>
<span class="p_add">+	launch |= (u64) mmu_get_ap(mmu_virtual_psize) &lt;&lt; PPC_BITLSHIFT(17);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* PID */</span>
<span class="p_add">+	launch |= pid &lt;&lt; PPC_BITLSHIFT(38);</span>
<span class="p_add">+</span>
<span class="p_add">+	return mmio_launch_invalidate(npu, launch, va);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define mn_to_npu_context(x) container_of(x, struct npu_context, mn)</span>
<span class="p_add">+</span>
<span class="p_add">+/* Invalidate either a single address or an entire PID depending on</span>
<span class="p_add">+ * the value of va.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static void mmio_invalidate(struct npu_context *npu_context, int va,</span>
<span class="p_add">+			unsigned long address)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int i, j, reg;</span>
<span class="p_add">+	struct npu *npu;</span>
<span class="p_add">+	struct pnv_phb *nphb;</span>
<span class="p_add">+	struct pci_dev *npdev;</span>
<span class="p_add">+	struct {</span>
<span class="p_add">+		struct npu *npu;</span>
<span class="p_add">+		int reg;</span>
<span class="p_add">+	} mmio_atsd_reg[NV_MAX_NPUS];</span>
<span class="p_add">+	unsigned long pid = npu_context-&gt;mm-&gt;context.id;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Loop over all the NPUs this process is active on and launch</span>
<span class="p_add">+	 * an invalidate.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	for (i = 0; i &lt;= max_npu2_index; i++) {</span>
<span class="p_add">+		mmio_atsd_reg[i].reg = -1;</span>
<span class="p_add">+		for (j = 0; j &lt; NV_MAX_LINKS; j++) {</span>
<span class="p_add">+			npdev = npu_context-&gt;npdev[i][j];</span>
<span class="p_add">+			if (!npdev)</span>
<span class="p_add">+				continue;</span>
<span class="p_add">+</span>
<span class="p_add">+			nphb = pci_bus_to_host(npdev-&gt;bus)-&gt;private_data;</span>
<span class="p_add">+			npu = &amp;nphb-&gt;npu;</span>
<span class="p_add">+			mmio_atsd_reg[i].npu = npu;</span>
<span class="p_add">+</span>
<span class="p_add">+			if (va)</span>
<span class="p_add">+				mmio_atsd_reg[i].reg =</span>
<span class="p_add">+					mmio_invalidate_va(npu, address, pid);</span>
<span class="p_add">+			else</span>
<span class="p_add">+				mmio_atsd_reg[i].reg =</span>
<span class="p_add">+					mmio_invalidate_pid(npu, pid);</span>
<span class="p_add">+</span>
<span class="p_add">+			/* The NPU hardware forwards the shootdown to</span>
<span class="p_add">+			 * all GPUs so we only have to launch one</span>
<span class="p_add">+			 * shootdown per NPU.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Unfortunately the nest mmu does not support flushing</span>
<span class="p_add">+	 * specific addresses so we have to flush the whole mm.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	radix__flush_tlb_mm(npu_context-&gt;mm);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Wait for all invalidations to complete */</span>
<span class="p_add">+	for (i = 0; i &lt;= max_npu2_index; i++) {</span>
<span class="p_add">+		if (mmio_atsd_reg[i].reg &lt; 0)</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		/* Wait for completion */</span>
<span class="p_add">+		npu = mmio_atsd_reg[i].npu;</span>
<span class="p_add">+		reg = mmio_atsd_reg[i].reg;</span>
<span class="p_add">+		while (__raw_readq(npu-&gt;mmio_atsd_regs[reg] + XTS_ATSD_STAT))</span>
<span class="p_add">+			cpu_relax();</span>
<span class="p_add">+		put_mmio_atsd_reg(npu, reg);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void pnv_npu2_mn_release(struct mmu_notifier *mn,</span>
<span class="p_add">+				struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct npu_context *npu_context = mn_to_npu_context(mn);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Call into device driver to stop requests to the NMMU */</span>
<span class="p_add">+	if (npu_context-&gt;release_cb)</span>
<span class="p_add">+		npu_context-&gt;release_cb(npu_context, npu_context-&gt;priv);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* There should be no more translation requests for this PID,</span>
<span class="p_add">+	 * but we need to ensure any entries for it are removed from</span>
<span class="p_add">+	 * the TLB.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	mmio_invalidate(npu_context, 0, 0);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void pnv_npu2_mn_change_pte(struct mmu_notifier *mn,</span>
<span class="p_add">+				struct mm_struct *mm,</span>
<span class="p_add">+				unsigned long address,</span>
<span class="p_add">+				pte_t pte)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct npu_context *npu_context = mn_to_npu_context(mn);</span>
<span class="p_add">+</span>
<span class="p_add">+	mmio_invalidate(npu_context, 1, address);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void pnv_npu2_mn_invalidate_page(struct mmu_notifier *mn,</span>
<span class="p_add">+					struct mm_struct *mm,</span>
<span class="p_add">+					unsigned long address)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct npu_context *npu_context = mn_to_npu_context(mn);</span>
<span class="p_add">+</span>
<span class="p_add">+	mmio_invalidate(npu_context, 1, address);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void pnv_npu2_mn_invalidate_range(struct mmu_notifier *mn,</span>
<span class="p_add">+					struct mm_struct *mm,</span>
<span class="p_add">+					unsigned long start, unsigned long end)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct npu_context *npu_context = mn_to_npu_context(mn);</span>
<span class="p_add">+	unsigned long address;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (address = start; address &lt;= end; address += PAGE_SIZE)</span>
<span class="p_add">+		mmio_invalidate(npu_context, 1, address);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static const struct mmu_notifier_ops nv_nmmu_notifier_ops = {</span>
<span class="p_add">+	.release = pnv_npu2_mn_release,</span>
<span class="p_add">+	.change_pte = pnv_npu2_mn_change_pte,</span>
<span class="p_add">+	.invalidate_page = pnv_npu2_mn_invalidate_page,</span>
<span class="p_add">+	.invalidate_range = pnv_npu2_mn_invalidate_range,</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Call into OPAL to setup the nmmu context for the current task in</span>
<span class="p_add">+ * the NPU. This must be called to setup the context tables before the</span>
<span class="p_add">+ * GPU issues ATRs. pdev should be a pointed to PCIe GPU device.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * A release callback should be registered to allow a device driver to</span>
<span class="p_add">+ * be notified that it should not launch any new translation requests</span>
<span class="p_add">+ * as the final TLB invalidate is about to occur.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Returns an error if there no contexts are currently available or a</span>
<span class="p_add">+ * npu_context which should be passed to pnv_npu2_handle_fault().</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * mmap_sem must be held in write mode.</span>
<span class="p_add">+ */</span>
<span class="p_add">+struct npu_context *pnv_npu2_init_context(struct pci_dev *gpdev,</span>
<span class="p_add">+			unsigned long flags,</span>
<span class="p_add">+			struct npu_context *(*cb)(struct npu_context *, void *),</span>
<span class="p_add">+			void *priv)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int rc;</span>
<span class="p_add">+	u32 nvlink_index;</span>
<span class="p_add">+	struct device_node *nvlink_dn;</span>
<span class="p_add">+	struct mm_struct *mm = current-&gt;mm;</span>
<span class="p_add">+	struct pnv_phb *nphb;</span>
<span class="p_add">+	struct npu *npu;</span>
<span class="p_add">+	struct npu_context *npu_context;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* At present we don&#39;t support gpus connected to multiple NPUs</span>
<span class="p_add">+	 * and I&#39;m not sure the hardware does either.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	struct pci_dev *npdev = pnv_pci_get_npu_dev(gpdev, 0);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!firmware_has_feature(FW_FEATURE_OPAL))</span>
<span class="p_add">+		return ERR_PTR(-ENODEV);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!npdev)</span>
<span class="p_add">+		/* No nvlink associated with this GPU device */</span>
<span class="p_add">+		return ERR_PTR(-ENODEV);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!mm) {</span>
<span class="p_add">+		/* kernel thread contexts are not supported */</span>
<span class="p_add">+		return ERR_PTR(-EINVAL);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	nphb = pci_bus_to_host(npdev-&gt;bus)-&gt;private_data;</span>
<span class="p_add">+	npu = &amp;nphb-&gt;npu;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Setup the NPU context table for a particular GPU. These</span>
<span class="p_add">+	 * need to be per-GPU as we need the tables to filter ATSDs</span>
<span class="p_add">+	 * when there are no active contexts on a particular GPU.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	rc = opal_npu_init_context(nphb-&gt;opal_id, mm-&gt;context.id, flags,</span>
<span class="p_add">+				PCI_DEVID(gpdev-&gt;bus-&gt;number, gpdev-&gt;devfn));</span>
<span class="p_add">+	if (rc &lt; 0)</span>
<span class="p_add">+		return ERR_PTR(-ENOSPC);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* We store the npu pci device so we can more easily get at</span>
<span class="p_add">+	 * the associated npus.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	npu_context = mm-&gt;context.npu_context;</span>
<span class="p_add">+	if (!npu_context) {</span>
<span class="p_add">+		npu_context = kzalloc(sizeof(struct npu_context), GFP_KERNEL);</span>
<span class="p_add">+		if (!npu_context)</span>
<span class="p_add">+			return ERR_PTR(-ENOMEM);</span>
<span class="p_add">+</span>
<span class="p_add">+		mm-&gt;context.npu_context = npu_context;</span>
<span class="p_add">+		npu_context-&gt;mm = mm;</span>
<span class="p_add">+		npu_context-&gt;mn.ops = &amp;nv_nmmu_notifier_ops;</span>
<span class="p_add">+		__mmu_notifier_register(&amp;npu_context-&gt;mn, mm);</span>
<span class="p_add">+		kref_init(&amp;npu_context-&gt;kref);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		kref_get(&amp;npu_context-&gt;kref);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	npu_context-&gt;release_cb = cb;</span>
<span class="p_add">+	npu_context-&gt;priv = priv;</span>
<span class="p_add">+	nvlink_dn = of_parse_phandle(npdev-&gt;dev.of_node, &quot;ibm,nvlink&quot;, 0);</span>
<span class="p_add">+	if (WARN_ON(of_property_read_u32(nvlink_dn, &quot;ibm,npu-link-index&quot;,</span>
<span class="p_add">+							&amp;nvlink_index)))</span>
<span class="p_add">+		return ERR_PTR(-ENODEV);</span>
<span class="p_add">+	npu_context-&gt;npdev[npu-&gt;index][nvlink_index] = npdev;</span>
<span class="p_add">+</span>
<span class="p_add">+	return npu_context;</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(pnv_npu2_init_context);</span>
<span class="p_add">+</span>
<span class="p_add">+static void pnv_npu2_release_context(struct kref *kref)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct npu_context *npu_context =</span>
<span class="p_add">+		container_of(kref, struct npu_context, kref);</span>
<span class="p_add">+</span>
<span class="p_add">+	npu_context-&gt;mm-&gt;context.npu_context = NULL;</span>
<span class="p_add">+	mmu_notifier_unregister(&amp;npu_context-&gt;mn,</span>
<span class="p_add">+				npu_context-&gt;mm);</span>
<span class="p_add">+</span>
<span class="p_add">+	kfree(npu_context);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void pnv_npu2_destroy_context(struct npu_context *npu_context,</span>
<span class="p_add">+			struct pci_dev *gpdev)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct pnv_phb *nphb, *phb;</span>
<span class="p_add">+	struct npu *npu;</span>
<span class="p_add">+	struct pci_dev *npdev = pnv_pci_get_npu_dev(gpdev, 0);</span>
<span class="p_add">+	struct device_node *nvlink_dn;</span>
<span class="p_add">+	u32 nvlink_index;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (WARN_ON(!npdev))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!firmware_has_feature(FW_FEATURE_OPAL))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	nphb = pci_bus_to_host(npdev-&gt;bus)-&gt;private_data;</span>
<span class="p_add">+	npu = &amp;nphb-&gt;npu;</span>
<span class="p_add">+	phb = pci_bus_to_host(gpdev-&gt;bus)-&gt;private_data;</span>
<span class="p_add">+	nvlink_dn = of_parse_phandle(npdev-&gt;dev.of_node, &quot;ibm,nvlink&quot;, 0);</span>
<span class="p_add">+	if (WARN_ON(of_property_read_u32(nvlink_dn, &quot;ibm,npu-link-index&quot;,</span>
<span class="p_add">+							&amp;nvlink_index)))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+	npu_context-&gt;npdev[npu-&gt;index][nvlink_index] = NULL;</span>
<span class="p_add">+	opal_npu_destroy_context(phb-&gt;opal_id, npu_context-&gt;mm-&gt;context.id,</span>
<span class="p_add">+				PCI_DEVID(gpdev-&gt;bus-&gt;number, gpdev-&gt;devfn));</span>
<span class="p_add">+	kref_put(&amp;npu_context-&gt;kref, pnv_npu2_release_context);</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(pnv_npu2_destroy_context);</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Assumes mmap_sem is held for the contexts associated mm.</span>
<span class="p_add">+ */</span>
<span class="p_add">+int pnv_npu2_handle_fault(struct npu_context *context, uintptr_t *ea,</span>
<span class="p_add">+			unsigned long *flags, unsigned long *status, int count)</span>
<span class="p_add">+{</span>
<span class="p_add">+	u64 rc = 0, result = 0;</span>
<span class="p_add">+	int i, is_write;</span>
<span class="p_add">+	struct page *page[1];</span>
<span class="p_add">+</span>
<span class="p_add">+	/* mmap_sem should be held so the struct_mm must be present */</span>
<span class="p_add">+	struct mm_struct *mm = context-&gt;mm;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!firmware_has_feature(FW_FEATURE_OPAL))</span>
<span class="p_add">+		return -ENODEV;</span>
<span class="p_add">+</span>
<span class="p_add">+	WARN_ON(!rwsem_is_locked(&amp;mm-&gt;mmap_sem));</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i &lt; count; i++) {</span>
<span class="p_add">+		is_write = flags[i] &amp; NPU2_WRITE;</span>
<span class="p_add">+		rc = get_user_pages_remote(NULL, mm, ea[i], 1,</span>
<span class="p_add">+					is_write ? FOLL_WRITE : 0,</span>
<span class="p_add">+					page, NULL, NULL);</span>
<span class="p_add">+</span>
<span class="p_add">+		/* To support virtualised environments we will have to</span>
<span class="p_add">+		 * do an access to the page to ensure it gets faulted</span>
<span class="p_add">+		 * into the hypervisor. For the moment virtualisation</span>
<span class="p_add">+		 * is not supported in other areas so leave the access</span>
<span class="p_add">+		 * out.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (rc != 1) {</span>
<span class="p_add">+			status[i] = rc;</span>
<span class="p_add">+			result = -EFAULT;</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		status[i] = 0;</span>
<span class="p_add">+		put_page(page[0]);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return result;</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(pnv_npu2_handle_fault);</span>
<span class="p_add">+</span>
<span class="p_add">+int pnv_npu2_init(struct pnv_phb *phb)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned int i;</span>
<span class="p_add">+	u64 mmio_atsd;</span>
<span class="p_add">+	struct device_node *dn;</span>
<span class="p_add">+	struct pci_dev *gpdev;</span>
<span class="p_add">+	static int npu_index;</span>
<span class="p_add">+	uint64_t rc = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	for_each_child_of_node(phb-&gt;hose-&gt;dn, dn) {</span>
<span class="p_add">+		gpdev = pnv_pci_get_gpu_dev(get_pci_dev(dn));</span>
<span class="p_add">+		if (gpdev) {</span>
<span class="p_add">+			rc = opal_npu_map_lpar(phb-&gt;opal_id,</span>
<span class="p_add">+				PCI_DEVID(gpdev-&gt;bus-&gt;number, gpdev-&gt;devfn),</span>
<span class="p_add">+				0, 0);</span>
<span class="p_add">+			if (rc)</span>
<span class="p_add">+				dev_err(&amp;gpdev-&gt;dev,</span>
<span class="p_add">+					&quot;Error %lld mapping device to LPAR\n&quot;,</span>
<span class="p_add">+					rc);</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; !of_property_read_u64_index(phb-&gt;hose-&gt;dn, &quot;ibm,mmio-atsd&quot;,</span>
<span class="p_add">+							i, &amp;mmio_atsd); i++)</span>
<span class="p_add">+		phb-&gt;npu.mmio_atsd_regs[i] = ioremap(mmio_atsd, 32);</span>
<span class="p_add">+</span>
<span class="p_add">+	pr_info(&quot;NPU%lld: Found %d MMIO ATSD registers&quot;, phb-&gt;opal_id, i);</span>
<span class="p_add">+	phb-&gt;npu.mmio_atsd_count = i;</span>
<span class="p_add">+	phb-&gt;npu.mmio_atsd_usage = 0;</span>
<span class="p_add">+	npu_index++;</span>
<span class="p_add">+	if (WARN_ON(npu_index &gt;= NV_MAX_NPUS))</span>
<span class="p_add">+		return -ENOSPC;</span>
<span class="p_add">+	max_npu2_index = npu_index;</span>
<span class="p_add">+	phb-&gt;npu.index = npu_index;</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_header">diff --git a/arch/powerpc/platforms/powernv/opal-wrappers.S b/arch/powerpc/platforms/powernv/opal-wrappers.S</span>
<span class="p_header">index da8a0f7..085677bfe 100644</span>
<span class="p_header">--- a/arch/powerpc/platforms/powernv/opal-wrappers.S</span>
<span class="p_header">+++ b/arch/powerpc/platforms/powernv/opal-wrappers.S</span>
<span class="p_chunk">@@ -301,3 +301,6 @@</span> <span class="p_context"> OPAL_CALL(opal_int_eoi,				OPAL_INT_EOI);</span>
 OPAL_CALL(opal_int_set_mfrr,			OPAL_INT_SET_MFRR);
 OPAL_CALL(opal_pci_tce_kill,			OPAL_PCI_TCE_KILL);
 OPAL_CALL(opal_nmmu_set_ptcr,			OPAL_NMMU_SET_PTCR);
<span class="p_add">+OPAL_CALL(opal_npu_init_context,		OPAL_NPU_INIT_CONTEXT);</span>
<span class="p_add">+OPAL_CALL(opal_npu_destroy_context,		OPAL_NPU_DESTROY_CONTEXT);</span>
<span class="p_add">+OPAL_CALL(opal_npu_map_lpar,			OPAL_NPU_MAP_LPAR);</span>
<span class="p_header">diff --git a/arch/powerpc/platforms/powernv/pci-ioda.c b/arch/powerpc/platforms/powernv/pci-ioda.c</span>
<span class="p_header">index e367382..9b24a52 100644</span>
<span class="p_header">--- a/arch/powerpc/platforms/powernv/pci-ioda.c</span>
<span class="p_header">+++ b/arch/powerpc/platforms/powernv/pci-ioda.c</span>
<span class="p_chunk">@@ -1262,6 +1262,8 @@</span> <span class="p_context"> static void pnv_pci_ioda_setup_PEs(void)</span>
 			/* PE#0 is needed for error reporting */
 			pnv_ioda_reserve_pe(phb, 0);
 			pnv_ioda_setup_npu_PEs(hose-&gt;bus);
<span class="p_add">+			if (phb-&gt;model == PNV_PHB_MODEL_NPU2)</span>
<span class="p_add">+				pnv_npu2_init(phb);</span>
 		}
 	}
 }
<span class="p_header">diff --git a/arch/powerpc/platforms/powernv/pci.h b/arch/powerpc/platforms/powernv/pci.h</span>
<span class="p_header">index e1d3e55..4eab713 100644</span>
<span class="p_header">--- a/arch/powerpc/platforms/powernv/pci.h</span>
<span class="p_header">+++ b/arch/powerpc/platforms/powernv/pci.h</span>
<span class="p_chunk">@@ -7,6 +7,9 @@</span> <span class="p_context"></span>

 struct pci_dn;

<span class="p_add">+/* Maximum possible number of ATSD MMIO registers per NPU */</span>
<span class="p_add">+#define NV_NMMU_ATSD_REGS 8</span>
<span class="p_add">+</span>
 enum pnv_phb_type {
 	PNV_PHB_IODA1	= 0,
 	PNV_PHB_IODA2	= 1,
<span class="p_chunk">@@ -174,6 +177,16 @@</span> <span class="p_context"> struct pnv_phb {</span>
 		struct OpalIoP7IOCErrorData 	hub_diag;
 	} diag;

<span class="p_add">+	/* Nvlink2 data */</span>
<span class="p_add">+	struct npu {</span>
<span class="p_add">+		int index;</span>
<span class="p_add">+		__be64 *mmio_atsd_regs[NV_NMMU_ATSD_REGS];</span>
<span class="p_add">+		unsigned int mmio_atsd_count;</span>
<span class="p_add">+</span>
<span class="p_add">+		/* Bitmask for MMIO register usage */</span>
<span class="p_add">+		unsigned long mmio_atsd_usage;</span>
<span class="p_add">+	} npu;</span>
<span class="p_add">+</span>
 #ifdef CONFIG_CXL_BASE
 	struct cxl_afu *cxl_afu;
 #endif
<span class="p_chunk">@@ -236,7 +249,7 @@</span> <span class="p_context"> extern long pnv_npu_set_window(struct pnv_ioda_pe *npe, int num,</span>
 extern long pnv_npu_unset_window(struct pnv_ioda_pe *npe, int num);
 extern void pnv_npu_take_ownership(struct pnv_ioda_pe *npe);
 extern void pnv_npu_release_ownership(struct pnv_ioda_pe *npe);
<span class="p_del">-</span>
<span class="p_add">+extern int pnv_npu2_init(struct pnv_phb *phb);</span>

 /* cxl functions */
 extern bool pnv_cxl_enable_device_hook(struct pci_dev *dev);

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



