
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[2/4] mm: Send one IPI per CPU to TLB flush all entries after unmapping pages - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [2/4] mm: Send one IPI per CPU to TLB flush all entries after unmapping pages</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=24451">Mel Gorman</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>July 6, 2015, 1:39 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1436189996-7220-3-git-send-email-mgorman@suse.de&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/6724471/mbox/"
   >mbox</a>
|
   <a href="/patch/6724471/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/6724471/">/patch/6724471/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork2.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork2.web.kernel.org (Postfix) with ESMTP id E27A9C05AC
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  6 Jul 2015 13:42:07 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 852B3206EA
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  6 Jul 2015 13:42:06 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id E273C206DC
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  6 Jul 2015 13:42:04 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1756627AbbGFNl7 (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 6 Jul 2015 09:41:59 -0400
Received: from cantor2.suse.de ([195.135.220.15]:49205 &quot;EHLO mx2.suse.de&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1754463AbbGFNkE (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 6 Jul 2015 09:40:04 -0400
X-Virus-Scanned: by amavisd-new at test-mx.suse.de
Received: from relay2.suse.de (charybdis-ext.suse.de [195.135.220.254])
	by mx2.suse.de (Postfix) with ESMTP id 075F9AD52;
	Mon,  6 Jul 2015 13:40:03 +0000 (UTC)
From: Mel Gorman &lt;mgorman@suse.de&gt;
To: Andrew Morton &lt;akpm@linux-foundation.org&gt;
Cc: Rik van Riel &lt;riel@redhat.com&gt;, Dave Hansen &lt;dave.hansen@intel.com&gt;,
	Ingo Molnar &lt;mingo@kernel.org&gt;,
	Linus Torvalds &lt;torvalds@linux-foundation.org&gt;,
	Linux-MM &lt;linux-mm@kvack.org&gt;,
	LKML &lt;linux-kernel@vger.kernel.org&gt;, Mel Gorman &lt;mgorman@suse.de&gt;
Subject: [PATCH 2/4] mm: Send one IPI per CPU to TLB flush all entries after
	unmapping pages
Date: Mon,  6 Jul 2015 14:39:54 +0100
Message-Id: &lt;1436189996-7220-3-git-send-email-mgorman@suse.de&gt;
X-Mailer: git-send-email 2.3.5
In-Reply-To: &lt;1436189996-7220-1-git-send-email-mgorman@suse.de&gt;
References: &lt;1436189996-7220-1-git-send-email-mgorman@suse.de&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-7.6 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=24451">Mel Gorman</a> - July 6, 2015, 1:39 p.m.</div>
<pre class="content">
An IPI is sent to flush remote TLBs when a page is unmapped that was
potentially accesssed by other CPUs. There are many circumstances where
this happens but the obvious one is kswapd reclaiming pages belonging to
a running process as kswapd and the task are likely running on separate CPUs.

On small machines, this is not a significant problem but as machine gets
larger with more cores and more memory, the cost of these IPIs can be
high. This patch uses a simple structure that tracks CPUs that potentially
have TLB entries for pages being unmapped. When the unmapping is complete,
the full TLB is flushed on the assumption that a refill cost is lower than
flushing individual entries.

Architectures wishing to do this must give the following guarantee.

	If a clean page is unmapped and not immediately flushed, the
	architecture must guarantee that a write to that linear address
	from a CPU with a cached TLB entry will trap a page fault.

This is essentially what the kernel already depends on but the window
is much larger with this patch applied and is worth highlighting. The
architecture should consider whether the cost of the full TLB flush is
higher than sending an IPI to flush each individual entry. An additional
architecture helper called flush_tlb_local is required. It&#39;s a trivial
wrapper with some accounting in the x86 case.

The impact of this patch depends on the workload as measuring any benefit
requires both mapped pages co-located on the LRU and memory pressure. The
case with the biggest impact is multiple processes reading mapped pages
taken from the vm-scalability test suite. The test case uses NR_CPU readers
of mapped files that consume 10*RAM.

Linear mapped reader on a 4-node machine with 64G RAM and 48 CPUs

                                           4.2.0-rc1          4.2.0-rc1
                                             vanilla       flushfull-v7
Ops lru-file-mmap-read-elapsed      159.62 (  0.00%)   120.68 ( 24.40%)
Ops lru-file-mmap-read-time_range    30.59 (  0.00%)     2.80 ( 90.85%)
Ops lru-file-mmap-read-time_stddv     6.70 (  0.00%)     0.64 ( 90.38%)

           4.2.0-rc1    4.2.0-rc1
             vanilla flushfull-v7
User          581.00       611.43
System       5804.93      4111.76
Elapsed       161.03       122.12

This is showing that the readers completed 24.40% faster with 29% less
system CPU time. From vmstats, it is known that the vanilla kernel was
interrupted roughly 900K times per second during the steady phase of the
test and the patched kernel was interrupts 180K times per second.

The impact is lower on a single socket machine.

                                           4.2.0-rc1          4.2.0-rc1
                                             vanilla       flushfull-v7
Ops lru-file-mmap-read-elapsed       25.33 (  0.00%)    20.38 ( 19.54%)
Ops lru-file-mmap-read-time_range     0.91 (  0.00%)     1.44 (-58.24%)
Ops lru-file-mmap-read-time_stddv     0.28 (  0.00%)     0.47 (-65.34%)

           4.2.0-rc1    4.2.0-rc1
             vanilla flushfull-v7
User           58.09        57.64
System        111.82        76.56
Elapsed        27.29        22.55

It&#39;s still a noticeable improvement with vmstat showing interrupts went
from roughly 500K per second to 45K per second.

The patch will have no impact on workloads with no memory pressure or
have relatively few mapped pages. It will have an unpredictable impact
on the workload running on the CPU being flushed as it&#39;ll depend on how
many TLB entries need to be refilled and how long that takes. Worst case,
the TLB will be completely cleared of active entries when the target PFNs
were not resident at all.
<span class="signed-off-by">
Signed-off-by: Mel Gorman &lt;mgorman@suse.de&gt;</span>
<span class="reviewed-by">Reviewed-by: Rik van Riel &lt;riel@redhat.com&gt;</span>
---
 arch/x86/Kconfig                |   1 +
 arch/x86/include/asm/tlbflush.h |   6 +++
 include/linux/rmap.h            |   3 ++
 include/linux/sched.h           |  16 +++++++
 init/Kconfig                    |  10 ++++
 mm/internal.h                   |  11 +++++
 mm/rmap.c                       | 103 +++++++++++++++++++++++++++++++++++++++-
 mm/vmscan.c                     |  23 ++++++++-
 8 files changed, 171 insertions(+), 2 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig</span>
<span class="p_header">index 55bced17dc95..d646d6f26a16 100644</span>
<span class="p_header">--- a/arch/x86/Kconfig</span>
<span class="p_header">+++ b/arch/x86/Kconfig</span>
<span class="p_chunk">@@ -41,6 +41,7 @@</span> <span class="p_context"> config X86</span>
 	select ARCH_USE_CMPXCHG_LOCKREF		if X86_64
 	select ARCH_USE_QUEUED_RWLOCKS
 	select ARCH_USE_QUEUED_SPINLOCKS
<span class="p_add">+	select ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH if SMP</span>
 	select ARCH_WANT_FRAME_POINTERS
 	select ARCH_WANT_IPC_PARSE_VERSION	if X86_32
 	select ARCH_WANT_OPTIONAL_GPIOLIB
<span class="p_header">diff --git a/arch/x86/include/asm/tlbflush.h b/arch/x86/include/asm/tlbflush.h</span>
<span class="p_header">index cd791948b286..6df2029405a3 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/tlbflush.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/tlbflush.h</span>
<span class="p_chunk">@@ -261,6 +261,12 @@</span> <span class="p_context"> static inline void reset_lazy_tlbstate(void)</span>
 
 #endif	/* SMP */
 
<span class="p_add">+/* Not inlined due to inc_irq_stat not being defined yet */</span>
<span class="p_add">+#define flush_tlb_local() {		\</span>
<span class="p_add">+	inc_irq_stat(irq_tlb_count);	\</span>
<span class="p_add">+	local_flush_tlb();		\</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 #ifndef CONFIG_PARAVIRT
 #define flush_tlb_others(mask, mm, start, end)	\
 	native_flush_tlb_others(mask, mm, start, end)
<span class="p_header">diff --git a/include/linux/rmap.h b/include/linux/rmap.h</span>
<span class="p_header">index c89c53a113a8..29446aeef36e 100644</span>
<span class="p_header">--- a/include/linux/rmap.h</span>
<span class="p_header">+++ b/include/linux/rmap.h</span>
<span class="p_chunk">@@ -89,6 +89,9 @@</span> <span class="p_context"> enum ttu_flags {</span>
 	TTU_IGNORE_MLOCK = (1 &lt;&lt; 8),	/* ignore mlock */
 	TTU_IGNORE_ACCESS = (1 &lt;&lt; 9),	/* don&#39;t age */
 	TTU_IGNORE_HWPOISON = (1 &lt;&lt; 10),/* corrupted page is recoverable */
<span class="p_add">+	TTU_BATCH_FLUSH = (1 &lt;&lt; 11),	/* Batch TLB flushes where possible</span>
<span class="p_add">+					 * and caller guarantees they will</span>
<span class="p_add">+					 * do a final flush if necessary */</span>
 };
 
 #ifdef CONFIG_MMU
<span class="p_header">diff --git a/include/linux/sched.h b/include/linux/sched.h</span>
<span class="p_header">index ae21f1591615..1a83fb44ab34 100644</span>
<span class="p_header">--- a/include/linux/sched.h</span>
<span class="p_header">+++ b/include/linux/sched.h</span>
<span class="p_chunk">@@ -1341,6 +1341,18 @@</span> <span class="p_context"> enum perf_event_task_context {</span>
 	perf_nr_task_contexts,
 };
 
<span class="p_add">+/* Track pages that require TLB flushes */</span>
<span class="p_add">+struct tlbflush_unmap_batch {</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Each bit set is a CPU that potentially has a TLB entry for one of</span>
<span class="p_add">+	 * the PFNs being flushed. See set_tlb_ubc_flush_pending().</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	struct cpumask cpumask;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* True if any bit in cpumask is set */</span>
<span class="p_add">+	bool flush_required;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
 struct task_struct {
 	volatile long state;	/* -1 unrunnable, 0 runnable, &gt;0 stopped */
 	void *stack;
<span class="p_chunk">@@ -1701,6 +1713,10 @@</span> <span class="p_context"> struct task_struct {</span>
 	unsigned long numa_pages_migrated;
 #endif /* CONFIG_NUMA_BALANCING */
 
<span class="p_add">+#ifdef CONFIG_ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH</span>
<span class="p_add">+	struct tlbflush_unmap_batch tlb_ubc;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 	struct rcu_head rcu;
 
 	/*
<span class="p_header">diff --git a/init/Kconfig b/init/Kconfig</span>
<span class="p_header">index af09b4fb43d2..0aa5be1e617d 100644</span>
<span class="p_header">--- a/init/Kconfig</span>
<span class="p_header">+++ b/init/Kconfig</span>
<span class="p_chunk">@@ -891,6 +891,16 @@</span> <span class="p_context"> config ARCH_SUPPORTS_NUMA_BALANCING</span>
 	bool
 
 #
<span class="p_add">+# For architectures that prefer to flush all TLBs after a number of pages</span>
<span class="p_add">+# are unmapped instead of sending one IPI per page to flush. The architecture</span>
<span class="p_add">+# must provide guarantees on what happens if a clean TLB cache entry is</span>
<span class="p_add">+# written after the unmap. Details are in mm/rmap.c near the check for</span>
<span class="p_add">+# should_defer_flush. The architecture should also consider if the full flush</span>
<span class="p_add">+# and the refill costs are offset by the savings of sending fewer IPIs.</span>
<span class="p_add">+config ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH</span>
<span class="p_add">+	bool</span>
<span class="p_add">+</span>
<span class="p_add">+#</span>
 # For architectures that know their GCC __int128 support is sound
 #
 config ARCH_SUPPORTS_INT128
<span class="p_header">diff --git a/mm/internal.h b/mm/internal.h</span>
<span class="p_header">index 36b23f1e2ca6..bd6372ac5f7f 100644</span>
<span class="p_header">--- a/mm/internal.h</span>
<span class="p_header">+++ b/mm/internal.h</span>
<span class="p_chunk">@@ -426,4 +426,15 @@</span> <span class="p_context"> unsigned long reclaim_clean_pages_from_list(struct zone *zone,</span>
 #define ALLOC_CMA		0x80 /* allow allocations from CMA areas */
 #define ALLOC_FAIR		0x100 /* fair zone allocation */
 
<span class="p_add">+enum ttu_flags;</span>
<span class="p_add">+struct tlbflush_unmap_batch;</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH</span>
<span class="p_add">+void try_to_unmap_flush(void);</span>
<span class="p_add">+#else</span>
<span class="p_add">+static inline void try_to_unmap_flush(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* CONFIG_ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH */</span>
 #endif	/* __MM_INTERNAL_H */
<span class="p_header">diff --git a/mm/rmap.c b/mm/rmap.c</span>
<span class="p_header">index 171b68768df1..d54f47666af5 100644</span>
<span class="p_header">--- a/mm/rmap.c</span>
<span class="p_header">+++ b/mm/rmap.c</span>
<span class="p_chunk">@@ -62,6 +62,8 @@</span> <span class="p_context"></span>
 
 #include &lt;asm/tlbflush.h&gt;
 
<span class="p_add">+#include &lt;trace/events/tlb.h&gt;</span>
<span class="p_add">+</span>
 #include &quot;internal.h&quot;
 
 static struct kmem_cache *anon_vma_cachep;
<span class="p_chunk">@@ -583,6 +585,88 @@</span> <span class="p_context"> vma_address(struct page *page, struct vm_area_struct *vma)</span>
 	return address;
 }
 
<span class="p_add">+#ifdef CONFIG_ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH</span>
<span class="p_add">+static void percpu_flush_tlb_batch_pages(void *data)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * All TLB entries are flushed on the assumption that it is</span>
<span class="p_add">+	 * cheaper to flush all TLBs and let them be refilled than</span>
<span class="p_add">+	 * flushing individual PFNs. Note that we do not track mm&#39;s</span>
<span class="p_add">+	 * to flush as that might simply be multiple full TLB flushes</span>
<span class="p_add">+	 * for no gain.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	count_vm_tlb_event(NR_TLB_REMOTE_FLUSH_RECEIVED);</span>
<span class="p_add">+	flush_tlb_local();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Flush TLB entries for recently unmapped pages from remote CPUs. It is</span>
<span class="p_add">+ * important if a PTE was dirty when it was unmapped that it&#39;s flushed</span>
<span class="p_add">+ * before any IO is initiated on the page to prevent lost writes. Similarly,</span>
<span class="p_add">+ * it must be flushed before freeing to prevent data leakage.</span>
<span class="p_add">+ */</span>
<span class="p_add">+void try_to_unmap_flush(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct tlbflush_unmap_batch *tlb_ubc = &amp;current-&gt;tlb_ubc;</span>
<span class="p_add">+	int cpu;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!tlb_ubc-&gt;flush_required)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	trace_tlb_flush(TLB_REMOTE_SHOOTDOWN, -1UL);</span>
<span class="p_add">+</span>
<span class="p_add">+	cpu = get_cpu();</span>
<span class="p_add">+	if (cpumask_test_cpu(cpu, &amp;tlb_ubc-&gt;cpumask))</span>
<span class="p_add">+		percpu_flush_tlb_batch_pages(&amp;tlb_ubc-&gt;cpumask);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (cpumask_any_but(&amp;tlb_ubc-&gt;cpumask, cpu) &lt; nr_cpu_ids) {</span>
<span class="p_add">+		smp_call_function_many(&amp;tlb_ubc-&gt;cpumask,</span>
<span class="p_add">+			percpu_flush_tlb_batch_pages, (void *)tlb_ubc, true);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	cpumask_clear(&amp;tlb_ubc-&gt;cpumask);</span>
<span class="p_add">+	tlb_ubc-&gt;flush_required = false;</span>
<span class="p_add">+	put_cpu();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void set_tlb_ubc_flush_pending(struct mm_struct *mm,</span>
<span class="p_add">+		struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct tlbflush_unmap_batch *tlb_ubc = &amp;current-&gt;tlb_ubc;</span>
<span class="p_add">+</span>
<span class="p_add">+	cpumask_or(&amp;tlb_ubc-&gt;cpumask, &amp;tlb_ubc-&gt;cpumask, mm_cpumask(mm));</span>
<span class="p_add">+	tlb_ubc-&gt;flush_required = true;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Returns true if the TLB flush should be deferred to the end of a batch of</span>
<span class="p_add">+ * unmap operations to reduce IPIs.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static bool should_defer_flush(struct mm_struct *mm, enum ttu_flags flags)</span>
<span class="p_add">+{</span>
<span class="p_add">+	bool should_defer = false;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!(flags &amp; TTU_BATCH_FLUSH))</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* If remote CPUs need to be flushed then defer batch the flush */</span>
<span class="p_add">+	if (cpumask_any_but(mm_cpumask(mm), get_cpu()) &lt; nr_cpu_ids)</span>
<span class="p_add">+		should_defer = true;</span>
<span class="p_add">+	put_cpu();</span>
<span class="p_add">+</span>
<span class="p_add">+	return should_defer;</span>
<span class="p_add">+}</span>
<span class="p_add">+#else</span>
<span class="p_add">+static void set_tlb_ubc_flush_pending(struct mm_struct *mm,</span>
<span class="p_add">+		struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static bool should_defer_flush(struct mm_struct *mm, enum ttu_flags flags)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return false;</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif /* CONFIG_ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH */</span>
<span class="p_add">+</span>
 /*
  * At what user virtual address is page expected in vma?
  * Caller should check the page is actually part of the vma.
<span class="p_chunk">@@ -1220,7 +1304,24 @@</span> <span class="p_context"> static int try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
 
 	/* Nuke the page table entry. */
 	flush_cache_page(vma, address, page_to_pfn(page));
<span class="p_del">-	pteval = ptep_clear_flush(vma, address, pte);</span>
<span class="p_add">+	if (should_defer_flush(mm, flags)) {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * We clear the PTE but do not flush so potentially a remote</span>
<span class="p_add">+		 * CPU could still be writing to the page. If the entry was</span>
<span class="p_add">+		 * previously clean then the architecture must guarantee that</span>
<span class="p_add">+		 * a clear-&gt;dirty transition on a cached TLB entry is written</span>
<span class="p_add">+		 * through and traps if the PTE is unmapped.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		pteval = ptep_get_and_clear(mm, address, pte);</span>
<span class="p_add">+</span>
<span class="p_add">+		/* Potentially writable TLBs must be flushed before IO */</span>
<span class="p_add">+		if (pte_dirty(pteval))</span>
<span class="p_add">+			flush_tlb_page(vma, address);</span>
<span class="p_add">+		else</span>
<span class="p_add">+			set_tlb_ubc_flush_pending(mm, page);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		pteval = ptep_clear_flush(vma, address, pte);</span>
<span class="p_add">+	}</span>
 
 	/* Move the dirty bit to the physical page now the pte is gone. */
 	if (pte_dirty(pteval))
<span class="p_header">diff --git a/mm/vmscan.c b/mm/vmscan.c</span>
<span class="p_header">index e61445dce04e..e4f1df1052a2 100644</span>
<span class="p_header">--- a/mm/vmscan.c</span>
<span class="p_header">+++ b/mm/vmscan.c</span>
<span class="p_chunk">@@ -1061,7 +1061,8 @@</span> <span class="p_context"> static unsigned long shrink_page_list(struct list_head *page_list,</span>
 		 * processes. Try to unmap it here.
 		 */
 		if (page_mapped(page) &amp;&amp; mapping) {
<span class="p_del">-			switch (try_to_unmap(page, ttu_flags)) {</span>
<span class="p_add">+			switch (try_to_unmap(page,</span>
<span class="p_add">+					ttu_flags|TTU_BATCH_FLUSH)) {</span>
 			case SWAP_FAIL:
 				goto activate_locked;
 			case SWAP_AGAIN:
<span class="p_chunk">@@ -1212,6 +1213,7 @@</span> <span class="p_context"> keep:</span>
 	}
 
 	mem_cgroup_uncharge_list(&amp;free_pages);
<span class="p_add">+	try_to_unmap_flush();</span>
 	free_hot_cold_page_list(&amp;free_pages, true);
 
 	list_splice(&amp;ret_pages, page_list);
<span class="p_chunk">@@ -2155,6 +2157,23 @@</span> <span class="p_context"> out:</span>
 	}
 }
 
<span class="p_add">+#ifdef CONFIG_ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH</span>
<span class="p_add">+static void init_tlb_ubc(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * This deliberately does not clear the cpumask as it&#39;s expensive</span>
<span class="p_add">+	 * and unnecessary. If there happens to be data in there then the</span>
<span class="p_add">+	 * first SWAP_CLUSTER_MAX pages will send an unnecessary IPI and</span>
<span class="p_add">+	 * then will be cleared.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	current-&gt;tlb_ubc.flush_required = false;</span>
<span class="p_add">+}</span>
<span class="p_add">+#else</span>
<span class="p_add">+static inline void init_tlb_ubc(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif /* CONFIG_ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH */</span>
<span class="p_add">+</span>
 /*
  * This is a basic per-zone page freer.  Used by both kswapd and direct reclaim.
  */
<span class="p_chunk">@@ -2189,6 +2208,8 @@</span> <span class="p_context"> static void shrink_lruvec(struct lruvec *lruvec, int swappiness,</span>
 	scan_adjusted = (global_reclaim(sc) &amp;&amp; !current_is_kswapd() &amp;&amp;
 			 sc-&gt;priority == DEF_PRIORITY);
 
<span class="p_add">+	init_tlb_ubc();</span>
<span class="p_add">+</span>
 	blk_start_plug(&amp;plug);
 	while (nr[LRU_INACTIVE_ANON] || nr[LRU_ACTIVE_FILE] ||
 					nr[LRU_INACTIVE_FILE]) {

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



