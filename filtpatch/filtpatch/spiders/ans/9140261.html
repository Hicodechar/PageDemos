
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[RFC,2/4] mm: Change the interface for __tlb_remove_page - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [RFC,2/4] mm: Change the interface for __tlb_remove_page</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=1141">Aneesh Kumar K.V</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>May 30, 2016, 5:44 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1464587062-17745-2-git-send-email-aneesh.kumar@linux.vnet.ibm.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9140261/mbox/"
   >mbox</a>
|
   <a href="/patch/9140261/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9140261/">/patch/9140261/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	E51DA60759 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 30 May 2016 05:45:12 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id D9EF7280B2
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 30 May 2016 05:45:12 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id CCEF528218; Mon, 30 May 2016 05:45:12 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id A13F6280B2
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 30 May 2016 05:45:11 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S932616AbcE3For (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 30 May 2016 01:44:47 -0400
Received: from e19.ny.us.ibm.com ([129.33.205.209]:42926 &quot;EHLO
	e19.ny.us.ibm.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S932589AbcE3Foo (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 30 May 2016 01:44:44 -0400
Received: from localhost
	by e19.ny.us.ibm.com with IBM ESMTP SMTP Gateway: Authorized Use
	Only! Violators will be prosecuted
	for &lt;linux-kernel@vger.kernel.org&gt; from
	&lt;aneesh.kumar@linux.vnet.ibm.com&gt;; Mon, 30 May 2016 01:44:43 -0400
Received: from d01dlp03.pok.ibm.com (9.56.250.168)
	by e19.ny.us.ibm.com (146.89.104.206) with IBM ESMTP SMTP Gateway:
	Authorized Use Only! Violators will be prosecuted; 
	Mon, 30 May 2016 01:44:40 -0400
X-IBM-Helo: d01dlp03.pok.ibm.com
X-IBM-MailFrom: aneesh.kumar@linux.vnet.ibm.com
X-IBM-RcptTo: linux-mm@kvack.org; akpm@linux-foundation.org;
	linux-arch@vger.kernel.org; linux-kernel@vger.kernel.org
Received: from b01cxnp23033.gho.pok.ibm.com (b01cxnp23033.gho.pok.ibm.com
	[9.57.198.28])
	by d01dlp03.pok.ibm.com (Postfix) with ESMTP id 2C8E3C9003E;
	Mon, 30 May 2016 01:44:32 -0400 (EDT)
Received: from b01ledav006.gho.pok.ibm.com (b01ledav006.gho.pok.ibm.com
	[9.57.199.111])
	by b01cxnp23033.gho.pok.ibm.com (8.14.9/8.14.9/NCO v10.0) with ESMTP
	id u4U5idVo33751098; Mon, 30 May 2016 05:44:39 GMT
Received: from b01ledav006.gho.pok.ibm.com (unknown [127.0.0.1])
	by IMSVA (Postfix) with ESMTP id 8C19CAC040;
	Mon, 30 May 2016 01:44:39 -0400 (EDT)
Received: from skywalker.in.ibm.com (unknown [9.124.219.185])
	by b01ledav006.gho.pok.ibm.com (Postfix) with ESMTP id 6892FAC03C;
	Mon, 30 May 2016 01:44:37 -0400 (EDT)
From: &quot;Aneesh Kumar K.V&quot; &lt;aneesh.kumar@linux.vnet.ibm.com&gt;
To: akpm@linux-foundation.org, linux-arch@vger.kernel.org
Cc: linux-mm@kvack.org, linux-kernel@vger.kernel.org,
	&quot;Aneesh Kumar K.V&quot; &lt;aneesh.kumar@linux.vnet.ibm.com&gt;
Subject: [RFC PATCH 2/4] mm: Change the interface for __tlb_remove_page
Date: Mon, 30 May 2016 11:14:20 +0530
Message-Id: &lt;1464587062-17745-2-git-send-email-aneesh.kumar@linux.vnet.ibm.com&gt;
X-Mailer: git-send-email 2.7.4
In-Reply-To: &lt;1464587062-17745-1-git-send-email-aneesh.kumar@linux.vnet.ibm.com&gt;
References: &lt;1464587062-17745-1-git-send-email-aneesh.kumar@linux.vnet.ibm.com&gt;
X-TM-AS-GCONF: 00
X-Content-Scanned: Fidelis XPS MAILER
x-cbid: 16053005-0057-0000-0000-000004761E4A
X-IBM-AV-DETECTION: SAVI=unused REMOTE=unused XFE=unused
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=1141">Aneesh Kumar K.V</a> - May 30, 2016, 5:44 a.m.</div>
<pre class="content">
This update the generic and arch specific implementation to return true
if we need to do a tlb flush. That means if a __tlb_remove_page indicate
a flush is needed, the page we try to remove need to be tracked and
added again after the flush. We need to track it because we have already
update the pte to none and we can&#39;t just loop back.

This changes is done to enable us to do a tlb_flush when we try to flush
a range that consists of different page sizes. For architectures like
ppc64, we can do a range based tlb flush and we need to track page size
for that. When we try to remove a huge page, we will force a tlb flush
and starts a new mmu gather.
<span class="signed-off-by">
Signed-off-by: Aneesh Kumar K.V &lt;aneesh.kumar@linux.vnet.ibm.com&gt;</span>
---
 arch/arm/include/asm/tlb.h  | 11 +++++++----
 arch/ia64/include/asm/tlb.h | 13 ++++++++-----
 arch/s390/include/asm/tlb.h |  4 ++--
 arch/sh/include/asm/tlb.h   |  2 +-
 arch/um/include/asm/tlb.h   |  2 +-
 include/asm-generic/tlb.h   | 18 ++++++++++++++++--
 mm/memory.c                 | 20 ++++++++++++++------
 7 files changed, 49 insertions(+), 21 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/arm/include/asm/tlb.h b/arch/arm/include/asm/tlb.h</span>
<span class="p_header">index 3cadb726ec88..45dea952b0e6 100644</span>
<span class="p_header">--- a/arch/arm/include/asm/tlb.h</span>
<span class="p_header">+++ b/arch/arm/include/asm/tlb.h</span>
<span class="p_chunk">@@ -209,17 +209,20 @@</span> <span class="p_context"> tlb_end_vma(struct mmu_gather *tlb, struct vm_area_struct *vma)</span>
 		tlb_flush(tlb);
 }
 
<span class="p_del">-static inline int __tlb_remove_page(struct mmu_gather *tlb, struct page *page)</span>
<span class="p_add">+static inline bool __tlb_remove_page(struct mmu_gather *tlb, struct page *page)</span>
 {
<span class="p_add">+	if (tlb-&gt;nr == tlb-&gt;max)</span>
<span class="p_add">+		return true;</span>
 	tlb-&gt;pages[tlb-&gt;nr++] = page;
<span class="p_del">-	VM_BUG_ON(tlb-&gt;nr &gt; tlb-&gt;max);</span>
<span class="p_del">-	return tlb-&gt;max - tlb-&gt;nr;</span>
<span class="p_add">+	return false;</span>
 }
 
 static inline void tlb_remove_page(struct mmu_gather *tlb, struct page *page)
 {
<span class="p_del">-	if (!__tlb_remove_page(tlb, page))</span>
<span class="p_add">+	if (__tlb_remove_page(tlb, page)) {</span>
 		tlb_flush_mmu(tlb);
<span class="p_add">+		__tlb_remove_page(tlb, page);</span>
<span class="p_add">+	}</span>
 }
 
 static inline void __pte_free_tlb(struct mmu_gather *tlb, pgtable_t pte,
<span class="p_header">diff --git a/arch/ia64/include/asm/tlb.h b/arch/ia64/include/asm/tlb.h</span>
<span class="p_header">index 39d64e0df1de..85005ab513e9 100644</span>
<span class="p_header">--- a/arch/ia64/include/asm/tlb.h</span>
<span class="p_header">+++ b/arch/ia64/include/asm/tlb.h</span>
<span class="p_chunk">@@ -205,17 +205,18 @@</span> <span class="p_context"> tlb_finish_mmu(struct mmu_gather *tlb, unsigned long start, unsigned long end)</span>
  * must be delayed until after the TLB has been flushed (see comments at the beginning of
  * this file).
  */
<span class="p_del">-static inline int __tlb_remove_page(struct mmu_gather *tlb, struct page *page)</span>
<span class="p_add">+static inline bool __tlb_remove_page(struct mmu_gather *tlb, struct page *page)</span>
 {
<span class="p_add">+	if (tlb-&gt;nr == tlb-&gt;max)</span>
<span class="p_add">+		return true;</span>
<span class="p_add">+</span>
 	tlb-&gt;need_flush = 1;
 
 	if (!tlb-&gt;nr &amp;&amp; tlb-&gt;pages == tlb-&gt;local)
 		__tlb_alloc_page(tlb);
 
 	tlb-&gt;pages[tlb-&gt;nr++] = page;
<span class="p_del">-	VM_BUG_ON(tlb-&gt;nr &gt; tlb-&gt;max);</span>
<span class="p_del">-</span>
<span class="p_del">-	return tlb-&gt;max - tlb-&gt;nr;</span>
<span class="p_add">+	return false;</span>
 }
 
 static inline void tlb_flush_mmu_tlbonly(struct mmu_gather *tlb)
<span class="p_chunk">@@ -235,8 +236,10 @@</span> <span class="p_context"> static inline void tlb_flush_mmu(struct mmu_gather *tlb)</span>
 
 static inline void tlb_remove_page(struct mmu_gather *tlb, struct page *page)
 {
<span class="p_del">-	if (!__tlb_remove_page(tlb, page))</span>
<span class="p_add">+	if (__tlb_remove_page(tlb, page)) {</span>
 		tlb_flush_mmu(tlb);
<span class="p_add">+		__tlb_remove_page(tlb, page);</span>
<span class="p_add">+	}</span>
 }
 
 /*
<span class="p_header">diff --git a/arch/s390/include/asm/tlb.h b/arch/s390/include/asm/tlb.h</span>
<span class="p_header">index 7a92e69c50bc..6b98cb3601d5 100644</span>
<span class="p_header">--- a/arch/s390/include/asm/tlb.h</span>
<span class="p_header">+++ b/arch/s390/include/asm/tlb.h</span>
<span class="p_chunk">@@ -87,10 +87,10 @@</span> <span class="p_context"> static inline void tlb_finish_mmu(struct mmu_gather *tlb,</span>
  * tlb_ptep_clear_flush. In both flush modes the tlb for a page cache page
  * has already been freed, so just do free_page_and_swap_cache.
  */
<span class="p_del">-static inline int __tlb_remove_page(struct mmu_gather *tlb, struct page *page)</span>
<span class="p_add">+static inline bool __tlb_remove_page(struct mmu_gather *tlb, struct page *page)</span>
 {
 	free_page_and_swap_cache(page);
<span class="p_del">-	return 1; /* avoid calling tlb_flush_mmu */</span>
<span class="p_add">+	return false; /* avoid calling tlb_flush_mmu */</span>
 }
 
 static inline void tlb_remove_page(struct mmu_gather *tlb, struct page *page)
<span class="p_header">diff --git a/arch/sh/include/asm/tlb.h b/arch/sh/include/asm/tlb.h</span>
<span class="p_header">index 62f80d2a9df9..3dec5e0734f5 100644</span>
<span class="p_header">--- a/arch/sh/include/asm/tlb.h</span>
<span class="p_header">+++ b/arch/sh/include/asm/tlb.h</span>
<span class="p_chunk">@@ -101,7 +101,7 @@</span> <span class="p_context"> static inline void tlb_flush_mmu(struct mmu_gather *tlb)</span>
 static inline int __tlb_remove_page(struct mmu_gather *tlb, struct page *page)
 {
 	free_page_and_swap_cache(page);
<span class="p_del">-	return 1; /* avoid calling tlb_flush_mmu */</span>
<span class="p_add">+	return false; /* avoid calling tlb_flush_mmu */</span>
 }
 
 static inline void tlb_remove_page(struct mmu_gather *tlb, struct page *page)
<span class="p_header">diff --git a/arch/um/include/asm/tlb.h b/arch/um/include/asm/tlb.h</span>
<span class="p_header">index 16eb63fac57d..c6638f8e5e90 100644</span>
<span class="p_header">--- a/arch/um/include/asm/tlb.h</span>
<span class="p_header">+++ b/arch/um/include/asm/tlb.h</span>
<span class="p_chunk">@@ -102,7 +102,7 @@</span> <span class="p_context"> static inline int __tlb_remove_page(struct mmu_gather *tlb, struct page *page)</span>
 {
 	tlb-&gt;need_flush = 1;
 	free_page_and_swap_cache(page);
<span class="p_del">-	return 1; /* avoid calling tlb_flush_mmu */</span>
<span class="p_add">+	return false; /* avoid calling tlb_flush_mmu */</span>
 }
 
 static inline void tlb_remove_page(struct mmu_gather *tlb, struct page *page)
<span class="p_header">diff --git a/include/asm-generic/tlb.h b/include/asm-generic/tlb.h</span>
<span class="p_header">index 9dbb739cafa0..2ac8fe202e9a 100644</span>
<span class="p_header">--- a/include/asm-generic/tlb.h</span>
<span class="p_header">+++ b/include/asm-generic/tlb.h</span>
<span class="p_chunk">@@ -107,6 +107,11 @@</span> <span class="p_context"> struct mmu_gather {</span>
 	struct mmu_gather_batch	local;
 	struct page		*__pages[MMU_GATHER_BUNDLE];
 	unsigned int		batch_count;
<span class="p_add">+	/*</span>
<span class="p_add">+	 * __tlb_adjust_range  will track the new addr here,</span>
<span class="p_add">+	 * that that we can adjust the range after the flush</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	unsigned long addr;</span>
 };
 
 #define HAVE_GENERIC_MMU_GATHER
<span class="p_chunk">@@ -115,7 +120,7 @@</span> <span class="p_context"> void tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm, unsigned long</span>
 void tlb_flush_mmu(struct mmu_gather *tlb);
 void tlb_finish_mmu(struct mmu_gather *tlb, unsigned long start,
 							unsigned long end);
<span class="p_del">-int __tlb_remove_page(struct mmu_gather *tlb, struct page *page);</span>
<span class="p_add">+bool __tlb_remove_page(struct mmu_gather *tlb, struct page *page);</span>
 
 /* tlb_remove_page
  *	Similar to __tlb_remove_page but will call tlb_flush_mmu() itself when
<span class="p_chunk">@@ -123,8 +128,11 @@</span> <span class="p_context"> int __tlb_remove_page(struct mmu_gather *tlb, struct page *page);</span>
  */
 static inline void tlb_remove_page(struct mmu_gather *tlb, struct page *page)
 {
<span class="p_del">-	if (!__tlb_remove_page(tlb, page))</span>
<span class="p_add">+	if (__tlb_remove_page(tlb, page)) {</span>
 		tlb_flush_mmu(tlb);
<span class="p_add">+		__tlb_adjust_range(tlb, tlb-&gt;addr);</span>
<span class="p_add">+		__tlb_remove_page(tlb, page);</span>
<span class="p_add">+	}</span>
 }
 
 static inline void __tlb_adjust_range(struct mmu_gather *tlb,
<span class="p_chunk">@@ -132,6 +140,12 @@</span> <span class="p_context"> static inline void __tlb_adjust_range(struct mmu_gather *tlb,</span>
 {
 	tlb-&gt;start = min(tlb-&gt;start, address);
 	tlb-&gt;end = max(tlb-&gt;end, address + PAGE_SIZE);
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Track the last address with which we adjusted the range. This</span>
<span class="p_add">+	 * will be used later to adjust again after a mmu_flush due to</span>
<span class="p_add">+	 * failed __tlb_remove_page</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	tlb-&gt;addr = address;</span>
 }
 
 static inline void __tlb_reset_range(struct mmu_gather *tlb)
<span class="p_header">diff --git a/mm/memory.c b/mm/memory.c</span>
<span class="p_header">index 15322b73636b..a01db5bc756b 100644</span>
<span class="p_header">--- a/mm/memory.c</span>
<span class="p_header">+++ b/mm/memory.c</span>
<span class="p_chunk">@@ -292,23 +292,24 @@</span> <span class="p_context"> void tlb_finish_mmu(struct mmu_gather *tlb, unsigned long start, unsigned long e</span>
  *	handling the additional races in SMP caused by other CPUs caching valid
  *	mappings in their TLBs. Returns the number of free page slots left.
  *	When out of page slots we must call tlb_flush_mmu().
<span class="p_add">+ *returns true if the caller should flush.</span>
  */
<span class="p_del">-int __tlb_remove_page(struct mmu_gather *tlb, struct page *page)</span>
<span class="p_add">+bool __tlb_remove_page(struct mmu_gather *tlb, struct page *page)</span>
 {
 	struct mmu_gather_batch *batch;
 
 	VM_BUG_ON(!tlb-&gt;end);
 
 	batch = tlb-&gt;active;
<span class="p_del">-	batch-&gt;pages[batch-&gt;nr++] = page;</span>
 	if (batch-&gt;nr == batch-&gt;max) {
 		if (!tlb_next_batch(tlb))
<span class="p_del">-			return 0;</span>
<span class="p_add">+			return true;</span>
 		batch = tlb-&gt;active;
 	}
 	VM_BUG_ON_PAGE(batch-&gt;nr &gt; batch-&gt;max, page);
 
<span class="p_del">-	return batch-&gt;max - batch-&gt;nr;</span>
<span class="p_add">+	batch-&gt;pages[batch-&gt;nr++] = page;</span>
<span class="p_add">+	return false;</span>
 }
 
 #endif /* HAVE_GENERIC_MMU_GATHER */
<span class="p_chunk">@@ -1109,6 +1110,7 @@</span> <span class="p_context"> static unsigned long zap_pte_range(struct mmu_gather *tlb,</span>
 	pte_t *start_pte;
 	pte_t *pte;
 	swp_entry_t entry;
<span class="p_add">+	struct page *pending_page = NULL;</span>
 
 again:
 	init_rss_vec(rss);
<span class="p_chunk">@@ -1160,8 +1162,9 @@</span> <span class="p_context"> again:</span>
 			page_remove_rmap(page, false);
 			if (unlikely(page_mapcount(page) &lt; 0))
 				print_bad_pte(vma, addr, ptent, page);
<span class="p_del">-			if (unlikely(!__tlb_remove_page(tlb, page))) {</span>
<span class="p_add">+			if (unlikely(__tlb_remove_page(tlb, page))) {</span>
 				force_flush = 1;
<span class="p_add">+				pending_page = page;</span>
 				addr += PAGE_SIZE;
 				break;
 			}
<span class="p_chunk">@@ -1202,7 +1205,12 @@</span> <span class="p_context"> again:</span>
 	if (force_flush) {
 		force_flush = 0;
 		tlb_flush_mmu_free(tlb);
<span class="p_del">-</span>
<span class="p_add">+		if (pending_page) {</span>
<span class="p_add">+			/* remove the page with new size */</span>
<span class="p_add">+			__tlb_adjust_range(tlb, tlb-&gt;addr);</span>
<span class="p_add">+			__tlb_remove_page(tlb, pending_page);</span>
<span class="p_add">+			pending_page = NULL;</span>
<span class="p_add">+		}</span>
 		if (addr != end)
 			goto again;
 	}

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



