
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>linux-next: manual merge of the akpm-current tree with the tip tree - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    linux-next: manual merge of the akpm-current tree with the tip tree</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Aug. 14, 2017, 7:38 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20170814193828.GN6524@worktop.programming.kicks-ass.net&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9899977/mbox/"
   >mbox</a>
|
   <a href="/patch/9899977/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9899977/">/patch/9899977/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	505B1602CA for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 14 Aug 2017 19:38:43 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 41D832871A
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 14 Aug 2017 19:38:43 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 3436128720; Mon, 14 Aug 2017 19:38:43 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.8 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	RCVD_IN_DNSWL_HI,T_DKIM_INVALID autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 5DBD928717
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 14 Aug 2017 19:38:42 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752255AbdHNTij (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 14 Aug 2017 15:38:39 -0400
Received: from bombadil.infradead.org ([65.50.211.133]:47890 &quot;EHLO
	bombadil.infradead.org&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1752159AbdHNTih (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 14 Aug 2017 15:38:37 -0400
DKIM-Signature: v=1; a=rsa-sha256; q=dns/txt; c=relaxed/relaxed;
	d=infradead.org; s=bombadil.20170209;
	h=In-Reply-To:Content-Transfer-Encoding
	:Content-Type:MIME-Version:References:Message-ID:Subject:Cc:To:From:Date:
	Sender:Reply-To:Content-ID:Content-Description:Resent-Date:Resent-From:
	Resent-Sender:Resent-To:Resent-Cc:Resent-Message-ID:List-Id:List-Help:
	List-Unsubscribe:List-Subscribe:List-Post:List-Owner:List-Archive;
	bh=m4LGrAMgoLPMXTa1xvLxkMKuwJcvdQJCRlWpUrFLCnE=;
	b=jhn42l9jE/W2xpc64g4OdB66V9
	fPidHP3bMKZb0K35ddFmsuwO0MDg073VT8HHai+NQVhMjtkoeFuXKtRy5ttF1OXWiGLGmpS5Fsv6e
	J0Q/xmljpFwMkFijzWppDUEFzwr9EKGUJzrDhhs200dgdrJGOvQXsP3oM6g/ty+AnWdWwrx/3ALVf
	rQCmV8EIMkEnOMJ32KDFEdjGGJlm977h3UGjKWYlz+V3i+CvkIKoZtV4c1/Cz92/ZYTEaeAtYzTm4
	vgnKYk0QHAkE+rMu18ulwOKgycl52uFEMqLXtw8by1GPdZgc+KdgYE3+24nuPK8+BAktwq9rq9ziq
	ByicxpNQ==;
Received: from host-877adb.obg1.zeelandnet.nl ([62.238.53.109] helo=worktop)
	by bombadil.infradead.org with esmtpsa (Exim 4.87 #1 (Red Hat
	Linux)) id 1dhLC2-0001sl-JD; Mon, 14 Aug 2017 19:38:31 +0000
Received: by worktop (Postfix, from userid 1000)
	id CB40D6E08A0; Mon, 14 Aug 2017 21:38:28 +0200 (CEST)
Date: Mon, 14 Aug 2017 21:38:28 +0200
From: Peter Zijlstra &lt;peterz@infradead.org&gt;
To: Nadav Amit &lt;namit@vmware.com&gt;
Cc: Minchan Kim &lt;minchan@kernel.org&gt;, Ingo Molnar &lt;mingo@kernel.org&gt;,
	Stephen Rothwell &lt;sfr@canb.auug.org.au&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	Thomas Gleixner &lt;tglx@linutronix.de&gt;,
	Ingo Molnar &lt;mingo@elte.hu&gt;, &quot;H. Peter Anvin&quot; &lt;hpa@zytor.com&gt;,
	Linux-Next Mailing List &lt;linux-next@vger.kernel.org&gt;,
	Linux Kernel Mailing List &lt;linux-kernel@vger.kernel.org&gt;,
	Linus &lt;torvalds@linux-foundation.org&gt;
Subject: Re: linux-next: manual merge of the akpm-current tree with the tip
	tree
Message-ID: &lt;20170814193828.GN6524@worktop.programming.kicks-ass.net&gt;
References: &lt;20170811175326.36d546dc@canb.auug.org.au&gt;
	&lt;20170811093449.w5wttpulmwfykjzm@hirez.programming.kicks-ass.net&gt;
	&lt;20170811214556.322b3c4e@canb.auug.org.au&gt;
	&lt;20170811115607.p2vgqcp7w3wurhvw@gmail.com&gt;
	&lt;20170811140450.irhxa2bhdpmmhhpv@hirez.programming.kicks-ass.net&gt;
	&lt;DE232310-8D7E-4074-ACFE-FE6416B13A3F@vmware.com&gt;
	&lt;20170813125019.ihqjud37ytgri7bn@hirez.programming.kicks-ass.net&gt;
	&lt;20170814031613.GD25427@bbox&gt;
	&lt;0F858068-D41D-46E3-B4A8-8A95B4EDB94F@vmware.com&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=utf-8
Content-Disposition: inline
Content-Transfer-Encoding: 8bit
In-Reply-To: &lt;0F858068-D41D-46E3-B4A8-8A95B4EDB94F@vmware.com&gt;
User-Agent: Mutt/1.5.22.1 (2013-10-16)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - Aug. 14, 2017, 7:38 p.m.</div>
<pre class="content">
On Mon, Aug 14, 2017 at 05:07:19AM +0000, Nadav Amit wrote:
<span class="quote">&gt; &gt;&gt; So I&#39;m not entirely clear about this yet.</span>
<span class="quote">&gt; &gt;&gt; </span>
<span class="quote">&gt; &gt;&gt; How about:</span>
<span class="quote">&gt; &gt;&gt; </span>
<span class="quote">&gt; &gt;&gt; </span>
<span class="quote">&gt; &gt;&gt; 	CPU0				CPU1</span>
<span class="quote">&gt; &gt;&gt; </span>
<span class="quote">&gt; &gt;&gt; 					tlb_gather_mmu()</span>
<span class="quote">&gt; &gt;&gt; </span>
<span class="quote">&gt; &gt;&gt; 					lock PTLn</span>
<span class="quote">&gt; &gt;&gt; 					no mod</span>
<span class="quote">&gt; &gt;&gt; 					unlock PTLn</span>
<span class="quote">&gt; &gt;&gt; </span>
<span class="quote">&gt; &gt;&gt; 	tlb_gather_mmu()</span>
<span class="quote">&gt; &gt;&gt; </span>
<span class="quote">&gt; &gt;&gt; 					lock PTLm</span>
<span class="quote">&gt; &gt;&gt; 					mod</span>
<span class="quote">&gt; &gt;&gt; 					include in tlb range</span>
<span class="quote">&gt; &gt;&gt; 					unlock PTLm</span>
<span class="quote">&gt; &gt;&gt; </span>
<span class="quote">&gt; &gt;&gt; 	lock PTLn</span>
<span class="quote">&gt; &gt;&gt; 	mod</span>
<span class="quote">&gt; &gt;&gt; 	unlock PTLn</span>
<span class="quote">&gt; &gt;&gt; </span>
<span class="quote">&gt; &gt;&gt; 					tlb_finish_mmu()</span>
<span class="quote">&gt; &gt;&gt; 					  force = mm_tlb_flush_nested(tlb-&gt;mm);</span>
<span class="quote">&gt; &gt;&gt; 					  arch_tlb_finish_mmu(force);</span>
<span class="quote">&gt; &gt;&gt; </span>
<span class="quote">&gt; &gt;&gt; </span>
<span class="quote">&gt; &gt;&gt; 	... more ...</span>
<span class="quote">&gt; &gt;&gt; </span>
<span class="quote">&gt; &gt;&gt; 	tlb_finish_mmu()</span>
<span class="quote">&gt; &gt;&gt; </span>
<span class="quote">&gt; &gt;&gt; </span>
<span class="quote">&gt; &gt;&gt; </span>
<span class="quote">&gt; &gt;&gt; In this case you also want CPU1&#39;s mm_tlb_flush_nested() call to return</span>
<span class="quote">&gt; &gt;&gt; true, right?</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; No, because CPU 1 mofified pte and added it into tlb range</span>
<span class="quote">&gt; &gt; so regardless of nested, it will flush TLB so there is no stale</span>
<span class="quote">&gt; &gt; TLB problem.</span>
<span class="quote">
&gt; To clarify: the main problem that these patches address is when the first</span>
<span class="quote">&gt; CPU updates the PTE, and second CPU sees the updated value and thinks: “the</span>
<span class="quote">&gt; PTE is already what I wanted - no flush is needed”.</span>

OK, that simplifies things.
<span class="quote">
&gt; For some reason (I would assume intentional), all the examples here first</span>
<span class="quote">&gt; “do not modify” the PTE, and then modify it - which is not an “interesting”</span>
<span class="quote">&gt; case.</span>

Depends on what you call &#39;interesting&#39; :-) They are &#39;interesting&#39; to
make work from a memory ordering POV. And since I didn&#39;t get they were
excluded from the set, I worried.

In fact, if they were to be included, I couldn&#39;t make it work at all. So
I&#39;m really glad to hear we can disregard them.
<span class="quote">
&gt; However, based on what I understand on the memory barriers, I think</span>
<span class="quote">&gt; there is indeed a missing barrier before reading it in</span>
<span class="quote">&gt; mm_tlb_flush_nested(). IIUC using smp_mb__after_unlock_lock() in this case,</span>
<span class="quote">&gt; before reading, would solve the problem with least impact on systems with</span>
<span class="quote">&gt; strong memory ordering.</span>

No, all is well. If, as you say, we&#39;re naturally constrained to the case
where we only care about prior modification we can rely on the RCpc PTL
locks.

Consider:


	CPU0				CPU1

					tlb_gather_mmu()

	tlb_gather_mmu()
	  inc	--------.
			| (inc is constrained by RELEASE)
	lock PTLn	|
	mod		^
	unlock PTLn -----------------&gt;	lock PTLn
				v	no mod
				|	unlock PTLn
				|
				|	lock PTLm
				|	mod
				|	include in tlb range
				|	unlock PTLm
				|
	(read is constrained	|
	          by ACQUIRE)	|
				|	tlb_finish_mmu()
				`----	  force = mm_tlb_flush_nested(tlb-&gt;mm);
					  arch_tlb_finish_mmu(force);


	... more ...

	tlb_finish_mmu()


Then CPU1&#39;s acquire of PTLn orders against CPU0&#39;s release of that same
PTLn which guarantees we observe both its (prior) modified PTE and the
mm-&gt;tlb_flush_pending increment from tlb_gather_mmu().

So all we need for mm_tlb_flush_nested() to work is having acquired the
right PTL at least once before calling it.

At the same time, the decrements need to be after the TLB invalidate is
complete, this ensures that _IF_ we observe the decrement, we must&#39;ve
also observed the corresponding invalidate.

Something like the below is then sufficient.

---
Subject: mm: Clarify tlb_flush_pending barriers
<span class="from">From: Peter Zijlstra &lt;peterz@infradead.org&gt;</span>
Date: Fri, 11 Aug 2017 16:04:50 +0200

Better document the ordering around tlb_flush_pending.
<span class="signed-off-by">
Signed-off-by: Peter Zijlstra (Intel) &lt;peterz@infradead.org&gt;</span>
---
 include/linux/mm_types.h |   78 +++++++++++++++++++++++++++--------------------
 1 file changed, 45 insertions(+), 33 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=159401">Nadav Amit</a> - Aug. 15, 2017, 7:51 a.m.</div>
<pre class="content">
Peter Zijlstra &lt;peterz@infradead.org&gt; wrote:
<span class="quote">
&gt; On Mon, Aug 14, 2017 at 05:07:19AM +0000, Nadav Amit wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; So I&#39;m not entirely clear about this yet.</span>
<span class="quote">&gt;&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt; How about:</span>
<span class="quote">&gt;&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt; 	CPU0				CPU1</span>
<span class="quote">&gt;&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt; 					tlb_gather_mmu()</span>
<span class="quote">&gt;&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt; 					lock PTLn</span>
<span class="quote">&gt;&gt;&gt;&gt; 					no mod</span>
<span class="quote">&gt;&gt;&gt;&gt; 					unlock PTLn</span>
<span class="quote">&gt;&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt; 	tlb_gather_mmu()</span>
<span class="quote">&gt;&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt; 					lock PTLm</span>
<span class="quote">&gt;&gt;&gt;&gt; 					mod</span>
<span class="quote">&gt;&gt;&gt;&gt; 					include in tlb range</span>
<span class="quote">&gt;&gt;&gt;&gt; 					unlock PTLm</span>
<span class="quote">&gt;&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt; 	lock PTLn</span>
<span class="quote">&gt;&gt;&gt;&gt; 	mod</span>
<span class="quote">&gt;&gt;&gt;&gt; 	unlock PTLn</span>
<span class="quote">&gt;&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt; 					tlb_finish_mmu()</span>
<span class="quote">&gt;&gt;&gt;&gt; 					  force = mm_tlb_flush_nested(tlb-&gt;mm);</span>
<span class="quote">&gt;&gt;&gt;&gt; 					  arch_tlb_finish_mmu(force);</span>
<span class="quote">&gt;&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt; 	... more ...</span>
<span class="quote">&gt;&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt; 	tlb_finish_mmu()</span>
<span class="quote">&gt;&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt; In this case you also want CPU1&#39;s mm_tlb_flush_nested() call to return</span>
<span class="quote">&gt;&gt;&gt;&gt; true, right?</span>
<span class="quote">&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt; No, because CPU 1 mofified pte and added it into tlb range</span>
<span class="quote">&gt;&gt;&gt; so regardless of nested, it will flush TLB so there is no stale</span>
<span class="quote">&gt;&gt;&gt; TLB problem.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; To clarify: the main problem that these patches address is when the first</span>
<span class="quote">&gt;&gt; CPU updates the PTE, and second CPU sees the updated value and thinks: “the</span>
<span class="quote">&gt;&gt; PTE is already what I wanted - no flush is needed”.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; OK, that simplifies things.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; For some reason (I would assume intentional), all the examples here first</span>
<span class="quote">&gt;&gt; “do not modify” the PTE, and then modify it - which is not an “interesting”</span>
<span class="quote">&gt;&gt; case.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Depends on what you call &#39;interesting&#39; :-) They are &#39;interesting&#39; to</span>
<span class="quote">&gt; make work from a memory ordering POV. And since I didn&#39;t get they were</span>
<span class="quote">&gt; excluded from the set, I worried.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; In fact, if they were to be included, I couldn&#39;t make it work at all. So</span>
<span class="quote">&gt; I&#39;m really glad to hear we can disregard them.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; However, based on what I understand on the memory barriers, I think</span>
<span class="quote">&gt;&gt; there is indeed a missing barrier before reading it in</span>
<span class="quote">&gt;&gt; mm_tlb_flush_nested(). IIUC using smp_mb__after_unlock_lock() in this case,</span>
<span class="quote">&gt;&gt; before reading, would solve the problem with least impact on systems with</span>
<span class="quote">&gt;&gt; strong memory ordering.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; No, all is well. If, as you say, we&#39;re naturally constrained to the case</span>
<span class="quote">&gt; where we only care about prior modification we can rely on the RCpc PTL</span>
<span class="quote">&gt; locks.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Consider:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	CPU0				CPU1</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 					tlb_gather_mmu()</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	tlb_gather_mmu()</span>
<span class="quote">&gt; 	  inc	--------.</span>
<span class="quote">&gt; 			| (inc is constrained by RELEASE)</span>
<span class="quote">&gt; 	lock PTLn	|</span>
<span class="quote">&gt; 	mod		^</span>
<span class="quote">&gt; 	unlock PTLn -----------------&gt;	lock PTLn</span>
<span class="quote">&gt; 				v	no mod</span>
<span class="quote">&gt; 				|	unlock PTLn</span>
<span class="quote">&gt; 				|</span>
<span class="quote">&gt; 				|	lock PTLm</span>
<span class="quote">&gt; 				|	mod</span>
<span class="quote">&gt; 				|	include in tlb range</span>
<span class="quote">&gt; 				|	unlock PTLm</span>
<span class="quote">&gt; 				|</span>
<span class="quote">&gt; 	(read is constrained	|</span>
<span class="quote">&gt; 	          by ACQUIRE)	|</span>
<span class="quote">&gt; 				|	tlb_finish_mmu()</span>
<span class="quote">&gt; 				`----	  force = mm_tlb_flush_nested(tlb-&gt;mm);</span>
<span class="quote">&gt; 					  arch_tlb_finish_mmu(force);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	... more ...</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	tlb_finish_mmu()</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Then CPU1&#39;s acquire of PTLn orders against CPU0&#39;s release of that same</span>
<span class="quote">&gt; PTLn which guarantees we observe both its (prior) modified PTE and the</span>
<span class="quote">&gt; mm-&gt;tlb_flush_pending increment from tlb_gather_mmu().</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So all we need for mm_tlb_flush_nested() to work is having acquired the</span>
<span class="quote">&gt; right PTL at least once before calling it.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; At the same time, the decrements need to be after the TLB invalidate is</span>
<span class="quote">&gt; complete, this ensures that _IF_ we observe the decrement, we must&#39;ve</span>
<span class="quote">&gt; also observed the corresponding invalidate.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Something like the below is then sufficient.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt; Subject: mm: Clarify tlb_flush_pending barriers</span>
<span class="quote">&gt; From: Peter Zijlstra &lt;peterz@infradead.org&gt;</span>
<span class="quote">&gt; Date: Fri, 11 Aug 2017 16:04:50 +0200</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Better document the ordering around tlb_flush_pending.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Peter Zijlstra (Intel) &lt;peterz@infradead.org&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt; include/linux/mm_types.h |   78 +++++++++++++++++++++++++++--------------------</span>
<span class="quote">&gt; 1 file changed, 45 insertions(+), 33 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; --- a/include/linux/mm_types.h</span>
<span class="quote">&gt; +++ b/include/linux/mm_types.h</span>
<span class="quote">&gt; @@ -526,30 +526,6 @@ extern void tlb_gather_mmu(struct mmu_ga</span>
<span class="quote">&gt; extern void tlb_finish_mmu(struct mmu_gather *tlb,</span>
<span class="quote">&gt; 				unsigned long start, unsigned long end);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -/*</span>
<span class="quote">&gt; - * Memory barriers to keep this state in sync are graciously provided by</span>
<span class="quote">&gt; - * the page table locks, outside of which no page table modifications happen.</span>
<span class="quote">&gt; - * The barriers are used to ensure the order between tlb_flush_pending updates,</span>
<span class="quote">&gt; - * which happen while the lock is not taken, and the PTE updates, which happen</span>
<span class="quote">&gt; - * while the lock is taken, are serialized.</span>
<span class="quote">&gt; - */</span>
<span class="quote">&gt; -static inline bool mm_tlb_flush_pending(struct mm_struct *mm)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	/*</span>
<span class="quote">&gt; -	 * Must be called with PTL held; such that our PTL acquire will have</span>
<span class="quote">&gt; -	 * observed the store from set_tlb_flush_pending().</span>
<span class="quote">&gt; -	 */</span>
<span class="quote">&gt; -	return atomic_read(&amp;mm-&gt;tlb_flush_pending) &gt; 0;</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -/*</span>
<span class="quote">&gt; - * Returns true if there are two above TLB batching threads in parallel.</span>
<span class="quote">&gt; - */</span>
<span class="quote">&gt; -static inline bool mm_tlb_flush_nested(struct mm_struct *mm)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	return atomic_read(&amp;mm-&gt;tlb_flush_pending) &gt; 1;</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; static inline void init_tlb_flush_pending(struct mm_struct *mm)</span>
<span class="quote">&gt; {</span>
<span class="quote">&gt; 	atomic_set(&amp;mm-&gt;tlb_flush_pending, 0);</span>
<span class="quote">&gt; @@ -558,7 +534,6 @@ static inline void init_tlb_flush_pendin</span>
<span class="quote">&gt; static inline void inc_tlb_flush_pending(struct mm_struct *mm)</span>
<span class="quote">&gt; {</span>
<span class="quote">&gt; 	atomic_inc(&amp;mm-&gt;tlb_flush_pending);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; 	/*</span>
<span class="quote">&gt; 	 * The only time this value is relevant is when there are indeed pages</span>
<span class="quote">&gt; 	 * to flush. And we&#39;ll only flush pages after changing them, which</span>
<span class="quote">&gt; @@ -580,24 +555,61 @@ static inline void inc_tlb_flush_pending</span>
<span class="quote">&gt; 	 *	flush_tlb_range();</span>
<span class="quote">&gt; 	 *	atomic_dec(&amp;mm-&gt;tlb_flush_pending);</span>
<span class="quote">&gt; 	 *</span>
<span class="quote">&gt; -	 * So the =true store is constrained by the PTL unlock, and the =false</span>
<span class="quote">&gt; -	 * store is constrained by the TLB invalidate.</span>
<span class="quote">&gt; +	 * Where the increment if constrained by the PTL unlock, it thus</span>
<span class="quote">&gt; +	 * ensures that the increment is visible if the PTE modification is</span>
<span class="quote">&gt; +	 * visible. After all, if there is no PTE modification, nobody cares</span>
<span class="quote">&gt; +	 * about TLB flushes either.</span>
<span class="quote">&gt; +	 *</span>
<span class="quote">&gt; +	 * This very much relies on users (mm_tlb_flush_pending() and</span>
<span class="quote">&gt; +	 * mm_tlb_flush_nested()) only caring about _specific_ PTEs (and</span>
<span class="quote">&gt; +	 * therefore specific PTLs), because with SPLIT_PTE_PTLOCKS and RCpc</span>
<span class="quote">&gt; +	 * locks (PPC) the unlock of one doesn&#39;t order against the lock of</span>
<span class="quote">&gt; +	 * another PTL.</span>
<span class="quote">&gt; +	 *</span>
<span class="quote">&gt; +	 * The decrement is ordered by the flush_tlb_range(), such that</span>
<span class="quote">&gt; +	 * mm_tlb_flush_pending() will not return false unless all flushes have</span>
<span class="quote">&gt; +	 * completed.</span>
<span class="quote">&gt; 	 */</span>
<span class="quote">&gt; }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -/* Clearing is done after a TLB flush, which also provides a barrier. */</span>
<span class="quote">&gt; static inline void dec_tlb_flush_pending(struct mm_struct *mm)</span>
<span class="quote">&gt; {</span>
<span class="quote">&gt; 	/*</span>
<span class="quote">&gt; -	 * Guarantee that the tlb_flush_pending does not not leak into the</span>
<span class="quote">&gt; -	 * critical section, since we must order the PTE change and changes to</span>
<span class="quote">&gt; -	 * the pending TLB flush indication. We could have relied on TLB flush</span>
<span class="quote">&gt; -	 * as a memory barrier, but this behavior is not clearly documented.</span>
<span class="quote">&gt; +	 * See inc_tlb_flush_pending().</span>
<span class="quote">&gt; +	 *</span>
<span class="quote">&gt; +	 * This cannot be smp_mb__before_atomic() because smp_mb() simply does</span>
<span class="quote">&gt; +	 * not order against TLB invalidate completion, which is what we need.</span>
<span class="quote">&gt; +	 *</span>
<span class="quote">&gt; +	 * Therefore we must rely on tlb_flush_*() to guarantee order.</span>
<span class="quote">&gt; 	 */</span>
<span class="quote">&gt; -	smp_mb__before_atomic();</span>
<span class="quote">&gt; 	atomic_dec(&amp;mm-&gt;tlb_flush_pending);</span>
<span class="quote">&gt; }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; +static inline bool mm_tlb_flush_pending(struct mm_struct *mm)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * Must be called after having acquired the PTL; orders against that</span>
<span class="quote">&gt; +	 * PTLs release and therefore ensures that if we observe the modified</span>
<span class="quote">&gt; +	 * PTE we must also observe the increment from inc_tlb_flush_pending().</span>
<span class="quote">&gt; +	 *</span>
<span class="quote">&gt; +	 * That is, it only guarantees to return true if there is a flush</span>
<span class="quote">&gt; +	 * pending for _this_ PTL.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	return atomic_read(&amp;mm-&gt;tlb_flush_pending);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline bool mm_tlb_flush_nested(struct mm_struct *mm)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * Similar to mm_tlb_flush_pending(), we must have acquired the PTL</span>
<span class="quote">&gt; +	 * for which there is a TLB flush pending in order to guarantee</span>
<span class="quote">&gt; +	 * we&#39;ve seen both that PTE modification and the increment.</span>
<span class="quote">&gt; +	 *</span>
<span class="quote">&gt; +	 * (no requirement on actually still holding the PTL, that is irrelevant)</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	return atomic_read(&amp;mm-&gt;tlb_flush_pending) &gt; 1;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; struct vm_fault;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; struct vm_special_mapping {</span>

Thanks for the detailed explanation. I will pay more attention next time.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">--- a/include/linux/mm_types.h</span>
<span class="p_header">+++ b/include/linux/mm_types.h</span>
<span class="p_chunk">@@ -526,30 +526,6 @@</span> <span class="p_context"> extern void tlb_gather_mmu(struct mmu_ga</span>
 extern void tlb_finish_mmu(struct mmu_gather *tlb,
 				unsigned long start, unsigned long end);
 
<span class="p_del">-/*</span>
<span class="p_del">- * Memory barriers to keep this state in sync are graciously provided by</span>
<span class="p_del">- * the page table locks, outside of which no page table modifications happen.</span>
<span class="p_del">- * The barriers are used to ensure the order between tlb_flush_pending updates,</span>
<span class="p_del">- * which happen while the lock is not taken, and the PTE updates, which happen</span>
<span class="p_del">- * while the lock is taken, are serialized.</span>
<span class="p_del">- */</span>
<span class="p_del">-static inline bool mm_tlb_flush_pending(struct mm_struct *mm)</span>
<span class="p_del">-{</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Must be called with PTL held; such that our PTL acquire will have</span>
<span class="p_del">-	 * observed the store from set_tlb_flush_pending().</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	return atomic_read(&amp;mm-&gt;tlb_flush_pending) &gt; 0;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
<span class="p_del">- * Returns true if there are two above TLB batching threads in parallel.</span>
<span class="p_del">- */</span>
<span class="p_del">-static inline bool mm_tlb_flush_nested(struct mm_struct *mm)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return atomic_read(&amp;mm-&gt;tlb_flush_pending) &gt; 1;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 static inline void init_tlb_flush_pending(struct mm_struct *mm)
 {
 	atomic_set(&amp;mm-&gt;tlb_flush_pending, 0);
<span class="p_chunk">@@ -558,7 +534,6 @@</span> <span class="p_context"> static inline void init_tlb_flush_pendin</span>
 static inline void inc_tlb_flush_pending(struct mm_struct *mm)
 {
 	atomic_inc(&amp;mm-&gt;tlb_flush_pending);
<span class="p_del">-</span>
 	/*
 	 * The only time this value is relevant is when there are indeed pages
 	 * to flush. And we&#39;ll only flush pages after changing them, which
<span class="p_chunk">@@ -580,24 +555,61 @@</span> <span class="p_context"> static inline void inc_tlb_flush_pending</span>
 	 *	flush_tlb_range();
 	 *	atomic_dec(&amp;mm-&gt;tlb_flush_pending);
 	 *
<span class="p_del">-	 * So the =true store is constrained by the PTL unlock, and the =false</span>
<span class="p_del">-	 * store is constrained by the TLB invalidate.</span>
<span class="p_add">+	 * Where the increment if constrained by the PTL unlock, it thus</span>
<span class="p_add">+	 * ensures that the increment is visible if the PTE modification is</span>
<span class="p_add">+	 * visible. After all, if there is no PTE modification, nobody cares</span>
<span class="p_add">+	 * about TLB flushes either.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * This very much relies on users (mm_tlb_flush_pending() and</span>
<span class="p_add">+	 * mm_tlb_flush_nested()) only caring about _specific_ PTEs (and</span>
<span class="p_add">+	 * therefore specific PTLs), because with SPLIT_PTE_PTLOCKS and RCpc</span>
<span class="p_add">+	 * locks (PPC) the unlock of one doesn&#39;t order against the lock of</span>
<span class="p_add">+	 * another PTL.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * The decrement is ordered by the flush_tlb_range(), such that</span>
<span class="p_add">+	 * mm_tlb_flush_pending() will not return false unless all flushes have</span>
<span class="p_add">+	 * completed.</span>
 	 */
 }
 
<span class="p_del">-/* Clearing is done after a TLB flush, which also provides a barrier. */</span>
 static inline void dec_tlb_flush_pending(struct mm_struct *mm)
 {
 	/*
<span class="p_del">-	 * Guarantee that the tlb_flush_pending does not not leak into the</span>
<span class="p_del">-	 * critical section, since we must order the PTE change and changes to</span>
<span class="p_del">-	 * the pending TLB flush indication. We could have relied on TLB flush</span>
<span class="p_del">-	 * as a memory barrier, but this behavior is not clearly documented.</span>
<span class="p_add">+	 * See inc_tlb_flush_pending().</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * This cannot be smp_mb__before_atomic() because smp_mb() simply does</span>
<span class="p_add">+	 * not order against TLB invalidate completion, which is what we need.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * Therefore we must rely on tlb_flush_*() to guarantee order.</span>
 	 */
<span class="p_del">-	smp_mb__before_atomic();</span>
 	atomic_dec(&amp;mm-&gt;tlb_flush_pending);
 }
 
<span class="p_add">+static inline bool mm_tlb_flush_pending(struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Must be called after having acquired the PTL; orders against that</span>
<span class="p_add">+	 * PTLs release and therefore ensures that if we observe the modified</span>
<span class="p_add">+	 * PTE we must also observe the increment from inc_tlb_flush_pending().</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * That is, it only guarantees to return true if there is a flush</span>
<span class="p_add">+	 * pending for _this_ PTL.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	return atomic_read(&amp;mm-&gt;tlb_flush_pending);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline bool mm_tlb_flush_nested(struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Similar to mm_tlb_flush_pending(), we must have acquired the PTL</span>
<span class="p_add">+	 * for which there is a TLB flush pending in order to guarantee</span>
<span class="p_add">+	 * we&#39;ve seen both that PTE modification and the increment.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * (no requirement on actually still holding the PTL, that is irrelevant)</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	return atomic_read(&amp;mm-&gt;tlb_flush_pending) &gt; 1;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 struct vm_fault;
 
 struct vm_special_mapping {

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



