
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[RFC,06/14] sparc64: general shared context tsb creation and support - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [RFC,06/14] sparc64: general shared context tsb creation and support</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=124511">Mike Kravetz</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Dec. 16, 2016, 6:35 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1481913337-9331-7-git-send-email-mike.kravetz@oracle.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9478221/mbox/"
   >mbox</a>
|
   <a href="/patch/9478221/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9478221/">/patch/9478221/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	7BF4C60828 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 16 Dec 2016 18:37:32 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 7212D28780
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 16 Dec 2016 18:37:32 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 6680228783; Fri, 16 Dec 2016 18:37:32 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	UNPARSEABLE_RELAY autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id A7BD228782
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 16 Dec 2016 18:37:31 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1761933AbcLPSh0 (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Fri, 16 Dec 2016 13:37:26 -0500
Received: from aserp1040.oracle.com ([141.146.126.69]:39504 &quot;EHLO
	aserp1040.oracle.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1761615AbcLPShH (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Fri, 16 Dec 2016 13:37:07 -0500
Received: from aserv0021.oracle.com (aserv0021.oracle.com [141.146.126.233])
	by aserp1040.oracle.com (Sentrion-MTA-4.3.2/Sentrion-MTA-4.3.2)
	with ESMTP id uBGIZwBU024945
	(version=TLSv1 cipher=DHE-RSA-AES256-SHA bits=256 verify=OK);
	Fri, 16 Dec 2016 18:35:58 GMT
Received: from userv0122.oracle.com (userv0122.oracle.com [156.151.31.75])
	by aserv0021.oracle.com (8.13.8/8.14.4) with ESMTP id uBGIZvtd021666
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-SHA bits=256 verify=OK);
	Fri, 16 Dec 2016 18:35:58 GMT
Received: from abhmp0002.oracle.com (abhmp0002.oracle.com [141.146.116.8])
	by userv0122.oracle.com (8.14.4/8.14.4) with ESMTP id uBGIZvUq020100; 
	Fri, 16 Dec 2016 18:35:57 GMT
Received: from monkey.oracle.com (/50.188.161.229)
	by default (Oracle Beehive Gateway v4.0)
	with ESMTP ; Fri, 16 Dec 2016 10:35:57 -0800
From: Mike Kravetz &lt;mike.kravetz@oracle.com&gt;
To: sparclinux@vger.kernel.org, linux-mm@kvack.org,
	linux-kernel@vger.kernel.org
Cc: &quot;David S . Miller&quot; &lt;davem@davemloft.net&gt;,
	Bob Picco &lt;bob.picco@oracle.com&gt;, Nitin Gupta &lt;nitin.m.gupta@oracle.com&gt;,
	Vijay Kumar &lt;vijay.ac.kumar@oracle.com&gt;,
	Julian Calaby &lt;julian.calaby@gmail.com&gt;,
	Adam Buchbinder &lt;adam.buchbinder@gmail.com&gt;,
	&quot;Kirill A . Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt;,
	Michal Hocko &lt;mhocko@suse.com&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	Mike Kravetz &lt;mike.kravetz@oracle.com&gt;
Subject: [RFC PATCH 06/14] sparc64: general shared context tsb creation and
	support
Date: Fri, 16 Dec 2016 10:35:29 -0800
Message-Id: &lt;1481913337-9331-7-git-send-email-mike.kravetz@oracle.com&gt;
X-Mailer: git-send-email 2.7.4
In-Reply-To: &lt;1481913337-9331-1-git-send-email-mike.kravetz@oracle.com&gt;
References: &lt;1481913337-9331-1-git-send-email-mike.kravetz@oracle.com&gt;
X-Source-IP: aserv0021.oracle.com [141.146.126.233]
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=124511">Mike Kravetz</a> - Dec. 16, 2016, 6:35 p.m.</div>
<pre class="content">
Take into account the shared context TSB when creating and updating
TSBs.  Existing routines are modified to key off the TSB index or
PTE flag (_PAGE_SHR_CTX_4V) to determine this is a shared context
operation.

With shared context support the sun4v TSB descriptor array could
contain a &#39;hole&#39; if there is a shared context TSB and no huge page
TSB. An array with a hole can not be bassed to the hypervisor, so
make sure no hole exists in the array.

For shared context TSBs, the context index in the hypervisor descriptor
structure is set to 1.  This indicates the context ID stored in context
register 1 should be used for TLB matching.

This commit does NOT load the shared context TSB into the hv MMU.
<span class="signed-off-by">
Signed-off-by: Mike Kravetz &lt;mike.kravetz@oracle.com&gt;</span>
---
 arch/sparc/mm/fault_64.c    | 10 ++++++++++
 arch/sparc/mm/hugetlbpage.c | 20 ++++++++++++++++----
 arch/sparc/mm/init_64.c     | 42 +++++++++++++++++++++++++++++++++++++++---
 arch/sparc/mm/tsb.c         | 41 ++++++++++++++++++++++++++++++++++++++++-
 4 files changed, 105 insertions(+), 8 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=8352">Sam Ravnborg</a> - Dec. 17, 2016, 7:53 a.m.</div>
<pre class="content">
Hi Mike
<span class="quote">
&gt; --- a/arch/sparc/mm/hugetlbpage.c</span>
<span class="quote">&gt; +++ b/arch/sparc/mm/hugetlbpage.c</span>
<span class="quote">&gt; @@ -162,8 +162,14 @@ void set_huge_pte_at(struct mm_struct *mm, unsigned long addr,</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	pte_t orig;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (!pte_present(*ptep) &amp;&amp; pte_present(entry))</span>
<span class="quote">&gt; -		mm-&gt;context.hugetlb_pte_count++;</span>
<span class="quote">&gt; +	if (!pte_present(*ptep) &amp;&amp; pte_present(entry)) {</span>
<span class="quote">&gt; +#if defined(CONFIG_SHARED_MMU_CTX)</span>
<span class="quote">&gt; +		if (pte_val(entry) | _PAGE_SHR_CTX_4V)</span>
<span class="quote">&gt; +			mm-&gt;context.shared_hugetlb_pte_count++;</span>
<span class="quote">&gt; +		else</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; +			mm-&gt;context.hugetlb_pte_count++;</span>
<span class="quote">&gt; +	}</span>

This kind of conditional code it just too ugly to survive...
Could a static inline be used to help you here?
The compiler will inline it so there should not be any run-time cost
<span class="quote">
&gt;  </span>
<span class="quote">&gt;  	mm_rss -= saved_thp_pte_count * (HPAGE_SIZE / PAGE_SIZE);</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt; @@ -544,8 +576,10 @@ int init_new_context(struct task_struct *tsk, struct mm_struct *mm)</span>
<span class="quote">&gt;  	 * us, so we need to zero out the TSB pointer or else tsb_grow()</span>
<span class="quote">&gt;  	 * will be confused and think there is an older TSB to free up.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt; -	for (i = 0; i &lt; MM_NUM_TSBS; i++)</span>
<span class="quote">&gt; +	for (i = 0; i &lt; MM_NUM_TSBS; i++) {</span>
<span class="quote">&gt;  		mm-&gt;context.tsb_block[i].tsb = NULL;</span>
<span class="quote">&gt; +		mm-&gt;context.tsb_descr[i].tsb_base = 0UL;</span>
<span class="quote">&gt; +	}</span>
This change seems un-related to the rest?

	Sam
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=124511">Mike Kravetz</a> - Dec. 19, 2016, 12:52 a.m.</div>
<pre class="content">
On 12/16/2016 11:53 PM, Sam Ravnborg wrote:
<span class="quote">&gt; Hi Mike</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; --- a/arch/sparc/mm/hugetlbpage.c</span>
<span class="quote">&gt;&gt; +++ b/arch/sparc/mm/hugetlbpage.c</span>
<span class="quote">&gt;&gt; @@ -162,8 +162,14 @@ void set_huge_pte_at(struct mm_struct *mm, unsigned long addr,</span>
<span class="quote">&gt;&gt;  {</span>
<span class="quote">&gt;&gt;  	pte_t orig;</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; -	if (!pte_present(*ptep) &amp;&amp; pte_present(entry))</span>
<span class="quote">&gt;&gt; -		mm-&gt;context.hugetlb_pte_count++;</span>
<span class="quote">&gt;&gt; +	if (!pte_present(*ptep) &amp;&amp; pte_present(entry)) {</span>
<span class="quote">&gt;&gt; +#if defined(CONFIG_SHARED_MMU_CTX)</span>
<span class="quote">&gt;&gt; +		if (pte_val(entry) | _PAGE_SHR_CTX_4V)</span>
<span class="quote">&gt;&gt; +			mm-&gt;context.shared_hugetlb_pte_count++;</span>
<span class="quote">&gt;&gt; +		else</span>
<span class="quote">&gt;&gt; +#endif</span>
<span class="quote">&gt;&gt; +			mm-&gt;context.hugetlb_pte_count++;</span>
<span class="quote">&gt;&gt; +	}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This kind of conditional code it just too ugly to survive...</span>
<span class="quote">&gt; Could a static inline be used to help you here?</span>
<span class="quote">&gt; The compiler will inline it so there should not be any run-time cost</span>

Yes, this can be cleaned up in that way.
<span class="quote">
&gt; </span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  	mm_rss -= saved_thp_pte_count * (HPAGE_SIZE / PAGE_SIZE);</span>
<span class="quote">&gt;&gt;  #endif</span>
<span class="quote">&gt;&gt; @@ -544,8 +576,10 @@ int init_new_context(struct task_struct *tsk, struct mm_struct *mm)</span>
<span class="quote">&gt;&gt;  	 * us, so we need to zero out the TSB pointer or else tsb_grow()</span>
<span class="quote">&gt;&gt;  	 * will be confused and think there is an older TSB to free up.</span>
<span class="quote">&gt;&gt;  	 */</span>
<span class="quote">&gt;&gt; -	for (i = 0; i &lt; MM_NUM_TSBS; i++)</span>
<span class="quote">&gt;&gt; +	for (i = 0; i &lt; MM_NUM_TSBS; i++) {</span>
<span class="quote">&gt;&gt;  		mm-&gt;context.tsb_block[i].tsb = NULL;</span>
<span class="quote">&gt;&gt; +		mm-&gt;context.tsb_descr[i].tsb_base = 0UL;</span>
<span class="quote">&gt;&gt; +	}</span>
<span class="quote">&gt; This change seems un-related to the rest?</span>

Correct.  I was experimenting with some other ways of managing the tsb_descr
array that got dropped, but forgot to remove this.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/sparc/mm/fault_64.c b/arch/sparc/mm/fault_64.c</span>
<span class="p_header">index 643c149..2b82cdb 100644</span>
<span class="p_header">--- a/arch/sparc/mm/fault_64.c</span>
<span class="p_header">+++ b/arch/sparc/mm/fault_64.c</span>
<span class="p_chunk">@@ -493,6 +493,16 @@</span> <span class="p_context"> asmlinkage void __kprobes do_sparc64_fault(struct pt_regs *regs)</span>
 			hugetlb_setup(regs);
 
 	}
<span class="p_add">+#if defined(CONFIG_SHARED_MMU_CTX)</span>
<span class="p_add">+	mm_rss = mm-&gt;context.shared_hugetlb_pte_count * REAL_HPAGE_PER_HPAGE;</span>
<span class="p_add">+	if (unlikely(mm_shared_ctx_val(mm) &amp;&amp; mm_rss &gt;</span>
<span class="p_add">+		     mm-&gt;context.tsb_block[MM_TSB_HUGE_SHARED].tsb_rss_limit)) {</span>
<span class="p_add">+		if (mm-&gt;context.tsb_block[MM_TSB_HUGE_SHARED].tsb)</span>
<span class="p_add">+			tsb_grow(mm, MM_TSB_HUGE_SHARED, mm_rss);</span>
<span class="p_add">+		else</span>
<span class="p_add">+			hugetlb_shared_setup(regs);</span>
<span class="p_add">+	}</span>
<span class="p_add">+#endif</span>
 #endif
 exit_exception:
 	exception_exit(prev_state);
<span class="p_header">diff --git a/arch/sparc/mm/hugetlbpage.c b/arch/sparc/mm/hugetlbpage.c</span>
<span class="p_header">index 988acc8b..2039d45 100644</span>
<span class="p_header">--- a/arch/sparc/mm/hugetlbpage.c</span>
<span class="p_header">+++ b/arch/sparc/mm/hugetlbpage.c</span>
<span class="p_chunk">@@ -162,8 +162,14 @@</span> <span class="p_context"> void set_huge_pte_at(struct mm_struct *mm, unsigned long addr,</span>
 {
 	pte_t orig;
 
<span class="p_del">-	if (!pte_present(*ptep) &amp;&amp; pte_present(entry))</span>
<span class="p_del">-		mm-&gt;context.hugetlb_pte_count++;</span>
<span class="p_add">+	if (!pte_present(*ptep) &amp;&amp; pte_present(entry)) {</span>
<span class="p_add">+#if defined(CONFIG_SHARED_MMU_CTX)</span>
<span class="p_add">+		if (pte_val(entry) | _PAGE_SHR_CTX_4V)</span>
<span class="p_add">+			mm-&gt;context.shared_hugetlb_pte_count++;</span>
<span class="p_add">+		else</span>
<span class="p_add">+#endif</span>
<span class="p_add">+			mm-&gt;context.hugetlb_pte_count++;</span>
<span class="p_add">+	}</span>
 
 	addr &amp;= HPAGE_MASK;
 	orig = *ptep;
<span class="p_chunk">@@ -180,8 +186,14 @@</span> <span class="p_context"> pte_t huge_ptep_get_and_clear(struct mm_struct *mm, unsigned long addr,</span>
 	pte_t entry;
 
 	entry = *ptep;
<span class="p_del">-	if (pte_present(entry))</span>
<span class="p_del">-		mm-&gt;context.hugetlb_pte_count--;</span>
<span class="p_add">+	if (pte_present(entry)) {</span>
<span class="p_add">+#if defined(CONFIG_SHARED_MMU_CTX)</span>
<span class="p_add">+		if (pte_val(entry) | _PAGE_SHR_CTX_4V)</span>
<span class="p_add">+			mm-&gt;context.shared_hugetlb_pte_count--;</span>
<span class="p_add">+		else</span>
<span class="p_add">+#endif</span>
<span class="p_add">+			mm-&gt;context.hugetlb_pte_count--;</span>
<span class="p_add">+	}</span>
 
 	addr &amp;= HPAGE_MASK;
 	*ptep = __pte(0UL);
<span class="p_header">diff --git a/arch/sparc/mm/init_64.c b/arch/sparc/mm/init_64.c</span>
<span class="p_header">index bb9a6ee..2b310e5 100644</span>
<span class="p_header">--- a/arch/sparc/mm/init_64.c</span>
<span class="p_header">+++ b/arch/sparc/mm/init_64.c</span>
<span class="p_chunk">@@ -346,6 +346,21 @@</span> <span class="p_context"> void update_mmu_cache(struct vm_area_struct *vma, unsigned long address, pte_t *</span>
 	spin_lock_irqsave(&amp;mm-&gt;context.lock, flags);
 
 #if defined(CONFIG_HUGETLB_PAGE) || defined(CONFIG_TRANSPARENT_HUGEPAGE)
<span class="p_add">+#if defined(CONFIG_SHARED_MMU_CTX)</span>
<span class="p_add">+	if ((mm-&gt;context.hugetlb_pte_count || mm-&gt;context.thp_pte_count ||</span>
<span class="p_add">+	    mm-&gt;context.shared_hugetlb_pte_count) &amp;&amp; is_hugetlb_pte(pte)) {</span>
<span class="p_add">+		/* We are fabricating 8MB pages using 4MB real hw pages.  */</span>
<span class="p_add">+		pte_val(pte) |= (address &amp; (1UL &lt;&lt; REAL_HPAGE_SHIFT));</span>
<span class="p_add">+		if (is_sharedctx_pte(pte))</span>
<span class="p_add">+			__update_mmu_tsb_insert(mm, MM_TSB_HUGE_SHARED,</span>
<span class="p_add">+					REAL_HPAGE_SHIFT, address,</span>
<span class="p_add">+					pte_val(pte));</span>
<span class="p_add">+		else</span>
<span class="p_add">+			__update_mmu_tsb_insert(mm, MM_TSB_HUGE,</span>
<span class="p_add">+					REAL_HPAGE_SHIFT, address,</span>
<span class="p_add">+					pte_val(pte));</span>
<span class="p_add">+	} else</span>
<span class="p_add">+#else</span>
 	if ((mm-&gt;context.hugetlb_pte_count || mm-&gt;context.thp_pte_count) &amp;&amp;
 	    is_hugetlb_pte(pte)) {
 		/* We are fabricating 8MB pages using 4MB real hw pages.  */
<span class="p_chunk">@@ -354,6 +369,7 @@</span> <span class="p_context"> void update_mmu_cache(struct vm_area_struct *vma, unsigned long address, pte_t *</span>
 					address, pte_val(pte));
 	} else
 #endif
<span class="p_add">+#endif</span>
 		__update_mmu_tsb_insert(mm, MM_TSB_BASE, PAGE_SHIFT,
 					address, pte_val(pte));
 
<span class="p_chunk">@@ -2915,7 +2931,7 @@</span> <span class="p_context"> static void context_reload(void *__data)</span>
 		load_secondary_context(mm);
 }
 
<span class="p_del">-void hugetlb_setup(struct pt_regs *regs)</span>
<span class="p_add">+static void __hugetlb_setup_common(struct pt_regs *regs, unsigned long tsb_idx)</span>
 {
 	struct mm_struct *mm = current-&gt;mm;
 	struct tsb_config *tp;
<span class="p_chunk">@@ -2933,15 +2949,18 @@</span> <span class="p_context"> void hugetlb_setup(struct pt_regs *regs)</span>
 		die_if_kernel(&quot;HugeTSB in atomic&quot;, regs);
 	}
 
<span class="p_del">-	tp = &amp;mm-&gt;context.tsb_block[MM_TSB_HUGE];</span>
<span class="p_add">+	tp = &amp;mm-&gt;context.tsb_block[tsb_idx];</span>
 	if (likely(tp-&gt;tsb == NULL))
<span class="p_del">-		tsb_grow(mm, MM_TSB_HUGE, 0);</span>
<span class="p_add">+		tsb_grow(mm, tsb_idx, 0);</span>
 
 	tsb_context_switch(mm);
 	smp_tsb_sync(mm);
 
 	/* On UltraSPARC-III+ and later, configure the second half of
 	 * the Data-TLB for huge pages.
<span class="p_add">+	 *</span>
<span class="p_add">+	 * Note that the following does not execute on platforms where</span>
<span class="p_add">+	 * shared context is supported.</span>
 	 */
 	if (tlb_type == cheetah_plus) {
 		bool need_context_reload = false;
<span class="p_chunk">@@ -2974,6 +2993,23 @@</span> <span class="p_context"> void hugetlb_setup(struct pt_regs *regs)</span>
 			on_each_cpu(context_reload, mm, 0);
 	}
 }
<span class="p_add">+</span>
<span class="p_add">+void hugetlb_setup(struct pt_regs *regs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__hugetlb_setup_common(regs, MM_TSB_HUGE);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#if defined(CONFIG_SHARED_MMU_CTX)</span>
<span class="p_add">+void hugetlb_shared_setup(struct pt_regs *regs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__hugetlb_setup_common(regs, MM_TSB_HUGE_SHARED);</span>
<span class="p_add">+}</span>
<span class="p_add">+#else</span>
<span class="p_add">+void hugetlb_shared_setup(struct pt_regs *regs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	BUG();</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
 #endif
 
 static struct resource code_resource = {
<span class="p_header">diff --git a/arch/sparc/mm/tsb.c b/arch/sparc/mm/tsb.c</span>
<span class="p_header">index 8c2d148..0b684de 100644</span>
<span class="p_header">--- a/arch/sparc/mm/tsb.c</span>
<span class="p_header">+++ b/arch/sparc/mm/tsb.c</span>
<span class="p_chunk">@@ -108,6 +108,12 @@</span> <span class="p_context"> void flush_tsb_user(struct tlb_batch *tb)</span>
 			base = __pa(base);
 		__flush_tsb_one(tb, REAL_HPAGE_SHIFT, base, nentries);
 	}
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * FIXME</span>
<span class="p_add">+	 * I don&#39;t &quot;think&quot; we want to flush shared context tsb entries here.</span>
<span class="p_add">+	 * There should at least be a comment.</span>
<span class="p_add">+	 */</span>
 #endif
 	spin_unlock_irqrestore(&amp;mm-&gt;context.lock, flags);
 }
<span class="p_chunk">@@ -133,6 +139,11 @@</span> <span class="p_context"> void flush_tsb_user_page(struct mm_struct *mm, unsigned long vaddr, bool huge)</span>
 			base = __pa(base);
 		__flush_tsb_one_entry(base, vaddr, REAL_HPAGE_SHIFT, nentries);
 	}
<span class="p_add">+	/*</span>
<span class="p_add">+	 * FIXME</span>
<span class="p_add">+	 * Again, we should give more thought to the need for flushing</span>
<span class="p_add">+	 * shared context pages.  At least a comment is needed.</span>
<span class="p_add">+	 */</span>
 #endif
 	spin_unlock_irqrestore(&amp;mm-&gt;context.lock, flags);
 }
<span class="p_chunk">@@ -159,6 +170,7 @@</span> <span class="p_context"> static void setup_tsb_params(struct mm_struct *mm, unsigned long tsb_idx, unsign</span>
 		break;
 #if defined(CONFIG_HUGETLB_PAGE) || defined(CONFIG_TRANSPARENT_HUGEPAGE)
 	case MM_TSB_HUGE:
<span class="p_add">+	case MM_TSB_HUGE_SHARED:</span>
 		base = TSBMAP_4M_BASE;
 		break;
 #endif
<span class="p_chunk">@@ -251,6 +263,7 @@</span> <span class="p_context"> static void setup_tsb_params(struct mm_struct *mm, unsigned long tsb_idx, unsign</span>
 			break;
 #if defined(CONFIG_HUGETLB_PAGE) || defined(CONFIG_TRANSPARENT_HUGEPAGE)
 		case MM_TSB_HUGE:
<span class="p_add">+		case MM_TSB_HUGE_SHARED:</span>
 			hp-&gt;pgsz_idx = HV_PGSZ_IDX_HUGE;
 			break;
 #endif
<span class="p_chunk">@@ -260,12 +273,21 @@</span> <span class="p_context"> static void setup_tsb_params(struct mm_struct *mm, unsigned long tsb_idx, unsign</span>
 		hp-&gt;assoc = 1;
 		hp-&gt;num_ttes = tsb_bytes / 16;
 		hp-&gt;ctx_idx = 0;
<span class="p_add">+</span>
<span class="p_add">+#if defined(CONFIG_SHARED_MMU_CTX)</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * For shared context TSBs, adjust the context register index</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (mm-&gt;context.shared_ctx &amp;&amp; tsb_idx == MM_TSB_HUGE_SHARED)</span>
<span class="p_add">+			hp-&gt;ctx_idx = 1;</span>
<span class="p_add">+#endif</span>
 		switch (tsb_idx) {
 		case MM_TSB_BASE:
 			hp-&gt;pgsz_mask = HV_PGSZ_MASK_BASE;
 			break;
 #if defined(CONFIG_HUGETLB_PAGE) || defined(CONFIG_TRANSPARENT_HUGEPAGE)
 		case MM_TSB_HUGE:
<span class="p_add">+		case MM_TSB_HUGE_SHARED:</span>
 			hp-&gt;pgsz_mask = HV_PGSZ_MASK_HUGE;
 			break;
 #endif
<span class="p_chunk">@@ -520,12 +542,18 @@</span> <span class="p_context"> int init_new_context(struct task_struct *tsk, struct mm_struct *mm)</span>
 #if defined(CONFIG_HUGETLB_PAGE) || defined(CONFIG_TRANSPARENT_HUGEPAGE)
 	unsigned long saved_hugetlb_pte_count;
 	unsigned long saved_thp_pte_count;
<span class="p_add">+#if defined(CONFIG_SHARED_MMU_CTX)</span>
<span class="p_add">+	unsigned long saved_shared_hugetlb_pte_count;</span>
<span class="p_add">+#endif</span>
 #endif
 	unsigned int i;
 
 	spin_lock_init(&amp;mm-&gt;context.lock);
 
 	mm-&gt;context.sparc64_ctx_val = 0UL;
<span class="p_add">+#if defined(CONFIG_SHARED_MMU_CTX)</span>
<span class="p_add">+	mm-&gt;context.shared_ctx = NULL;</span>
<span class="p_add">+#endif</span>
 
 #if defined(CONFIG_HUGETLB_PAGE) || defined(CONFIG_TRANSPARENT_HUGEPAGE)
 	/* We reset them to zero because the fork() page copying
<span class="p_chunk">@@ -536,6 +564,10 @@</span> <span class="p_context"> int init_new_context(struct task_struct *tsk, struct mm_struct *mm)</span>
 	saved_thp_pte_count = mm-&gt;context.thp_pte_count;
 	mm-&gt;context.hugetlb_pte_count = 0;
 	mm-&gt;context.thp_pte_count = 0;
<span class="p_add">+#if defined(CONFIG_SHARED_MMU_CTX)</span>
<span class="p_add">+	saved_shared_hugetlb_pte_count = mm-&gt;context.shared_hugetlb_pte_count;</span>
<span class="p_add">+	mm-&gt;context.shared_hugetlb_pte_count = 0;</span>
<span class="p_add">+#endif</span>
 
 	mm_rss -= saved_thp_pte_count * (HPAGE_SIZE / PAGE_SIZE);
 #endif
<span class="p_chunk">@@ -544,8 +576,10 @@</span> <span class="p_context"> int init_new_context(struct task_struct *tsk, struct mm_struct *mm)</span>
 	 * us, so we need to zero out the TSB pointer or else tsb_grow()
 	 * will be confused and think there is an older TSB to free up.
 	 */
<span class="p_del">-	for (i = 0; i &lt; MM_NUM_TSBS; i++)</span>
<span class="p_add">+	for (i = 0; i &lt; MM_NUM_TSBS; i++) {</span>
 		mm-&gt;context.tsb_block[i].tsb = NULL;
<span class="p_add">+		mm-&gt;context.tsb_descr[i].tsb_base = 0UL;</span>
<span class="p_add">+	}</span>
 
 	/* If this is fork, inherit the parent&#39;s TSB size.  We would
 	 * grow it to that size on the first page fault anyways.
<span class="p_chunk">@@ -557,6 +591,11 @@</span> <span class="p_context"> int init_new_context(struct task_struct *tsk, struct mm_struct *mm)</span>
 		tsb_grow(mm, MM_TSB_HUGE,
 			 (saved_hugetlb_pte_count + saved_thp_pte_count) *
 			 REAL_HPAGE_PER_HPAGE);
<span class="p_add">+#if defined(CONFIG_SHARED_MMU_CTX)</span>
<span class="p_add">+	if (unlikely(saved_shared_hugetlb_pte_count))</span>
<span class="p_add">+		tsb_grow(mm, MM_TSB_HUGE_SHARED,</span>
<span class="p_add">+			saved_shared_hugetlb_pte_count * REAL_HPAGE_PER_HPAGE);</span>
<span class="p_add">+#endif</span>
 #endif
 
 	if (unlikely(!mm-&gt;context.tsb_block[MM_TSB_BASE].tsb))

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



